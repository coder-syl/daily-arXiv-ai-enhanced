{"id": "2601.00051", "pdf": "https://arxiv.org/pdf/2601.00051", "abs": "https://arxiv.org/abs/2601.00051", "authors": ["Yabo Chen", "Yuanzhi Liang", "Jiepeng Wang", "Tingxi Chen", "Junfei Cheng", "Zixiao Gu", "Yuyang Huang", "Zicheng Jiang", "Wei Li", "Tian Li", "Weichen Li", "Zuoxin Li", "Guangce Liu", "Jialun Liu", "Junqi Liu", "Haoyuan Wang", "Qizhen Weng", "Xuan'er Wu", "Xunzhi Xiang", "Xiaoyan Yang", "Xin Zhang", "Shiwen Zhang", "Junyu Zhou", "Chengcheng Zhou", "Haibin Huang", "Chi Zhang", "Xuelong Li"], "title": "TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model", "categories": ["cs.CV"], "comment": null, "summary": "World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.", "AI": {"tldr": "TeleWorld \u662f\u4e00\u4e2a\u5b9e\u65f6\u591a\u6a21\u60014D\u4e16\u754c\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210-\u91cd\u5efa-\u5f15\u5bfc\u95ed\u73af\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u957f\u65f6\u7a0b\u7684\u52a8\u6001\u573a\u666f\u751f\u6210\u4e0e\u8bb0\u5fc6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u5177\u9ad8\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u5728\u5b9e\u65f6\u4ea4\u4e92\u3001\u957f\u65f6\u7a0b\u4e00\u81f4\u6027\u53ca\u52a8\u6001\u573a\u666f\u6301\u4e45\u8bb0\u5fc6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u4f5c\u4e3a\u5b9e\u7528\u7684\u4e16\u754c\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u7edf\u4e00\u89c6\u9891\u751f\u6210\u3001\u52a8\u6001\u91cd\u5efa\u4e0e\u957f\u671f\u8bb0\u5fc6\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa TeleWorld \u6846\u67b6\uff0c\u91c7\u7528\u201c\u751f\u6210-\u91cd\u5efa-\u5f15\u5bfc\u201d\u8303\u5f0f\uff1a\u5c06\u751f\u6210\u7684\u89c6\u9891\u6d41\u6301\u7eed\u91cd\u5efa\u4e3a\u52a8\u60014D\u65f6\u7a7a\u8868\u793a\uff0c\u5e76\u4ee5\u6b64\u5f15\u5bfc\u540e\u7eed\u751f\u6210\u4ee5\u7ef4\u6301\u4e00\u81f4\u6027\uff1b\u7ed3\u5408\u57fa\u4e8e\u81ea\u56de\u5f52\u6269\u6563\u7684\u89c6\u9891\u6a21\u578b\u3001Macro-from-Micro Planning\uff08MMPL\uff09\u5206\u5c42\u89c4\u5212\u7b56\u7565\u4ee5\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\uff0c\u4ee5\u53ca\u9ad8\u6548 Distribution Matching Distillation\uff08DMD\uff09\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTeleWorld \u5728\u9759\u6001\u4e0e\u52a8\u6001\u573a\u666f\u7406\u89e3\u3001\u957f\u671f\u4e00\u81f4\u6027\u53ca\u5b9e\u65f6\u751f\u6210\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u6574\u5408\u4e86\u52a8\u6001\u5bf9\u8c61\u5efa\u6a21\u4e0e\u9759\u6001\u573a\u666f\u8868\u793a\u3002", "conclusion": "TeleWorld \u4e3a\u6784\u5efa\u5177\u5907\u4ea4\u4e92\u80fd\u529b\u3001\u8bb0\u5fc6\u673a\u5236\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u7684\u5b9e\u7528\u578b\u4e16\u754c\u6a21\u578b\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u6709\u671b\u63a8\u52a8\u591a\u6a21\u6001\u751f\u6210\u4e0e\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002", "summary_cn": "\u4e16\u754c\u6a21\u578b\u65e8\u5728\u8d4b\u4e88\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4ee5\u8fde\u8d2f\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u65b9\u5f0f\u8868\u793a\u3001\u751f\u6210\u5e76\u4e0e\u52a8\u6001\u73af\u5883\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u5c3d\u7ba1\u8fd1\u671f\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u5b9e\u65f6\u4ea4\u4e92\u3001\u957f\u65f6\u7a0b\u4e00\u81f4\u6027\u4ee5\u53ca\u5bf9\u52a8\u6001\u573a\u666f\u7684\u6301\u4e45\u8bb0\u5fc6\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\uff0c\u963b\u788d\u4e86\u5176\u5411\u5b9e\u7528\u4e16\u754c\u6a21\u578b\u7684\u6f14\u8fdb\u3002\u5728\u672c\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TeleWorld\u2014\u2014\u4e00\u4e2a\u5b9e\u65f6\u591a\u6a21\u60014D\u4e16\u754c\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4e00\u4e2a\u95ed\u73af\u7cfb\u7edf\u4e2d\u7edf\u4e00\u4e86\u89c6\u9891\u751f\u6210\u3001\u52a8\u6001\u573a\u666f\u91cd\u5efa\u548c\u957f\u671f\u4e16\u754c\u8bb0\u5fc6\u3002TeleWorld \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u751f\u6210-\u91cd\u5efa-\u5f15\u5bfc\u201d\u8303\u5f0f\uff1a\u751f\u6210\u7684\u89c6\u9891\u6d41\u88ab\u6301\u7eed\u91cd\u5efa\u4e3a\u52a8\u60014D\u65f6\u7a7a\u8868\u793a\uff0c\u8fdb\u800c\u5f15\u5bfc\u540e\u7eed\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u7ef4\u6301\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002\u4e3a\u652f\u6301\u4f4e\u5ef6\u8fdf\u7684\u957f\u65f6\u7a0b\u751f\u6210\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u6269\u6563\u7684\u89c6\u9891\u6a21\u578b\uff0c\u5e76\u7ed3\u5408 Macro-from-Micro Planning\uff08MMPL\uff09\u5206\u5c42\u89c4\u5212\u65b9\u6cd5\uff08\u5c06\u8bef\u5dee\u7d2f\u79ef\u4ece\u5e27\u7ea7\u964d\u4f4e\u5230\u7247\u6bb5\u7ea7\uff09\u4ee5\u53ca\u9ad8\u6548\u7684 Distribution Matching Distillation\uff08DMD\uff09\u6280\u672f\uff0c\u4ece\u800c\u5728\u5b9e\u9645\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u5408\u6210\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u7edf\u4e00\u76844D\u6846\u67b6\u5185\u65e0\u7f1d\u6574\u5408\u4e86\u52a8\u6001\u5bf9\u8c61\u5efa\u6a21\u4e0e\u9759\u6001\u573a\u666f\u8868\u793a\uff0c\u63a8\u52a8\u4e16\u754c\u6a21\u578b\u5411\u5b9e\u7528\u3001\u53ef\u4ea4\u4e92\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u5411\u53d1\u5c55\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTeleWorld \u5728\u9759\u6001\u4e0e\u52a8\u6001\u4e16\u754c\u7406\u89e3\u3001\u957f\u671f\u4e00\u81f4\u6027\u4ee5\u53ca\u5b9e\u65f6\u751f\u6210\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u6807\u5fd7\u7740\u8fc8\u5411\u5177\u5907\u4ea4\u4e92\u80fd\u529b\u548c\u8bb0\u5fc6\u673a\u5236\u7684\u5b9e\u7528\u578b\u4e16\u754c\u6a21\u578b\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u751f\u6210\u4e0e\u5177\u8eab\u667a\u80fd\u3002"}}
{"id": "2601.00090", "pdf": "https://arxiv.org/pdf/2601.00090", "abs": "https://arxiv.org/abs/2601.00090", "authors": ["Anne Harrington", "A. Sophia Koepke", "Shyamgopal Karthik", "Trevor Darrell", "Alexei A. Efros"], "title": "It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4f18\u5316\u566a\u58f0\u800c\u975e\u4f9d\u8d56\u4f20\u7edf\u5f15\u5bfc\u6216\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u76f8\u540c\u63d0\u793a\u4e0b\u751f\u6210\u7684\u56fe\u50cf\u5b58\u5728\u660e\u663e\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8f93\u51fa\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u566a\u58f0\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7b80\u5355\u7684\u566a\u58f0\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u63a2\u7d22\u5177\u6709\u4e0d\u540c\u9891\u7387\u7279\u6027\u7684\u566a\u58f0\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u751f\u6210\u7ed3\u679c\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u566a\u58f0\u4f18\u5316\u662f\u4e00\u79cd\u6709\u6548\u7f13\u89e3\u6a21\u5f0f\u5d29\u6e83\u3001\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u4fdd\u771f\u5ea6\u7684\u53ef\u884c\u9014\u5f84\uff0c\u4e14\u4e0d\u540c\u9891\u7387\u7279\u6027\u7684\u566a\u58f0\u521d\u59cb\u5316\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "summary_cn": "\u5f53\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7ed9\u5b9a\u76f8\u540c\u6587\u672c\u63d0\u793a\u65f6\u751f\u6210\u7684\u591a\u4e2a\u56fe\u50cf\u8868\u73b0\u51fa\u4ee4\u4eba\u60ca\u8bb6\u7684\u6a21\u5f0f\u5d29\u6e83\u73b0\u8c61\u3002\u5c3d\u7ba1\u5148\u524d\u5de5\u4f5c\u5c1d\u8bd5\u901a\u8fc7\u5f15\u5bfc\u673a\u5236\u8c03\u63a7\u6a21\u578b\u6216\u751f\u6210\u5927\u91cf\u5019\u9009\u56fe\u50cf\u540e\u518d\u8fdb\u884c\u7b5b\u9009\u6765\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u672c\u6587\u5219\u53e6\u8f9f\u8e4a\u5f84\uff0c\u901a\u8fc7\u566a\u58f0\u4f18\u5316\u6765\u63d0\u5347\u751f\u6210\u7ed3\u679c\u7684\u591a\u6837\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u566a\u58f0\u4f18\u5316\u76ee\u6807\u80fd\u591f\u5728\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u7f13\u89e3\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002\u6211\u4eec\u8fd8\u5206\u6790\u4e86\u566a\u58f0\u7684\u9891\u7387\u7279\u6027\uff0c\u5e76\u8868\u660e\u91c7\u7528\u5177\u6709\u4e0d\u540c\u9891\u7387\u5206\u5e03\u7684\u66ff\u4ee3\u566a\u58f0\u521d\u59cb\u5316\u65b9\u5f0f\u53ef\u4ee5\u6539\u8fdb\u4f18\u5316\u8fc7\u7a0b\u548c\u641c\u7d22\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u566a\u58f0\u4f18\u5316\u5728\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u7ed3\u679c\u3002"}}
{"id": "2601.00092", "pdf": "https://arxiv.org/pdf/2601.00092", "abs": "https://arxiv.org/abs/2601.00092", "authors": ["Pan Wang", "Yang Liu", "Guile Wu", "Eduardo R. Corral-Soto", "Chengjie Huang", "Binbin Xu", "Dongfeng Bai", "Xu Yan", "Yuan Ren", "Xingxin Chen", "Yizhe Wu", "Tao Huang", "Wenjun Wan", "Xin Wu", "Pei Zhou", "Xuyang Dai", "Kangbo Lv", "Hongbo Zhang", "Yosef Fried", "Aixue Ye", "Bailan Feng", "Zhenyu Chen", "Zhen Li", "Yingcong Chen", "Yiyi Liao", "Bingbing Liu"], "title": "Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Spatial4D-Bench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u4efb\u52a1\u76844D\u7a7a\u95f4\u667a\u80fd\u8bc4\u6d4b\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u57284D\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u4e3b\u6d41MLLMs\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u4eba\u7c7b\u5929\u7136\u5177\u59074D\u7a7a\u95f4\u667a\u80fd\uff0c\u80fd\u591f\u7406\u89e3\u7269\u4f53\u968f\u65f6\u95f4\u53d8\u5316\u7684\u8fd0\u52a8\u4e0e\u5f62\u6001\uff0c\u800c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002\u5f53\u524d\u7684\u7a7a\u95f4\u667a\u80fd\u8bc4\u6d4b\u57fa\u51c6\u901a\u5e38\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u6709\u9650\uff0c\u96be\u4ee5\u5168\u9762\u8861\u91cfMLLMs\u76844D\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86Spatial4D-Bench\uff0c\u5305\u542b\u7ea64\u4e07\u4e2a\u95ee\u7b54\u5bf9\uff0c\u8986\u76d618\u4e2a\u660e\u786e\u5b9a\u4e49\u7684\u4efb\u52a1\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4efb\u52a1\u7cfb\u7edf\u5212\u5206\u4e3a\u516d\u7c7b\u8ba4\u77e5\u7ef4\u5ea6\uff1a\u7269\u4f53\u7406\u89e3\u3001\u573a\u666f\u7406\u89e3\u3001\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u3001\u65f6\u7a7a\u5173\u7cfb\u7406\u89e3\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u65f6\u7a7a\u63a8\u7406\u3002", "result": "\u5728Spatial4D-Bench\u4e0a\u5bf9\u591a\u79cd\u524d\u6cbf\u5f00\u6e90\u548c\u95ed\u6e90MLLM\u8fdb\u884c\u8bc4\u6d4b\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u6a21\u578b\u5728\u8def\u5f84\u89c4\u5212\u3001\u52a8\u4f5c\u8bc6\u522b\u3001\u7269\u7406\u5408\u7406\u6027\u63a8\u7406\u7b49\u591a\u4e2a4D\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u5b58\u5728\u660e\u663e\u77ed\u677f\u3002", "conclusion": "\u5f53\u524dMLLM\u5c1a\u672a\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u76844D\u7a7a\u95f4\u667a\u80fd\uff1bSpatial4D-Bench\u53ef\u4f5c\u4e3a\u7ed3\u6784\u5316\u3001\u5168\u9762\u7684\u8bc4\u6d4b\u5de5\u5177\uff0c\u63a8\u52a8\u672a\u6765\u66f4\u5f3a\u5927MLLM\u7684\u7814\u53d1\u3002", "summary_cn": "4D\u7a7a\u95f4\u667a\u80fd\u6d89\u53ca\u611f\u77e5\u548c\u5904\u7406\u7269\u4f53\u5982\u4f55\u968f\u65f6\u95f4\u79fb\u52a8\u6216\u53d8\u5316\u3002\u4eba\u7c7b\u5929\u7136\u5177\u59074D\u7a7a\u95f4\u667a\u80fd\uff0c\u652f\u6491\u7740\u5e7f\u6cdb\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u76844D\u7a7a\u95f4\u667a\u80fd\uff1f\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Spatial4D-Bench\u2014\u2014\u4e00\u4e2a\u901a\u7528\u76844D\u7a7a\u95f4\u667a\u80fd\u8bc4\u6d4b\u57fa\u51c6\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30MLLMs\u76844D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u4e0e\u73b0\u6709\u89c4\u6a21\u8f83\u5c0f\u6216\u591a\u6837\u6027\u6709\u9650\u7684\u7a7a\u95f4\u667a\u80fd\u57fa\u51c6\u4e0d\u540c\uff0cSpatial4D-Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u4efb\u52a1\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5305\u542b\u7ea640,000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d618\u4e2a\u660e\u786e\u5b9a\u4e49\u7684\u4efb\u52a1\u3002\u6211\u4eec\u5c06\u8fd9\u4e9b\u4efb\u52a1\u7cfb\u7edf\u5730\u7ec4\u7ec7\u4e3a\u516d\u7c7b\u8ba4\u77e5\u7ef4\u5ea6\uff1a\u7269\u4f53\u7406\u89e3\u3001\u573a\u666f\u7406\u89e3\u3001\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u3001\u65f6\u7a7a\u5173\u7cfb\u7406\u89e3\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u65f6\u7a7a\u63a8\u7406\u3002\u56e0\u6b64\uff0cSpatial4D-Bench\u4e3a\u8bc4\u4f30MLLMs\u7684\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u4e14\u5168\u9762\u7684\u57fa\u51c6\uff0c\u8986\u76d6\u4e86\u4e0e\u4eba\u7c7b\u7a7a\u95f4\u667a\u80fd\u76f8\u5ab2\u7f8e\u7684\u5e7f\u6cdb\u4efb\u52a1\u3002\u6211\u4eec\u5728Spatial4D-Bench\u4e0a\u5bf9\u591a\u79cd\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90MLLM\u8fdb\u884c\u4e86\u8bc4\u6d4b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u8def\u5f84\u89c4\u5212\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u7269\u7406\u5408\u7406\u6027\u63a8\u7406\u7b49\u591a\u79cd4D\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u5c40\u9650\u6027\u3002\u6211\u4eec\u5e0c\u671b\u672c\u7814\u7a76\u7684\u53d1\u73b0\u80fd\u4e3a\u793e\u533a\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5e76\u671f\u671b\u8be5\u57fa\u51c6\u80fd\u591f\u4fc3\u8fdb\u66f4\u5f3a\u5927MLLM\u7684\u53d1\u5c55\uff0c\u4ee5\u8fc8\u5411\u4eba\u7c7b\u6c34\u5e73\u76844D\u7a7a\u95f4\u667a\u80fd\u3002\u66f4\u591a\u8d44\u6e90\u8bf7\u53c2\u89c1\u6211\u4eec\u7684\u9879\u76ee\u9875\u9762\u3002"}}
{"id": "2601.00123", "pdf": "https://arxiv.org/pdf/2601.00123", "abs": "https://arxiv.org/abs/2601.00123", "authors": ["Hyunho Lee", "Wenwen Li"], "title": "A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data", "categories": ["cs.CV"], "comment": "50 pages, 12 figures, 6 tables", "summary": "Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMAGNet\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528SAR\u4f5c\u4e3a\u4e3b\u8f93\u5165\uff0c\u5e76\u81ea\u9002\u5e94\u878d\u5408\u53ef\u80fd\u90e8\u5206\u7f3a\u5931\u7684MSI\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u6d2a\u6c34\u540e\u6c34\u4f53\u8303\u56f4\u5236\u56fe\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u6d2a\u6c34\u5e94\u6025\u54cd\u5e94\u9636\u6bb5\uff0c\u53ca\u65f6\u51c6\u786e\u7684\u6c34\u4f53\u8303\u56f4\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136SAR\u6570\u636e\u5e38\u7528\u4e8e\u6b64\u4efb\u52a1\uff0c\u4f46\u7ed3\u5408SAR\u4e0eMSI\uff08\u591a\u5149\u8c31\u6210\u50cf\uff09\u6570\u636e\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u66f4\u5177\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u707e\u540e\u53ef\u7528\u5f71\u50cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u7136\u800c\uff0c\u5982\u4f55\u5728MSI\u6570\u636e\u90e8\u5206\u7f3a\u5931\u65f6\u81ea\u9002\u5e94\u5730\u5c06\u5176\u6574\u5408\u5230\u57fa\u4e8eSAR\u7684\u5236\u56fe\u6d41\u7a0b\u4e2d\uff0c\u4ecd\u662f\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u7a7a\u95f4\u63a9\u7801\u81ea\u9002\u5e94\u95e8\u63a7\u7f51\u7edc\uff08SMAGNet\uff09\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8be5\u6a21\u578b\u4ee5SAR\u6570\u636e\u4e3a\u4e3b\u8981\u8f93\u5165\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u5730\u6574\u5408\u4e92\u8865\u7684MSI\u6570\u636e\uff0c\u5373\u4f7fMSI\u6570\u636e\u90e8\u5206\u6216\u5b8c\u5168\u7f3a\u5931\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "result": "\u5728C2S-MS Floods\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSMAGNet\u5728\u4e0d\u540cMSI\u6570\u636e\u53ef\u7528\u6027\u6c34\u5e73\u4e0b\u5747\u4f18\u4e8e\u5176\u4ed6\u591a\u6a21\u6001\u6a21\u578b\u3002\u5373\u4f7fMSI\u6570\u636e\u5b8c\u5168\u7f3a\u5931\uff0c\u5176\u6027\u80fd\u4e5f\u4e0e\u4ec5\u4f7f\u7528SAR\u6570\u636e\u8bad\u7ec3\u7684U-Net\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "SMAGNet\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u7f3a\u5931\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u4e5f\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u6d2a\u6c34\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "summary_cn": "\u5728\u6d2a\u6c34\u4e8b\u4ef6\u671f\u95f4\u7ed8\u5236\u6c34\u4f53\u8303\u56f4\u5bf9\u4e8e\u51cf\u707e\u3001\u51c6\u5907\u3001\u54cd\u5e94\u548c\u6062\u590d\u7b49\u6240\u6709\u9636\u6bb5\u7684\u6709\u6548\u707e\u5bb3\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u662f\u5728\u54cd\u5e94\u9636\u6bb5\uff0c\u53ca\u65f6\u51c6\u786e\u7684\u4fe1\u606f\u5c24\u4e3a\u91cd\u8981\uff0c\u6b64\u65f6\u4e3b\u8981\u91c7\u7528\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u6570\u636e\u6765\u751f\u6210\u6c34\u4f53\u8303\u56f4\u56fe\u3002\u6700\u8fd1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u65b9\u6cd5\u5229\u7528SAR\u548c\u591a\u5149\u8c31\u6210\u50cf\uff08MSI\uff09\u6570\u636e\u7684\u4e92\u8865\u7279\u6027\uff0c\u5df2\u6210\u4e3a\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63a8\u8fdb\u6c34\u4f53\u8303\u56f4\u5236\u56fe\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u7b56\u7565\u3002\u5f53\u5728\u6d2a\u5cf0\u671f\u95f4\u6216\u4e4b\u540e\u4e0d\u4e45\u83b7\u53d6\u7684\u53ca\u65f6\u707e\u540e\u89c2\u6d4b\u6570\u636e\u6709\u9650\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5c24\u5176\u6709\u76ca\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u5229\u7528\u6240\u6709\u53ef\u7528\u5f71\u50cf\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u707e\u540e\u6c34\u4f53\u8303\u56f4\u5236\u56fe\u3002\u7136\u800c\uff0c\u5982\u4f55\u5c06\u90e8\u5206\u53ef\u7528\u7684MSI\u6570\u636e\u81ea\u9002\u5e94\u5730\u6574\u5408\u5230\u57fa\u4e8eSAR\u7684\u707e\u540e\u6c34\u4f53\u8303\u56f4\u5236\u56fe\u8fc7\u7a0b\u4e2d\uff0c\u76ee\u524d\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7a7a\u95f4\u63a9\u7801\u81ea\u9002\u5e94\u95e8\u63a7\u7f51\u7edc\uff08SMAGNet\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5SAR\u6570\u636e\u4f5c\u4e3a\u707e\u540e\u6c34\u4f53\u8303\u56f4\u5236\u56fe\u7684\u4e3b\u8981\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u878d\u5408\u6574\u5408\u4e92\u8865\u7684MSI\u6570\u636e\u3002\u5728C2S-MS Floods\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSMAGNet\u5728\u4e0d\u540cMSI\u6570\u636e\u53ef\u7528\u6027\u6c34\u5e73\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5373\u4f7fMSI\u6570\u636e\u5b8c\u5168\u7f3a\u5931\uff0cSMAGNet\u7684\u6027\u80fd\u5728\u7edf\u8ba1\u4e0a\u4ecd\u4e0e\u4ec5\u4f7f\u7528SAR\u6570\u636e\u8bad\u7ec3\u7684U-Net\u6a21\u578b\u76f8\u5f53\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cSMAGNet\u4e0d\u4ec5\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u7f3a\u5931\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u4e5f\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u6d2a\u6c34\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2601.00139", "pdf": "https://arxiv.org/pdf/2601.00139", "abs": "https://arxiv.org/abs/2601.00139", "authors": ["Brady Zhou", "Philipp Kr\u00e4henb\u00fchl"], "title": "Compressed Map Priors for 3D Perception", "categories": ["cs.CV"], "comment": "Tech report; code https://github.com/bradyz/compressed_map_priors", "summary": "Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\\text{KB}/\\text{km}^2$, a $20\\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u538b\u7f29\u5730\u56fe\u5148\u9a8c\uff08CMP\uff09\uff0c\u4e00\u79cd\u4ece\u5386\u53f2\u884c\u9a76\u6570\u636e\u4e2d\u5b66\u4e60\u7a7a\u95f4\u5148\u9a8c\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u4ec5\u970032KB/km\u00b2\u5b58\u50a8\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4e3b\u6d413D\u611f\u77e5\u7cfb\u7edf\u4e2d\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u591a\u79cd\u67b6\u6784\u76843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u7cfb\u7edf\u901a\u5e38\u5c06\u6bcf\u4e2a\u5730\u70b9\u89c6\u4e3a\u9996\u6b21\u8bbf\u95ee\uff0c\u5ffd\u7565\u4e86\u5927\u91cf\u9053\u8def\u5df2\u88ab\u53cd\u590d\u884c\u9a76\u7684\u4e8b\u5b9e\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u5386\u53f2\u904d\u5386\u4fe1\u606f\u6765\u63d0\u5347\u611f\u77e5\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u538b\u7f29\u5730\u56fe\u5148\u9a8c\uff08Compressed Map Priors, CMP\uff09\u6846\u67b6\uff0c\u5229\u7528\u4e8c\u503c\u5316\u54c8\u5e0c\u56fe\u4ece\u5386\u53f2\u8f68\u8ff9\u4e2d\u5b66\u4e60\u7a7a\u95f4\u5148\u9a8c\uff0c\u5b58\u50a8\u5f00\u9500\u4ec5\u4e3a32KB/km\u00b2\uff0c\u6bd4\u7a20\u5bc6\u5b58\u50a8\u51cf\u5c1120\u500d\uff0c\u5e76\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u67093D\u611f\u77e5\u7cfb\u7edf\u4e2d\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cCMP\u5728\u591a\u79cd3D\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u4e2d\u5747\u5e26\u6765\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u51e0\u4e4e\u4e0d\u589e\u52a0\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u5229\u7528\u5386\u53f2\u904d\u5386\u6570\u636e\u6784\u5efa\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u5148\u9a8c\u662f\u4e00\u79cd\u9ad8\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a763D\u611f\u77e5\u80fd\u529b\u7684\u65b9\u6cd5\uff0cCMP\u4e3a\u6b64\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u4eba\u7c7b\u9a7e\u9a76\u5458\u5f88\u5c11\u524d\u5f80\u4ece\u672a\u6709\u4eba\u53bb\u8fc7\u7684\u5730\u65b9\u3002\u6bd5\u7adf\uff0c\u6bcf\u5929\u90fd\u6709\u6210\u5343\u4e0a\u4e07\u7684\u9a7e\u9a76\u5458\u4f7f\u7528\u7e41\u5fd9\u7684\u57ce\u5e02\u9053\u8def\uff0c\u5176\u4e2d\u53ea\u6709\u4e00\u4eba\u80fd\u58f0\u79f0\u81ea\u5df1\u662f\u7b2c\u4e00\u4e2a\u3002\u81ea\u52a8\u9a7e\u9a76\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u4e5f\u662f\u5982\u6b64\u3002\u7edd\u5927\u591a\u6570\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u7cfb\u7edf\u7684\u90e8\u7f72\u533a\u57df\u90fd\u66fe\u88ab\u8bbf\u95ee\u8fc7\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u89c6\u89c9\u7cfb\u7edf\u5374\u8868\u73b0\u5f97\u597d\u50cf\u6bcf\u6b21\u90fd\u662f\u7b2c\u4e00\u6b21\u9047\u5230\u67d0\u4e2a\u5730\u70b9\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u538b\u7f29\u5730\u56fe\u5148\u9a8c\uff08Compressed Map Priors, CMP\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u53ef\u4ece\u5386\u53f2\u884c\u9a76\u8f68\u8ff9\u4e2d\u5b66\u4e60\u7a7a\u95f4\u5148\u9a8c\u3002\u8be5\u5730\u56fe\u5148\u9a8c\u91c7\u7528\u4e8c\u503c\u5316\u54c8\u5e0c\u56fe\uff0c\u6bcf\u5e73\u65b9\u516c\u91cc\u4ec5\u970032KB\u5b58\u50a8\u7a7a\u95f4\uff0c\u76f8\u6bd4\u7a20\u5bc6\u5b58\u50a8\u65b9\u5f0f\u51cf\u5c11\u4e8620\u500d\u3002\u538b\u7f29\u5730\u56fe\u5148\u9a8c\u80fd\u591f\u8f7b\u677e\u96c6\u6210\u5230\u4e3b\u6d413D\u611f\u77e5\u7cfb\u7edf\u4e2d\uff0c\u51e0\u4e4e\u4e0d\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4e14\u4e00\u81f4\u5730\u63d0\u5347\u4e86\u591a\u79cd\u67b6\u6784\u76843D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.00141", "pdf": "https://arxiv.org/pdf/2601.00141", "abs": "https://arxiv.org/abs/2601.00141", "authors": ["Lawrence Han"], "title": "Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.", "AI": {"tldr": "GLASS\u662f\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u89c6\u56fe\u4e0e\u5c40\u90e8\u9ad8\u5206\u8fa8\u7387\u88c1\u526a\u7684\u56fe\u50cf\u68c0\u6d4b\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u91c7\u6837\u548c\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u878d\u5408\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u5728\u8ba1\u7b97\u53ef\u884c\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347AI\u751f\u6210\u56fe\u50cf\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u5bfc\u81f4\u4e22\u5931\u5173\u952e\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5f71\u54cd\u68c0\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51faGLASS\u67b6\u6784\uff0c\u5c06\u5168\u5c40\u7f29\u653e\u89c6\u56fe\u4e0e\u591a\u4e2a\u901a\u8fc7\u7a7a\u95f4\u5206\u5c42\u91c7\u6837\u83b7\u5f97\u7684\u539f\u59cb\u5206\u8fa8\u7387\u5c40\u90e8\u88c1\u526a\u76f8\u7ed3\u5408\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u5c40\u90e8\u533a\u57df\u8fdb\u884c\u52a0\u6743\u805a\u5408\uff1b\u8be5\u67b6\u6784\u53ef\u5d4c\u5165\u5404\u7c7b\u89c6\u89c9\u6a21\u578b\uff08\u5982ViT\u3001ResNet\u3001ConvNeXt\uff09\u4e2d\u5904\u7406\u4efb\u610f\u5c3a\u5bf8\u56fe\u50cf\u3002", "result": "\u5728\u591a\u79cd\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cGLASS\u5728\u5408\u7406\u8ba1\u7b97\u5f00\u9500\u4e0b\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d6\u5f97\u66f4\u9ad8\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "GLASS\u6709\u6548\u4fdd\u7559\u5e76\u5229\u7528\u4e86\u56fe\u50cf\u4e2d\u7684\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\uff0c\u4e3aAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97AI\u751f\u6210\u7684\u56fe\u50cf\u8d8a\u6765\u8d8a\u903c\u771f\u4e14\u5177\u6709\u9ad8\u5206\u8fa8\u7387\u3002\u5927\u591a\u6570AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u67b6\u6784\u901a\u5e38\u5728\u5c06\u56fe\u50cf\u8f93\u5165\u6a21\u578b\u524d\u5bf9\u5176\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u4e22\u5931\u3002\u672c\u6587\u63d0\u51fa\u4e86GLASS\uff08Global-Local Attention with Stratified Sampling\uff09\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u5168\u5c40\u7f29\u653e\u89c6\u56fe\u4e0e\u591a\u4e2a\u968f\u673a\u91c7\u6837\u7684\u5c40\u90e8\u88c1\u526a\u533a\u57df\u3002\u8fd9\u4e9b\u88c1\u526a\u533a\u57df\u662f\u901a\u8fc7\u7a7a\u95f4\u5206\u5c42\u91c7\u6837\u9ad8\u6548\u9009\u53d6\u7684\u539f\u59cb\u5206\u8fa8\u7387\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bc4\u5206\u673a\u5236\u8fdb\u884c\u805a\u5408\u3002GLASS\u53ef\u96c6\u6210\u5230\u89c6\u89c9\u6a21\u578b\u4e2d\uff0c\u4ee5\u5229\u7528\u4efb\u610f\u5c3a\u5bf8\u56fe\u50cf\u4e2d\u7684\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\u3002\u672c\u6587\u91c7\u7528Vision Transformer\u3001ResNet\u548cConvNeXt\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u53ef\u884c\u7684\u8ba1\u7b97\u7ea6\u675f\u4e0b\uff0cGLASS\u76f8\u6bd4\u6807\u51c6\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.00150", "pdf": "https://arxiv.org/pdf/2601.00150", "abs": "https://arxiv.org/abs/2601.00150", "authors": ["Yehui Yang", "Dalu Yang", "Wenshuo Zhou", "Fangxin Shang", "Yifan Liu", "Jie Ren", "Haojun Fei", "Qing Yang", "Tao Chen"], "title": "FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.MM"], "comment": null, "summary": "As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FCMBench-V1.0\uff0c\u4e00\u4e2a\u9762\u5411\u91d1\u878d\u4fe1\u8d37\u573a\u666f\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b18\u7c7b\u6838\u5fc3\u8bc1\u4ef6\u30014,043\u5f20\u9690\u79c1\u5408\u89c4\u56fe\u50cf\u548c8,446\u4e2a\u95ee\u7b54\u6837\u672c\uff0c\u901a\u8fc7\u611f\u77e5\u3001\u63a8\u7406\u548c\u9c81\u68d2\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u91d1\u878d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u6b64\u57fa\u51c6\u4e0b\u7684\u6027\u80fd\u5dee\u8ddd\u4e0e\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001AI\u5728\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u548c\u6587\u6863\u5ba1\u6838\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e9f\u9700\u4e00\u4e2a\u9886\u57df\u4e13\u7528\u7684\u57fa\u51c6\u6765\u53cd\u6620\u91d1\u878d\u4fe1\u8d37\u7279\u6709\u7684\u6587\u6863\u4e0e\u5de5\u4f5c\u6d41\u3001\u5305\u542b\u4fe1\u8d37\u76f8\u5173\u7684\u7406\u89e3\u80fd\u529b\u4e0e\u73b0\u5b9e\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4fdd\u969c\u9690\u79c1\u5408\u89c4\u7684\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u4ef7\u503c\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86FCMBench-V1.0\uff0c\u6db5\u76d618\u7c7b\u6838\u5fc3\u8bc1\u4ef6\uff0c\u91c7\u7528\u95ed\u5408\u7684\u5408\u6210-\u62cd\u6444\u6d41\u7a0b\u751f\u62104,043\u5f20\u9690\u79c1\u5408\u89c4\u56fe\u50cf\u548c8,446\u4e2a\u95ee\u7b54\u6837\u672c\u3002\u8bc4\u4f30\u6846\u67b6\u5305\u62ec\u611f\u77e5\uff083\u9879\u57fa\u7840\u4efb\u52a1\uff09\u3001\u63a8\u7406\uff084\u9879\u4fe1\u8d37\u4e13\u7528\u4efb\u52a1\uff09\u548c\u9c81\u68d2\u6027\uff0810\u79cd\u73b0\u5b9e\u91c7\u96c6\u4f2a\u5f71\uff09\u4e09\u4e2a\u7ef4\u5ea6\u3002\u6240\u6709\u6837\u672c\u901a\u8fc7\u4eba\u5de5\u5408\u6210\u6a21\u677f\u5e76\u5185\u90e8\u62cd\u6444\u751f\u6210\uff0c\u907f\u514d\u4f7f\u7528\u7f51\u7edc\u6216\u516c\u5f00\u56fe\u50cf\u4ee5\u9632\u6b62\u9884\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\u3002", "result": "\u572823\u4e2a\u6765\u81ea14\u5bb6\u9876\u5c16\u673a\u6784\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u5546\u4e1a\u6a21\u578bGemini 3 Pro\u53d6\u5f97\u6700\u4f73F1\u5f97\u5206\uff0864.61%\uff09\uff0c\u5f00\u6e90\u57fa\u7ebfQwen3-VL-235B\u5f97\u5206\u4e3a57.27%\uff0c\u800c\u4f5c\u8005\u63d0\u51fa\u7684\u91d1\u878d\u4e13\u7528\u6a21\u578bQfin-VL-Instruct\u4ee564.92%\u4f4d\u5c45\u699c\u9996\u3002\u9c81\u68d2\u6027\u6d4b\u8bd5\u663e\u793a\uff0c\u5373\u4f7f\u9876\u5c16\u6a21\u578b\u5728\u91c7\u96c6\u4f2a\u5f71\u4e0b\u4e5f\u51fa\u73b0\u660e\u663e\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "FCMBench-V1.0\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u4fe1\u8d37\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u548c\u9c81\u68d2\u6027\u77ed\u677f\uff0c\u4e3a\u8be5\u9886\u57df\u6a21\u578b\u5f00\u53d1\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\u3002", "summary_cn": "\u968f\u7740\u591a\u6a21\u6001AI\u5728\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u548c\u6587\u6863\u5ba1\u6838\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u9886\u57df\u4e13\u7528\u7684\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5e94\uff081\uff09\u53cd\u6620\u91d1\u878d\u4fe1\u8d37\u7533\u8bf7\u4e2d\u7279\u6709\u7684\u6587\u6863\u548c\u5de5\u4f5c\u6d41\u7a0b\uff0c\uff082\uff09\u5305\u542b\u4fe1\u8d37\u76f8\u5173\u7684\u7406\u89e3\u80fd\u529b\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u9c81\u68d2\u6027\uff0c\uff083\uff09\u5728\u4e0d\u727a\u7272\u5b9e\u9645\u6548\u7528\u7684\u524d\u63d0\u4e0b\u786e\u4fdd\u9690\u79c1\u5408\u89c4\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FCMBench-V1.0\u2014\u2014\u4e00\u4e2a\u9762\u5411\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u5927\u89c4\u6a21\u91d1\u878d\u4fe1\u8d37\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u6db5\u76d618\u7c7b\u6838\u5fc3\u8bc1\u4ef6\uff0c\u5305\u542b4,043\u5f20\u9690\u79c1\u5408\u89c4\u56fe\u50cf\u548c8,446\u4e2a\u95ee\u7b54\u6837\u672c\u3002FCMBench\u8bc4\u4f30\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u611f\u77e5\u3001\u63a8\u7406\u548c\u9c81\u68d2\u6027\uff0c\u5177\u4f53\u5305\u62ec3\u9879\u57fa\u7840\u611f\u77e5\u4efb\u52a1\u30014\u9879\u9700\u8981\u5bf9\u89c6\u89c9\u8bc1\u636e\u8fdb\u884c\u51b3\u7b56\u5bfc\u5411\u7406\u89e3\u7684\u4fe1\u8d37\u4e13\u7528\u63a8\u7406\u4efb\u52a1\uff0c\u4ee5\u53ca\u7528\u4e8e\u9c81\u68d2\u6027\u538b\u529b\u6d4b\u8bd5\u768410\u79cd\u73b0\u5b9e\u4e16\u754c\u91c7\u96c6\u4f2a\u5f71\u7c7b\u578b\u3002\u4e3a\u4e86\u517c\u987e\u5408\u89c4\u6027\u4e0e\u771f\u5b9e\u6027\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u5c01\u95ed\u7684\u5408\u6210-\u62cd\u6444\u6d41\u7a0b\u6784\u5efa\u6240\u6709\u6837\u672c\uff1a\u624b\u52a8\u5408\u6210\u5e26\u6709\u865a\u62df\u5185\u5bb9\u7684\u6587\u6863\u6a21\u677f\uff0c\u5e76\u5728\u5185\u90e8\u62cd\u6444\u7b26\u5408\u573a\u666f\u7684\u56fe\u50cf\u3002\u8be5\u8bbe\u8ba1\u8fd8\u901a\u8fc7\u907f\u514d\u4f7f\u7528\u7f51\u7edc\u6765\u6e90\u6216\u516c\u5f00\u53d1\u5e03\u7684\u56fe\u50cf\uff0c\u7f13\u89e3\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\u95ee\u9898\u3002FCMBench\u80fd\u591f\u6709\u6548\u533a\u5206\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u548c\u9c81\u68d2\u6027\u8868\u73b0\u3002\u6211\u4eec\u5728\u6765\u81ea14\u5bb6\u9876\u7ea7AI\u516c\u53f8\u548c\u7814\u7a76\u673a\u6784\u768423\u4e2a\u6700\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u5176\u4e2d\uff0c\u5546\u4e1a\u6a21\u578bGemini 3 Pro\u53d6\u5f97\u4e86\u6700\u4f73F1\uff08%\uff09\u5206\u6570\uff0864.61\uff09\uff0c\u5f00\u6e90\u57fa\u7ebfQwen3-VL-235B\u8868\u73b0\u6700\u4f73\uff0857.27\uff09\uff0c\u800c\u6211\u4eec\u63d0\u51fa\u7684\u91d1\u878d\u4fe1\u8d37\u4e13\u7528\u6a21\u578bQfin-VL-Instruct\u83b7\u5f97\u4e86\u6574\u4f53\u6700\u9ad8\u5206\uff0864.92\uff09\u3002\u9c81\u68d2\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728\u9762\u5bf9\u91c7\u96c6\u4f2a\u5f71\u65f6\u4e5f\u4f1a\u51fa\u73b0\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2601.00156", "pdf": "https://arxiv.org/pdf/2601.00156", "abs": "https://arxiv.org/abs/2601.00156", "authors": ["Kaiwen Zheng", "Junchen Fu", "Songpei Xu", "Yaoqing He", "Joemon M. Jose", "Han Hu", "Xuri Ge"], "title": "Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFaceFocalDesc\u4efb\u52a1\uff0c\u5373\u5bf9\u4efb\u610f\u9009\u5b9a\u7684\u4eba\u8138\u533a\u57df\u751f\u6210\u548c\u8bc6\u522b\u5305\u542b\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff08AU\uff09\u3001\u60c5\u7eea\u72b6\u6001\u548c\u5e74\u9f84\u4f30\u8ba1\u7684\u591a\u5c5e\u6027\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u6784\u5efa\u4e86\u76f8\u5e94\u6570\u636e\u96c6\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u57fa\u4e8eQwen2.5-VL\u5fae\u8c03\u7684Focal-RegionFace\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u5fae\u8c03\u5b9e\u73b0\u5bf9\u5c40\u90e8\u7279\u5f81\u7684\u805a\u7126\uff0c\u5728\u65b0\u57fa\u51c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u5206\u6790\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u6574\u5f20\u4eba\u8138\uff0c\u7f3a\u4e4f\u5bf9\u4efb\u610f\u5c40\u90e8\u533a\u57df\u8fdb\u884c\u7ec6\u7c92\u5ea6\u591a\u5c5e\u6027\u7406\u89e3\u7684\u80fd\u529b\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u7cfb\u7edf\u82e5\u80fd\u805a\u7126\u4e8e\u7279\u5b9a\u9762\u90e8\u533a\u57df\uff0c\u5c06\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7684\u7406\u89e3\u4e0e\u53ef\u63a7\u6027\uff0c\u56e0\u6b64\u63d0\u51faFaceFocalDesc\u8fd9\u4e00\u65b0\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4efb\u610f\u4eba\u8138\u533a\u57df\u7684\u591a\u5c5e\u6027\u63cf\u8ff0\u65b0\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u533a\u57df\u7ea7\u6807\u6ce8\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faFocal-RegionFace\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u57fa\u4e8eQwen2.5-VL\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u4e2a\u6e10\u8fdb\u5f0f\u5fae\u8c03\u9636\u6bb5\u9010\u6b65\u589e\u5f3a\u5bf9\u5c40\u90e8\u9762\u90e8\u7279\u5f81\u7684\u5173\u6ce8\u80fd\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5e74\u9f84\u4f30\u8ba1\u3001\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff08FAU\uff09\u548c\u60c5\u7eea\u68c0\u6d4b\u3002", "result": "Focal-RegionFace\u5728\u65b0\u6784\u5efa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4f20\u7edf\u5e38\u7528\u6307\u6807\u548c\u65b0\u63d0\u51fa\u7684\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u3001\u591a\u5c5e\u6027\u3001\u533a\u57df\u805a\u7126\u7684\u4eba\u8138\u5206\u6790\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6709\u6548\u6027\u4e0e\u901a\u7528\u6027\uff0c\u9a8c\u8bc1\u4e86\u5c40\u90e8\u805a\u7126\u7b56\u7565\u5728\u9762\u90e8\u72b6\u6001\u7406\u89e3\u4e2d\u7684\u4f18\u52bf\u3002", "summary_cn": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u90e8\u5206\u6790\u4e2d\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\uff1a\u4e3a\u4efb\u610f\u9009\u5b9a\u7684\u4eba\u8138\u533a\u57df\u751f\u6210\u5e76\u8bc6\u522b\u5305\u542b\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff08AUs\uff09\u3001\u60c5\u7eea\u72b6\u6001\u548c\u5e74\u9f84\u4f30\u8ba1\u7684\u591a\u5c5e\u6027\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff08\u79f0\u4e3aFaceFocalDesc\uff09\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u7cfb\u7edf\u82e5\u80fd\u805a\u7126\u4e8e\u4e2a\u4f53\u9762\u90e8\u533a\u57df\uff0c\u5c06\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u6df1\u5165\u7684\u7406\u89e3\u4e0e\u66f4\u5f3a\u7684\u63a7\u5236\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u9762\u5411\u4efb\u610f\u4eba\u8138\u533a\u57df\u7684\u65b0\u578b\u591a\u5c5e\u6027\u63cf\u8ff0\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u533a\u57df\u7ea7\u6807\u6ce8\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eQwen2.5-VL\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u2014\u2014Focal-RegionFace\uff0c\u7528\u4e8e\u9762\u90e8\u72b6\u6001\u5206\u6790\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u591a\u4e2a\u6e10\u8fdb\u5f0f\u5fae\u8c03\u9636\u6bb5\uff0c\u9010\u6b65\u4f18\u5316\u5bf9\u5c40\u90e8\u9762\u90e8\u7279\u5f81\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5e74\u9f84\u4f30\u8ba1\u3001\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff08FAU\uff09\u8bc6\u522b\u548c\u60c5\u7eea\u68c0\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFocal-RegionFace\u5728\u65b0\u6784\u5efa\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u8bba\u662f\u5728\u4f20\u7edf\u5e7f\u6cdb\u4f7f\u7528\u7684\u6307\u6807\u8fd8\u662f\u65b0\u63d0\u51fa\u7684\u6307\u6807\u4e0a\uff0c\u5747\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u5145\u5206\u9a8c\u8bc1\u4e86\u5176\u5728\u7ec6\u7c92\u5ea6\u591a\u5c5e\u6027\u4eba\u8138\u533a\u57df\u805a\u7126\u5206\u6790\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2601.00194", "pdf": "https://arxiv.org/pdf/2601.00194", "abs": "https://arxiv.org/abs/2601.00194", "authors": ["Salma Gonzalez-Sabbagh", "Antonio Robles-Kelly", "Shang Gao"], "title": "DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery", "categories": ["cs.CV"], "comment": "11 pages, 6 figures", "summary": "Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDichroGAN\uff0c\u4e00\u79cd\u7528\u4e8e\u4ece\u536b\u661f\u56fe\u50cf\u6062\u590d\u6d77\u5e95\u771f\u5b9e\u989c\u8272\u7684\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u5927\u6c14\u8f90\u5c04\u4eae\u5ea6\u4e0e\u6c34\u4e0b\u5149\u4f20\u8f93\uff0c\u6709\u6548\u53bb\u9664\u5149\u5438\u6536\u548c\u6563\u5c04\u5f71\u54cd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6c34\u4f53\u4e2d\u5149\u968f\u6df1\u5ea6\u5448\u6307\u6570\u8870\u51cf\uff0c\u4ece\u536b\u661f\u5f71\u50cf\u4e2d\u6062\u590d\u6d77\u5e95\u5728\u7a7a\u6c14\u4e2d\u7684\u771f\u5b9e\u989c\u8272\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faDichroGAN\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u540c\u6b65\u8bad\u7ec3\uff1a\u524d\u4e24\u4e2a\u751f\u6210\u5668\u5229\u7528\u9ad8\u5149\u8c31\u56fe\u50cf\u7acb\u65b9\u4f53\u5206\u522b\u4f30\u8ba1\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u53cd\u5c04\u4ee5\u83b7\u5f97\u5927\u6c14\u573a\u666f\u8f90\u5c04\u4eae\u5ea6\uff1b\u540e\u4e24\u4e2a\u751f\u6210\u5668\u5206\u522b\u5904\u7406\u5305\u542b\u5404\u6ce2\u6bb5\u7279\u5f81\u7684\u573a\u666f\u8f90\u5c04\u4eae\u5ea6\u548c\u4f30\u8ba1\u6c34\u4e0b\u5149\u4f20\u8f93\uff0c\u4f9d\u636e\u6c34\u4e0b\u6210\u50cf\u65b9\u7a0b\u8054\u5408\u53bb\u9664\u5149\u5438\u6536\u4e0e\u6563\u5c04\u6548\u5e94\u3002", "result": "\u5728\u57fa\u4e8ePRISMA\u536b\u661f\u6784\u5efa\u7684\u5c0f\u578bRGB-\u5149\u8c31\u914d\u5bf9\u6570\u636e\u96c6\u4ee5\u53ca\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDichroGAN\u5728\u6d77\u5e95\u989c\u8272\u6062\u590d\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6c34\u4e0b\u590d\u539f\u65b9\u6cd5\u3002", "conclusion": "DichroGAN\u80fd\u6709\u6548\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u6062\u590d\u6d77\u5e95\u5728\u7a7a\u6c14\u4e2d\u7684\u771f\u5b9e\u989c\u8272\uff0c\u4e3a\u9065\u611f\u6d77\u6d0b\u89c2\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\u3002", "summary_cn": "\u7531\u4e8e\u6c34\u4f53\u4e2d\u5149\u968f\u6df1\u5ea6\u5448\u6307\u6570\u8870\u51cf\uff0c\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u6062\u590d\u6d77\u5e95\u5728\u7a7a\u6c14\u4e2d\u7684\u771f\u5b9e\u989c\u8272\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86DichroGAN\uff0c\u4e00\u79cd\u4e3a\u6b64\u76ee\u7684\u8bbe\u8ba1\u7684\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGAN\uff09\u3002DichroGAN\u91c7\u7528\u4e24\u9636\u6bb5\u540c\u6b65\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\uff0c\u4e24\u4e2a\u751f\u6210\u5668\u5229\u7528\u9ad8\u5149\u8c31\u56fe\u50cf\u7acb\u65b9\u4f53\u4f30\u8ba1\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u53cd\u5c04\uff0c\u4ece\u800c\u83b7\u5f97\u5927\u6c14\u573a\u666f\u8f90\u5c04\u4eae\u5ea6\uff1b\u968f\u540e\uff0c\u7b2c\u4e09\u4e2a\u751f\u6210\u5668\u63a5\u6536\u5305\u542b\u5404\u5149\u8c31\u6ce2\u6bb5\u7279\u5f81\u7684\u751f\u6210\u573a\u666f\u8f90\u5c04\u4eae\u5ea6\u4f5c\u4e3a\u8f93\u5165\uff0c\u800c\u7b2c\u56db\u4e2a\u751f\u6210\u5668\u5219\u4f30\u8ba1\u6c34\u4e0b\u5149\u4f20\u8f93\u3002\u8fd9\u4e9b\u751f\u6210\u5668\u534f\u540c\u5de5\u4f5c\uff0c\u4f9d\u636e\u6c34\u4e0b\u56fe\u50cf\u5f62\u6210\u65b9\u7a0b\uff0c\u53bb\u9664\u5149\u5438\u6536\u548c\u6563\u5c04\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u6062\u590d\u6d77\u5e95\u5728\u7a7a\u6c14\u4e2d\u7684\u771f\u5b9e\u989c\u8272\u3002DichroGAN\u5728\u7531PRISMA\u536b\u661f\u56fe\u50cf\u6784\u5efa\u7684\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542bRGB\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u5149\u8c31\u6ce2\u6bb5\u548c\u63a9\u819c\u3002\u5728\u536b\u661f\u548c\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDichroGAN\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6c34\u4e0b\u590d\u539f\u6280\u672f\u5177\u6709\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002"}}
{"id": "2601.00204", "pdf": "https://arxiv.org/pdf/2601.00204", "abs": "https://arxiv.org/abs/2601.00204", "authors": ["Xiaokun Sun", "Zeyu Cai", "Hao Tang", "Ying Tai", "Jian Yang", "Zhenyu Zhang"], "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing", "categories": ["cs.CV"], "comment": "Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/", "summary": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.", "AI": {"tldr": "MorphAny3D \u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684 3D \u5f62\u53d8\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6f5c\u5728\uff08SLAT\uff09\u8868\u793a\uff0c\u901a\u8fc7\u65b0\u9896\u7684 Morphing Cross-Attention \u548c Temporal-Fused Self-Attention \u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u4e14\u65f6\u95f4\u5e73\u6ed1\u7684 3D \u5f62\u53d8\uff0c\u652f\u6301\u8de8\u7c7b\u522b\u5f62\u53d8\u3001\u89e3\u8026\u5f62\u53d8\u548c 3D \u98ce\u683c\u8fc1\u79fb\u3002", "motivation": "3D \u5f62\u53d8\u5728\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u4e14\u65f6\u95f4\u5e73\u6ed1\u7684\u53d8\u5f62\u65b9\u9762\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u8de8\u7c7b\u522b\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa MorphAny3D \u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6f5c\u5728\uff08SLAT\uff09\u8868\u793a\uff0c\u5728 3D \u751f\u6210\u5668\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u667a\u80fd\u878d\u5408\u6e90\u548c\u76ee\u6807 SLAT \u7279\u5f81\u3002\u5177\u4f53\u5305\u62ec Morphing Cross-Attention (MCA) \u7528\u4e8e\u7ed3\u6784\u4e00\u81f4\u6027\uff0cTemporal-Fused Self-Attention (TFSA) \u7528\u4e8e\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u59ff\u6001\u6821\u6b63\u7b56\u7565\u7f13\u89e3\u5f62\u53d8\u8fc7\u7a0b\u4e2d\u7684\u59ff\u6001\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5305\u62ec\u8de8\u7c7b\u522b\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u4e2d\uff0c\u5747\u80fd\u751f\u6210\u6700\u5148\u8fdb\u7684\u5f62\u53d8\u5e8f\u5217\uff0c\u5e76\u652f\u6301\u89e3\u8026\u5f62\u53d8\u548c 3D \u98ce\u683c\u8fc1\u79fb\u7b49\u9ad8\u7ea7\u5e94\u7528\u3002", "conclusion": "MorphAny3D \u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684 3D \u5f62\u53d8\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u4e14\u65f6\u95f4\u5e73\u6ed1\u7684\u5f62\u53d8\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u5e76\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u57fa\u4e8e SLAT \u7684\u751f\u6210\u6a21\u578b\u3002", "summary_cn": "\u7531\u4e8e\u96be\u4ee5\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u4e14\u65f6\u95f4\u5e73\u6ed1\u7684\u53d8\u5f62\uff0c\u5c24\u5176\u662f\u8de8\u7c7b\u522b\u7684\u53d8\u5f62\uff0c3D \u5f62\u53d8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86 MorphAny3D\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6f5c\u5728\uff08SLAT\uff09\u8868\u793a\u6765\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684 3D \u5f62\u53d8\u3002\u6211\u4eec\u7684\u5173\u952e\u89c1\u89e3\u662f\uff0c\u5728 3D \u751f\u6210\u5668\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u667a\u80fd\u5730\u6df7\u5408\u6e90\u548c\u76ee\u6807 SLAT \u7279\u5f81\uff0c\u53ef\u4ee5\u81ea\u7136\u5730\u4ea7\u751f\u5408\u7406\u7684\u5f62\u53d8\u5e8f\u5217\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 Morphing Cross-Attention\uff08MCA\uff09\uff0c\u7528\u4e8e\u878d\u5408\u6e90\u548c\u76ee\u6807\u4fe1\u606f\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u4e00\u81f4\u6027\uff1b\u4ee5\u53ca Temporal-Fused Self-Attention\uff08TFSA\uff09\uff0c\u901a\u8fc7\u6574\u5408\u524d\u5e8f\u5e27\u7684\u7279\u5f81\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u4e00\u79cd\u59ff\u6001\u6821\u6b63\u7b56\u7565\u8fdb\u4e00\u6b65\u7f13\u89e3\u4e86\u5f62\u53d8\u6b65\u9aa4\u4e2d\u7684\u59ff\u6001\u6a21\u7cca\u95ee\u9898\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5373\u4f7f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u7c7b\u522b\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u751f\u6210\u6700\u5148\u8fdb\u7684\u5f62\u53d8\u5e8f\u5217\u3002MorphAny3D \u8fd8\u652f\u6301\u89e3\u8026\u5f62\u53d8\u548c 3D \u98ce\u683c\u8fc1\u79fb\u7b49\u9ad8\u7ea7\u5e94\u7528\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u57fa\u4e8e SLAT \u7684\u751f\u6210\u6a21\u578b\u3002\u9879\u76ee\u9875\u9762\uff1ahttps://xiaokunsun.github.io/MorphAny3D.github.io/\u3002"}}
