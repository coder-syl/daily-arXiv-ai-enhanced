<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Ground: Map-Free LiDAR Relocalization for UAVs](https://arxiv.org/abs/2602.13267)
*Hengyu Mu,Jianshi Wu,Yuxin Guo,XianLian Lin,Qingyong Hu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: 本文提出MAILS，一种面向无人机的无图LiDAR重定位框架，通过局部保持滑动窗口注意力、坐标无关特征初始化和局部不变位置编码提升在稀疏点云、大偏航角与高度变化下的定位精度，并构建了首个大规模无人机LiDAR重定位数据集。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR重定位方法主要针对自动驾驶场景设计，在无人机（UAV）应用中因点云稀疏、大范围偏航旋转和高度变化等因素导致精度显著下降；同时缺乏能反映真实无人机飞行特性的公开数据集。

Method: 提出MAILS框架：1）引入局部保持滑动窗口注意力模块，从稀疏点云中提取局部判别性几何特征；2）设计坐标无关的特征初始化模块和局部不变的位置编码机制，以增强对大偏航角和高度变化的鲁棒性；3）构建包含四个场景和多种飞行轨迹的大规模无人机LiDAR重定位数据集。

Result: 大量实验表明，所提方法在真实无人机场景下实现了高精度定位，显著优于现有技术。

Conclusion: MAILS有效解决了无人机在GNSS拒止环境下高精度LiDAR重定位的挑战，其提出的模块设计和新构建的数据集为后续研究提供了重要基础。

Abstract: Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.

Abstract (中文翻译): 定位是无人机（UAV）系统的一项基本能力。无图LiDAR重定位为在GNSS信号弱或不可用环境中实现高精度定位提供了一种有效解决方案。然而，现有的LiDAR重定位方法主要针对自动驾驶场景设计，在无人机应用中精度显著下降。本文提出了MAILS，一种面向无人机的新型无图LiDAR重定位框架。首先引入局部保持滑动窗口注意力模块，从稀疏点云中提取局部判别性几何特征。为应对无人机飞行过程中遇到的大范围偏航旋转和高度变化，进一步设计了坐标无关的特征初始化模块和局部不变的位置编码机制，共同显著增强了特征提取的鲁棒性。此外，现有的基于LiDAR的重定位数据集未能捕捉真实无人机飞行特性（如不规则轨迹和高度变化）。为填补这一空白，我们构建了一个大规模无人机LiDAR定位数据集，包含四个场景和多种飞行轨迹，旨在真实条件下评估无人机重定位性能。大量实验表明，我们的方法实现了令人满意的定位精度，并显著且稳定地优于现有技术。我们的代码和数据集将很快发布。

</details>


### [2] [Explanatory Interactive Machine Learning for Bias Mitigation in Visual Gender Classification](https://arxiv.org/abs/2602.13286)
*Nathanya Satriani,Djordje Slijepčević,Markus Schedl,Matthias Zeppelzauer*

Main category: cs.CV

TL;DR: 本文研究了可解释交互式学习（XIL）在缓解视觉分类器中偏见和虚假相关性方面的有效性，特别是在性别分类任务中。通过评估两种现有XIL方法（CAIPI和RRR）及一种新提出的混合方法，实验表明XIL能有效引导模型关注相关特征并提升公平性，其中CAIPI甚至可能提高准确率。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，模型常因数据偏差而学习到虚假相关性，例如在性别分类任务中依赖非相关特征（如背景或配饰）。为解决此问题，作者探索利用可解释交互式学习（XIL）让用户通过反馈模型解释来引导训练，使模型聚焦于用户认为相关的特征，从而减轻偏见、提升公平性。

Method: 作者评估了两种前沿的XIL策略：CAIPI 和 Right for the Right Reasons (RRR)，并提出了一种结合两者的新混合方法。通过将模型生成的解释（使用GradCAM和BLA）与分割掩码进行定量比较，来衡量模型是否聚焦于相关图像区域。

Result: 实验结果表明，所研究的XIL方法能有效（i）引导模型关注与预测相关的图像特征（尤其CAIPI效果显著），以及（ii）减少模型偏见（即平衡男女预测的误分类率）。此外，CAIPI在提升公平性的同时，甚至可能提高分类准确率，而其他方法则伴随轻微性能下降。

Conclusion: XIL方法在提升视觉分类器（尤其是性别分类器）的透明度和公平性方面具有显著潜力。尽管通常会带来轻微的性能损失，但CAIPI方法展示了在改善公平性的同时维持甚至提升准确率的可能性，为构建更可靠、可信的AI系统提供了新途径。

Abstract: Explanatory interactive learning (XIL) enables users to guide model training in machine learning (ML) by providing feedback on the model's explanations, thereby helping it to focus on features that are relevant to the prediction from the user's perspective. In this study, we explore the capability of this learning paradigm to mitigate bias and spurious correlations in visual classifiers, specifically in scenarios prone to data bias, such as gender classification. We investigate two methodologically different state-of-the-art XIL strategies, i.e., CAIPI and Right for the Right Reasons (RRR), as well as a novel hybrid approach that combines both strategies. The results are evaluated quantitatively by comparing segmentation masks with explanations generated using Gradient-weighted Class Activation Mapping (GradCAM) and Bounded Logit Attention (BLA). Experimental results demonstrate the effectiveness of these methods in (i) guiding ML models to focus on relevant image features, particularly when CAIPI is used, and (ii) reducing model bias (i.e., balancing the misclassification rates between male and female predictions). Our analysis further supports the potential of XIL methods to improve fairness in gender classifiers. Overall, the increased transparency and fairness obtained by XIL leads to slight performance decreases with an exception being CAIPI, which shows potential to even improve classification accuracy.

Abstract (中文翻译): 可解释交互式学习（XIL）使用户能够通过提供对模型解释的反馈来指导机器学习（ML）模型的训练，从而帮助模型从用户视角聚焦于与预测相关的特征。在本研究中，我们探索了这种学习范式在缓解视觉分类器中的偏见和虚假相关性方面的能力，特别是在易受数据偏见影响的场景（如性别分类）中。我们研究了两种方法论上不同的前沿XIL策略，即CAIPI和“基于正确理由的正确”（RRR），以及一种结合这两种策略的新颖混合方法。通过将分割掩码与使用梯度加权类激活映射（GradCAM）和有界Logit注意力（BLA）生成的解释进行比较，对结果进行了定量评估。实验结果证明了这些方法在（i）引导ML模型关注相关图像特征（尤其是使用CAIPI时）以及（ii）减少模型偏见（即平衡男性和女性预测之间的误分类率）方面的有效性。我们的分析进一步支持了XIL方法在改善性别分类器公平性方面的潜力。总体而言，XIL所带来的透明度和公平性的提升通常伴随着轻微的性能下降，但CAIPI是个例外，它显示出甚至能提高分类准确率的潜力。

</details>


### [3] [COOPERTRIM: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception](https://arxiv.org/abs/2602.13287)
*Shilpa Mukhopadhyay,Amit Roy-Chowdhury,Hang Qiu*

Main category: cs.CV

TL;DR: 本文提出COOPERTRIM，一种基于时序感知的自适应特征选择框架，通过引入时序不确定性度量和数据驱动机制，在保持感知精度的同时大幅降低协作感知中的通信带宽需求。


<details>
  <summary>Details</summary>
Motivation: 协作感知受限于有限的通信带宽与丰富的传感器信息之间的矛盾，现有方法虽尝试选择性共享特征，但仍难以满足当前无线技术的带宽限制。因此，需要一种更高效的方法来缓解这一矛盾。

Method: 提出COOPERTRIM框架，利用环境动态的时序连续性，避免重复传输静态信息；引入新的保形时序不确定性度量来评估特征相关性，并采用数据驱动机制动态决定共享特征数量。

Result: 在语义分割和3D检测任务上，COOPERTRIM分别最多减少80.28%和72.52%的带宽，同时保持相当的精度；相比其他选择策略，在节省最多72%带宽的同时提升IoU达45.54%；结合压缩策略可将带宽降至1.46%而不损失IoU性能。

Conclusion: COOPERTRIM能灵活适应环境动态、定位误差和通信延迟，在显著降低带宽的同时维持感知性能，为协作感知的实际部署提供了可行路径。

Abstract: Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other's live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information hinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To fundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness, agents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature relevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation and 3D detection as example tasks. Across multiple open-source cooperative segmentation and detection models, COOPERTRIM achieves up to 80.28% and 72.52% bandwidth reduction respectively while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Combined with compression strategies, COOPERTRIM can further reduce bandwidth usage to as low as 1.46% without compromising IoU performance. Qualitative results show COOPERTRIM gracefully adapts to environmental dynamics, localization error, and communication latency, demonstrating flexibility and paving the way for real-world deployment.

Abstract (中文翻译): 协作感知使自主智能体能够通过无线通信共享编码表示，以增强彼此的实时态势感知能力。然而，有限的通信带宽与丰富的传感器信息之间的矛盾阻碍了其实际部署。近期研究探索了每帧仅共享部分特征的选择策略，力求在降低通信开销的同时保持性能。但其带宽需求仍对当前无线技术构成压力。为从根本上缓解这一矛盾，本文采取主动策略，利用时间连续性识别捕捉环境动态的特征，避免重复传输静态信息。通过引入时间感知，智能体可根据环境复杂度动态调整共享特征的数量。我们将这一思想实例化为自适应选择框架COOPERTRIM，该框架提出了一种新颖的保形时序不确定性度量以评估特征相关性，并采用数据驱动机制动态确定共享数量。我们在语义分割和3D检测任务上评估了COOPERTRIM。在多个开源协作分割与检测模型上，COOPERTRIM分别最多实现了80.28%和72.52%的带宽缩减，同时保持了相当的精度。相较于其他选择策略，COOPERTRIM在最多减少72%带宽的情况下，IoU指标最高提升45.54%。结合压缩策略后，COOPERTRIM可进一步将带宽使用降至1.46%，且不牺牲IoU性能。定性结果表明，COOPERTRIM能够优雅地适应环境动态变化、定位误差和通信延迟，展现出良好的灵活性，为实际部署铺平了道路。

</details>


### [4] [Evaluating the Impact of Post-Training Quantization on Reliable VQA with Multimodal LLMs](https://arxiv.org/abs/2602.13289)
*Paul Jonas Kurz,Tobias Jan Wieczorek,Mohamed A. Abdelsalam,Rahaf Aljundi,Marcus Rohrbach*

Main category: cs.CV

TL;DR: 本文首次系统研究了后训练量化（PTQ）对多模态大语言模型（MLLM）在视觉问答任务中准确性和可靠性的影响，发现量化会降低两者性能，但结合数据感知量化方法（MBQ）和Selector置信度估计器可在大幅压缩模型的同时有效缓解可靠性下降。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在部署时面临两个关键挑战：一是模型过度自信，产生高置信度但错误的回答，可靠性不足；二是模型体积庞大，难以部署在边缘设备上，需进行压缩。本文旨在探索这两个问题的交集，即量化压缩如何影响模型的准确性和可靠性。

Method: 作者对Qwen2-VL-7B和Idefics3-8B两个MLLM采用无数据（HQQ）和有数据（MBQ）的后训练量化方法进行不同比特宽度的压缩，并将Selector置信度估计器适配到量化后的多模态场景中，评估其在不同量化级别和分布外（OOD）情况下的鲁棒性。

Result: 实验发现PTQ会同时损害模型的准确性和可靠性，而数据感知的量化方法能缓解这种负面影响。引入Selector置信度估计器能显著减轻可靠性下降的问题。其中，int4 MBQ与Selector的组合在效率与可靠性之间取得了最佳平衡，在内存需求减少约75%的情况下，性能接近未压缩模型。

Conclusion: 本研究是首个系统性地将量化与多模态模型可靠性联系起来的工作，证明了通过结合先进的量化技术和置信度校准方法，可以在大幅压缩模型的同时维持其可靠性和准确性，为MLLM在资源受限设备上的高效可靠部署提供了可行路径。

Abstract: Multimodal Large Language Models (MLLM) are increasingly deployed in domains where both reliability and efficiency are critical. However, current models remain overconfident, producing highly certain but incorrect answers. At the same time, their large size limits deployment on edge devices, necessitating compression. We study the intersection of these two challenges by analyzing how Post-Training Quantization (PTQ) compression affects both accuracy and reliability in Visual Question Answering (VQA). We evaluate two MLLMs, Qwen2-VL-7B and Idefics3-8B, quantized with data-free (HQQ) and data-aware (MBQ) methods across multiple bit widths. To counteract the reduction in reliability caused by quantization, we adapt the Selector confidence estimator for quantized multimodal settings and test its robustness across various quantization levels and out-of-distribution (OOD) scenarios. We find that PTQ degrades both accuracy and reliability. Data-aware methods soften the effect thereof. The Selector substantially mitigates the reliability impact. The combination of int4 MBQ and the Selector achieves the best efficiency-reliability trade-off, closing in on uncompressed performance at approx. 75% less memory demand. Overall, we present the first systematic study linking quantization and reliability in multimodal settings.

Abstract (中文翻译): 多模态大语言模型（MLLM）正越来越多地被部署在对可靠性和效率都至关重要的领域。然而，当前的模型仍然过于自信，会产生高度确定但错误的答案。同时，其庞大的体量限制了在边缘设备上的部署，因此需要进行压缩。我们通过分析后训练量化（PTQ）压缩对视觉问答（VQA）任务中准确性和可靠性的影响，来研究这两个挑战的交集。我们评估了两种MLLM（Qwen2-VL-7B和Idefics3-8B），它们分别使用无数据（HQQ）和有数据（MBQ）的方法在多种比特宽度下进行了量化。为了抵消量化所导致的可靠性下降，我们对Selector置信度估计器进行了适配，使其适用于量化后的多模态环境，并测试了其在不同量化级别和分布外（OOD）场景下的鲁棒性。我们发现，PTQ会同时降低准确性和可靠性，而有数据的方法可以缓和这种影响。Selector则能显著减轻对可靠性的影响。int4 MBQ与Selector的组合实现了最佳的效率-可靠性权衡，在内存需求减少约75%的情况下，性能接近未压缩模型。总体而言，我们首次系统性地研究了多模态环境下量化与可靠性的关联。

</details>


### [5] [NutVLM: A Self-Adaptive Defense Framework against Full-Dimension Attacks for Vision Language Models in Autonomous Driving](https://arxiv.org/abs/2602.13293)
*Xiaoxu Peng,Dong Zhou,Jianwen Zhang,Guanghui Sun,Anh Tu Ngo,Anupam Chattopadhyay*

Main category: cs.CV

TL;DR: 本文提出NutVLM，一种自适应防御框架，用于保护自动驾驶中视觉语言模型（VLM）免受局部和全局对抗攻击，通过检测-净化机制与专家引导的提示调优，在不牺牲干净样本性能的前提下提升整体鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有针对VLM的防御方法在应对对抗攻击（如物理贴片和全局扰动）时效果有限，且难以兼顾鲁棒性与干净样本下的性能，因此亟需一种兼顾安全性和效率的全面防御方案。

Method: NutVLM包含两个核心组件：1）NutNet++，用于三分类检测良性样本、局部贴片和全局扰动；2）对局部威胁采用灰度掩码净化，对全局扰动则使用专家引导的对抗提示调优（EAPT），通过梯度优化和离散投影生成“纠正性驾驶提示”，避免全模型微调。

Result: 在Dolphins基准上，NutVLM在Accuracy、Language Score和GPT Score等指标上整体提升4.89%，验证了其有效性。

Conclusion: NutVLM为智能交通系统中的VLM提供了一种可扩展、高效且兼顾性能与安全的防御方案。

Abstract: Vision Language Models (VLMs) have advanced perception in autonomous driving (AD), but they remain vulnerable to adversarial threats. These risks range from localized physical patches to imperceptible global perturbations. Existing defense methods for VLMs remain limited and often fail to reconcile robustness with clean-sample performance. To bridge these gaps, we propose NutVLM, a comprehensive self-adaptive defense framework designed to secure the entire perception-decision lifecycle. Specifically, we first employ NutNet++ as a sentinel, which is a unified detection-purification mechanism. It identifies benign samples, local patches, and global perturbations through three-way classification. Subsequently, localized threats are purified via efficient grayscale masking, while global perturbations trigger Expert-guided Adversarial Prompt Tuning (EAPT). Instead of the costly parameter updates of full-model fine-tuning, EAPT generates "corrective driving prompts" via gradient-based latent optimization and discrete projection. These prompts refocus the VLM's attention without requiring exhaustive full-model retraining. Evaluated on the Dolphins benchmark, our NutVLM yields a 4.89% improvement in overall metrics (e.g., Accuracy, Language Score, and GPT Score). These results validate NutVLM as a scalable security solution for intelligent transportation. Our code is available at https://github.com/PXX/NutVLM.

Abstract (中文翻译): 视觉语言模型（VLM）已推动自动驾驶（AD）感知能力的发展，但仍易受对抗性威胁影响，这些威胁包括局部物理贴片乃至人眼不可察觉的全局扰动。当前针对VLM的防御方法仍较为有限，往往难以在鲁棒性与干净样本性能之间取得平衡。为弥合这一差距，我们提出了NutVLM——一种全面的自适应防御框架，旨在保障整个感知-决策生命周期的安全。具体而言，我们首先引入NutNet++作为哨兵机制，这是一种统一的检测-净化方法，通过三分类识别良性样本、局部贴片和全局扰动。随后，针对局部威胁采用高效的灰度掩码进行净化，而全局扰动则触发专家引导的对抗提示调优（EAPT）。EAPT无需像全模型微调那样进行昂贵的参数更新，而是通过基于梯度的潜在空间优化与离散投影生成“纠正性驾驶提示”，从而重新聚焦VLM的注意力，避免了耗时的全模型重训练。在Dolphins基准上的评估表明，NutVLM在整体指标（如准确率、语言评分和GPT评分）上提升了4.89%。这些结果验证了NutVLM作为智能交通系统中可扩展安全解决方案的有效性。代码已开源：https://github.com/PXX/NutVLM。

</details>


### [6] [VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction](https://arxiv.org/abs/2602.13294)
*Jiarong Liang,Max Ku,Ka-Hei Hui,Ping Nie,Wenhu Chen*

Main category: cs.CV

TL;DR: 本文提出VisPhyWorld框架，通过要求多模态大语言模型（MLLMs）从视觉输入生成可执行的物理模拟代码，来评估其真实物理推理能力，并构建了包含209个场景的VisPhyBench基准。实验表明，当前最先进的MLLMs虽具备良好的语义理解能力，但在推断物理参数和模拟一致动态方面仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估多模态大语言模型物理推理能力的基准（如VQA、VoE）大多依赖识别式任务，无法强制模型形成明确且可检验的物理假设，因此难以判断模型是否真正理解物理规律。

Method: 提出VisPhyWorld框架，要求模型根据视觉观察生成可执行的物理模拟器代码；该代码可直接检验、编辑和证伪，从而将物理推理与渲染分离。基于此构建VisPhyBench基准，包含209个源自108个物理模板的评估场景，并采用系统化协议评估模型在外观重建和物理运动模拟方面的表现。

Result: 所提评估流程在基准上生成有效重建视频的成功率达97.7%；实验显示，尽管最先进MLLMs在语义场景理解方面表现良好，但在准确推断物理参数和模拟一致物理动态方面仍存在显著困难。

Conclusion: 通过生成可执行代码的方式能更有效地评估模型的物理推理能力，当前MLLMs尚未真正掌握物理动态建模，未来需在显式物理表示和可验证推理方面加强。

Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.

Abstract (中文翻译): 评估多模态大语言模型（MLLMs）是否真正具备物理动态推理能力仍具挑战性。大多数现有基准依赖于识别式协议，例如视觉问答（VQA）和期望违反（VoE），这些问题往往无需模型提出明确且可检验的物理假设即可作答。我们提出了VisPhyWorld——一种基于执行的框架，通过要求模型从视觉观察中生成可执行的模拟器代码来评估其物理推理能力。通过生成可运行的代码，所推断的世界表征可被直接检查、编辑和证伪，从而将物理推理与渲染过程分离。基于该框架，我们构建了VisPhyBench，包含源自108个物理模板的209个评估场景，以及一套系统化评估协议，用于衡量模型在外观重建和生成物理合理运动方面的表现。我们的流程在该基准上生成有效重建视频的成功率达97.7%。实验表明，尽管当前最先进的MLLMs在语义场景理解方面表现强劲，但在准确推断物理参数和模拟一致的物理动态方面仍存在困难。

</details>


### [7] [MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models](https://arxiv.org/abs/2602.13296)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 本文提出一种基于物理意义的高分辨距离像（HRRP）生成数据评估方法，通过将HRRP分解为掩码、特征和噪声三个部分，设计了两个可解释的新评估指标，并在真实数据集上验证其判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有HRRP生成数据的评估方法依赖分类模型（“黑箱”方法），缺乏可解释性和多层次评估能力，难以有效衡量生成数据的质量。

Method: 将HRRP数据分解为掩码、特征和噪声三个组成部分，并基于这些成分的物理含义设计两种新的评估指标。

Result: 在具有挑战性的昂贵真实数据集上验证了所提指标的有效性，展示了其对生成HRRP数据的判别能力。

Conclusion: 所提出的基于物理分解的评估指标能提供更可解释、多层次的HRRP生成质量评价，优于传统依赖分类器的黑箱方法。

Abstract: High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.

Abstract (中文翻译): 高分辨距离像（HRRP）数据在雷达自动目标识别（RATR）中日益流行。随着利用HRRP进行分类模型研究的兴趣增加，使用生成模型填补数据集空缺的工作近期取得了可喜进展。然而，即使对于人脸图像等显式数据，生成数据的评估仍是一个具有挑战性的问题。目前HRRP生成领域的前沿评估方法依赖于分类模型，这类“黑箱”模型既无法对生成数据提供可解释性，也无法实现多层次评估。本文聚焦于将HRRP数据分解为三个组成部分：掩码、特征和噪声。基于这种分解，我们提出了两种基于数据物理含义的评估指标。我们利用一个昂贵的真实数据集，在一项具有挑战性的任务上评估所提出的指标，并证明了这些指标的判别能力。

</details>


### [8] [Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset](https://arxiv.org/abs/2602.13297)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 本文在大规模海上数据库上研究高分辨距离像（HRRP）合成，发现目标几何尺寸和观测角度是影响HRRP的关键因素，基于此条件生成的HRRP能有效复现真实数据中的几何趋势，强调了采集几何在鲁棒HRRP生成中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 高分辨距离像（HRRP）对采集条件高度敏感，导致在不同操作场景下鲁棒性受限；现有条件生成方法受限于小规模、高度特定的数据集，难以推广。

Method: 在具有沿海监视多样性的大规模海上数据库上，以舰船几何尺寸和期望观测角度作为条件变量，训练生成模型以合成HRRP。

Result: 所生成的HRRP能够复现真实数据中预期的视线几何趋势，验证了几何因素在HRRP生成中的主导作用。

Conclusion: 采集几何（特别是目标尺寸与观测角度）是实现鲁棒HRRP生成的关键，为未来雷达自动目标识别系统提供了重要指导。

Abstract: High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.

Abstract (中文翻译): 高分辨距离像（HRRP）可实现雷达自动目标识别的快速机载处理，但其对采集条件的高度敏感性限制了在不同作战场景下的鲁棒性。条件式HRRP生成可缓解这一问题，但以往研究受限于规模小且高度特定的数据集。本文在代表沿海监视多样性的大规模海上数据库上研究HRRP合成。分析表明，决定场景变化的根本因素是几何特性：即舰船尺寸和所需观测角度。基于这些变量进行条件建模，我们训练了生成模型，并证明所合成的信号能够复现真实数据中观察到的视线几何趋势。这些结果突显了采集几何在鲁棒HRRP生成中的核心作用。

</details>


### [9] [Effect of Convolutional Depth on Image Recognition Performance: VGG vs. ResNet vs. GoogLeNet](https://arxiv.org/abs/2602.13298)
*Manfred M. Fischer,Joshua Pitts*

Main category: cs.CV

TL;DR: 该论文通过控制变量实验比较了VGG、ResNet和GoogLeNet三种经典CNN架构，发现网络的“有效深度”（而非名义深度）才是决定性能提升、优化稳定性和计算效率的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管增加卷积网络的深度在图像识别中取得了显著进展，但更深的网络并不总能带来更高的准确率、稳定的优化或高效的计算。因此，有必要系统研究深度如何真正影响网络性能。

Method: 对VGG、ResNet和GoogLeNet三种典型CNN架构进行受控对比实验，在统一训练协议下，区分“名义深度”与“有效深度”，分析深度对分类性能、收敛行为和计算效率的影响。

Result: 普通深层网络（如VGG）会出现准确率早期饱和和优化不稳定；而残差（ResNet）和Inception结构（GoogLeNet）能以更低的有效深度将增加的深度转化为更高的准确率，并实现更优的准确率-计算开销权衡。

Conclusion: 在卷积神经网络中，真正决定深度作为有效扩展维度的是“有效深度”，而非“名义深度”；架构设计中的机制（如残差连接、Inception模块）对有效深度的形成至关重要。

Abstract: Increasing convolutional depth has been central to advances in image recognition, yet deeper networks do not uniformly yield higher accuracy, stable optimization, or efficient computation. We present a controlled comparative study of three canonical convolutional neural network architectures - VGG, ResNet, and GoogLeNet - to isolate how depth influences classification performance, convergence behavior, and computational efficiency. By standardizing training protocols and explicitly distinguishing between nominal and effective depth, we show that the benefits of depth depend critically on architectural mechanisms that constrain its effective manifestation during training rather than on nominal depth alone. Although plain deep networks exhibit early accuracy saturation and optimization instability, residual and inception-based architectures consistently translate additional depth into improved accuracy at lower effective depth and favorable accuracy-compute trade-offs. These findings demonstrate that effective depth, not nominal depth, is the operative quantity governing depth's role as a productive scaling dimension in convolutional networks.

Abstract (中文翻译): 增加卷积深度一直是图像识别领域取得进展的核心，然而更深的网络并不总能带来更高的准确率、稳定的优化或高效的计算。本文对三种经典卷积神经网络架构——VGG、ResNet和GoogLeNet——进行了受控的对比研究，以厘清深度如何影响分类性能、收敛行为和计算效率。通过标准化训练协议并明确区分名义深度与有效深度，我们表明深度带来的收益关键取决于架构机制是否能在训练过程中有效约束其实际表现，而非仅依赖于名义深度。尽管普通深层网络表现出早期准确率饱和和优化不稳定性，基于残差和Inception的架构却能持续地将额外深度转化为更高的准确率，同时在更低的有效深度下实现更有利的准确率-计算开销权衡。这些发现表明，在卷积网络中，有效深度而非名义深度，才是决定深度作为有效扩展维度的关键因素。

</details>


### [10] [KidMesh: Computational Mesh Reconstruction for Pediatric Congenital Hydronephrosis Using Deep Neural Networks](https://arxiv.org/abs/2602.13299)
*Haoran Sun,Zhanpeng Zhu,Anguo Zhang,Bo Liu,Zhaohua Lin,Liqin Huang,Mingjing Yang,Lei Liu,Shan Lin,Wangbin Ding*

Main category: cs.CV

TL;DR: 本文提出了一种名为KidMesh的端到端深度学习方法，可直接从磁共振尿路造影（MRU）图像中自动重建小儿先天性肾积水（CH）的三维网格模型，无需依赖复杂的后处理或精确的网格标注，重建速度快、精度高，并支持后续尿动力学仿真。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素的分割方法虽能从MRU中提取CH区域，但仅关注形态学特征，难以直接用于功能评估（如尿动力学模拟），且需额外复杂后处理才能生成网格模型。此外，获取精确的网格级标注困难，限制了端到端网格重建方法的发展。

Method: 提出KidMesh方法：首先从MRU图像中提取特征图，通过网格采样将其转换为特征顶点，再根据这些顶点对模板网格进行形变，生成特定CH的三维网格。同时设计了一种无需精确网格标注的训练策略，以应对MRU切片稀疏导致标注困难的问题。

Result: KidMesh平均0.4秒内完成网格重建，无需后处理即可达到与传统方法相当的性能；重建网格无自交，仅3.7%和0.2%的顶点误差分别超过3.2mm和6.4mm；光栅化后与人工标注的Dice得分为0.86；并可用于肾尿流仿真。

Conclusion: KidMesh提供了一种高效、准确且无需复杂后处理的CH网格重建方案，能够支持临床尿动力学分析，具有良好的应用前景。

Abstract: Pediatric congenital hydronephrosis (CH) is a common urinary tract disorder, primarily caused by obstruction at the renal pelvis-ureter junction. Magnetic resonance urography (MRU) can visualize hydronephrosis, including renal pelvis and calyces, by utilizing the natural contrast provided by water. Existing voxel-based segmentation approaches can extract CH regions from MRU, facilitating disease diagnosis and prognosis. However, these segmentation methods predominantly focus on morphological features, such as size, shape, and structure. To enable functional assessments, such as urodynamic simulations, external complex post-processing steps are required to convert these results into mesh-level representations. To address this limitation, we propose an end-to-end method based on deep neural networks, namely KidMesh, which could automatically reconstruct CH meshes directly from MRU. Generally, KidMesh extracts feature maps from MRU images and converts them into feature vertices through grid sampling. It then deforms a template mesh according to these feature vertices to generate the specific CH meshes of MRU images. Meanwhile, we develop a novel schema to train KidMesh without relying on accurate mesh-level annotations, which are difficult to obtain due to the sparsely sampled MRU slices. Experimental results show that KidMesh could reconstruct CH meshes in an average of 0.4 seconds, and achieve comparable performance to conventional methods without requiring post-processing. The reconstructed meshes exhibited no self-intersections, with only 3.7% and 0.2% of the vertices having error distances exceeding 3.2mm and 6.4mm, respectively. After rasterization, these meshes achieved a Dice score of 0.86 against manually delineated CH masks. Furthermore, these meshes could be used in renal urine flow simulations, providing valuable urodynamic information for clinical practice.

Abstract (中文翻译): 小儿先天性肾积水（CH）是一种常见的泌尿系统疾病，主要由肾盂-输尿管连接处梗阻引起。磁共振尿路造影（MRU）利用水的天然对比度，可清晰显示包括肾盂和肾盏在内的肾积水区域。现有的基于体素的分割方法能够从MRU中提取CH区域，辅助疾病诊断与预后评估，但这些方法主要关注大小、形状和结构等形态学特征。若要进行尿动力学模拟等功能性评估，仍需通过复杂的外部后处理步骤将分割结果转换为网格表示。为解决这一局限，本文提出一种基于深度神经网络的端到端方法——KidMesh，可直接从MRU图像中自动重建CH的三维网格模型。具体而言，KidMesh首先从MRU图像中提取特征图，并通过网格采样将其转化为特征顶点，随后依据这些顶点对模板网格进行形变，从而生成对应MRU图像的特定CH网格。同时，我们设计了一种新颖的训练策略，无需依赖难以获取的精确网格级标注（因MRU切片采样稀疏所致）。实验结果表明，KidMesh平均仅需0.4秒即可完成CH网格重建，在无需后处理的情况下性能与传统方法相当；重建的网格无自交现象，仅有3.7%和0.2%的顶点误差距离分别超过3.2mm和6.4mm；经光栅化后，与人工勾画的CH掩膜相比，Dice得分为0.86。此外，这些网格可用于肾脏尿液流动仿真，为临床提供有价值的尿动力学信息。

</details>
