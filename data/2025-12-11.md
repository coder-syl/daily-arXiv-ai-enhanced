<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn,Yura Choi,Hyeonbeom Choi,Seongwon Cho,San Kim,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文发现当前视频多模态大模型（VLMMs）在事件时序理解方面存在不足，即使输入帧被扰乱仍能取得高分，表明其依赖场景先验而非真实时序推理。为此，作者提出了新基准VECTOR以评估时序理解能力，并提出MECOT方法，通过事件级指令微调与推理时的思维链提示提升模型对事件顺序的感知，在VECTOR及现有视频任务上均取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大模型在标准评测中表现良好，但其是否真正理解视频中多个事件的时间顺序尚不清楚。作者通过实验发现模型在帧被打乱的情况下仍能取得高分，说明其可能依赖常识先验而非真实时序建模，因此亟需一个专门评估时序理解能力的新基准。

Method: 作者构建了名为VECTOR的新基准用于评估模型对事件时序的理解；并提出MECOT方法，包括：(1) 使用逐事件的详细视频描述进行指令微调；(2) 在推理阶段引入思维链（Chain-of-Thought）提示，以增强模型的时序意识。

Result: 在VECTOR基准上，多种现有VLMM均表现不佳，而MECOT显著优于先前方法；同时，MECOT也在现有视频理解基准上带来性能提升，验证了其对时序理解的有效性。

Conclusion: 当前VLMM在事件时序理解方面存在明显缺陷，仅靠场景先验不足以完成真正的时间推理。通过针对性的数据和训练策略（如MECOT），可有效提升模型的时序建模能力，为未来视频理解研究提供新方向。

Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.

</details>


### [2] [Training Multi-Image Vision Agents via End2End Reinforcement Learning](https://arxiv.org/abs/2512.08980)
*Chengqi Dong,Chuhuai Yue,Hang He,Rongge Mao,Fenghe Tang,S Kevin Zhou,Zekun Xu,Xiaohan Wang,Jiajun Chai,Wei Lin,Guojun Yin*

Main category: cs.CV

TL;DR: 本文提出IMAgent，一个基于端到端强化学习的开源视觉语言模型（VLM）智能体，专为复杂多图像问答任务设计。通过多智能体系统生成高质量多图像问答数据集MIFG-QA，并引入两种专用工具增强模型对图像内容的关注，结合两级掩码策略实现稳定工具调用，无需监督微调，在单图和多图任务上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有开源VLM智能体大多仅支持单图像输入，难以应对现实世界中复杂的多图像问答任务；同时，随着推理步骤加深，模型容易忽略视觉输入，限制了其工具使用能力。

Method: 提出IMAgent框架：1）利用多智能体系统生成具挑战性的多图像问答对，构建含1万样本的数据集MIFG-QA；2）设计两种专用工具用于视觉反思与确认，引导模型在推理中主动关注图像；3）采用动作-轨迹两级掩码策略，通过纯强化学习实现稳定的工具调用行为，无需监督微调。

Result: IMAgent在现有单图像基准上保持强性能，同时在新提出的多图像数据集上取得显著提升，验证了方法的有效性。

Conclusion: IMAgent成功解决了多图像理解与工具调用中的关键挑战，展示了纯强化学习在训练VLM智能体中的潜力，并为社区提供了可复现的开源方案与数据集。

Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.

</details>


### [3] [Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding](https://arxiv.org/abs/2512.08981)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 本文提出了一种名为UTIE的新方法，通过利用视觉-语言模型（VLMs）将其他人口群体的文本语义信息融入人脸嵌入中，以模糊人口属性、增强身份相关特征，从而在保持甚至提升人脸识别准确率的同时，显著降低跨群体的人脸识别偏见。


<details>
  <summary>Details</summary>
Motivation: 现有人脸识别系统常因人口统计学信息与身份特征在嵌入空间中纠缠而导致对不同群体的识别性能不均，尤其在多元文化城市中问题突出；为缓解这一偏见，需解耦身份与人口属性信息。

Method: 提出Unified Text-Image Embedding（UTIE）策略，利用视觉-语言模型（如CLIP、OpenCLIP、SigLIP）的跨模态对齐能力，将来自其他人口群体的文本描述特征注入当前人脸嵌入，使其在保留身份信息的同时弱化特定人口属性，实现更公平的表示。

Result: 在RFW和BFW两个公平性评测基准上，UTIE在三种VLM上均有效降低了偏见指标，同时维持甚至提升了人脸验证准确率。

Conclusion: UTIE通过引入跨群体文本语义信息成功实现了更公平且高精度的人脸识别，为缓解FR系统中的人口偏见提供了有效且实用的解决方案。

Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.

</details>


### [4] [Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement](https://arxiv.org/abs/2512.08982)
*Jian Xu,Wei Chen,Shigui Li,Delu Zeng,John Paisley,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文提出Consist-Retinex，首次将一致性模型应用于Retinex低光增强任务，通过双目标一致性损失和自适应噪声强调采样策略，实现单步生成的SOTA性能，且训练成本仅为传统扩散模型的1/8。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的低光图像增强方法虽效果良好，但需数百次迭代采样，难以实际部署；而当前一致性模型仅用于无条件生成，尚未探索其在有条件增强任务中的应用。

Method: 提出Consist-Retinex框架，包含两项核心创新：(1) 双目标一致性损失，在随机时间采样下结合时间一致性和真值对齐，提供全谱监督；(2) 自适应噪声强调采样策略，优先训练大噪声区域以支持单步条件生成。

Result: 在VE-LOL-L数据集上，Consist-Retinex以单步采样达到PSNR 25.51、FID 44.73，优于Diff-Retinex++（PSNR 23.41、FID 49.59），且训练开销仅为1000步Diff-Retinex基线的1/8。

Conclusion: Consist-Retinex成功将一致性建模引入Retinex低光增强，显著提升推理效率与训练经济性，同时保持甚至超越现有扩散模型的增强质量。

Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.

</details>


### [5] [HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification](https://arxiv.org/abs/2512.08983)
*Maoyu Wang,Yao Lu,Bo Zhou,Zhuangzhi Chen,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: 本文提出了一种名为HSCP的分层谱聚类剪枝框架，通过结合层剪枝与通道剪枝，在显著压缩模型规模和计算量的同时提升识别准确率，并在低信噪比环境下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统无人机识别方法在复杂环境中难以提取可靠信号特征且难以满足实时性要求；基于深度学习的射频指纹识别虽提升了准确率，但模型大、计算需求高，难以部署于资源受限的边缘设备。现有剪枝方法难以同时优化压缩率、硬件加速与识别精度。

Method: 提出HSCP框架：第一阶段利用基于CKA（Centered Kernel Alignment）引导的谱聚类识别并移除冗余层；第二阶段将相同策略应用于通道维度以消除更细粒度的冗余；最后采用抗噪微调策略增强鲁棒性。

Result: 在UAV-M100基准上，HSCP在ResNet18上实现了86.39%的参数减少和84.44%的FLOPs减少，同时相比未剪枝基线提升了1.49%的准确率，并在低信噪比下仍保持优异鲁棒性。

Conclusion: HSCP有效解决了现有剪枝方法在压缩率、推理效率与识别精度之间难以兼顾的问题，为资源受限场景下的高效无人机射频指纹识别提供了可行方案。

Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.

</details>


### [6] [RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition](https://arxiv.org/abs/2512.08984)
*Nirhoshan Sivaroopan,Hansi Karunarathna,Chamara Madarasingha,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.CV

TL;DR: 本文提出RAG-HAR，一种无需训练的检索增强框架，利用大语言模型（LLM）进行人类活动识别（HAR），在六个基准上达到SOTA性能，且无需模型训练或微调。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在HAR任务中依赖特定数据集训练、大量标注数据和高计算资源，限制了其泛化能力和实用性。

Method: RAG-HAR通过计算轻量级统计描述符，从向量数据库中检索语义相似样本，并结合上下文证据利用LLM进行活动识别；进一步通过提示优化和LLM生成的活动描述符构建富含上下文的向量数据库。

Result: 在六个多样化的HAR基准上实现最先进性能，且能识别并合理标注多种未见过的人类活动。

Conclusion: RAG-HAR无需训练即可实现高性能和强泛化能力，显著提升了HAR系统的实用性和部署灵活性。

Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.

</details>


### [7] [An Efficient Test-Time Scaling Approach for Image Generation](https://arxiv.org/abs/2512.08985)
*Vignesh Sundaresha,Akash Haridas,Vikram Appia,Lav Varshney*

Main category: cs.CV

TL;DR: 本文提出了一种名为Verifier-Threshold的方法，通过在图像生成模型的去噪过程中更有效地重新分配测试时计算资源，在保持GenEval基准性能不变的前提下，将计算时间减少了2-4倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在为扩散和流模型分配非均匀推理计算预算时依赖贪心算法，导致计算资源分配效率低下，亟需改进。

Method: 提出Verifier-Threshold方法，自动重新分配测试时计算资源，优化不同去噪步骤中的计算预算分配。

Result: 在GenEval基准上达到与当前最先进方法相同的性能，同时将计算时间减少2-4倍。

Conclusion: 通过更智能地分配测试时计算资源，Verifier-Threshold方法显著提升了图像生成模型的推理效率，证明了优化计算分配策略的有效性。

Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.

</details>


### [8] [Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy](https://arxiv.org/abs/2512.08986)
*Anca Mihai,Adrian Groza*

Main category: cs.CV

TL;DR: 本文提出了一种用于糖尿病视网膜病变（DR）图像数据的质量控制框架，通过可解释的特征分类器筛选低质量图像、结合图像增强与深度学习辅助标注，并利用标注者一致性评估确保用于AI训练和评估的数据具有高质量。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变若未及早诊断可能导致视力丧失，而AI辅助诊断依赖高质量标注数据；然而，由于视网膜结构复杂，人工采集和标注过程中易出现错误，因此需要一个可靠的质量控制机制。

Method: 该方法包括三个步骤：1）使用基于可解释特征的分类器（结合图像处理与对比学习提取特征）过滤不合格图像；2）对保留图像进行增强并采用深度学习辅助人工标注；3）通过所推导的一致性公式评估标注者间一致性，以判断标注是否可用。

Result: 该框架有效筛选出高质量图像并提升标注可靠性，从而为AI模型训练和评估提供更可信的数据基础。

Conclusion: 所提出的质量控制框架有助于提高糖尿病视网膜病变AI诊断系统中训练和评估数据的质量，进而提升模型性能与临床实用性。

Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.

</details>


### [9] [3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization](https://arxiv.org/abs/2512.08987)
*Yuze Hao,Linchao Zhu,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为3DID的三维逆向设计框架，通过结合连续潜在表示与物理感知优化策略，直接在三维设计空间中进行高效搜索，从而生成高保真度且多样化的三维几何结构。


<details>
  <summary>Details</summary>
Motivation: 现有三维逆向设计方法通常依赖二维投影或对已有三维形状微调，牺牲了体积细节并限制了从零开始的设计探索能力；同时，三维设计空间的指数级增长使得传统搜索方法不可行。

Method: 提出3DID框架：首先学习一个统一的物理-几何嵌入，在连续潜在空间中紧凑地表示形状和物理场数据；然后采用两阶段优化策略——第一阶段使用梯度引导的扩散采样器探索全局潜在流形，第二阶段进行目标驱动、拓扑保持的精细化调整。

Result: 3DID在解的质量和设计多样性方面均优于现有方法，能够生成高保真度的三维几何结构。

Conclusion: 所提出的3DID框架有效克服了当前三维逆向设计方法在细节保留与设计自由度方面的局限，实现了真正意义上的从零开始三维逆向设计。

Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.

</details>


### [10] [Enhancing Knowledge Transfer in Hyperspectral Image Classification via Cross-scene Knowledge Integration](https://arxiv.org/abs/2512.08989)
*Lu Huo,Wenjian Huang,Jianguo Zhang,Min Xu,Haimin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为跨场景知识整合（CKI）的新框架，以解决高光谱图像分类中跨域知识迁移的挑战，特别是在标签空间不重叠且目标域包含私有类别的完全异构场景下。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高光谱图像跨域迁移时受限于同质域假设或仅包含共现类别的异构场景，无法有效利用目标域私有信息，尤其在源域和目标域标签空间无重叠时表现不佳。

Method: 提出CKI框架，包含三个核心组件：(1) 光谱特征对齐（ASC），通过域无关投影减少光谱差异；(2) 跨场景知识共享偏好（CKSP），利用源相似性机制（SSM）缓解语义不一致；(3) 互补信息整合（CII），最大化利用目标域特有信息。

Result: 大量实验表明，CKI在多种跨场景高光谱图像分类任务中达到最先进的性能，并展现出良好的稳定性。

Conclusion: CKI有效解决了完全异构场景下的高光谱图像跨域知识迁移问题，通过显式整合目标域私有知识显著提升了分类性能。

Abstract: Knowledge transfer has strong potential to improve hyperspectral image (HSI) classification, yet two inherent challenges fundamentally restrict effective cross-domain transfer: spectral variations caused by different sensors and semantic inconsistencies across heterogeneous scenes. Existing methods are limited by transfer settings that assume homogeneous domains or heterogeneous scenarios with only co-occurring categories. When label spaces do not overlap, they further rely on complete source-domain coverage and therefore overlook critical target-private information. To overcome these limitations and enable knowledge transfer in fully heterogeneous settings, we propose Cross-scene Knowledge Integration (CKI), a framework that explicitly incorporates target-private knowledge during transfer. CKI includes: (1) Alignment of Spectral Characteristics (ASC) to reduce spectral discrepancies through domain-agnostic projection; (2) Cross-scene Knowledge Sharing Preference (CKSP), which resolves semantic mismatch via a Source Similarity Mechanism (SSM); and (3) Complementary Information Integration (CII) to maximize the use of target-specific complementary cues. Extensive experiments verify that CKI achieves state-of-the-art performance with strong stability across diverse cross-scene HSI scenarios.

</details>
