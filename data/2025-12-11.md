<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn,Yura Choi,Hyeonbeom Choi,Seongwon Cho,San Kim,Jonghyun Choi*

Main category: cs.CV

TL;DR: 现有视频大模型（VLMMs）在理解事件时序方面能力不足，本文提出新基准VECTOR评估该能力，并引入MECOT方法通过事件级描述与思维链提示提升模型时序理解表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLMMs在视频理解任务中表现出色，但其对多个事件时间顺序的准确捕捉能力尚未充分研究；实验发现即使打乱视频帧顺序，模型仍能在现有基准上取得良好表现，说明其可能依赖场景先验知识而非真实时序推理。

Method: 提出名为VECTOR的新基准用于评估VLMMs的事件时序理解能力；并提出MECOT方法，包括：(1) 使用逐事件的详细视频描述进行指令微调；(2) 在推理阶段采用思维链（Chain-of-Thought）提示以增强模型对时间顺序的感知。

Result: 在VECTOR基准上，多种VLMMs表现不佳，而MECOT显著优于现有方法，同时也在已有视频理解基准上带来性能提升。

Conclusion: VLMMs当前缺乏真正的事件时序理解能力，所提出的VECTOR基准和MECOT方法有效提升了模型在此方面的能力，验证了显式建模时序信息的重要性。

Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.

Abstract (中文翻译): 视频大模型（VLMMs）在视频理解方面展现出卓越性能，但其准确捕捉多个事件时间顺序的能力仍未得到充分探索。我们通过全面实验有趣地发现，即使视频帧被打乱，模型在现有基准上依然表现优异。这表明VLMMs可能并不依赖对视觉事件的准确顺序处理，而是利用典型场景的先验知识来回答问题。为评估VLMMs的时间理解能力，我们提出了VECTOR基准，专门用于明确测试模型识别事件时间顺序的能力。在该基准上，我们观察到多种VLMMs通常难以正确理解事件顺序。为此，我们提出了MECOT（多事件指令微调结合思维链）方法，该方法（1）使用详细的逐事件视频描述对模型进行训练，（2）在推理阶段采用思维链提示以增强模型的时间感知能力。MECOT不仅在VECTOR上优于先前方法，还在现有视频基准上提升了性能，表明其在增强时间理解方面的有效性。我们已开源代码、模型和数据集。

</details>


### [2] [Training Multi-Image Vision Agents via End2End Reinforcement Learning](https://arxiv.org/abs/2512.08980)
*Chengqi Dong,Chuhuai Yue,Hang He,Rongge Mao,Fenghe Tang,S Kevin Zhou,Zekun Xu,Xiaohan Wang,Jiajun Chai,Wei Lin,Guojun Yin*

Main category: cs.CV

TL;DR: 本文提出IMAgent，一个基于端到端强化学习训练的开源视觉智能体，专为复杂多图像问答任务设计，通过多智能体系统生成高质量多图像数据集MIFG-QA，并引入视觉反思与确认工具以增强模型对图像内容的关注，在无需监督微调的情况下实现稳定工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的智能体大多仅支持单图输入，难以应对现实世界中的多图像问答任务；同时，随着推理步骤加深，模型可能忽略视觉输入，限制了其在复杂任务中的表现。

Method: 提出IMAgent框架，采用端到端强化学习训练；利用多智能体系统生成具挑战性的多图像问答对（构建MIFG-QA数据集）；设计两种专用工具用于视觉反思与确认；引入动作-轨迹两级掩码策略以稳定纯RL训练下的工具使用行为。

Result: IMAgent在保持现有单图基准性能的同时，在新提出的多图像数据集上取得显著提升，验证了方法的有效性。

Conclusion: IMAgent成功解决了多图像理解中工具使用和视觉注意力分配的问题，为未来研究提供了可复现的开源方案和实用见解。

Abstract: Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.

Abstract (中文翻译): 近期基于视觉语言模型（VLM）的智能体试图通过工具使用来复现OpenAI O3的“结合图像进行思考”能力，但大多数开源方法仅限于单张图像输入，在真实世界的多图像问答任务中表现不足。为此，我们提出了IMAgent——一个专为复杂多图像任务设计、通过端到端强化学习训练的开源视觉智能体。我们利用多智能体系统生成具有挑战性且视觉信息丰富的多图像问答对，以充分激发基础VLM的工具使用潜力，并通过人工验证构建了包含1万样本的MIFG-QA数据集用于训练与评估。考虑到VLM在更深层次推理过程中可能逐渐忽略视觉输入，我们开发了两种专门用于视觉反思与确认的工具，使模型在推理过程中能主动重新聚焦于图像内容。得益于精心设计的动作-轨迹两级掩码策略，IMAgent能够在不依赖昂贵监督微调数据的情况下，仅通过纯强化学习实现稳定的工具使用行为。大量实验表明，IMAgent在现有单图像基准上保持强劲性能的同时，在我们提出的多图像数据集上取得了显著提升，我们的分析也为研究社区提供了可操作的洞见。代码和数据将很快发布。

</details>


### [3] [Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding](https://arxiv.org/abs/2512.08981)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 本文提出一种名为UTIE的新方法，通过利用视觉-语言模型将其他人口统计群体的文本语义信息融入人脸嵌入，使人脸表征在保留身份信息的同时模糊人口属性，从而降低人脸识别系统中的偏见。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统常因人口统计特征与身份特征在嵌入空间中纠缠而导致偏见，尤其在多元文化城市中影响显著。这种纠缠会使人脸验证在不同群体间表现不均，亟需缓解。

Method: 提出Unified Text-Image Embedding（UTIE）策略，利用视觉-语言模型（如CLIP、OpenCLIP、SigLIP）的跨模态对齐能力，将其他人口群体的文本描述特征注入当前人脸嵌入，诱导人口统计模糊性，强化身份相关特征。

Result: 在RFW和BFW两个公平性评测基准上实验表明，UTIE能持续降低偏见指标，同时保持甚至在某些情况下提升人脸验证准确率。

Conclusion: UTIE有效缓解了人脸识别中的人口统计偏见，证明了利用跨模态语义信息实现更公平表征的可行性。

Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.

Abstract (中文翻译): 人脸识别（FR）系统常常容易受到人口统计偏见的影响，部分原因在于人脸嵌入中人口统计特定信息与身份相关特征相互纠缠。这种偏见在大型多元文化城市中尤为关键，尤其是在生物识别技术在智慧城市基础设施中扮演重要角色的场景下。这种纠缠可能导致人口属性在嵌入空间中掩盖身份线索，从而造成不同人口群体之间验证性能的差异。为解决这一问题，我们提出了一种新策略——统一文本-图像嵌入（Unified Text-Image Embedding, UTIE），通过将其他人口群体的相关信息融入人脸嵌入，诱导其产生人口统计模糊性，从而促使嵌入更强调身份相关特征，实现更公平的跨群体验证性能。UTIE利用视觉-语言模型（VLMs）的零样本能力和跨模态语义对齐特性。鉴于VLMs天然地被训练用于对齐视觉与文本表示，我们将每个群体的人脸嵌入与从其他人口群体提取的文本衍生人口特征相结合，以鼓励在人口属性方面更具中立性的表征。我们在三个VLM（CLIP、OpenCLIP和SigLIP）上，使用两个广泛采用的偏见评估基准（RFW和BFW）对UTIE进行了评估。实验结果表明，UTIE在持续降低偏见指标的同时，还能维持甚至在若干情况下提升人脸识别的准确率。

</details>


### [4] [Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement](https://arxiv.org/abs/2512.08982)
*Jian Xu,Wei Chen,Shigui Li,Delu Zeng,John Paisley,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文提出 Consist-Retinex，首次将一致性模型应用于 Retinex 框架下的低光图像增强，通过双目标一致性损失和自适应噪声强调采样策略，在单步生成下实现 SOTA 性能，并大幅降低训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的低光增强方法虽效果良好，但需数百次迭代采样，难以实际部署；而当前一致性模型仅用于无条件生成，尚未探索其在有条件增强任务中的应用。

Method: 提出 Consist-Retinex 框架，包含两项核心创新：(1) 双目标一致性损失，在随机时间采样下结合时间一致性和真值对齐，提供全谱监督；(2) 自适应噪声强调采样策略，优先训练对单步条件生成至关重要的大噪声区域。

Result: 在 VE-LOL-L 数据集上，Consist-Retinex 单步采样即达到 SOTA 性能（PSNR: 25.51 vs. 23.41，FID: 44.73 vs. 49.59），且训练开销仅为 1000 步 Diff-Retinex 基线的 1/8。

Conclusion: Consist-Retinex 成功将一致性建模引入 Retinex 低光增强任务，显著提升推理效率与训练经济性，为条件图像增强提供新范式。

Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.

Abstract (中文翻译): 扩散模型通过 Retinex 分解在低光图像增强任务中取得了显著成功，但其需要数百次迭代采样步骤，严重限制了实际部署。尽管近期的一致性模型在无条件合成任务中展现出有前景的单步生成能力，但其在条件增强任务中的应用仍未被探索。我们提出了 Consist-Retinex，这是首个将一致性建模应用于 Retinex 低光增强的框架。我们的关键洞察在于：条件增强所需的训练动态与无条件生成存在根本差异——标准一致性训练聚焦于数据流形附近的低噪声区域，而条件映射则高度依赖连接退化输入与增强输出的大噪声区域。为此，我们引入两项核心创新：(1) 一种双目标一致性损失，在随机时间采样下结合时间一致性与真值对齐，提供全谱监督以确保稳定收敛；(2) 一种自适应噪声强调采样策略，优先训练对单步条件生成至关重要的大噪声区域。在 VE-LOL-L 数据集上，Consist-Retinex 仅用单步采样即达到当前最优性能（PSNR：25.51 对比 Diff-Retinex++ 的 23.41，FID：44.73 对比 49.59），同时训练开销仅为 1000 步 Diff-Retinex 基线的八分之一。

</details>


### [5] [HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification](https://arxiv.org/abs/2512.08983)
*Maoyu Wang,Yao Lu,Bo Zhou,Zhuangzhi Chen,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: 本文提出HSCP框架，结合谱聚类与CKA指导的层剪枝和通道剪枝，在显著压缩模型的同时提升准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统无人机识别方法在复杂环境中难以提取可靠信号特征且难以满足实时性；基于深度学习的RFFI方法虽提升准确率，但模型大、计算量高，难以部署于资源受限的边缘设备；现有剪枝方法难以同时优化压缩率、硬件加速和识别准确率。

Method: 提出HSCP（Hierarchical Spectral Clustering Pruning）框架：第一阶段利用CKA指导的谱聚类进行层剪枝，第二阶段在通道维度应用相同策略进行细粒度剪枝，并采用抗噪微调策略增强鲁棒性。

Result: 在UAV-M100数据集上，HSCP在ResNet18上实现86.39%参数减少和84.44% FLOPs减少，同时准确率比原始模型提升1.49%，并在低信噪比环境下保持优异鲁棒性。

Conclusion: HSCP有效解决了现有剪枝方法在压缩率、推理效率与准确率之间的权衡问题，为资源受限场景下的无人机射频指纹识别提供了高效可行的解决方案。

Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.

Abstract (中文翻译): 随着无人机（UAV）的快速发展以及低空安全威胁日益复杂化，传统无人机识别方法在复杂环境中难以提取可靠的信号特征，也难以满足实时性要求。近年来，基于深度学习的射频指纹识别（RFFI）方法显著提升了识别准确率，但其模型规模庞大、计算需求高，阻碍了在资源受限的边缘设备上的部署。尽管模型剪枝为降低复杂度提供了一种通用解决方案，但现有的权重、通道和层剪枝技术难以同时优化压缩率、硬件加速能力和识别准确率。为此，本文提出了HSCP（分层谱聚类剪枝）框架，将层剪枝与通道剪枝相结合，以实现极致压缩、高性能和高效推理。在第一阶段，HSCP采用以中心核对齐（CKA）为指导的谱聚类方法识别并移除冗余层；随后，在通道维度应用相同策略以消除更细粒度的冗余。为确保鲁棒性，我们进一步采用了抗噪声的微调策略。在UAV-M100基准数据集上的实验表明，HSCP优于现有的通道和层剪枝方法。具体而言，在ResNet18上，HSCP实现了86.39%的参数减少和84.44%的FLOPs减少，同时相比未剪枝基线模型，准确率提升了1.49%，即使在低信噪比环境下仍保持优越的鲁棒性。

</details>


### [6] [RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition](https://arxiv.org/abs/2512.08984)
*Nirhoshan Sivaroopan,Hansi Karunarathna,Chamara Madarasingha,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.CV

TL;DR: 提出了一种无需训练的检索增强框架 RAG-HAR，利用大语言模型（LLM）进行人类活动识别，在六个 HAR 基准上达到 SOTA 性能，且无需模型训练或微调。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在人类活动识别（HAR）中依赖特定数据集训练、大量标注数据和高计算资源，限制了其通用性和实用性。

Method: RAG-HAR 是一个无需训练的检索增强框架：首先计算轻量级统计描述符，从向量数据库中检索语义相似样本，并利用这些上下文证据通过大语言模型进行活动识别；进一步结合提示优化和 LLM 生成的活动描述符，构建富含上下文的向量数据库以提升准确性。

Result: 在六个多样化的人类活动识别基准上达到最先进的性能，且无需模型训练或微调；还能识别并合理标注多种未见过的人类活动。

Conclusion: RAG-HAR 展现出卓越的鲁棒性与实用价值，突破了传统 HAR 方法对训练和标注数据的依赖，为零样本或少样本场景下的活动识别提供了新范式。

Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.

Abstract (中文翻译): 人类活动识别（HAR）是医疗健康、康复训练、健身追踪和智能环境等应用的基础，但现有的深度学习方法通常需要针对特定数据集进行训练、依赖大量标注数据并消耗大量计算资源。我们提出了 RAG-HAR，一种无需训练的检索增强框架，利用大语言模型（LLMs）进行 HAR。RAG-HAR 首先计算轻量级统计描述符，从向量数据库中检索语义上相似的样本，并利用这些上下文证据进行基于大语言模型的活动识别。我们进一步通过提示优化以及引入基于 LLM 的活动描述符来增强 RAG-HAR，该描述符可生成富含上下文信息的向量数据库，从而提供准确且高度相关的上下文信息。借助这些机制，RAG-HAR 在六个多样化的 HAR 基准测试中实现了最先进的性能。最重要的是，RAG-HAR 在无需模型训练或微调的情况下取得了这些改进，突显了其鲁棒性和实际应用价值。RAG-HAR 能够超越已知行为，实现对多种未见过的人类活动的识别和有意义的标注。

</details>


### [7] [An Efficient Test-Time Scaling Approach for Image Generation](https://arxiv.org/abs/2512.08985)
*Vignesh Sundaresha,Akash Haridas,Vikram Appia,Lav Varshney*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Verifier-Threshold 的新方法，通过在测试时自动重新分配计算资源，显著提升了图像生成模型（如扩散和流模型）的效率，在保持 GenEval 基准性能不变的情况下，将计算时间减少了2到4倍。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在测试阶段虽可通过搜索噪声样本提升性能，但当前非均匀推理计算预算分配策略依赖贪心算法，导致计算资源分配效率低下。

Method: 提出 Verifier-Threshold 方法，自动在测试时重新分配计算资源，优化不同去噪步骤中的计算预算分配。

Result: 在 GenEval 基准上达到相同性能时，相比当前最先进方法，计算时间减少2至4倍。

Conclusion: Verifier-Threshold 方法有效解决了测试时计算资源分配低效的问题，显著提升了图像生成模型的推理效率。

Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.

Abstract (中文翻译): 图像生成已成为大型生成式AI模型的一项主流应用。正如测试时计算和推理有助于提升语言模型的能力一样，类似的优势也在图像生成模型中得到了体现。特别是，对扩散模型和流模型的噪声样本进行搜索已被证明能够很好地随测试时计算量扩展。尽管近期的研究探索了在不同去噪步骤中分配非均匀的推理计算预算，但这些方法依赖于贪心算法，导致计算预算分配效率低下。在本研究中，我们深入探讨了这一问题并提出了相应的解决方案。我们提出了Verifier-Threshold方法，该方法能够自动重新分配测试时的计算资源，从而显著提升效率。在GenEval基准测试中保持相同性能的前提下，我们的方法相比当前最先进的方法实现了2至4倍的计算时间减少。

</details>


### [8] [Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy](https://arxiv.org/abs/2512.08986)
*Anca Mihai,Adrian Groza*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.

Abstract (中文翻译): Error

</details>


### [9] [3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization](https://arxiv.org/abs/2512.08987)
*Yuze Hao,Linchao Zhu,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为3DID的新框架，通过结合连续潜在表示与物理感知优化策略，直接在三维设计空间中进行逆向设计，从而生成高保真度的3D几何结构，在解的质量和设计多样性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的3D逆向设计方法通常依赖2D投影或对已有3D形状微调，牺牲了体素细节并限制了从零开始的设计探索能力。为克服这些局限，作者旨在开发一种能直接在完整3D空间中进行高效、高质量逆向设计的方法。

Method: 提出3D Inverse Design（3DID）框架：首先学习一个统一的物理-几何嵌入，将形状与物理场数据紧凑地编码到连续潜在空间；然后采用两阶段优化策略——第一阶段使用梯度引导的扩散采样器探索全局潜在流形，第二阶段通过目标驱动且保持拓扑结构的精细化调整候选设计。

Result: 3DID能够生成高保真度的3D几何结构，在解的质量和设计多样性方面均优于现有方法。

Conclusion: 所提出的3DID框架有效解决了现有3D逆向设计方法在体积细节和设计自由度方面的不足，实现了真正意义上的从零开始的3D逆向设计。

Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.

Abstract (中文翻译): 逆向设计旨在优化物理系统的输入变量以实现特定目标函数，通常被表述为搜索或优化问题。然而，在三维域中，设计空间呈指数级增长，使得基于网格的穷举搜索变得不可行。近年来，深度学习的发展通过提供强大的生成先验和可微代理模型，加速了逆向设计进程。然而，当前方法往往通过二维投影近似三维设计空间，或对现有三维形状进行微调。这些方法牺牲了体积细节，并限制了设计探索，无法实现真正从零开始的三维设计。本文提出了一种三维逆向设计（3DID）框架，通过将连续潜在表示与物理感知优化策略相结合，直接在三维设计空间中进行导航。我们首先学习一个统一的物理-几何嵌入，将形状和物理场数据紧凑地捕捉到连续潜在空间中。随后，我们引入一种两阶段策略进行物理感知优化：第一阶段，梯度引导的扩散采样器探索全局潜在流形；第二阶段，目标驱动且保持拓扑结构的精细化过程进一步雕琢每个候选方案以逼近目标。这使得3DID能够生成高保真度的三维几何结构，在解的质量和设计多样性方面均优于现有方法。

</details>


### [10] [Enhancing Knowledge Transfer in Hyperspectral Image Classification via Cross-scene Knowledge Integration](https://arxiv.org/abs/2512.08989)
*Lu Huo,Wenjian Huang,Jianguo Zhang,Min Xu,Haimin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为跨场景知识整合（CKI）的新框架，用于解决高光谱图像分类中因传感器差异和场景语义不一致导致的跨域知识迁移难题，尤其在源域与目标域标签空间无重叠的情况下仍能有效利用目标私有知识，显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱图像跨域迁移方法受限于同质域假设或仅包含共现类别的异质场景设定，在标签空间不重叠时依赖源域对所有类别全覆盖，忽略了目标域私有信息，难以应对真实复杂的跨场景迁移任务。

Method: 所提CKI框架包含三个核心组件：(1) 光谱特征对齐（ASC），通过领域无关投影减少光谱差异；(2) 跨场景知识共享偏好（CKSP），借助源相似性机制（SSM）缓解语义错配；(3) 互补信息整合（CII），最大化利用目标域特有的互补线索。

Result: 大量实验表明，CKI在多种跨场景高光谱图像分类任务中均取得当前最优性能，并展现出优异的稳定性。

Conclusion: CKI通过显式建模目标私有知识，有效解决了完全异质场景下的高光谱图像跨域知识迁移问题，为实际应用提供了更鲁棒和通用的解决方案。

Abstract: Knowledge transfer has strong potential to improve hyperspectral image (HSI) classification, yet two inherent challenges fundamentally restrict effective cross-domain transfer: spectral variations caused by different sensors and semantic inconsistencies across heterogeneous scenes. Existing methods are limited by transfer settings that assume homogeneous domains or heterogeneous scenarios with only co-occurring categories. When label spaces do not overlap, they further rely on complete source-domain coverage and therefore overlook critical target-private information. To overcome these limitations and enable knowledge transfer in fully heterogeneous settings, we propose Cross-scene Knowledge Integration (CKI), a framework that explicitly incorporates target-private knowledge during transfer. CKI includes: (1) Alignment of Spectral Characteristics (ASC) to reduce spectral discrepancies through domain-agnostic projection; (2) Cross-scene Knowledge Sharing Preference (CKSP), which resolves semantic mismatch via a Source Similarity Mechanism (SSM); and (3) Complementary Information Integration (CII) to maximize the use of target-specific complementary cues. Extensive experiments verify that CKI achieves state-of-the-art performance with strong stability across diverse cross-scene HSI scenarios.

Abstract (中文翻译): 知识迁移在提升高光谱图像（HSI）分类性能方面具有巨大潜力，但两个固有挑战从根本上限制了有效的跨域迁移：由不同传感器引起的光谱差异以及异质场景间的语义不一致性。现有方法受限于迁移设定，这些设定要么假设域是同质的，要么仅适用于具有共现类别的异质场景。当标签空间不重叠时，这些方法进一步依赖于源域对所有类别的完整覆盖，因而忽略了关键的目标域私有信息。为克服这些局限，并实现在完全异质设定下的知识迁移，我们提出了跨场景知识整合（CKI）框架，该框架在迁移过程中显式地融合了目标域私有知识。CKI包括：(1) 光谱特征对齐（ASC），通过领域无关投影减少光谱差异；(2) 跨场景知识共享偏好（CKSP），通过源相似性机制（SSM）解决语义错配问题；以及(3) 互补信息整合（CII），以最大化利用目标域特有的互补线索。大量实验验证了CKI在多种跨场景高光谱图像场景中均取得了最先进的性能，并具有很强的稳定性。

</details>
