<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 本文指出当前多模态大语言模型（MLLMs）在理解真实STEM手写解答方面存在严重但被忽视的识别错误，导致其在自动评分等高风险教育应用中可靠性不足；为此作者发布了包含1300+份真实学生手写解答的数据集EDU-CIRCUIT-HW，并通过同时评估上游识别准确性和下游评分性能揭示了这一问题，最后展示了仅需约4%人工干预即可显著提升AI评分系统鲁棒性的纠错方案。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs缺乏针对真实、无约束STEM手写解答（包含公式、图表和文本推理）的领域专用评测基准，且当前评估范式仅关注下游任务结果（如自动评分），无法全面衡量模型对复杂手写逻辑的整体理解能力，从而掩盖了其在高风险教育场景中的可靠性隐患。

Method: 发布新数据集EDU-CIRCUIT-HW（含1300+份大学STEM课程真实学生手写解答及其专家验证的逐字转录与评分报告），并基于此同时评估多种MLLMs的上游识别保真度和下游自动评分性能；进一步通过案例研究，利用识别出的错误模式进行预判性检测与修正，仅引入约4%的人工干预以提升系统鲁棒性。

Result: 评估揭示了MLLMs在识别学生手写内容时存在大规模潜在失败，表明其在高风险教育应用中的可靠性不足；而结合少量人工干预的纠错策略可显著提升AI评分系统在未见学生解答上的鲁棒性。

Conclusion: MLLMs目前尚不足以可靠地用于高风险教育场景中的自动评分等理解型任务，亟需更全面的评测基准与针对性的纠错机制；本研究提供的数据集、评估框架及轻量级干预方案为提升教育AI系统的可靠性提供了重要路径。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

Abstract (中文翻译): 多模态大语言模型（MLLMs）在革新传统教育和减轻教师负担方面具有巨大潜力。然而，由于缺乏真实且领域特定的基准数据集，准确解读包含交织的数学公式、图表和文本推理的非约束性STEM学生手写解答仍是一项重大挑战。此外，当前的评估范式主要依赖下游任务（如自动评分）的结果，而这些任务通常仅考察所识别内容的一个子集，因而无法全面捕捉MLLMs对复杂手写逻辑的整体理解能力。为弥合这一差距，我们发布了EDU-CIRCUIT-HW数据集，其中包含1300多份来自大学STEM课程的真实学生手写解答。利用专家验证的学生解答逐字转录文本和评分报告，我们同时评估了多种MLLMs的上游识别保真度和下游自动评分性能。我们的评估揭示了MLLMs在识别学生手写内容时存在惊人的潜在失败规模，凸显了这些模型在高风险教育场景中用于自动评分及其他理解导向型应用时的可靠性不足。作为解决方案，我们展示了一个案例研究，表明利用已识别的错误模式进行前瞻性检测与修正，仅需极少的人工干预（约占全部解答的4%），即可显著提升部署的AI赋能评分系统在未见学生解答上的鲁棒性。

</details>


### [2] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: 本文提出Simulate Anything框架，仅使用多视角环境视频和现成资产，通过3D高斯泼溅重建真实场景并结合生成模型构建物理逼真的可编辑仿真环境，用于高效生成高质量具身智能训练数据，在零样本任务中媲美甚至超越真实数据训练效果。


<details>
  <summary>Details</summary>
Motivation: 具身智能的可扩展性受限于真实世界交互数据的稀缺；现有仿真方法存在与真实环境在视觉和物理上的显著差距，并依赖昂贵传感器、精确机器人校准或深度测量，难以大规模应用。

Method: 提出一个以图形驱动的世界建模与仿真框架：首先利用3D高斯泼溅（3DGS）从多视角视频中重建出具有精细几何与外观的逼真场景；然后借助生成模型恢复物理上合理的表示，并通过精密校准目标将其集成到仿真环境中，实现重建场景与真实世界之间的准确尺度对齐。

Result: 基于该框架生成的仿真数据训练的视觉-语言-动作（VLA）模型在下游任务中表现出强大的零样本性能，其效果媲美甚至优于使用真实世界数据训练的模型。

Conclusion: 该工作表明，基于重建驱动的世界建模方法在可扩展、实用的具身智能训练方面具有巨大潜力。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

Abstract (中文翻译): 具身智能的可扩展性从根本上受到现实世界交互数据稀缺的限制。虽然仿真平台提供了一种有前景的替代方案，但现有方法通常在视觉和物理层面与真实环境存在显著差距，并且依赖昂贵的传感器、精确的机器人校准或深度测量，限制了其在大规模应用中的实用性。我们提出了“任意仿真”（Simulate Anything）——一种以图形驱动的世界建模与仿真框架，仅需多视角环境视频和现成资产即可高效生成高保真的具身训练数据。我们的方法利用3D高斯泼溅（3DGS）将真实世界环境重建为逼真的场景表示，从视频中无缝捕捉精细的几何结构与外观。随后，我们利用生成模型恢复出物理上逼真的表示，并通过一个精密校准目标将其集成到仿真环境中，从而实现重建场景与真实世界之间的精确尺度对齐。这些组件共同构成了一个统一、可编辑且物理可信的世界模型。在该仿真数据上训练的视觉-语言-动作（VLA）模型在下游任务中展现出强大的零样本性能，其效果媲美甚至超越使用真实世界数据训练所得的结果，凸显了基于重建驱动的世界建模在可扩展且实用的具身智能训练方面的潜力。

</details>


### [3] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出R3G框架，通过推理-检索-重排序三阶段提升视觉问答中图像检索与利用效果，在MRAG-Bench上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答系统在需要外部图像补充缺失视觉线索时，难以有效选择并整合相关图像到推理过程中。

Method: 提出R3G模块化框架：先生成简要推理计划以明确所需视觉线索，再采用两阶段策略（粗检索+细粒度重排序）选取证据图像。

Result: 在MRAG-Bench上，R3G在六个MLLM主干模型和九个子场景中均提升准确率，实现整体SOTA性能；消融实验证明充分性感知重排序与推理步骤互补。

Conclusion: R3G有效解决了视觉问答中图像检索与整合难题，通过结构化推理与重排序机制显著提升模型性能。

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

Abstract (中文翻译): 面向视觉的检索式VQA需要检索图像以提供缺失的视觉线索，并将其整合到推理过程中。然而，如何选择合适的图像并有效地将其融入模型推理仍具挑战性。为应对这一挑战，我们提出了R3G——一个模块化的“推理-检索-重排序”框架。该框架首先生成一个简要的推理计划，明确所需视觉线索，然后采用两阶段策略（先粗检索，再细粒度重排序）来选择证据图像。在MRAG-Bench上，R3G在六个多模态大语言模型（MLLM）主干和九个子场景中均提升了准确率，取得了当前最优的整体性能。消融实验表明，基于充分性感知的重排序与推理步骤具有互补性，有助于模型既选对图像又用好图像。代码与数据已开源：https://github.com/czh24/R3G。

</details>


### [4] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

TL;DR: 本文提出了HYPE-EDIT-1，一个包含100个参考图像编辑任务的基准，用于评估图像编辑模型在真实工作流中的成功率与有效成本，发现低价模型因重试和人工审核成本反而更贵。


<details>
  <summary>Details</summary>
Motivation: 当前公开演示的图像编辑模型通常展示的是最佳案例，无法反映真实工作流中因失败重试和人工审核带来的额外成本。因此需要一个更贴近实际应用场景的评估基准。

Method: 构建包含100个参考式营销/设计编辑任务的基准HYPE-EDIT-1，对每个任务生成10次独立输出，通过二元（通过/失败）判断计算单次尝试通过率、pass@10、带重试上限的期望尝试次数，以及结合模型调用价格和人工审核时间的有效成功成本。其中50个任务公开，50个任务作为私有测试集用于服务器端评估，并提供标准化的JSON格式和VLM/人工评判工具。

Result: 在评估的模型中，单次尝试通过率为34%–83%，每次成功编辑的有效成本为0.66–1.42美元。部分单图价格较低的模型，因重试和人工审核成本高，总体有效成本反而更高。

Conclusion: 仅看单图生成价格会误导模型选择；真实工作流中应综合考虑重试率和人工审核成本。HYPE-EDIT-1为图像编辑模型提供了更贴近实际应用的评估方式。

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

Abstract (中文翻译): 图像编辑模型的公开演示通常是最佳案例；而真实工作流程则需承担重试和审核时间的成本。我们提出了 HYPE-EDIT-1，一个包含100项基于参考图像的营销/设计编辑任务的基准测试，采用二元（通过/失败）评判标准。针对每项任务，我们生成10次独立输出，以估算单次尝试通过率、pass@10、在重试上限下的期望尝试次数，以及结合模型调用价格与人工审核时间的每次成功编辑的有效成本。我们公开发布其中50项任务，并保留另外50项作为私有测试集用于服务器端评估，同时提供标准化的JSON格式及支持视觉语言模型（VLM）和人工评判的工具链。在所评估的模型中，单次尝试通过率介于34%至83%之间，每次成功编辑的有效成本为0.66至1.42美元。考虑到重试和人工审核的总成本，那些单图定价较低的模型实际上反而更昂贵。

</details>


### [5] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

TL;DR: 提出了一种融合激光雷达与毫米波雷达信息的多模态无人机轨迹预测方法，在CVPR 2024 MMAUD数据集上验证，相比基线模型提升40%预测精度。


<details>
  <summary>Details</summary>
Motivation: 低空经济中对非法无人机的管理需求迫切，而单一传感器难以兼顾空间几何结构与动态反射特性，因此需要融合多模态感知信息以提升轨迹预测性能。

Method: 设计了名为“多模态深度融合框架”的网络，包含两个模态专用的特征提取编码器和一个双向交叉注意力融合模块，实现激光雷达与毫米波雷达点云在特征层面的互补与语义对齐。

Result: 在MMAUD数据集上的实验表明，所提方法相比基线模型轨迹预测精度提升40%；消融实验验证了不同损失函数和后处理策略的有效性。

Conclusion: 该模型能有效利用多模态数据，为低空经济中非法无人机的轨迹预测提供高效解决方案。

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

Abstract (中文翻译): 为满足低空经济中对非法无人机管理的需求，本文提出了一种基于激光雷达与毫米波雷达信息融合的多模态无人机轨迹预测方法。设计了一个名为“多模态深度融合框架”的深度融合网络，其整体架构包括两个模态专用的特征提取网络和一个双向交叉注意力融合模块，旨在充分利用激光雷达与雷达点云在空间几何结构和动态反射特性方面的互补信息。在特征提取阶段，模型对激光雷达和雷达分别采用结构相同但独立的特征编码器；特征提取后，通过双向交叉注意力机制实现两种模态间的信息互补与语义对齐。为验证所提模型的有效性，采用CVPR 2024 UG2+无人机跟踪与姿态估计挑战赛所用的MMAUD数据集作为训练与测试数据集。实验结果表明，所提出的多模态融合模型显著提升了轨迹预测精度，相较基线模型提升了40%。此外，还进行了消融实验，验证了不同损失函数和后处理策略在提升模型性能方面的有效性。所提模型能够有效利用多模态数据，为低空经济中非法无人机的轨迹预测提供了高效解决方案。

</details>


### [6] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE 是一个用于训练和评估视觉语言模型在具有空间约束的计数任务上的新数据集，能提升模型在分布外图像上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有计数任务数据集要么过于简单（如 VLMCountBench），要么缺乏对遮挡和空间构成的控制（如 TallyQA），难以有效训练和评估具备空间推理能力的视觉语言模型。

Method: 构建 SITUATE 数据集，包含受控的空间约束和遮挡条件下的计数任务，并在 Qwen VL 2.5 7B 模型上进行微调实验，同时与 Pixmo Count 及其他基准进行交叉验证。

Result: 在 SITUATE 上微调的模型在 Pixmo Count 测试集上表现更好，但反过来不成立；且在其他计数基准上也显示出更强的泛化能力。

Conclusion: SITUATE 数据集有效填补了简单合成数据与复杂真实数据之间的空白，有助于提升视觉语言模型在空间约束计数任务中的泛化性能。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

Abstract (中文翻译): 我们提出了 SITUATE，这是一个用于训练和评估视觉语言模型在具有空间约束的计数任务上的新型数据集。该数据集弥合了像 VLMCountBench 这类简单二维数据集与像 TallyQA 这类通常存在歧义且缺乏对遮挡和空间构成控制的真实数据集之间的差距。实验表明，我们的数据集有助于提升模型在分布外图像上的泛化能力：在 SITUATE 上微调的 Qwen VL 2.5 7B 模型在 Pixmo Count 测试数据上的准确率有所提升，但反之则不然。我们通过在其他已建立的计数基准上比较模型性能，并与从 Pixmo Count 中提取的同等规模微调集进行对比，进一步验证了这一结论。

</details>


### [7] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

TL;DR: 低光照和自动图像采集会显著降低商业PAD系统的性能，仅有一个系统在所有测试场景中保持稳健。


<details>
  <summary>Details</summary>
Motivation: 现有远程身份验证（RIV）系统中的呈现攻击检测（PAD）子系统在不同环境和流程条件下（如低光照、自动拍摄）的鲁棒性不足，亟需评估其实际部署中的可靠性。

Method: 通过构建贴近真实RIV应用场景的测试场景，评估多个商用PAD系统在低光照和自动图像采集条件下的性能表现，并分析错误率变化。

Result: 在低光照条件下，PAD系统的错误率预计增加约4倍；在自动采集模式下，错误率翻倍。所测系统中仅有一个在所有扰动下保持最大真实呈现分类错误率低于3%。

Conclusion: 为确保PAD系统在现实应用中的可靠性和鲁棒性，必须在多样化的环境和操作条件下进行全面测试。

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

Abstract (中文翻译): 呈现攻击检测（PAD）子系统是高效且用户友好的远程身份验证（RIV）系统的重要组成部分。然而，在各种环境和操作条件下确保其鲁棒性能仍是一个关键挑战。本文通过一个贴近真实RIV应用场景的测试，研究了低光照条件和自动图像采集对商用PAD系统鲁棒性的影响。结果表明，当应用于低光照或自动采集场景时，PAD系统的性能显著下降：在低光照条件下，模型预测的错误率增加约四倍；在自动采集工作流中，错误率则翻倍。具体而言，在所测试的系统中，仅有一个系统对这些扰动具有鲁棒性，在所有场景中其最大真实呈现分类错误率均低于3%。本研究强调了在多样化环境中进行测试的重要性，以确保PAD系统在现实应用中具备可靠且鲁棒的性能。

</details>


### [8] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

TL;DR: 本文提出了一种结合地理空间辅助信息的新型视觉Transformer模型，通过地理空间嵌入机制和引导注意力模块，显著提升了遥感图像在多模态地理空间理解任务（如疾病流行率预测）中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言和多模态模型主要优化视觉与文本内容的语义对齐，缺乏对结构化地理空间信息的有效表示和推理能力，难以满足遥感图像分析中对地理空间理解的需求。

Method: 提出一种地理空间嵌入机制，将多样化的地理空间数据转换为与图像块空间对齐的嵌入块；设计引导注意力模块，基于辅助数据相关性动态计算注意力权重，并为不同注意力头分配不同角色以捕捉互补信息。

Result: 所提框架在疾病流行率预测任务上优于现有的预训练地理空间基础模型，验证了其在多模态地理空间理解方面的有效性。

Conclusion: 通过引入地理空间嵌入和引导注意力机制，该模型有效提升了遥感图像处理中对结构化地理空间信息的理解与利用能力，为多模态地理空间分析提供了新思路。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

Abstract (中文翻译): 视觉Transformer在遥感图像分析（尤其是目标检测与分割）中取得了重大进展。近期的视觉-语言及多模态模型通过引入字幕、问答对和元数据等辅助信息，进一步拓展了其能力，使应用范围超越了传统计算机视觉任务。然而，这些模型通常针对视觉与文本内容之间的语义对齐进行优化，而非地理空间理解，因此不适合表示或推理结构化的地理空间图层。本研究提出了一种新模型，利用辅助地理空间信息指导遥感图像处理。我们的方法引入了一种地理空间嵌入机制，将多样化的地理空间数据转换为与图像块空间对齐的嵌入块。为促进跨模态交互，我们设计了一个引导注意力模块，通过计算与辅助数据相关性来动态生成注意力权重，从而引导模型关注最相关的区域。此外，该模块为各个注意力头分配了不同的角色，使模型能够捕捉辅助信息的互补方面，并提升预测结果的可解释性。实验结果表明，所提出的框架在疾病流行率预测任务上优于现有的预训练地理空间基础模型，凸显了其在多模态地理空间理解方面的有效性。

</details>


### [9] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

TL;DR: 在商业奶牛场中，8–10 m²/犊牛的空间配额最有利于促进玩耍行为（福利指标），且通过计算机视觉可实现自动化监测。


<details>
  <summary>Details</summary>
Motivation: 玩耍行为是奶牛犊良好福利的积极指标，但在商业条件下，尤其是在中高空间配额（6–20 m²/犊牛）下，空间对玩耍行为的影响尚不明确。此外，缺乏可扩展的自动监测方法限制了大规模应用。

Method: 研究在荷兰14个商业农场对60头群养奶牛犊进行视频观察，空间范围为2.66–17.98 m²/犊牛。使用详细行为谱记录玩耍行为（占观察时段的百分比，%OP），并采用线性混合模型（以农场为随机效应）分析空间与玩耍的关系。同时开发了一个基于108小时人工标注数据（来自6个农场）训练的计算机视觉流水线，并在独立测试集上验证其性能。

Result: 犊牛平均花费1.0%的观察时间（约17小时中的10分钟）玩耍。玩耍行为与空间呈非线性关系：在8–10 m²/犊牛时达到峰值（1.6% OP），而在6–8 m²和12–14 m²时最低（<0.6% OP）。该关系在控制年龄、健康状况和群体规模后仍显著。计算机视觉模型在活跃玩耍检测上达到97.6%准确率和99.4%召回率。

Conclusion: 8–10 m²/犊牛是兼顾动物福利与经济可行性的实用空间目标；自动化计算机视觉系统能将小规模人工标注扩展为持续的福利评估工具。

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

Abstract (中文翻译): 玩耍行为是奶牛犊良好福利的积极指标，但在商业养殖条件下，尤其是在中高空间配额（6–20 平方米/犊牛）范围内，空间配额对玩耍行为的影响尚未充分阐明。本研究调查了荷兰14个商业农场中60头群养奶牛犊的空间配额（2.66–17.98 平方米/犊牛）与玩耍行为之间的关系，并开发了一套用于可扩展监测的自动化计算机视觉流程。研究人员使用详细的行为谱对视频观察结果进行分析，将玩耍行为表示为观察时段的百分比（%OP）。统计分析采用以农场为随机效应的线性混合模型。计算机视觉流程基于来自6个农场共108小时的人工标注数据进行训练，并在独立测试数据上进行验证。该计算机视觉分类器在活跃玩耍行为检测上达到了97.6%的准确率和99.4%的召回率。犊牛平均在观察时段中花费1.0%的时间玩耍，相当于每17小时约10分钟。空间与玩耍行为之间呈非线性关系：在8–10 平方米/犊牛时玩耍水平最高（1.6% OP），而在6–8 平方米和12–14 平方米时最低（<0.6% OP）。在控制年龄、健康状况和群体规模后，空间效应仍然显著。总之，这些发现表明，8–10 平方米/犊牛是一个兼顾动物福利效益与经济可行性的实用目标，同时证明自动化监测能够将小规模人工标注项目扩展为持续的动物福利评估系统。

</details>


### [10] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

TL;DR: 本文提出一个结合多视角摄影测量、3D重建和深度学习的AI平台，用于客观、可重复地评估烧伤，支持治疗决策与愈合追踪。


<details>
  <summary>Details</summary>
Motivation: 传统视觉检查和2D摄影在烧伤评估中主观性强、难以进行纵向比较，缺乏客观、可重复的量化手段。

Method: 整合多视角摄影测量、3D表面重建和基于深度学习的分割技术，利用普通相机拍摄的多角度图像构建患者专属3D烧伤模型，并映射至解剖结构以计算客观指标（如表面积、TBSA、深度相关几何代理和体积变化）；通过空间对齐实现多次重建间的纵向对比。

Result: 系统能稳定重建3D模型，一致计算临床指标，并展现出符合临床预期的愈合趋势，支持非侵入式、可扩展的烧伤评估。

Conclusion: 该平台为急性和门诊烧伤护理提供了一种客观、几何感知的评估与决策支持工具，具有良好的临床应用前景。

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

Abstract (中文翻译): 准确且可重复的烧伤评估对于治疗规划、愈合监测和法医记录至关重要，然而传统的目视检查和二维摄影具有主观性，且难以进行纵向比较。本文提出了一种由人工智能驱动的烧伤评估与管理平台，该平台在一个结构化的临床工作流程中整合了多视角摄影测量、三维表面重建以及基于深度学习的分割技术。系统利用消费级相机拍摄的标准多角度图像，重建患者特定的三维烧伤表面，并将烧伤区域映射到解剖结构上，以计算真实世界单位下的客观指标，包括表面积、总体表面积百分比（TBSA）、与深度相关的几何代理指标以及体积变化。通过空间对齐连续的三维重建结果，可量化随时间推移的愈合进展，从而客观追踪创面收缩和深度减少情况。该平台还支持结构化患者信息录入、引导式图像采集、三维分析与可视化、治疗建议以及自动化报告生成。基于模拟的评估表明，该系统能够实现稳定的三维重建、一致的指标计算以及临床上合理的纵向变化趋势，为急性期和门诊烧伤护理提供了一种可扩展、非侵入式的客观、几何感知型烧伤评估与决策支持方法。

</details>
