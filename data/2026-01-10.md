<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段对齐框架，通过构建分层细粒度评价标准并引入复杂偏好优化（CPO）方法，显著提升了扩散模型在绘画生成任务中与人类专家知识的对齐程度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型的后训练对齐依赖于简化的信号（如标量奖励或二元偏好），难以捕捉人类专家所具备的层次化、细粒度的复杂判断。

Method: 首先与领域专家共同构建一个树状结构的分层细粒度图像质量评价标准；然后提出两阶段对齐框架：第一阶段通过监督微调将领域知识注入辅助扩散模型，第二阶段提出复杂偏好优化（CPO）方法，将DPO扩展至非二元、分层的评价标准，同时最大化正向属性概率并最小化负向属性概率。

Result: 在绘画生成领域的实验表明，CPO显著提升了生成质量和与专家知识的对齐效果。

Conclusion: 该方法为扩散模型与细粒度、层次化人类专业知识的对齐提供了新途径。

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

Abstract (中文翻译): 扩散模型的后训练对齐依赖于简化的信号，例如标量奖励或二元偏好，这限制了其与复杂人类专业知识（具有层次性和细粒度）的对齐能力。为解决此问题，我们首先与领域专家共同构建了一套层次化、细粒度的评估标准，将图像质量分解为多个以树状结构组织的正向和负向属性。在此基础上，我们提出了一个两阶段对齐框架：首先，通过监督微调（Supervised Fine-Tuning）将领域知识注入到一个辅助扩散模型中；其次，我们提出了复杂偏好优化（Complex Preference Optimization, CPO）方法，将DPO扩展至适用于我们的非二元、层次化标准。具体而言，我们将对齐问题重新形式化为利用辅助扩散模型同时最大化正向属性概率并最小化负向属性概率。我们在绘画生成领域实例化了该方法，并基于所构建的标准，使用带有细粒度属性标注的绘画数据集进行了CPO训练。大量实验表明，CPO显著提升了生成质量和与专家知识的对齐程度，为细粒度标准对齐开辟了新途径。

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: 本文提出一种基于RGB空间中五进制像素强度组合的新文本嵌入方法，可在单个像素内完整编码一个字符，避免了传统LSB/MSB、变换域或深度学习方法中的多像素操作或高计算开销，且图像失真极小。


<details>
  <summary>Details</summary>
Motivation: 现有文本隐写方法（如LSB、MSB、PVD、变换域、量化、边缘区域方法及深度学习等）普遍存在依赖多像素强度翻转、引入噪声、计算复杂度高或需多步处理等问题，亟需一种高效、低失真且计算轻量的替代方案。

Method: 在RGB空间中，对每个R、G、B通道引入五种受控的像素强度变化，形成最多125种不同的三通道组合，并将这些组合映射到字母、数字、空格和常用特殊字符；通过单个RGB像素即可完整嵌入一个文本符号。

Result: 通过MSE、MAE、SNR、PSNR、SSIM、直方图比较和热力图分析等多种指标评估，嵌入前后图像无明显失真；相比传统方法，该方法在单像素内完成符号编码，提升了嵌入效率并降低了计算开销。

Conclusion: 所提五进制像素强度组合方法能高效、低失真地将文本嵌入图像，克服了现有方法在计算复杂度、噪声引入和多像素依赖等方面的局限性，具有良好的实用潜力。

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

Abstract (中文翻译): 本文提出了一种利用RGB空间中五进制像素强度组合将文本数据嵌入图像的新技术。现有方法主要依赖最低有效位（LSB）和最高有效位（MSB）操作、像素值差分（PVD）、RGB通道的空间扰动、变换域方法、量化方法、基于边缘和区域的方法，以及近期兴起的深度学习和生成式AI技术在图像空间域中隐藏文本信息。其中大多数方法依赖于多个像素的像素强度翻转（如LSB及其组合方法）或变换系数，常导致噪声形式的失真。此外，现有方法的编解码过程多为确定性，而在深度学习和生成式AI等高级模型中则计算开销较大。所提出的方法在RGB空间中采用五进制像素强度组合，即在R、G、B每个通道中引入五种受控的不同像素强度变化，从而形成最多125种不同的像素强度组合。这些组合被映射到文本符号，能够表示大小写字母、数字、空格及常用特殊字符。通过MSE、MAE、SNR、PSNR、SSIM、直方图比较和热力图分析等多种指标对原始图像与嵌入后图像进行评估，结果显示图像未产生显著失真。此外，该方法通过单个RGB像素即可完整编码一个文本符号，相较于通常需要多个像素或多步处理的LSB/MSB方法，以及计算开销更高的变换域和学习型方法，实现了更高的嵌入效率。

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文提出通过后训练实现文本与图像的完全统一生成，使模型在单次推理中自主从文本推理过渡到图像合成，提升跨模态耦合能力，并在多个T2I基准上取得改进。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态生成架构通常依赖显式的模态切换，在生成推理文本后再手动切换至图像生成，这种分离式、顺序式的推理过程限制了跨模态耦合，无法实现自动的多模态生成。

Method: 采用离线、奖励加权的后训练方法，利用完全自生成的合成数据，探索联合文本-图像生成对T2I性能的影响、各模态在后训练中的重要性，以及不同后训练数据策略的效果。

Result: 使用针对特定局限性设计的后训练数据集，相比通用图文语料或基准对齐数据效果更优；在四个多样化的T2I基准上实现了多模态图像生成性能的提升。

Conclusion: 奖励加权的双模态后训练结合策略性设计的数据能有效提升统一多模态生成模型的性能，实现更紧密的跨模态协同与自动多模态输出。

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

Abstract (中文翻译): 最近，能够联合生成文本和图像的统一多模态生成架构已成为文本到图像（T2I）合成的一个有前景的方向。然而，许多现有系统依赖于显式的模态切换机制，即先生成推理文本，再手动切换到图像生成阶段。这种分离的、顺序式的推理过程限制了跨模态之间的耦合，并阻碍了自动多模态生成的实现。本研究探索通过后训练实现完全统一的文本-图像生成，使模型能够在单次推理过程中自主地从文本推理过渡到视觉合成。我们考察了联合文本-图像生成对T2I性能的影响，以及在后训练过程中各模态的相对重要性。此外，我们还探索了不同的后训练数据策略，结果表明，针对特定局限性设计的定向数据集相较于广泛的图像-标题语料库或与基准对齐的数据能取得更优的效果。通过采用离线、奖励加权的后训练方法，并结合完全自生成的合成数据，我们的方法在四个多样化的T2I基准上均提升了多模态图像生成性能，验证了对两种模态进行奖励加权以及策略性设计后训练数据的有效性。

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: ReHyAt 是一种混合注意力机制，结合了 softmax 注意力的高质量与线性注意力的高效性，实现常数内存占用和线性复杂度，支持从现有模型高效蒸馏，在显著降低训练成本的同时保持 SOTA 视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于 Transformer 的视频扩散模型虽性能领先，但其二次方复杂度的注意力机制严重限制了长序列视频生成的可扩展性。

Method: 提出 ReHyAt（Recurrent Hybrid Attention）机制，融合 softmax 注意力与线性注意力，支持分块递归计算和常量内存使用，并设计轻量级蒸馏与微调流程，可高效迁移现有 softmax 模型的知识。

Result: 在 VBench 和 VBench-2.0 基准及人类偏好研究中，ReHyAt 在将注意力复杂度从二次降至线性的同时，实现了 SOTA 视频生成质量，训练成本降至约 160 GPU 小时。

Conclusion: ReHyAt 有效解决了视频扩散模型中的可扩展性问题，为未来双向 softmax 模型提供了一种高效、实用的部署路径，适用于长时长及端侧视频生成任务。

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

Abstract (中文翻译): 近期视频扩散模型的研究趋势转向基于 Transformer 的架构，在视频生成方面达到了最先进的水平，但其注意力机制具有二次方复杂度，严重限制了对更长序列的可扩展性。我们提出了 ReHyAt（Recurrent Hybrid Attention，递归混合注意力）机制，该机制结合了 softmax 注意力的高保真度与线性注意力的高效性，支持分块递归重构并实现恒定内存占用。与同期仅使用线性注意力的 SANA Video 不同，ReHyAt 的混合设计能够高效地从现有的基于 softmax 的模型中进行知识蒸馏，将训练成本降低两个数量级至约 160 GPU 小时，同时在生成质量上具有竞争力。我们提出的轻量级蒸馏与微调流程，可作为通用方案应用于未来的先进双向 softmax 模型。在 VBench 和 VBench-2.0 基准测试以及人类偏好研究中的实验表明，ReHyAt 在将注意力计算复杂度从二次方降至线性的同时，实现了最先进的视频生成质量，为长时长视频生成和端侧部署提供了实用的可扩展性。项目页面见 https://qualcomm-ai-research.github.io/rehyat。

</details>


### [5] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: 本文提出了一种基于残差向量量化的新型渐进式3D高斯泼溅压缩方法，利用多分辨率哈希网格引导的自回归熵模型，显著提升压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅模型在中大型场景中存储开销大，难以部署于云端或流媒体服务；当前基于标量量化的渐进压缩方法未能充分利用高维特征向量间的相关性，限制了率失真性能。

Method: 采用残差向量量化（RVQ）替代传统标量量化，并引入由多分辨率哈希网格引导的自回归熵模型，以准确预测逐层传输索引的条件概率，实现粗略层与细化层的高效压缩。

Result: 所提方法能更有效地压缩3D高斯泼溅的图元特征，在保持高质量视图合成的同时显著降低比特率。

Conclusion: 通过引入残差向量量化和自回归熵建模，该工作在3D高斯泼溅的渐进压缩中实现了优于现有方法的率失真性能，为实际部署提供了可行方案。

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

Abstract (中文翻译): 近期在3D高斯泼溅方面的进展已实现实时、高保真的新视角合成。然而，这些模型在中大型场景中具有较高的存储需求，阻碍了其在云服务和流媒体中的部署。一些最新的渐进压缩技术依赖于渐进掩码和标量量化方法，利用空间上下文模型来降低高斯属性的比特率。尽管有效，但标量量化可能无法最优地捕捉高维特征向量之间的相关性，从而可能限制率失真性能。  
在本研究中，我们提出了一种新颖的3D高斯泼溅渐进式编解码器，用更强大的残差向量量化方法替代传统方法来压缩图元特征。我们的核心贡献是一种由多分辨率哈希网格引导的自回归熵模型，能够准确预测每个连续传输索引的条件概率，从而高效压缩粗略层和细化层。

</details>


### [6] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: 在五个孟加拉国图像数据集上的实验表明，使用微调的迁移学习（尤其是ResNet-18）显著优于从头训练的自定义CNN和特征提取方法，在准确率上提升3%–76%，其中ResNet-18在Road Damage BD数据集上达到100%准确率。尽管自定义CNN参数更少、训练更快，但迁移学习在复杂或小样本任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 在资源受限或特定领域（如孟加拉国本地应用场景）下，如何在自定义CNN与预训练模型之间做出合理选择，尚缺乏系统性比较研究。本文旨在通过实证分析为实践者提供选型依据。

Method: 对五个来自孟加拉国的图像分类数据集（Footpath Vision、Auto Rickshaw Detection、Mango Image Classification、Paddy Variety Recognition、Road Damage Detection），分别采用自定义CNN、基于ResNet-18和VGG-16的特征提取以及微调迁移学习三种方法进行训练与评估，并对比其准确率、模型大小和训练效率。

Result: 微调迁移学习在所有数据集上均优于其他方法，准确率提升3%–76%；ResNet-18微调在Road Damage BD数据集上达到100%准确率。自定义CNN参数量仅为3.4M，远小于预训练模型（11M–134M），在简单任务上训练更快。

Conclusion: 对于复杂或训练样本有限的任务，推荐使用微调的预训练模型（如ResNet-18）；若计算资源受限且任务较简单，可考虑轻量级自定义CNN。本研究为实际应用中的模型选型提供了实用指导。

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

Abstract (中文翻译): 本研究对自定义卷积神经网络（CNN）与流行的预训练架构（ResNet-18 和 VGG-16）进行了全面的对比分析，分别采用特征提取和迁移学习两种方法。我们在五个来自孟加拉国的多样化图像分类数据集上评估了这些模型，包括人行道视觉（Footpath Vision）、三轮车检测（Auto Rickshaw Detection）、芒果图像分类（Mango Image Classification）、水稻品种识别（Paddy Variety Recognition）和道路损坏检测（Road Damage Detection）。实验结果表明，在不同数据集上，经过微调的迁移学习方法始终优于从头构建的自定义CNN和特征提取方法，准确率提升幅度在3%至76%之间。值得注意的是，采用微调的ResNet-18在Road Damage BD数据集上达到了完美的100%准确率。尽管自定义CNN在模型规模（340万参数，而预训练模型为1100万至1.34亿参数）和简单任务的训练效率方面具有优势，但在复杂分类任务或训练数据有限的情况下，结合迁移学习的预训练模型表现出更优越的性能。本研究为从业者根据数据集特性、计算资源和性能需求选择合适的深度学习方法提供了实用见解。

</details>


### [7] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: 本文提出PackCache，一种无需训练的KV缓存管理方法，通过条件锚定、跨帧衰减建模和空间保留位置编码三机制，在统一自回归视频生成中显著提升推理效率，尤其在长序列生成中加速效果显著。


<details>
  <summary>Details</summary>
Motivation: 统一自回归模型在处理多模态任务（如文本、图像、视频）时依赖KV缓存机制以降低注意力计算复杂度，但KV缓存随生成长度线性增长，成为推理效率和生成长度的主要瓶颈。特别是在视频生成中，这一问题尤为突出。

Method: 作者分析发现KV缓存中的token具有明显的时空特性：文本和条件图像token作为持久语义锚点持续获得高注意力，而对先前帧的注意力随时间距离自然衰减。基于此，提出PackCache方法，包含三个协同机制：条件锚定保留语义参考、跨帧衰减建模按时间距离分配缓存预算、空间保留位置编码在缓存移除后维持3D结构一致性。

Result: 在48帧长序列视频生成中，PackCache实现端到端1.7–2.2倍加速；在A40和H200上，对最耗时的最后四帧分别实现2.6倍和3.7倍加速。

Conclusion: PackCache有效缓解了统一自回归视频生成中KV缓存带来的效率瓶颈，为更长序列的高效视频生成提供了可行路径。

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

Abstract (中文翻译): 统一自回归模型是一种基于Transformer的框架，将多样化的多模态任务（例如文本、图像、视频）统一为共享token空间下的单一序列建模问题。这类模型依赖KV缓存机制将注意力计算复杂度从O(T²)降至O(T)；然而，KV缓存大小随生成token数量线性增长，迅速成为限制推理效率和生成长度的主要瓶颈。统一自回归视频生成同样面临这一限制。我们的分析表明，KV缓存中的token表现出明显的时空特性：(i) 文本和条件图像token作为持久的语义锚点，始终获得高注意力；(ii) 对先前帧的注意力随时间距离自然衰减。基于这些观察，我们提出了PackCache——一种无需训练的KV缓存管理方法，通过三种协同机制动态压缩KV缓存：保留语义参考的条件锚定、根据时间距离分配缓存预算的跨帧衰减建模，以及在缓存移除后维持一致3D结构的空间保留位置编码。在效率方面，PackCache在48帧长序列上实现了1.7–2.2倍的端到端生成加速，展现出其在支持更长序列视频生成方面的强大潜力。值得注意的是，在受KV缓存持续膨胀影响最大、因而计算成本最高的最后四帧部分，PackCache在A40和H200上分别实现了2.6倍和3.7倍的加速。

</details>


### [8] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 该论文提出利用EMOCA提取的3D面部表情与姿态系数，结合Transformer时序建模和跨模态注意力机制，在分心驾驶场景中实现高准确率的压力识别。


<details>
  <summary>Details</summary>
Motivation: 由于压力具有主观性且面部表情可被自主控制，从面部视频中可靠地识别压力十分困难；现有方法多依赖面部动作单元，而解耦的3D面部几何信息的作用尚未充分探索。

Method: 使用EMOCA提取3D表情与姿态系数，通过配对假设检验分析其在基线与压力阶段的差异，并构建基于Transformer的时序建模框架，评估单模态、早期融合与跨模态注意力策略。

Result: 56个系数中有41个在不同阶段表现出与生理指标相当的一致性压力响应；跨模态注意力融合EMOCA与生理信号效果最佳（AUROC 92%，准确率86.7%），EMOCA-注视融合也具竞争力（AUROC 91.8%）。

Conclusion: 时序建模与跨模态注意力机制能有效提升基于3D面部几何的压力识别性能。

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

Abstract (中文翻译): 从面部视频中可靠地识别压力具有挑战性，原因在于压力的主观性以及人们对面部表情的自主控制。尽管大多数方法依赖于面部动作单元（Facial Action Units），但解耦的3D面部几何信息在压力识别中的作用尚未得到充分探索。本文通过在分心驾驶情境下分析由EMOCA提取的3D表情与姿态系数来解决这一问题。对基线阶段与压力刺激阶段进行的配对假设检验表明，在56个系数中有41个展现出与生理指标相当的、具有阶段特异性的稳定压力反应。在此基础上，我们提出了一种基于Transformer的时序建模框架，并评估了单模态、早期融合以及跨模态注意力等融合策略。实验结果显示，EMOCA与生理信号的跨模态注意力融合取得了最佳性能（AUROC达92%，准确率为86.7%），而EMOCA与注视信息的融合也表现优异（AUROC为91.8%）。这凸显了时序建模与跨模态注意力机制在压力识别任务中的有效性。

</details>


### [9] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: 本文研究了如何通过少量配对样本，将主要在RGB图像上预训练的流匹配基础模型（FLUX.1 Kontext）利用低秩适配（LoRA）技术，快速适配为跨光谱翻译器（如RGB转红外或合成孔径雷达图像），并验证了生成的合成数据能有效提升下游目标检测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉领域的基础模型主要基于RGB数据训练，但许多安全关键应用（如自动驾驶、遥感）依赖红外（IR）和合成孔径雷达（SAR）等非可见光模态。如何高效地将现有RGB基础模型能力迁移到这些稀缺模态，是一个重要问题。

Method: 作者在FLUX.1 Kontext模型中插入低秩适配（LoRA）模块，仅使用每个域100对共测图像（KAIST数据集用于RGB-to-IR，M4-SAR用于RGB-to-SAR）进行微调，将RGB图像翻译成像素对齐的IR/SAR图像。利用LPIPS指标在50个保留样本上评估翻译质量，并以此选择最佳LoRA适配器用于下游检测任务。

Result: 实验表明，LPIPS是下游检测性能（mAP）的强代理指标。使用外部RGB数据集（LLVIP, FLIR ADAS）生成的合成IR图像提升了KAIST上的行人检测性能；合成SAR图像与少量真实SAR数据结合，显著提升了M4-SAR上的基础设施检测性能。

Conclusion: 少量样本的LoRA适配方法能有效将流匹配基础模型应用于非可见光模态，为跨光谱感知任务提供了一种高效且有前景的解决方案。

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

Abstract (中文翻译): 视觉领域的基础模型主要在RGB数据上进行训练，而许多安全关键型应用则依赖于红外（IR）和合成孔径雷达（SAR）等非可见光模态。我们研究了是否可以将一个主要在RGB图像上预训练的单一流匹配基础模型，仅通过少量共测样本重新用作跨光谱翻译器，以及由此生成的合成数据能否增强下游检测任务。我们从FLUX.1 Kontext模型出发，插入低秩适配（LoRA）模块，并在每个域仅使用100对配对图像进行微调，分别在KAIST数据集上实现RGB到IR的转换，以及在M4-SAR数据集上实现RGB到SAR的转换。经过适配的模型能够将RGB图像翻译为像素对齐的IR/SAR图像，从而复用现有的边界框，并纯粹在目标模态下训练目标检测模型。在一系列LoRA超参数组合中，我们发现仅基于50个保留样本计算的LPIPS指标是下游性能的强有力代理：更低的LPIPS值始终预示着YOLOv11n在IR和SAR上以及DETR在KAIST IR测试数据上获得更高的mAP。使用基于LPIPS选出的最佳LoRA适配器，由外部RGB数据集（LLVIP、FLIR ADAS）生成的合成IR图像提升了KAIST红外行人检测性能；而合成SAR图像与有限的真实SAR数据结合时，显著提升了M4-SAR上的基础设施检测性能。我们的结果表明，对流匹配基础模型进行少量样本的LoRA适配，是为非可见光模态提供基础模型级支持的一条有前景的路径。

</details>


### [10] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: 预训练CNN在准确率和收敛速度上优于自定义CNN，但后者参数更少、计算更高效。


<details>
  <summary>Details</summary>
Motivation: 在图像分类任务中，选择从头设计自定义CNN还是使用预训练模型是一个关键的实际问题，需权衡性能与效率。

Method: 作者设计了一个自定义CNN并从头训练，同时在相同实验设置下使用迁移学习对VGG-16、ResNet-50和MobileNet等主流架构进行训练，并通过准确率、精确率、召回率和F1分数等指标进行对比评估。

Result: 预训练模型在分类准确率和收敛速度方面显著优于自定义CNN，尤其在数据有限时；而自定义CNN则以更少参数和更低计算复杂度实现了有竞争力的性能。

Conclusion: 该研究揭示了模型复杂度、性能和计算效率之间的权衡，为图像分类任务中CNN架构的选择提供了实用指导。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

Abstract (中文翻译): 卷积神经网络（CNN）在图像分类任务中展现出卓越的性能；然而，在从头设计自定义CNN与采用成熟的预训练架构之间如何选择，仍是一个重要的实际考量。本文对一种自定义设计的CNN与若干广泛使用的深度学习架构（包括VGG-16、ResNet-50和MobileNet）在图像分类任务中的表现进行了比较分析。自定义CNN从零开始开发和训练，而主流架构则在相同的实验设置下采用迁移学习方式进行训练。所有模型均使用准确率、精确率、召回率和F1分数等标准性能指标进行评估。实验结果表明，预训练CNN架构在分类准确率和收敛速度方面始终优于自定义CNN，尤其在训练数据有限的情况下更为明显。然而，自定义CNN在参数数量显著更少、计算复杂度更低的前提下，仍展现出具有竞争力的性能。本研究揭示了模型复杂度、性能与计算效率之间的权衡关系，并为图像分类问题中选择合适的CNN架构提供了实用见解。

</details>
