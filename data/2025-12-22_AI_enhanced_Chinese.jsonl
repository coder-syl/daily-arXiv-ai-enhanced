{"id": "2512.16925", "pdf": "https://arxiv.org/pdf/2512.16925", "abs": "https://arxiv.org/abs/2512.16925", "authors": ["SunYoung Park", "Jong-Hyeon Lee", "Youngjune Kim", "Daegyu Sung", "Younghyun Yu", "Young-rok Cha", "Jeongho Ju"], "title": "V-Agent: An Interactive Video Search System Using Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.MA"], "comment": "CIKM 2025 MMGENSR Workshop", "summary": "We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.", "AI": {"tldr": "V-Agent \u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u89c6\u9891\u68c0\u7d22\u5e73\u53f0\uff0c\u7ed3\u5408\u5fae\u8c03\u540e\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u6a21\u578b\uff0c\u5728 MultiVENT 2.0 \u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u7684\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u5728\u5904\u7406\u591a\u6a21\u6001\u5185\u5bb9\uff08\u5982\u89c6\u89c9\u548c\u8bed\u97f3\uff09\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7cbe\u51c6\u68c0\u7d22\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa V-Agent \u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u4e0e\u8bed\u97f3\u4fe1\u606f\u63d0\u5347\u89c6\u9891\u641c\u7d22\u80fd\u529b\u3002", "method": "V-Agent \u5229\u7528\u5c11\u91cf\u89c6\u9891\u504f\u597d\u6570\u636e\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u6a21\u578b\u751f\u6210\u7684\u68c0\u7d22\u5411\u91cf\u3002\u8be5\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u534f\u540c\u5de5\u4f5c\u7684\u667a\u80fd\u4f53\uff1a\u8def\u7531\u667a\u80fd\u4f53\u3001\u641c\u7d22\u667a\u80fd\u4f53\u548c\u804a\u5929\u667a\u80fd\u4f53\u3002\u5176\u4e2d\uff0c\u641c\u7d22\u667a\u80fd\u4f53\u4f7f\u7528 VLM \u5bf9\u89c6\u9891\u5e27\u548c ASR \u8bed\u97f3\u8f6c\u5f55\u5206\u522b\u7f16\u7801\u81f3\u5171\u4eab\u7684\u591a\u6a21\u6001\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u91cd\u6392\u5e8f\u6a21\u5757\u4ee5\u4f18\u5316\u68c0\u7d22\u7ed3\u679c\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728 MultiVENT 2.0 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "V-Agent \u5c55\u793a\u4e86\u5728\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5904\u7406\u591a\u6a21\u6001\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u652f\u6301\u7528\u6237\u4e0e\u7cfb\u7edf\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86 V-Agent\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5e73\u53f0\uff0c\u4e13\u4e3a\u9ad8\u7ea7\u89c6\u9891\u641c\u7d22\u548c\u7528\u6237\u4e0e\u7cfb\u7edf\u4e4b\u95f4\u7684\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u800c\u8bbe\u8ba1\u3002\u901a\u8fc7\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f7f\u7528\u5c0f\u578b\u89c6\u9891\u504f\u597d\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u6a21\u578b\u63d0\u4f9b\u7684\u68c0\u7d22\u5411\u91cf\uff0c\u6211\u4eec\u514b\u670d\u4e86\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u7cfb\u7edf\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002\u57fa\u4e8e VLM \u7684\u68c0\u7d22\u6a21\u578b\u80fd\u591f\u5c06\u6765\u81ea\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u5757\u7684\u89c6\u9891\u5e27\u548c\u97f3\u9891\u8f6c\u5f55\u5206\u522b\u5d4c\u5165\u5230\u5171\u4eab\u7684\u591a\u6a21\u6001\u8868\u793a\u7a7a\u95f4\u4e2d\uff0c\u4f7f V-Agent \u80fd\u591f\u540c\u65f6\u7406\u89e3\u89c6\u89c9\u548c\u8bed\u97f3\u5185\u5bb9\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u9891\u641c\u7d22\u3002\u8be5\u7cfb\u7edf\u7531\u4e09\u4e2a\u667a\u80fd\u4f53\u7ec4\u6210\u2014\u2014\u8def\u7531\u667a\u80fd\u4f53\u3001\u641c\u7d22\u667a\u80fd\u4f53\u548c\u804a\u5929\u667a\u80fd\u4f53\u2014\u2014\u5b83\u4eec\u534f\u540c\u5de5\u4f5c\uff0c\u901a\u8fc7\u4f18\u5316\u641c\u7d22\u7ed3\u679c\u5e76\u4e0e\u7528\u6237\u6c9f\u901a\u6765\u6ee1\u8db3\u7528\u6237\u610f\u56fe\u3002\u641c\u7d22\u667a\u80fd\u4f53\u7ed3\u5408 VLM \u68c0\u7d22\u6a21\u578b\u548c\u989d\u5916\u7684\u91cd\u6392\u5e8f\u6a21\u5757\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u89c6\u9891\u68c0\u7d22\u8d28\u91cf\u3002\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u5728 MultiVENT 2.0 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u51f8\u663e\u4e86\u5176\u5728\u5b66\u672f\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.16947", "pdf": "https://arxiv.org/pdf/2512.16947", "abs": "https://arxiv.org/abs/2512.16947", "authors": ["Reza Chandra", "Adang Suhendra", "Lintang Yuniar Banowosari", "Prihandoko"], "title": "Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content", "categories": ["cs.CV"], "comment": null, "summary": "In 2020, a total of 59,741 websites were blocked by the Indonesian government due to containing negative content, including pornography, with 14,266 websites falling into this category. However, these blocked websites could still be accessed by the public using virtual private networks (VPNs). This prompted the research idea to quickly identify pornographic content. This study aims to develop a system capable of identifying websites suspected of containing pornographic image content, using a deep learning approach with convolutional neural network (CNN) and visual geometry group 16 (VGG-16) model. The two models were then explored comprehensively and holistically to determine which model was most effective in detecting pornographic content quickly. Based on the findings of the comparison between testing the CNN and VGG-16 models, research results showed that the best test results were obtained in the eighth experiment using the CNN model at an epoch value level of 50 and a learning rate of 0.001 of 0.9487 or 94.87%. This can be interpreted that the CNN model is more effective in detecting pornographic content quickly and accurately compared to using the VGG-16 model.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86CNN\u548cVGG-16\u6a21\u578b\u5728\u5feb\u901f\u8bc6\u522b\u542b\u8272\u60c5\u56fe\u50cf\u7f51\u7ad9\u4e2d\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660eCNN\u6a21\u578b\uff08\u572850\u4e2aepoch\u3001\u5b66\u4e60\u73870.001\u4e0b\uff09\u51c6\u786e\u7387\u8fbe94.87%\uff0c\u4f18\u4e8eVGG-16\u3002", "motivation": "\u5370\u5c3c\u653f\u5e9c\u867d\u5c01\u9501\u4e86\u5927\u91cf\u542b\u8272\u60c5\u5185\u5bb9\u7684\u7f51\u7ad9\uff0c\u4f46\u516c\u4f17\u4ecd\u53ef\u901a\u8fc7VPN\u8bbf\u95ee\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u5feb\u901f\u8bc6\u522b\u6b64\u7c7b\u5185\u5bb9\u7684\u6280\u672f\u624b\u6bb5\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u522b\u6784\u5efa\u5e76\u5168\u9762\u8bc4\u4f30\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4e0eVGG-16\u6a21\u578b\uff0c\u4ee5\u8bc6\u522b\u7591\u4f3c\u5305\u542b\u8272\u60c5\u56fe\u50cf\u7684\u7f51\u7ad9\u3002", "result": "\u5728\u7b2c\u516b\u6b21\u5b9e\u9a8c\u4e2d\uff0cCNN\u6a21\u578b\u5728epoch\u4e3a50\u3001\u5b66\u4e60\u7387\u4e3a0.001\u65f6\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u51c6\u786e\u7387\u8fbe94.87%\uff0c\u4f18\u4e8eVGG-16\u6a21\u578b\u3002", "conclusion": "CNN\u6a21\u578b\u5728\u5feb\u901f\u4e14\u51c6\u786e\u5730\u68c0\u6d4b\u8272\u60c5\u5185\u5bb9\u65b9\u9762\u6bd4VGG-16\u6a21\u578b\u66f4\u6709\u6548\u3002", "summary_cn": "2020\u5e74\uff0c\u5370\u5ea6\u5c3c\u897f\u4e9a\u653f\u5e9c\u5171\u5c4f\u853d\u4e8659,741\u4e2a\u542b\u6709\u8d1f\u9762\u5185\u5bb9\uff08\u5305\u62ec\u8272\u60c5\u5185\u5bb9\uff09\u7684\u7f51\u7ad9\uff0c\u5176\u4e2d14,266\u4e2a\u5c5e\u4e8e\u8272\u60c5\u7c7b\u522b\u3002\u7136\u800c\uff0c\u516c\u4f17\u4ecd\u53ef\u901a\u8fc7\u865a\u62df\u79c1\u4eba\u7f51\u7edc\uff08VPN\uff09\u8bbf\u95ee\u8fd9\u4e9b\u88ab\u5c4f\u853d\u7684\u7f51\u7ad9\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u5feb\u901f\u8bc6\u522b\u8272\u60c5\u5185\u5bb9\u7684\u60f3\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u8bc6\u522b\u7591\u4f3c\u5305\u542b\u8272\u60c5\u56fe\u50cf\u5185\u5bb9\u7f51\u7ad9\u7684\u7cfb\u7edf\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9\u51e0\u4f55\u7ec416\uff08VGG-16\uff09\u6a21\u578b\u3002\u968f\u540e\u5bf9\u8fd9\u4e24\u79cd\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u800c\u6574\u4f53\u7684\u63a2\u7d22\uff0c\u4ee5\u786e\u5b9a\u54ea\u79cd\u6a21\u578b\u5728\u5feb\u901f\u68c0\u6d4b\u8272\u60c5\u5185\u5bb9\u65b9\u9762\u66f4\u4e3a\u6709\u6548\u3002\u6839\u636e\u5bf9CNN\u548cVGG-16\u6a21\u578b\u7684\u5bf9\u6bd4\u6d4b\u8bd5\u7ed3\u679c\uff0c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7b2c\u516b\u6b21\u5b9e\u9a8c\u4e2d\uff0cCNN\u6a21\u578b\u5728epoch\u503c\u4e3a50\u3001\u5b66\u4e60\u7387\u4e3a0.001\u65f6\u53d6\u5f97\u4e86\u6700\u4f73\u6d4b\u8bd5\u7ed3\u679c\uff0c\u51c6\u786e\u7387\u8fbe\u52300.9487\uff08\u537394.87%\uff09\u3002\u8fd9\u8868\u660e\uff0c\u4e0eVGG-16\u6a21\u578b\u76f8\u6bd4\uff0cCNN\u6a21\u578b\u5728\u5feb\u901f\u4e14\u51c6\u786e\u5730\u68c0\u6d4b\u8272\u60c5\u5185\u5bb9\u65b9\u9762\u66f4\u4e3a\u6709\u6548\u3002"}}
{"id": "2512.16948", "pdf": "https://arxiv.org/pdf/2512.16948", "abs": "https://arxiv.org/abs/2512.16948", "authors": ["Qi Xu", "Shuai Gong", "Xuming Ran", "Haihua Luo", "Yangfan Hu"], "title": "AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals", "categories": ["cs.CV"], "comment": null, "summary": "While deep learning models have shown strong performance in simulating neural responses, they often fail to clearly separate stable visual encoding from condition-specific adaptation, which limits their ability to generalize across stimuli and individuals. We introduce the Adaptive Visual Model (AVM), a structure-preserving framework that enables condition-aware adaptation through modular subnetworks, without modifying the core representation. AVM keeps a Vision Transformer-based encoder frozen to capture consistent visual features, while independently trained modulation paths account for neural response variations driven by stimulus content and subject identity. We evaluate AVM in three experimental settings, including stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all of which involve structured changes in inputs and individuals. Across two large-scale mouse V1 datasets, AVM outperforms the state-of-the-art V1T model by approximately 2% in predictive correlation, demonstrating robust generalization, interpretable condition-wise modulation, and high architectural efficiency. Specifically, AVM achieves a 9.1% improvement in explained variance (FEVE) under the cross-dataset adaptation setting. These results suggest that AVM provides a unified framework for adaptive neural modeling across biological and experimental conditions, offering a scalable solution under structural constraints. Its design may inform future approaches to cortical modeling in both neuroscience and biologically inspired AI systems.", "AI": {"tldr": "AVM is a modular, structure-preserving deep learning framework that separates stable visual encoding from condition-specific adaptation using a frozen Vision Transformer encoder and independent modulation subnetworks, achieving better generalization and interpretability across stimuli, subjects, and datasets in mouse V1 neural response prediction.", "motivation": "Existing deep learning models struggle to disentangle stable visual encoding from condition-specific neural adaptation, limiting their generalization across different stimuli and individuals.", "method": "The Adaptive Visual Model (AVM) uses a frozen Vision Transformer encoder for consistent visual features and adds independently trained modular subnetworks to modulate responses based on stimulus content and subject identity, preserving core architecture while enabling condition-aware adaptation.", "result": "AVM outperforms the state-of-the-art V1T model by ~2% in predictive correlation on two large-scale mouse V1 datasets and achieves a 9.1% improvement in explained variance (FEVE) in cross-dataset adaptation, showing strong generalization, interpretable modulation, and high efficiency.", "conclusion": "AVM offers a unified, scalable framework for adaptive neural modeling under structural constraints, with implications for both neuroscience and biologically inspired AI.", "summary_cn": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6a21\u62df\u795e\u7ecf\u53cd\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u96be\u4ee5\u6e05\u6670\u5730\u533a\u5206\u7a33\u5b9a\u7684\u89c6\u89c9\u7f16\u7801\u4e0e\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u523a\u6fc0\u548c\u4e2a\u4f53\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u89c6\u89c9\u6a21\u578b\uff08AVM\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u6784\u4fdd\u6301\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7684\u5b50\u7f51\u7edc\u5b9e\u73b0\u5bf9\u6761\u4ef6\u611f\u77e5\u7684\u9002\u5e94\uff0c\u800c\u65e0\u9700\u4fee\u6539\u6838\u5fc3\u8868\u5f81\u3002AVM\u5c06\u57fa\u4e8eVision Transformer\u7684\u7f16\u7801\u5668\u51bb\u7ed3\u4ee5\u6355\u6349\u4e00\u81f4\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u540c\u65f6\u4f7f\u7528\u72ec\u7acb\u8bad\u7ec3\u7684\u8c03\u5236\u8def\u5f84\u6765\u89e3\u91ca\u7531\u523a\u6fc0\u5185\u5bb9\u548c\u4e2a\u4f53\u8eab\u4efd\u5f15\u8d77\u7684\u795e\u7ecf\u53cd\u5e94\u53d8\u5316\u3002\u6211\u4eec\u5728\u4e09\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u4e86AVM\uff0c\u5305\u62ec\u523a\u6fc0\u5c42\u9762\u7684\u53d8\u5316\u3001\u8de8\u4e2a\u4f53\u6cdb\u5316\u548c\u8de8\u6570\u636e\u96c6\u9002\u5e94\uff0c\u8fd9\u4e9b\u8bbe\u7f6e\u5747\u6d89\u53ca\u8f93\u5165\u548c\u4e2a\u4f53\u7684\u7ed3\u6784\u6027\u53d8\u5316\u3002\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u5c0f\u9f20V1\u6570\u636e\u96c6\u4e0a\uff0cAVM\u5728\u9884\u6d4b\u76f8\u5173\u6027\u4e0a\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684V1T\u6a21\u578b\u9ad8\u51fa\u7ea62%\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u7684\u6761\u4ef6\u8c03\u5236\u673a\u5236\u4ee5\u53ca\u9ad8\u67b6\u6784\u6548\u7387\u3002\u7279\u522b\u662f\u5728\u8de8\u6570\u636e\u96c6\u9002\u5e94\u573a\u666f\u4e0b\uff0cAVM\u5728\u89e3\u91ca\u65b9\u5dee\uff08FEVE\uff09\u4e0a\u63d0\u5347\u4e869.1%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cAVM\u4e3a\u5728\u751f\u7269\u548c\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u8fdb\u884c\u81ea\u9002\u5e94\u795e\u7ecf\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u7ed3\u6784\u7ea6\u675f\u4e0b\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5176\u8bbe\u8ba1\u53ef\u80fd\u4e3a\u672a\u6765\u795e\u7ecf\u79d1\u5b66\u548c\u7c7b\u8111\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u76ae\u5c42\u5efa\u6a21\u65b9\u6cd5\u63d0\u4f9b\u542f\u53d1\u3002"}}
{"id": "2512.16950", "pdf": "https://arxiv.org/pdf/2512.16950", "abs": "https://arxiv.org/abs/2512.16950", "authors": ["Adrian Straker", "Paul Magdon", "Marco Zullich", "Maximilian Freudenberg", "Christoph Kleinn", "Johannes Breidenbach", "Stefano Puliti", "Nils N\u00f6lke"], "title": "Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections", "categories": ["cs.CV", "cs.AI"], "comment": "31 pages, 14 figures, submitted to Forestry: An International Journal of Forest Research", "summary": "Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Finer-CAM\u4e0eTLS\u70b9\u4e91\u6295\u5f71\u5206\u6bb5\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u6790\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6811\u79cd\u5206\u7c7b\u4e2d\u6240\u4f9d\u8d56\u7684\u7ed3\u6784\u7279\u5f81\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u4e3b\u8981\u5229\u7528\u6811\u51a0\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\uff0c\u5728\u90e8\u5206\u6811\u79cd\u4e2d\u830e\u5e72\u548c\u7ec6\u679d\u4e5f\u8d77\u91cd\u8981\u4f5c\u7528\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5224\u522b\u903b\u8f91\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8ba4\u77e5\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTLS\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6811\u79cd\u5206\u7c7b\u65b9\u6cd5\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5bf9\u6a21\u578b\u53ef\u9760\u6027\u3001\u504f\u5dee\u53ca\u6570\u636e\u5c40\u9650\u6027\u7684\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u5c06Finer-CAM\uff08\u7c7b\u6fc0\u6d3b\u6620\u5c04\uff09\u89e3\u91ca\u65b9\u6cd5\u4e0eTLS\u6295\u5f71\u4e2d\u7684\u7ed3\u6784\u7279\u5f81\u5206\u6bb5\u76f8\u7ed3\u5408\uff0c\u7cfb\u7edf\u8bc4\u4f30\u9a71\u52a8\u6811\u79cd\u5206\u7c7b\u7684\u5173\u952e\u7279\u5f81\uff1b\u4f7f\u75282,445\u68f5\u6b27\u6d32\u4e03\u79cd\u6811\u6728\u7684TLS\u6570\u636e\u8bad\u7ec3\u5e76\u4ea4\u53c9\u9a8c\u8bc1\u4e94\u4e2aYOLOv8\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u8fbe96%\uff08\u6807\u51c6\u5dee0.24%\uff09\uff1b630\u5f20\u663e\u8457\u56fe\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u6811\u51a0\u7279\u5f81\u8fdb\u884c\u5206\u7c7b\uff0c\u94f6\u6866\u3001\u5c71\u6bdb\u6989\u3001\u82f1\u56fd\u680e\u548c\u632a\u5a01\u4e91\u6749\u5c24\u4e3a\u660e\u663e\uff0c\u800c\u6b27\u6d32\u767d\u8721\u3001\u82cf\u683c\u5170\u677e\u548c\u82b1\u65d7\u677e\u5219\u66f4\u591a\u4f9d\u8d56\u830e\u5e72\u7279\u5f81\uff1b\u7ec6\u679d\u7ed3\u6784\u5bf9\u6a21\u578b\u51b3\u7b56\u6709\u663e\u8457\u8d21\u732e\uff1b\u6a21\u578b\u5bf9\u76f8\u4f3c\u6811\u79cd\u7684\u5224\u65ad\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u4e00\u81f4\u3002", "conclusion": "\u4e3a\u63d0\u5347\u6a21\u578b\u53ef\u4fe1\u5ea6\u5e76\u8bc6\u522b\u6f5c\u5728\u504f\u5dee\u4e0e\u5c40\u9650\uff0c\u9700\u6df1\u5165\u7406\u89e3\u6811\u79cd\u5206\u7c7b\u6a21\u578b\u7684\u51b3\u7b56\u673a\u5236\uff1b\u6240\u63d0\u65b9\u6cd5\u6709\u52a9\u4e8e\u63ed\u793a\u6a21\u578b\u4f9d\u8d56\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "summary_cn": "\u6811\u79cd\u5206\u7c7b\u51e0\u5341\u5e74\u6765\u4e00\u76f4\u662f\u68ee\u6797\u9065\u611f\u9886\u57df\u7684\u6838\u5fc3\u7814\u7a76\u65b9\u5411\u3002\u65b0\u578b\u4f20\u611f\u5668\u548c\u5206\u7c7b\u65b9\u6cd5\uff08\u5982\u5730\u57fa\u6fc0\u5149\u626b\u63cfTLS\u548c\u6df1\u5ea6\u5b66\u4e60\uff09\u867d\u5df2\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u4ecd\u4e0d\u900f\u660e\u3002\u8bf8\u5982Finer-CAM\uff08\u7c7b\u6fc0\u6d3b\u6620\u5c04\uff09\u7b49\u65b9\u6cd5\u53ef\u4ee5\u7a81\u51fa\u663e\u793aTLS\u6295\u5f71\u4e2d\u5bf9\u76ee\u6807\u6811\u79cd\u5206\u7c7b\u8d77\u5173\u952e\u4f5c\u7528\u3001\u800c\u5728\u5916\u89c2\u76f8\u4f3c\u7684\u5bf9\u6bd4\u6811\u79cd\u4e2d\u4e0d\u5e38\u89c1\u7684\u7279\u5f81\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06Finer-CAM\u89e3\u91ca\u7ed3\u679c\u4e0e\u4ee3\u8868\u6811\u6728\u7ed3\u6784\u7279\u5f81\u7684TLS\u6295\u5f71\u5206\u6bb5\u76f8\u8054\u7cfb\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u54ea\u4e9b\u7279\u5f81\u9a71\u52a8\u4e86\u6811\u79cd\u533a\u5206\u3002\u7814\u7a76\u5229\u7528\u6765\u81ea\u4e03\u79cd\u6b27\u6d32\u6811\u79cd\u51712,445\u68f5\u6811\u7684TLS\u6570\u636e\uff0c\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u8bad\u7ec3\u5e76\u9a8c\u8bc1\u4e86\u4e94\u4e2aYOLOv8\u6a21\u578b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523096%\uff08\u6807\u51c6\u5dee\u4e3a0.24%\uff09\u3002\u5bf9630\u5f20\u663e\u8457\u56fe\u7684\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56TLS\u6295\u5f71\u4e2d\u7684\u6811\u51a0\u7279\u5f81\u8fdb\u884c\u6811\u79cd\u5206\u7c7b\u3002\u8fd9\u4e00\u73b0\u8c61\u5728\u94f6\u6866\u3001\u6b27\u6d32\u5c71\u6bdb\u6989\u3001\u82f1\u56fd\u680e\u548c\u632a\u5a01\u4e91\u6749\u4e2d\u5c24\u4e3a\u663e\u8457\uff0c\u800c\u6b27\u6d32\u767d\u8721\u3001\u82cf\u683c\u5170\u677e\u548c\u82b1\u65d7\u677e\u7684\u5206\u7c7b\u5219\u66f4\u5e38\u4f9d\u8d56\u830e\u5e72\u7279\u5f81\u3002\u7279\u522b\u662f\u7ec6\u679d\u7684\u8868\u5f81\u5bf9\u6a21\u578b\u51b3\u7b56\u5177\u6709\u91cd\u8981\u8d21\u732e\u3002\u6b64\u5916\uff0c\u6a21\u578b\u8ba4\u4e3a\u5f7c\u6b64\u76f8\u4f3c\u7684\u6811\u79cd\uff0c\u4e5f\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8ba4\u77e5\u4e00\u81f4\u3002\u7814\u7a76\u7ed3\u679c\u8fdb\u4e00\u6b65\u5f3a\u8c03\uff0c\u6709\u5fc5\u8981\u52a0\u5f3a\u5bf9\u6811\u79cd\u5206\u7c7b\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u4ee5\u63ed\u793a\u6570\u636e\u96c6\u548c\u6a21\u578b\u7684\u5c40\u9650\u6027\u4e0e\u504f\u5dee\uff0c\u5e76\u589e\u5f3a\u5bf9\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u7684\u4fe1\u5fc3\u3002"}}
{"id": "2512.16954", "pdf": "https://arxiv.org/pdf/2512.16954", "abs": "https://arxiv.org/abs/2512.16954", "authors": ["Chayan Jain", "Rishant Sharma", "Archit Garg", "Ishan Bhanuka", "Pratik Narang", "Dhruv Kumar"], "title": "Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review", "summary": "Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5267\u672c\uff0c\u518d\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u521b\u5efa\u4e00\u81f4\u7684\u89d2\u8272\u89c6\u89c9\u951a\u70b9\uff0c\u6700\u540e\u5408\u6210\u5404\u573a\u666f\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5370\u5ea6\u4e0e\u897f\u65b9\u4e3b\u9898\u751f\u6210\u4e2d\u7684\u6587\u5316\u504f\u89c1\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891AI\u5728\u751f\u6210\u957f\u7bc7\u3001\u8fde\u8d2f\u4e14\u89d2\u8272\u4e00\u81f4\u7684\u89c6\u9891\u6545\u4e8b\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7c7b\u7535\u5f71\u5236\u4f5c\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u5267\u672c\uff0c\u518d\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e3a\u6bcf\u4e2a\u89d2\u8272\u751f\u6210\u4e00\u81f4\u7684\u89c6\u89c9\u951a\u70b9\uff0c\u6700\u540e\u7531\u89c6\u9891\u751f\u6210\u6a21\u578b\u9010\u573a\u666f\u5408\u6210\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u79fb\u9664\u89c6\u89c9\u951a\u5b9a\u673a\u5236\u4f1a\u5bfc\u81f4\u89d2\u8272\u4e00\u81f4\u6027\u8bc4\u5206\u4ece7.99\u9aa4\u964d\u81f30.55\uff1b\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5370\u5ea6\u4e0e\u897f\u65b9\u4e3b\u9898\u89c6\u9891\u751f\u6210\u4e2d\u5b58\u5728\u6587\u5316\u504f\u89c1\uff0c\u8868\u73b0\u4e3a\u89d2\u8272\u4e00\u81f4\u6027\u548c\u52a8\u6001\u7a0b\u5ea6\u7684\u5dee\u5f02\u3002", "conclusion": "\u591a\u9636\u6bb5\u5206\u89e3\u548c\u89c6\u89c9\u5148\u9a8c\u5bf9\u4fdd\u6301\u89d2\u8272\u8eab\u4efd\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u9700\u5173\u6ce8\u5e76\u89e3\u51b3\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\u95ee\u9898\u3002", "summary_cn": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891AI\u5728\u751f\u6210\u957f\u7bc7\u3001\u8fde\u8d2f\u4e14\u89d2\u8272\u4e00\u81f4\u7684\u89c6\u9891\u6545\u4e8b\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u7535\u5f71\u5236\u4f5c\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u91c7\u7528\u4e00\u6b65\u5230\u4f4d\u7684\u65b9\u5f0f\u751f\u6210\u89c6\u9891\uff0c\u800c\u662f\u9996\u5148\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8be6\u7ec6\u7684\u5236\u4f5c\u5267\u672c\uff0c\u518d\u4ee5\u6b64\u5267\u672c\u6307\u5bfc\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e3a\u6bcf\u4e2a\u89d2\u8272\u521b\u5efa\u4e00\u81f4\u7684\u89c6\u89c9\u5f62\u8c61\uff0c\u8fd9\u4e9b\u89c6\u89c9\u5f62\u8c61\u4f5c\u4e3a\u951a\u70b9\uff0c\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6a21\u578b\u9010\u4e2a\u5408\u6210\u6bcf\u4e2a\u573a\u666f\u3002\u6211\u4eec\u7684\u57fa\u7ebf\u5bf9\u6bd4\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u591a\u9636\u6bb5\u5206\u89e3\u7684\u5fc5\u8981\u6027\uff1a\u5177\u4f53\u800c\u8a00\uff0c\u82e5\u79fb\u9664\u89c6\u89c9\u951a\u5b9a\u673a\u5236\uff0c\u89d2\u8272\u4e00\u81f4\u6027\u8bc4\u5206\u5c06\u4ece7.99\u6025\u5267\u4e0b\u964d\u81f30.55\uff0c\u8bc1\u5b9e\u4e86\u89c6\u89c9\u5148\u9a8c\u5bf9\u4e8e\u8eab\u4efd\u4fdd\u6301\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5206\u6790\u4e86\u5f53\u524d\u6a21\u578b\u4e2d\u7684\u6587\u5316\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u5370\u5ea6\u4e3b\u9898\u4e0e\u897f\u65b9\u4e3b\u9898\u751f\u6210\u5728\u4e3b\u4f53\u4e00\u81f4\u6027\u548c\u52a8\u6001\u7a0b\u5ea6\u65b9\u9762\u5b58\u5728\u660e\u663e\u504f\u89c1\u3002"}}
{"id": "2512.16975", "pdf": "https://arxiv.org/pdf/2512.16975", "abs": "https://arxiv.org/abs/2512.16975", "authors": ["Haotian Ye", "Qiyuan He", "Jiaqi Han", "Puheng Li", "Jiaojiao Fan", "Zekun Hao", "Fitsum Reda", "Yogesh Balaji", "Huayu Chen", "Sheng Liu", "Angela Yao", "James Zou", "Stefano Ermon", "Haoxiang Wang", "Ming-Yu Liu"], "title": "InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInfoTok\uff0c\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u81ea\u9002\u5e94\u89c6\u9891\u5206\u8bcd\u6846\u67b6\uff0c\u901a\u8fc7\u6309\u4fe1\u606f\u4e30\u5bcc\u5ea6\u5206\u914dtoken\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u538b\u7f29\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5206\u8bcd\u5668\u91c7\u7528\u56fa\u5b9a\u538b\u7f29\u7387\uff0c\u65e0\u6cd5\u5e94\u5bf9\u89c6\u9891\u5185\u5bb9\u590d\u6742\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\u53d8\u5316\uff0c\u5bfc\u81f4\u5197\u4f59\u6216\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u57fa\u4e8e\u9999\u519c\u4fe1\u606f\u8bba\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684ELBO\uff08\u8bc1\u636e\u4e0b\u754c\uff09\u7b97\u6cd5\uff0c\u5e76\u6784\u5efa\u57fa\u4e8eTransformer\u7684\u81ea\u9002\u5e94\u538b\u7f29\u5668\u5b9e\u73b0\u81ea\u9002\u5e94\u5206\u8bcd\u3002", "result": "\u5728\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u8282\u770120%\u7684token\uff0c\u5b9e\u73b02.3\u500d\u538b\u7f29\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "InfoTok\u901a\u8fc7\u4f9d\u636e\u4fe1\u606f\u4e30\u5bcc\u5ea6\u5206\u914dtoken\uff0c\u5b9e\u73b0\u4e86\u66f4\u7d27\u51d1\u4e14\u51c6\u786e\u7684\u89c6\u9891\u8868\u793a\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "summary_cn": "\u51c6\u786e\u800c\u9ad8\u6548\u7684\u79bb\u6563\u89c6\u9891\u5206\u8bcd\u5bf9\u4e8e\u957f\u89c6\u9891\u5e8f\u5217\u5904\u7406\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u89c6\u9891\u56fa\u6709\u7684\u590d\u6742\u6027\u548c\u53ef\u53d8\u7684\u4fe1\u606f\u5bc6\u5ea6\u5bf9\u5f53\u524d\u7684\u5206\u8bcd\u5668\u6784\u6210\u4e86\u663e\u8457\u74f6\u9888\u2014\u2014\u8fd9\u4e9b\u5206\u8bcd\u5668\u4ee5\u56fa\u5b9a\u901f\u7387\u521a\u6027\u5730\u538b\u7f29\u6240\u6709\u5185\u5bb9\uff0c\u5bfc\u81f4\u5197\u4f59\u6216\u4fe1\u606f\u4e22\u5931\u3002\u53d7\u9999\u519c\u4fe1\u606f\u8bba\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86InfoTok\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u9002\u5e94\u89c6\u9891\u5206\u8bcd\u7684\u539f\u5219\u6027\u6846\u67b6\u3002\u6211\u4eec\u4e25\u683c\u8bc1\u660e\u4e86\u73b0\u6709\u7684\u4e0e\u6570\u636e\u65e0\u5173\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u8868\u793a\u957f\u5ea6\u4e0a\u662f\u6b21\u4f18\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u8bc1\u636e\u4e0b\u754c\uff08ELBO\uff09\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u3002\u5229\u7528\u8fd9\u4e00\u6846\u67b6\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u81ea\u9002\u5e94\u538b\u7f29\u5668\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u9002\u5e94\u5206\u8bcd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u8282\u7701\u4e8620%\u7684token\uff0c\u5e76\u5b9e\u73b0\u4e862.3\u500d\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4f18\u4e8e\u4ee5\u5f80\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u3002\u901a\u8fc7\u6839\u636e\u4fe1\u606f\u4e30\u5bcc\u5ea6\u5206\u914dtoken\uff0cInfoTok\u5b9e\u73b0\u4e86\u66f4\u7d27\u51d1\u4e14\u51c6\u786e\u7684\u89c6\u9891\u8868\u793a\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.16977", "pdf": "https://arxiv.org/pdf/2512.16977", "abs": "https://arxiv.org/abs/2512.16977", "authors": ["Hao Li", "Daiwei Lu", "Xing Yao", "Nicholas Kavoussi", "Ipek Oguz"], "title": "Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present Endo-SemiS, a semi-supervised segmentation framework for providing reliable segmentation of endoscopic video frames with limited annotation. EndoSemiS uses 4 strategies to improve performance by effectively utilizing all available data, particularly unlabeled data: (1) Cross-supervision between two individual networks that supervise each other; (2) Uncertainty-guided pseudo-labels from unlabeled data, which are generated by selecting high-confidence regions to improve their quality; (3) Joint pseudolabel supervision, which aggregates reliable pixels from the pseudo-labels of both networks to provide accurate supervision for unlabeled data; and (4) Mutual learning, where both networks learn from each other at the feature and image levels, reducing variance and guiding them toward a consistent solution. Additionally, a separate corrective network that utilizes spatiotemporal information from endoscopy video to improve segmentation performance. Endo-SemiS is evaluated on two clinical applications: kidney stone laser lithotomy from ureteroscopy and polyp screening from colonoscopy. Compared to state-of-the-art segmentation methods, Endo-SemiS substantially achieves superior results on both datasets with limited labeled data. The code is publicly available at https://github.com/MedICL-VU/Endo-SemiS", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEndo-SemiS\uff0c\u4e00\u79cd\u7528\u4e8e\u5185\u7aa5\u955c\u89c6\u9891\u5e27\u5206\u5272\u7684\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u53ef\u9760\u5206\u5272\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4ea4\u53c9\u76d1\u7763\u3001\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u3001\u8054\u5408\u4f2a\u6807\u7b7e\u76d1\u7763\u548c\u4e92\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u5229\u7528\u65f6\u7a7a\u4fe1\u606f\u7684\u6821\u6b63\u7f51\u7edc\uff0c\u5728\u4e24\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5185\u7aa5\u955c\u89c6\u9891\u5e27\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u5728\u6709\u9650\u6807\u6ce8\u6761\u4ef6\u4e0b\u6709\u6548\u5229\u7528\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u7684\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u3002", "method": "Endo-SemiS\u6846\u67b6\u91c7\u7528\u56db\u79cd\u7b56\u7565\uff1a(1) \u4e24\u4e2a\u72ec\u7acb\u7f51\u7edc\u95f4\u7684\u4ea4\u53c9\u76d1\u7763\uff1b(2) \u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u751f\u6210\uff1b(3) \u805a\u5408\u53cc\u7f51\u7edc\u53ef\u9760\u50cf\u7d20\u7684\u8054\u5408\u4f2a\u6807\u7b7e\u76d1\u7763\uff1b(4) \u7279\u5f81\u4e0e\u56fe\u50cf\u5c42\u9762\u7684\u4e92\u5b66\u4e60\u4ee5\u964d\u4f4e\u65b9\u5dee\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e00\u4e2a\u5229\u7528\u5185\u7aa5\u955c\u89c6\u9891\u65f6\u7a7a\u4fe1\u606f\u7684\u72ec\u7acb\u6821\u6b63\u7f51\u7edc\u3002", "result": "\u5728\u8f93\u5c3f\u7ba1\u955c\u80be\u7ed3\u77f3\u6fc0\u5149\u788e\u77f3\u672f\u548c\u7ed3\u80a0\u955c\u606f\u8089\u7b5b\u67e5\u4e24\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0cEndo-SemiS\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "Endo-SemiS\u901a\u8fc7\u6709\u6548\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u548c\u65f6\u7a7a\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4e3a\u5185\u7aa5\u955c\u89c6\u9891\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u534a\u76d1\u7763\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "summary_cn": "\u672c\u6587\u63d0\u51fa\u4e86Endo-SemiS\uff0c\u4e00\u79cd\u7528\u4e8e\u5185\u7aa5\u955c\u89c6\u9891\u5e27\u5206\u5272\u7684\u534a\u76d1\u7763\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ef\u9760\u7684\u5206\u5272\u7ed3\u679c\u3002Endo-SemiS\u901a\u8fc7\u56db\u79cd\u7b56\u7565\u6709\u6548\u5229\u7528\u6240\u6709\u53ef\u7528\u6570\u636e\uff08\u5c24\u5176\u662f\u672a\u6807\u6ce8\u6570\u636e\uff09\u6765\u63d0\u5347\u6027\u80fd\uff1a(1) \u4e24\u4e2a\u72ec\u7acb\u7f51\u7edc\u4e4b\u95f4\u7684\u4ea4\u53c9\u76d1\u7763\uff0c\u5f7c\u6b64\u76f8\u4e92\u76d1\u7763\uff1b(2) \u4ece\u672a\u6807\u6ce8\u6570\u636e\u4e2d\u751f\u6210\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u9009\u62e9\u9ad8\u7f6e\u4fe1\u5ea6\u533a\u57df\u6765\u63d0\u9ad8\u5176\u8d28\u91cf\uff1b(3) \u8054\u5408\u4f2a\u6807\u7b7e\u76d1\u7763\uff0c\u805a\u5408\u6765\u81ea\u4e24\u4e2a\u7f51\u7edc\u4f2a\u6807\u7b7e\u4e2d\u7684\u53ef\u9760\u50cf\u7d20\uff0c\u4e3a\u672a\u6807\u6ce8\u6570\u636e\u63d0\u4f9b\u51c6\u786e\u7684\u76d1\u7763\u4fe1\u53f7\uff1b(4) \u4e92\u5b66\u4e60\u673a\u5236\uff0c\u4f7f\u4e24\u4e2a\u7f51\u7edc\u5728\u7279\u5f81\u5c42\u9762\u548c\u56fe\u50cf\u5c42\u9762\u76f8\u4e92\u5b66\u4e60\uff0c\u4ece\u800c\u964d\u4f4e\u65b9\u5dee\u5e76\u5f15\u5bfc\u5b83\u4eec\u8fbe\u6210\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u72ec\u7acb\u7684\u6821\u6b63\u7f51\u7edc\uff0c\u5229\u7528\u5185\u7aa5\u955c\u89c6\u9891\u4e2d\u7684\u65f6\u7a7a\u4fe1\u606f\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002Endo-SemiS\u5728\u4e24\u4e2a\u4e34\u5e8a\u5e94\u7528\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1a\u8f93\u5c3f\u7ba1\u955c\u4e0b\u7684\u80be\u7ed3\u77f3\u6fc0\u5149\u788e\u77f3\u672f\u548c\u7ed3\u80a0\u955c\u4e0b\u7684\u606f\u8089\u7b5b\u67e5\u3002\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5206\u5272\u65b9\u6cd5\u76f8\u6bd4\uff0cEndo-SemiS\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u4f18\u7684\u7ed3\u679c\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u53d1\u5e03\u4e8ehttps://github.com/MedICL-VU/Endo-SemiS\u3002"}}
{"id": "2512.16978", "pdf": "https://arxiv.org/pdf/2512.16978", "abs": "https://arxiv.org/abs/2512.16978", "authors": ["Mohammed Irfan Kurpath", "Jaseel Muhammad Kaithakkodan", "Jinxing Zhou", "Sahal Shaji Mullappilly", "Mohammad Almansoori", "Noor Ahsan", "Beknur Kalmakhanbet", "Sambal Shikhar", "Rishabh Lalla", "Jean Lahoud", "Mariette Awad", "Fahad Shahbaz Khan", "Salman Khan", "Rao Muhammad Anwer", "Hisham Cholakkal"], "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos", "categories": ["cs.CV"], "comment": null, "summary": "Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LongShOTBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u8868\u73b0\u7684\u8bca\u65ad\u6027\u57fa\u51c6\uff0c\u6db5\u76d6\u5f00\u653e\u5f0f\u95ee\u9898\u3001\u591a\u8f6e\u5bf9\u8bdd\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\uff0c\u5e76\u5f15\u5165\u914d\u5957\u7cfb\u7edfLongShOTAgent\uff1b\u5b9e\u9a8c\u663e\u793a\u5f53\u524dSOTA\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u8be5\u4efb\u52a1\u7684\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u8981\u4e48\u4fa7\u91cd\u65f6\u95f4\u957f\u5ea6\uff0c\u8981\u4e48\u4fa7\u91cd\u591a\u6a21\u6001\u4e30\u5bcc\u6027\uff0c\u5f88\u5c11\u517c\u987e\u4e24\u8005\uff1b\u4e14\u591a\u4f9d\u8d56\u5355\u4e00\u51c6\u786e\u7387\u6307\u6807\uff0c\u96be\u4ee5\u63ed\u793a\u6a21\u578b\u5931\u8d25\u539f\u56e0\u3002", "method": "\u6784\u5efaLongShOTBench\u57fa\u51c6\uff1a\u901a\u8fc7\u53ef\u6269\u5c55\u3001\u4eba\u5de5\u9a8c\u8bc1\u7684\u6d41\u7a0b\u751f\u6210\u5305\u542b\u53c2\u8003\u7b54\u6848\u548c\u5206\u7ea7\u8bc4\u5206\u6807\u51c6\u7684\u6837\u672c\uff0c\u6db5\u76d6\u89c6\u9891\u3001\u97f3\u9891\u3001\u8bed\u97f3\u7684\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u53ca\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\uff1b\u540c\u65f6\u5f00\u53d1LongShOTAgent\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u5904\u7406\u3001\u641c\u7d22\u548c\u8fed\u4ee3\u4f18\u5316\u5206\u6790\u957f\u89c6\u9891\u3002", "result": "\u5728LongShOTBench\u4e0a\uff0cGemini-2.5-Flash\u5f97\u5206\u4e3a52.95%\uff0c\u5f00\u6e90\u6a21\u578b\u4f4e\u4e8e30%\uff0cLongShOTAgent\u8fbe\u523044.66%\uff0c\u8868\u660e\u5f53\u524dMLLM\u5728\u771f\u5b9e\u573a\u666f\u957f\u89c6\u9891\u7406\u89e3\u4e0a\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "LongShOTBench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002", "summary_cn": "\u957f\u683c\u5f0f\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u9700\u8981\u5c06\u89c6\u89c9\u3001\u8bed\u97f3\u548c\u73af\u5883\u97f3\u9891\u4fe1\u606f\u6574\u5408\u8d77\u6765\uff0c\u5e76\u8fdb\u884c\u8fde\u8d2f\u7684\u957f\u8ddd\u79bb\u63a8\u7406\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u8981\u4e48\u5f3a\u8c03\u65f6\u95f4\u957f\u5ea6\uff0c\u8981\u4e48\u5f3a\u8c03\u591a\u6a21\u6001\u4e30\u5bcc\u6027\uff0c\u4f46\u5f88\u5c11\u540c\u65f6\u517c\u987e\u4e24\u8005\uff1b\u5c3d\u7ba1\u90e8\u5206\u57fa\u51c6\u5f15\u5165\u4e86\u5f00\u653e\u5f0f\u95ee\u9898\u548c\u9ad8\u7ea7\u8bc4\u4f30\u6307\u6807\uff0c\u4f46\u5927\u591a\u4ecd\u4f9d\u8d56\u5355\u4e00\u5206\u6570\u7684\u51c6\u786e\u7387\uff0c\u96be\u4ee5\u63ed\u793a\u6a21\u578b\u7684\u5177\u4f53\u5931\u8d25\u6a21\u5f0f\u3002\u6211\u4eec\u63d0\u51fa\u4e86LongShOTBench\u2014\u2014\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\uff0c\u5305\u542b\u610f\u56fe\u9a71\u52a8\u7684\u5f00\u653e\u5f0f\u95ee\u9898\u3001\u5355\u8f6e\u4e0e\u591a\u8f6e\u5bf9\u8bdd\uff0c\u4ee5\u53ca\u9700\u8981\u8de8\u89c6\u9891\u3001\u97f3\u9891\u548c\u8bed\u97f3\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u5de5\u5177\u8c03\u7528\u7684\u4efb\u52a1\u3002\u6bcf\u4e2a\u6837\u672c\u5747\u914d\u6709\u53c2\u8003\u7b54\u6848\u548c\u5206\u7ea7\u8bc4\u5206\u6807\u51c6\uff0c\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u53ef\u8ffd\u6eaf\u7684\u8bc4\u4f30\u3002LongShOTBench\u901a\u8fc7\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7ecf\u4eba\u5de5\u9a8c\u8bc1\u7684\u6d41\u7a0b\u6784\u5efa\uff0c\u786e\u4fdd\u8986\u76d6\u5168\u9762\u4e14\u7ed3\u679c\u53ef\u590d\u73b0\uff0c\u6240\u6709\u6837\u672c\u5747\u7ecf\u8fc7\u4eba\u5de5\u6821\u9a8c\u4e0e\u4fee\u6b63\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86LongShOTAgent\uff0c\u4e00\u79cd\u901a\u8fc7\u9884\u5904\u7406\u3001\u641c\u7d22\u548c\u8fed\u4ee3\u4f18\u5316\u6765\u5206\u6790\u957f\u89c6\u9891\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u5728LongShOTBench\u4e0a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1aGemini-2.5-Flash\u5f97\u5206\u4e3a52.95%\uff0c\u5f00\u6e90\u6a21\u578b\u5f97\u5206\u5747\u4f4e\u4e8e30%\uff0c\u800cLongShOTAgent\u8fbe\u523044.66%\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u7684\u96be\u5ea6\u3002LongShOTBench\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347MLLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u7840\u3002\u6240\u6709\u8d44\u6e90\u5df2\u5728GitHub\u516c\u5f00\uff1ahttps://github.com/mbzuai-oryx/longshot\u3002"}}
{"id": "2512.17012", "pdf": "https://arxiv.org/pdf/2512.17012", "abs": "https://arxiv.org/abs/2512.17012", "authors": ["Chiao-An Yang", "Ryo Hachiuma", "Sifei Liu", "Subhashree Radhakrishnan", "Raymond A. Yeh", "Yu-Chiang Frank Wang", "Min-Hung Chen"], "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa4D-RGPT\u6a21\u578b\u3001P4D\u8bad\u7ec3\u6846\u67b6\u548cR4D-Bench\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57284D\uff083D+\u65f6\u95f4\uff09\u89c6\u9891\u7406\u89e3\u548c\u533a\u57df\u7ea7\u95ee\u7b54\u4e0a\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5904\u74063D\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8f83\u5f31\u76844D\u611f\u77e5\u548c\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff1b\u73b0\u67093D/4D\u89c6\u9891\u95ee\u7b54\uff08VQA\uff09\u57fa\u51c6\u4fa7\u91cd\u9759\u6001\u573a\u666f\u4e14\u7f3a\u4e4f\u533a\u57df\u7ea7\u63d0\u793a\u3002", "method": "\u63d0\u51fa\uff1a(a) 4D-RGPT\uff0c\u4e00\u79cd\u4e13\u7528\u4e8e\u4ece\u89c6\u9891\u8f93\u5165\u4e2d\u6355\u63494D\u8868\u5f81\u5e76\u589e\u5f3a\u65f6\u95f4\u611f\u77e5\u7684MLLM\uff1b(b) \u611f\u77e54D\u84b8\u998f\uff08P4D\uff09\uff0c\u4e00\u79cd\u5c06\u51bb\u7ed3\u4e13\u5bb6\u6a21\u578b\u4e2d\u76844D\u8868\u5f81\u8fc1\u79fb\u81f34D-RGPT\u7684\u8bad\u7ec3\u6846\u67b6\uff1b(c) R4D-Bench\uff0c\u4e00\u4e2a\u7ed3\u5408\u81ea\u52a8\u4e0e\u4eba\u5de5\u9a8c\u8bc1\u6784\u5efa\u7684\u3001\u652f\u6301\u533a\u57df\u7ea7\u63d0\u793a\u7684\u6df1\u5ea6\u611f\u77e5\u52a8\u6001\u573a\u666f\u8bc4\u6d4b\u57fa\u51c6\u3002", "result": "4D-RGPT\u5728\u73b0\u67094D VQA\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684R4D-Bench\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86MLLM\u5bf94D\u52a8\u6001\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u65b0\u57fa\u51c6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "summary_cn": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u57283D\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u53d7\u9650\u4e8e\u8f83\u5f31\u76844D\u611f\u77e5\u548c\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u3002\u73b0\u6709\u76843D\u548c4D\u89c6\u9891\u95ee\u7b54\uff08VQA\uff09\u57fa\u51c6\u4e5f\u4fa7\u91cd\u4e8e\u9759\u6001\u573a\u666f\uff0c\u4e14\u7f3a\u4e4f\u533a\u57df\u7ea7\u522b\u7684\u63d0\u793a\u673a\u5236\u3002\u6211\u4eec\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff1a(a) \u63d0\u51fa4D-RGPT\uff0c\u4e00\u79cd\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u4ece\u89c6\u9891\u8f93\u5165\u4e2d\u6355\u63494D\u8868\u5f81\u5e76\u589e\u5f3a\u65f6\u95f4\u611f\u77e5\u7684MLLM\uff1b(b) \u63d0\u51fa\u611f\u77e54D\u84b8\u998f\uff08P4D\uff09\uff0c\u4e00\u79cd\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u5c06\u51bb\u7ed3\u4e13\u5bb6\u6a21\u578b\u4e2d\u76844D\u8868\u5f81\u8fc1\u79fb\u52304D-RGPT\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u5168\u9762\u76844D\u611f\u77e5\uff1b(c) \u6784\u5efaR4D-Bench\uff0c\u4e00\u4e2a\u9762\u5411\u5177\u6709\u6df1\u5ea6\u611f\u77e5\u7684\u52a8\u6001\u573a\u666f\u5e76\u652f\u6301\u533a\u57df\u7ea7\u63d0\u793a\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u6df7\u5408\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u9a8c\u8bc1\u7684\u6d41\u7a0b\u6784\u5efa\u800c\u6210\u3002\u6211\u4eec\u76844D-RGPT\u5728\u73b0\u67094D VQA\u57fa\u51c6\u548c\u6240\u63d0\u51fa\u7684R4D-Bench\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.17021", "pdf": "https://arxiv.org/pdf/2512.17021", "abs": "https://arxiv.org/abs/2512.17021", "authors": ["Martin Schwartz", "Fajwel Fogel", "Nikola Besic", "Damien Robert", "Louis Geist", "Jean-Pierre Renaud", "Jean-Matthieu Monnet", "Clemens Mosig", "C\u00e9dric Vega", "Alexandre d'Aspremont", "Loic Landrieu", "Philippe Ciais"], "title": "FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "The recent decline of the European forest carbon sink highlights the need for spatially explicit and frequently updated forest monitoring tools. Yet, existing satellite-based disturbance products remain too coarse to detect changes at the scale of individual trees, typically below 100 m$^{2}$. Here, we introduce FORMSpoT (Forest Mapping with SPOT Time series), a decade-long (2014-2024) nationwide mapping of forest canopy height at 1.5 m resolution, together with annual disturbance polygons (FORMSpoT-$\u0394$) covering mainland France. Canopy heights were derived from annual SPOT-6/7 composites using a hierarchical transformer model (PVTv2) trained on high-resolution airborne laser scanning (ALS) data. To enable robust change detection across heterogeneous acquisitions, we developed a dedicated post-processing pipeline combining co-registration and spatio-temporal total variation denoising. Validation against ALS revisits across 19 sites and 5,087 National Forest Inventory plots shows that FORMSpoT-$\u0394$ substantially outperforms existing disturbance products. In mountainous forests, where disturbances are small and spatially fragmented, FORMSpoT-$\u0394$ achieves an F1-score of 0.44, representing an order of magnitude higher than existing benchmarks. By enabling tree-level monitoring of forest dynamics at national scale, FORMSpoT-$\u0394$ provides a unique tool to analyze management practices, detect early signals of forest decline, and better quantify carbon losses from subtle disturbances such as thinning or selective logging. These results underscore the critical importance of sustaining very high-resolution satellite missions like SPOT and open-data initiatives such as DINAMIS for monitoring forests under climate change.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFORMSpoT\u65b9\u6cd5\uff0c\u5229\u7528SPOT-6/7\u536b\u661f\u5341\u5e74\u65f6\u5e8f\u6570\u636e\u548c\u9ad8\u5206\u8fa8\u7387\u673a\u8f7d\u6fc0\u5149\u626b\u63cf\uff08ALS\uff09\u8bad\u7ec3\u7684\u5c42\u6b21\u5316Transformer\u6a21\u578b\uff0c\u57281.5\u7c73\u5206\u8fa8\u7387\u4e0b\u7ed8\u5236\u6cd5\u56fd\u5168\u56fd\u68ee\u6797\u51a0\u5c42\u9ad8\u5ea6\uff0c\u5e76\u901a\u8fc7\u4e13\u7528\u540e\u5904\u7406\u6d41\u7a0b\u751f\u6210\u5e74\u5ea6\u6270\u52a8\u56fe\uff08FORMSpoT-\u0394\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6270\u52a8\u4ea7\u54c1\uff0c\u5c24\u5176\u5728\u5c71\u533a\u5c0f\u5c3a\u5ea6\u6270\u52a8\u68c0\u6d4b\u4e2dF1\u5206\u6570\u8fbe0.44\uff0c\u4e3a\u68ee\u6797\u7ba1\u7406\u3001\u8870\u9000\u65e9\u671f\u9884\u8b66\u53ca\u78b3\u635f\u5931\u91cf\u5316\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "motivation": "\u6b27\u6d32\u68ee\u6797\u78b3\u6c47\u80fd\u529b\u8fd1\u671f\u4e0b\u964d\uff0c\u4e9f\u9700\u7a7a\u95f4\u663e\u5f0f\u4e14\u9ad8\u9891\u66f4\u65b0\u7684\u68ee\u6797\u76d1\u6d4b\u624b\u6bb5\uff1b\u800c\u73b0\u6709\u57fa\u4e8e\u536b\u661f\u7684\u6270\u52a8\u4ea7\u54c1\u5206\u8fa8\u7387\u4e0d\u8db3\uff0c\u96be\u4ee5\u68c0\u6d4b\u5355\u682a\u6811\u6728\u5c3a\u5ea6\uff08\u901a\u5e38\u5c0f\u4e8e100\u5e73\u65b9\u7c73\uff09\u7684\u53d8\u5316\u3002", "method": "\u5229\u75282014\u20132024\u5e74SPOT-6/7\u5e74\u5ea6\u5408\u6210\u5f71\u50cf\uff0c\u7ed3\u5408\u57fa\u4e8e\u9ad8\u5206\u8fa8\u7387\u673a\u8f7d\u6fc0\u5149\u626b\u63cf\uff08ALS\uff09\u6570\u636e\u8bad\u7ec3\u7684\u5c42\u6b21\u5316Transformer\u6a21\u578b\uff08PVTv2\uff09\u4f30\u7b971.5\u7c73\u5206\u8fa8\u7387\u7684\u68ee\u6797\u51a0\u5c42\u9ad8\u5ea6\uff1b\u5f00\u53d1\u5305\u542b\u914d\u51c6\u4e0e\u65f6\u7a7a\u603b\u53d8\u5206\u53bb\u566a\u7684\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u8de8\u5f02\u6784\u5f71\u50cf\u7684\u7a33\u5065\u53d8\u5316\u68c0\u6d4b\uff0c\u751f\u6210\u5e74\u5ea6\u6270\u52a8\u56feFORMSpoT-\u0394\u3002", "result": "\u572819\u4e2aALS\u91cd\u8bbf\u7ad9\u70b9\u548c5,087\u4e2a\u6cd5\u56fd\u56fd\u5bb6\u68ee\u6797\u6e05\u67e5\u6837\u5730\u4e2d\u9a8c\u8bc1\u8868\u660e\uff0cFORMSpoT-\u0394\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6270\u52a8\u4ea7\u54c1\uff1b\u5728\u6270\u52a8\u5c0f\u4e14\u7a7a\u95f4\u7834\u788e\u7684\u5c71\u5730\u68ee\u6797\u4e2d\uff0c\u5176F1\u5206\u6570\u8fbe0.44\uff0c\u6bd4\u73b0\u6709\u57fa\u51c6\u9ad8\u51fa\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "FORMSpoT-\u0394\u5b9e\u73b0\u4e86\u56fd\u5bb6\u7ea7\u5c3a\u5ea6\u7684\u6811\u6728\u7ea7\u522b\u68ee\u6797\u52a8\u6001\u76d1\u6d4b\uff0c\u4e3a\u5206\u6790\u68ee\u6797\u7ba1\u7406\u5b9e\u8df5\u3001\u65e9\u671f\u53d1\u73b0\u68ee\u6797\u8870\u9000\u4fe1\u53f7\u4ee5\u53ca\u66f4\u51c6\u786e\u91cf\u5316\u5982\u95f4\u4f10\u6216\u62e9\u4f10\u7b49\u7ec6\u5fae\u6270\u52a8\u9020\u6210\u7684\u78b3\u635f\u5931\u63d0\u4f9b\u4e86\u72ec\u7279\u5de5\u5177\uff1b\u7814\u7a76\u5f3a\u8c03\u4e86\u7ef4\u6301SPOT\u7b49\u8d85\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u4efb\u52a1\u53caDINAMIS\u7b49\u5f00\u653e\u6570\u636e\u8ba1\u5212\u5bf9\u6c14\u5019\u53d8\u5316\u4e0b\u68ee\u6797\u76d1\u6d4b\u7684\u91cd\u8981\u6027\u3002", "summary_cn": "\u8fd1\u671f\u6b27\u6d32\u68ee\u6797\u78b3\u6c47\u7684\u4e0b\u964d\u51f8\u663e\u4e86\u5bf9\u7a7a\u95f4\u663e\u5f0f\u4e14\u9891\u7e41\u66f4\u65b0\u7684\u68ee\u6797\u76d1\u6d4b\u5de5\u5177\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u4e8e\u536b\u661f\u7684\u6270\u52a8\u4ea7\u54c1\u5206\u8fa8\u7387\u4ecd\u7136\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u65e0\u6cd5\u68c0\u6d4b\u5355\u682a\u6811\u6728\u5c3a\u5ea6\uff08\u901a\u5e38\u5c0f\u4e8e100\u5e73\u65b9\u7c73\uff09\u7684\u53d8\u5316\u3002\u672c\u6587\u63d0\u51fa\u4e86FORMSpoT\uff08Forest Mapping with SPOT Time series\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5341\u5e74\uff082014\u20132024\u5e74\uff09SPOT\u536b\u661f\u65f6\u5e8f\u6570\u636e\uff0c\u4ee51.5\u7c73\u5206\u8fa8\u7387\u7ed8\u5236\u4e86\u6cd5\u56fd\u672c\u571f\u7684\u68ee\u6797\u51a0\u5c42\u9ad8\u5ea6\u56fe\uff0c\u5e76\u540c\u65f6\u751f\u6210\u4e86\u5e74\u5ea6\u6270\u52a8\u56fe\uff08FORMSpoT-\u0394\uff09\u3002\u51a0\u5c42\u9ad8\u5ea6\u7531\u5e74\u5ea6SPOT-6/7\u5408\u6210\u5f71\u50cf\u901a\u8fc7\u4e00\u4e2a\u5c42\u6b21\u5316Transformer\u6a21\u578b\uff08PVTv2\uff09\u63a8\u5bfc\u5f97\u51fa\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u673a\u8f7d\u6fc0\u5149\u626b\u63cf\uff08ALS\uff09\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u5b9e\u73b0\u8de8\u5f02\u6784\u5f71\u50cf\u91c7\u96c6\u6761\u4ef6\u4e0b\u7684\u7a33\u5065\u53d8\u5316\u68c0\u6d4b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u5957\u4e13\u7528\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u7ed3\u5408\u5f71\u50cf\u914d\u51c6\u4e0e\u65f6\u7a7a\u603b\u53d8\u5206\u53bb\u566a\u6280\u672f\u3002\u572819\u4e2aALS\u91cd\u8bbf\u7ad9\u70b9\u548c5,087\u4e2a\u6cd5\u56fd\u56fd\u5bb6\u68ee\u6797\u6e05\u67e5\u6837\u5730\u4e0a\u7684\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0cFORMSpoT-\u0394\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6270\u52a8\u4ea7\u54c1\u3002\u5728\u6270\u52a8\u89c4\u6a21\u5c0f\u4e14\u7a7a\u95f4\u7834\u788e\u7684\u5c71\u5730\u68ee\u6797\u4e2d\uff0cFORMSpoT-\u0394\u7684F1\u5206\u6570\u8fbe\u52300.44\uff0c\u6bd4\u73b0\u6709\u57fa\u51c6\u9ad8\u51fa\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u901a\u8fc7\u5b9e\u73b0\u56fd\u5bb6\u7ea7\u5c3a\u5ea6\u7684\u6811\u6728\u7ea7\u522b\u68ee\u6797\u52a8\u6001\u76d1\u6d4b\uff0cFORMSpoT-\u0394\u4e3a\u5206\u6790\u68ee\u6797\u7ba1\u7406\u5b9e\u8df5\u3001\u65e9\u671f\u8bc6\u522b\u68ee\u6797\u8870\u9000\u4fe1\u53f7\uff0c\u4ee5\u53ca\u66f4\u7cbe\u786e\u91cf\u5316\u7531\u95f4\u4f10\u6216\u62e9\u4f10\u7b49\u7ec6\u5fae\u6270\u52a8\u5f15\u8d77\u7684\u78b3\u635f\u5931\u63d0\u4f9b\u4e86\u72ec\u7279\u5de5\u5177\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u6301\u7eed\u652f\u6301SPOT\u7b49\u8d85\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u4efb\u52a1\u4ee5\u53caDINAMIS\u7b49\u5f00\u653e\u6570\u636e\u5021\u8bae\u5bf9\u4e8e\u5728\u6c14\u5019\u53d8\u5316\u80cc\u666f\u4e0b\u76d1\u6d4b\u68ee\u6797\u7684\u5173\u952e\u610f\u4e49\u3002"}}
