<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

TL;DR: 本文提出在下游任务模型训练中引入新型鲁棒性损失，有效降低计算病理学基础模型对技术变异的敏感性，在不重新训练基础模型的前提下提升模型鲁棒性与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前组织病理学基础模型不仅学习生物学相关特征，还捕获了前处理和扫描仪相关的技术变异，导致下游任务模型预测存在偏差，影响其在真实临床场景中的泛化能力。

Method: 在下游任务特定模型训练过程中引入新型鲁棒性损失；基于来自6155名患者的27,042张全切片图像（WSI），利用8种主流基础模型提取特征，训练数千个下游模型进行系统评估。

Result: 所提方法显著提升了模型对技术变异的鲁棒性，同时通过聚焦生物学相关特征提高了预测准确性。

Conclusion: 该方法无需重新训练基础模型即可有效缓解其在计算病理学中的鲁棒性问题，有助于开发适用于常规临床实践的可靠计算病理模型。

Abstract: Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

Abstract (中文翻译): 组织病理学中的基础模型有望促进高性能且可泛化的深度学习系统的发展。然而，当前的模型不仅捕捉了生物学相关的特征，还包含了前处理和扫描仪特异性带来的技术变异，从而导致基于这些基础模型特征训练的任务特定模型产生有偏预测。本文表明，在下游任务特定模型的训练过程中引入新颖的鲁棒性损失，可以降低模型对技术变异的敏感性。我们设计了一个全面的实验体系，包含来自6155名患者的27,042张全切片图像（WSI），并基于八种流行的计算病理学基础模型提取的特征训练了数千个模型。除了显著提升鲁棒性外，我们还观察到，通过聚焦于生物学相关特征，预测准确性也得到了提高。我们的方法成功缓解了计算病理学基础模型的鲁棒性问题，且无需重新训练基础模型本身，从而推动了适用于常规临床实践真实世界数据的鲁棒计算病理模型的开发。

</details>


### [2] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

TL;DR: MNAS-Unet 是一种结合蒙特卡洛树搜索（MCTS）与神经架构搜索（NAS）的新型医学图像分割框架，通过 MCTS 动态探索高效网络结构，在多个数据集上实现优于现有方法的分割精度，同时显著降低搜索成本和模型参数量。


<details>
  <summary>Details</summary>
Motivation: 传统 NAS 方法在医学图像分割中存在搜索效率低、计算资源消耗大等问题，限制了其实际应用。本文旨在通过引入 MCTS 提高架构搜索效率，并优化单元结构以兼顾精度与轻量化。

Method: 提出 MNAS-Unet 框架，将 MCTS 与 NAS 相结合，动态探索有潜力的网络架构；同时设计并优化 DownSC 和 UpSC 单元结构，以实现快速且精确的模型调整。

Result: 在 PROMISE12、Ultrasound Nerve 和 CHAOS 等医学图像数据集上，MNAS-Unet 的分割精度优于 NAS-Unet 及其他先进模型；架构搜索预算减少 54%（139 vs. 300 epochs），模型仅含 0.6M 参数，GPU 内存占用更低。

Conclusion: MNAS-Unet 在保持高分割精度的同时显著提升了搜索效率并降低了资源消耗，具备更强的实际部署潜力。

Abstract: This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

Abstract (中文翻译): 本文提出了一种新颖的医学图像分割框架 MNAS-Unet，该框架结合了蒙特卡洛树搜索（MCTS）与神经架构搜索（NAS）。MNAS-Unet 通过 MCTS 动态探索有前景的网络架构，显著提升了架构搜索的效率与准确性。此外，该方法还优化了 DownSC 和 UpSC 单元结构，从而实现快速而精确的模型调整。实验结果表明，MNAS-Unet 在包括 PROMISE12、超声神经（Ultrasound Nerve）和 CHAOS 在内的多个医学图像数据集上，其分割精度均优于 NAS-Unet 及其他当前最先进的模型。此外，与 NAS-Unet 相比，MNAS-Unet 在相同搜索设置下将架构搜索预算减少了 54%（提前在第 139 轮停止，而 NAS-Unet 需要 300 轮），同时获得了一个仅含 0.6M 参数的轻量级模型，并降低了 GPU 内存消耗，进一步提升了其实用性。这些结果表明，MNAS-Unet 能在实际资源约束下提高搜索效率，同时保持具有竞争力的分割精度。

</details>


### [3] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

TL;DR: AeroDGS is a physics-guided 4D Gaussian splatting method for monocular UAV videos that addresses depth ambiguity and unstable motion estimation in dynamic aerial scenes by combining geometry lifting and physics-based priors.


<details>
  <summary>Details</summary>
Motivation: Existing 4D reconstruction methods struggle with monocular aerial videos due to challenges like single-view capture, wide spatial coverage, small object footprint, and large motion disparity, leading to depth ambiguity and unreliable motion estimation.

Method: AeroDGS uses a Monocular Geometry Lifting module to reconstruct static and dynamic geometry from a single aerial sequence, and a Physics-Guided Optimization module that enforces ground-support, upright-stability, and trajectory-smoothness priors to resolve monocular ambiguity and ensure physically plausible motion.

Result: Experiments on synthetic and real UAV datasets show that AeroDGS outperforms state-of-the-art methods in reconstruction fidelity for dynamic aerial environments.

Conclusion: AeroDGS effectively addresses the ill-posed nature of monocular dynamic aerial reconstruction by integrating geometric inference with physical constraints, enabling robust and coherent 4D scene modeling from UAV videos.

Abstract: Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

Abstract (中文翻译): 近期在4D场景重建方面的进展显著提升了多个领域中的动态建模能力。然而，现有方法在航拍条件下仍存在局限性，尤其是在单视角拍摄、大空间范围、以及空间占比较小但运动差异较大的动态物体场景中。这些挑战导致严重的深度模糊和不稳定的运动估计，使得单目航拍重建本质上是一个病态问题。为此，我们提出了AeroDGS——一种面向单目无人机视频的物理引导4D高斯泼溅框架。AeroDGS引入了一个单目几何提升模块，可从单段航拍序列中重建可靠的静态与动态几何结构，为动态估计提供稳健基础。为进一步解决单目模糊问题，我们提出了一个物理引导优化模块，融合了可微分的地面支撑、直立稳定性及轨迹平滑性先验，将模糊的图像线索转化为物理一致的运动。该框架联合优化静态背景与动态实体，实现稳定几何结构与连贯的时间演化。此外，我们构建了一个覆盖多种飞行高度和运动条件的真实世界无人机数据集，用于评估动态航拍重建性能。在合成与真实无人机场景上的实验表明，AeroDGS优于当前最先进的方法，在动态航拍环境中实现了更优的重建保真度。

</details>


### [4] [Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention](https://arxiv.org/abs/2602.22381)
*Zhengkang Fan,Chengkun Sun,Russell Terry,Jie Xu,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出一种无需手动分割的深度学习框架，通过引入器官聚焦注意力（OFA）损失函数，在不依赖肿瘤区域分割的情况下实现肾肿瘤恶性程度预测，性能优于传统基于分割的方法。


<details>
  <summary>Details</summary>
Motivation: 现有影像学方法在术前预测肾肿瘤恶性程度方面准确性不足，而当前深度学习方法通常依赖耗时且需专家参与的手动分割来提升性能，限制了临床应用效率。

Method: 开发了一种基于器官聚焦注意力（OFA）损失函数的深度学习框架，使图像块仅关注同属器官的其他图像块，从而在部署阶段无需对3D肾CT图像进行分割即可进行恶性预测。

Result: 在UF IDR私有数据集上达到AUC 0.685、F1-score 0.872；在KiTS21公开数据集上达到AUC 0.760、F1-score 0.852，优于依赖分割裁剪的传统模型。

Conclusion: 该方法无需显式分割即可提高恶性预测准确性，为肾癌诊断提供了一种更高效、可靠的临床决策支持工具。

Abstract: Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.

Abstract (中文翻译): 准确预测肾肿瘤的恶性程度对于指导临床决策和优化治疗策略至关重要。然而，现有的影像学方法在手术干预前尚无法可靠地预测恶性程度。尽管深度学习在利用3D CT图像进行恶性预测方面展现出潜力，但传统方法通常依赖人工分割来隔离肿瘤区域并减少噪声，从而提升预测性能。然而，人工分割费时费力、成本高昂，且依赖专家知识。本研究开发了一种深度学习框架，采用器官聚焦注意力（OFA）损失函数，调整图像块的注意力机制，使其仅关注其他属于同一器官的图像块。因此，在部署阶段无需对3D肾CT图像进行分割即可完成恶性预测。所提框架在佛罗里达大学综合数据存储库（UF IDR）的私有数据集上取得了AUC为0.685、F1分数为0.872的结果，在公开可用的KiTS21数据集上取得了AUC为0.760、F1分数为0.852的结果。这些结果优于依赖基于分割裁剪进行降噪的传统模型，表明该框架在无需显式分割输入的情况下仍能提升预测准确性。研究结果表明，该方法为肾癌恶性程度预测提供了一种更高效、可靠的手段，有助于改善肾癌诊断中的临床决策。

</details>


### [5] [Vision Transformers Need More Than Registers](https://arxiv.org/abs/2602.22394)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文发现Vision Transformer（ViT）中的伪影问题源于其“懒惰聚合”行为，即利用语义无关的背景块作为捷径来表示全局语义，并提出通过选择性地将图像块特征整合到CLS token中以缓解该问题，在多种监督范式下均取得性能提升。


<details>
  <summary>Details</summary>
Motivation: ViT在大规模数据上预训练后虽能提供通用表征，但在不同监督范式和下游任务中普遍存在伪影（artifacts）现象，其根本机制尚不明确，亟需深入分析与解释。

Method: 通过系统分析ViT中的伪影现象，识别其源于全局注意力机制和粗粒度语义监督所导致的“懒惰聚合”行为；提出一种选择性整合图像块特征至CLS token的方法，以削弱背景主导的捷径影响。

Result: 所提方法在12个基准数据集上，无论是在标签监督、文本监督还是自监督设置下，均一致提升了ViT的性能。

Conclusion: 本研究揭示了ViT伪影的根本成因，并提供了一种有效缓解策略，为理解ViT的行为提供了新视角。

Abstract: Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.

Abstract (中文翻译): 视觉Transformer（ViT）在大规模数据上进行预训练后，能够为各种下游任务提供通用表征。然而，在不同监督范式和下游任务中，ViT中普遍观察到伪影现象。通过对ViT中伪影的系统分析，我们发现其根本机制尚未得到充分阐明。本文通过系统分析得出结论：这些伪影源于一种“懒惰聚合”行为——ViT在全局注意力机制和粗粒度语义监督的驱动下，利用语义无关的背景图像块作为捷径来表示全局语义。我们的解决方案选择性地将图像块特征整合到CLS token中，从而减少由背景主导的捷径所带来的影响，并在标签监督、文本监督和自监督三种范式下的12个基准数据集上均持续提升了性能。我们希望这项工作能为理解ViT的行为提供新的视角。

</details>


### [6] [CLIP Is Shortsighted: Paying Attention Beyond the First Sentence](https://arxiv.org/abs/2602.22419)
*Marc-Antoine Lavoie,Anas Mahmoud,Aldo Zaimi,Arsene Fansi Tchango,Steven L. Waslander*

Main category: cs.CV

TL;DR: CLIP模型因训练数据中长文本描述通常以简短摘要开头，导致模型过度关注开头句子而忽略后续细节。本文提出DeBias-CLIP，通过在训练中移除摘要句、采用句子子采样和文本填充策略，使监督信号均匀分布于所有文本位置，从而提升长文本检索性能，并在短文本检索和句子顺序鲁棒性上表现更优。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在预训练时主要使用图像与简短标题配对的数据，使其偏向于学习显著物体的简单描述，在处理复杂场景和密集描述时对齐效果较差。尽管近期工作通过在小规模长标题数据集上微调来缓解该问题，但作者发现这些长标题（无论是人工还是LLM生成）普遍存在“先摘要后细节”的结构偏差，导致模型在训练中依赖开头句子作为捷径，削弱了对全文的对齐能力。

Method: 提出DeBias-CLIP方法：在训练过程中移除长标题中的首句摘要；引入句子子采样（sentence sub-sampling）和文本token填充（text token padding）机制，使监督信号均匀分布在所有文本token位置，从而避免模型过度关注开头部分。

Result: DeBias-CLIP在长文本检索任务上达到SOTA性能，同时提升了短文本检索效果，并对句子顺序的排列变化表现出更强的鲁棒性。该方法可直接替代Long-CLIP，且无需增加任何可训练参数。

Conclusion: 通过消除长文本描述中的摘要先行偏差并重新分配监督信号，DeBias-CLIP有效提升了CLIP模型对复杂、密集文本描述的对齐能力，是一种高效且即插即用的改进方案。

Abstract: CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.

Abstract (中文翻译): CLIP模型通过在互联网规模数据上进行图像-文本对比学习，学习可迁移的多模态特征，广泛应用于零样本分类、多模态检索、文本到图像扩散模型以及作为大型视觉-语言模型中的图像编码器。然而，CLIP的预训练主要依赖于与简短标题配对的图像，这使得模型倾向于编码显著物体的简单描述，导致在复杂场景和密集描述上的对齐较为粗糙。尽管近期工作通过在小规模长标题数据集上微调来缓解这一问题，但我们发现一个重要的共性偏差：无论是人工撰写还是大语言模型（LLM）生成的长标题，通常都以一句概括性摘要开头，随后才是详细描述。我们证明这种结构在训练中会形成“捷径”，使模型注意力集中在开头句子和早期token上，削弱了对标题其余部分的对齐能力。为解决此问题，我们提出了DeBias-CLIP：在训练过程中移除摘要句，并采用句子子采样和文本token填充策略，将监督信号均匀分布到所有token位置。DeBias-CLIP在长文本检索任务上达到最先进水平，同时提升了短文本检索性能，并对句子顺序的排列变化更具鲁棒性。该方法可作为Long-CLIP的即插即用替代方案，且不引入额外的可训练参数。

</details>


### [7] [SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read](https://arxiv.org/abs/2602.22426)
*Yibo Peng,Peng Xia,Ding Zhong,Kaide Zeng,Siwei Han,Yiyang Zhou,Jiaqi Liu,Ruiyi Zhang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文揭示了多模态大语言模型（MLLMs）在处理图像中文本时存在“模态惰性”问题，即倾向于依赖文本提示中的参数捷径而非真正读取图像中的文字。为此，作者提出可视化问题（VQ）设置进行诊断，并设计了一种名为SimpleOCR的即插即用训练策略，通过将训练样本转换为VQ格式并引入随机样式，强制模型利用视觉文本提取能力。该方法在多个OOD基准上显著提升性能，且数据效率极高。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）是否真正“阅读”图像中的嵌入文本，还是仅仅依赖文本提示中的参数捷径，这一关键问题尚未得到解答。作者旨在诊断并解决MLLMs在视觉基础任务中存在的“模态惰性”问题。

Method: 作者提出了可视化问题（Visualized-Question, VQ）设置，将文本查询直接渲染到图像上，以结构性地强制模型进行视觉交互。基于此，设计了SimpleOCR训练策略：将训练样本转换为VQ格式，并使用随机样式，从而无效化基于纯文本的捷径，迫使模型激活和优化其视觉文本提取通路。该方法无需修改模型架构，即可即插即用。

Result: 在Qwen2.5-VL上的诊断实验显示，尽管模型具备强大的OCR能力，但在VQ设置下性能最多下降12.7%。应用SimpleOCR后，在四个代表性OOD基准上，模型性能比基线模型提升5.4%，比基于原始图像的GRPO方法提升2.7%。此外，SimpleOCR具有极高的数据效率，仅用8.5K样本（为近期RL方法的1/30）就取得了更优性能，并能与NoisyRollout等高级RL策略无缝结合，带来互补性提升。

Conclusion: MLLMs普遍存在“模态惰性”，即在可选情况下会回避使用视觉信息。通过引入VQ设置和SimpleOCR训练策略，可以有效强制模型利用其内在的视觉文本理解能力，显著提升模型在需要真实视觉接地的任务上的鲁棒性和泛化能力，且该方法高效、通用、易于集成。

Abstract: Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.

Abstract (中文翻译): 尽管多模态大语言模型（MLLMs）发展迅速，但一个关键问题仍未得到解答：这些模型究竟是真正“阅读”图像中嵌入的文本，还是仅仅依赖文本提示中的参数捷径？在本研究中，我们通过引入“可视化问题”（Visualized-Question, VQ）设置来诊断这一问题，该设置将文本查询直接渲染到图像上，从结构上强制模型进行视觉交互。我们对Qwen2.5-VL的诊断实验揭示了一个惊人的能力-利用率差距：尽管模型具备强大的OCR能力，但在VQ设置下性能最多下降12.7%，暴露了其根深蒂固的“模态惰性”。为弥合这一差距，我们提出了SimpleOCR——一种即插即用的训练策略，它通过对学习过程施加结构性约束来解决问题。通过将训练样本转换为VQ格式并采用随机样式，SimpleOCR有效地消除了基于文本的捷径，迫使模型激活并优化其视觉文本提取通路。实验证明，SimpleOCR在无需修改模型架构的情况下带来了稳健的性能提升。在四个具有代表性的分布外（OOD）基准上，其性能超越了基线模型5.4%，也优于基于原始图像的GRPO方法2.7%。同时，它展现出极高的数据效率，仅用8.5K个样本（为近期基于强化学习方法所需样本量的1/30）就实现了更优性能。此外，其即插即用的特性使其能够与NoisyRollout等先进的强化学习策略无缝集成，产生互补性的改进。代码已开源：https://github.com/aiming-lab/SimpleOCR。

</details>


### [8] [Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge](https://arxiv.org/abs/2602.22455)
*Giuseppe Lando,Rosario Forte,Antonino Furnari*

Main category: cs.CV

TL;DR: 该论文研究了在边缘设备上使用多模态大语言模型（MLLMs）实现实时在线情景记忆问答的可行性，提出了一种双线程流水线架构，在消费级和企业级GPU上分别实现了51.76%和54.40%的准确率，接近云端方案的56.00%，同时兼顾隐私与低延迟。


<details>
  <summary>Details</summary>
Motivation: 云端卸载虽常见，但存在隐私泄露和延迟高的问题，尤其对可穿戴助手而言。因此，作者希望探索在边缘设备上部署MLLM以实现高效、隐私保护的情景记忆问答系统。

Method: 提出一个包含两个异步线程的问答流水线：Descriptor Thread持续将视频流转换为轻量级文本记忆；QA Thread基于该文本记忆进行推理回答问题，并在QAEgo4D-Closed基准上评估不同资源限制下的MLLM性能。

Result: 在消费级8GB GPU上达到51.76%准确率，TTFT为0.41秒；在本地企业级服务器上达到54.40%准确率，TTFT为0.88秒；相比之下，云端方案准确率为56.00%。

Conclusion: 边缘部署MLLM用于情景记忆问答具有实际可行性，在性能接近云端的同时保障了隐私和低延迟，展示了其在可穿戴设备等场景中的应用潜力。

Abstract: We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.

Abstract (中文翻译): 我们研究了使用多模态大语言模型（MLLMs）进行实时在线情景记忆问答的可行性。尽管云端卸载很常见，但它在可穿戴助手中引发了隐私和延迟问题，因此我们探索在边缘设备上的实现方案。我们将流式处理约束整合到问答流水线中，该流水线由两个异步线程组成：一个描述符线程（Descriptor Thread）持续将视频转换为轻量级文本记忆，另一个问答线程（QA Thread）则基于该文本记忆进行推理以回答查询。我们在QAEgo4D-Closed基准上分析了MLLM在严格资源限制下的性能，结果表明其表现颇具前景，甚至可与云端解决方案相媲美。具体而言，在消费级8GB GPU上运行的端到端配置实现了51.76%的准确率，首字生成时间（TTFT）为0.41秒；而在本地企业级服务器上扩展部署时，准确率达到54.40%，TTFT为0.88秒。相比之下，云端解决方案的准确率为56.00%。这些具有竞争力的结果凸显了基于边缘的方案在隐私保护型情景记忆检索中的潜力。

</details>


### [9] [MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation](https://arxiv.org/abs/2602.22462)
*Raiyan Jahangir,Nafiz Imtiaz Khan,Amritanand Sudheerkumar,Vladimir Filkov*

Main category: cs.CV

TL;DR: MammoWise is a local, open-source multi-model pipeline that leverages Vision Language Models (VLMs) for mammography report generation and multi-task classification, supporting flexible prompting strategies and optional RAG, with demonstrated improvements via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current VLM-based solutions for mammography reporting often rely on closed or tightly coupled systems, limiting privacy, reproducibility, and adaptability; there is a need for an open, local, and flexible framework.

Method: The authors propose MammoWise, a local pipeline using open-source VLMs (e.g., MedGemma, LLaVA-Med, Qwen2.5-VL) that supports zero-shot, few-shot, and Chain-of-Thought prompting, optionally enhanced with multimodal Retrieval Augmented Generation (RAG). They evaluate performance on VinDr-Mammo and DMID datasets across report quality and classification tasks, and apply QLoRA fine-tuning to improve reliability.

Result: Report generation is consistently strong and improves with few-shot prompting and RAG. Classification performance varies by model and dataset. QLoRA fine-tuning of MedGemma yields high accuracy: BI-RADS (0.7545), breast density (0.8840), and calcification (0.9341), while maintaining report quality.

Conclusion: MammoWise offers a practical, extensible, and reproducible local framework for deploying VLMs in mammography reporting and classification, balancing performance, privacy, and adaptability.

Abstract: Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

Abstract (中文翻译): 乳腺X线摄影筛查具有高工作量、时间敏感性和繁重的文档记录需求。放射科医生必须将细微的视觉发现转化为一致的BI-RADS评估、乳腺密度分类和结构化叙述报告。尽管近期的视觉语言模型（VLM）已实现图像到文本的报告生成，但许多系统依赖封闭的云平台或高度耦合的架构，限制了隐私性、可复现性和适应性。我们提出了MammoWise——一个本地多模型流水线，将开源VLM转化为乳腺X线摄影报告生成器和多任务分类器。MammoWise支持任何由Ollama托管的VLM和乳腺X线摄影数据集，并支持零样本、少样本和思维链提示，还可选地结合向量数据库实现多模态检索增强生成（RAG）以提供病例特定上下文。我们在VinDr-Mammo和DMID数据集上评估了MedGemma、LLaVA-Med和Qwen2.5-VL，衡量指标包括报告质量（BERTScore、ROUGE-L）、BI-RADS分类、乳腺密度和关键发现识别。结果表明，报告生成效果稳定，并在少样本提示和RAG加持下进一步提升；分类任务可行但对模型和数据集选择敏感。通过对MedGemma进行参数高效微调（QLoRA），在保持报告质量的同时显著提升了可靠性，实现了BI-RADS准确率0.7545、乳腺密度准确率0.8840和钙化准确率0.9341。MammoWise为在统一且可复现的工作流中部署本地VLM用于乳腺X线摄影报告提供了一个实用且可扩展的框架。

</details>


### [10] [Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models](https://arxiv.org/abs/2602.22469)
*Niamul Hassan Samin,Md Arifur Rahman,Abdullah Ibne Hanif,Juena Ahmed Noshin,Md Ashikur Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的推理时干预方法——空间信用重分配（SCR），通过将早期Transformer层中集中在稀疏视觉块上的激活重新分配到其上下文，有效缓解视觉语言模型（VLMs）中的幻觉问题，在多个基准上显著降低幻觉率，同时几乎不影响生成质量，并具有较低的推理开销。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）常在输入图像中不存在的对象上产生幻觉。作者发现这一问题源于“空间信用坍缩”：在早期Transformer层中，激活信用过度集中在稀疏的视觉块上，抑制了上下文证据，导致模型更依赖语言先验。

Method: 提出Spatial Credit Redistribution (SCR)方法，在推理阶段无需训练地将高注意力源块的隐藏状态激活重新分配给其上下文，该过程由低熵输入引导。通过注意力机制选择源块，而非随机选择。

Result: 在POPE和CHAIR基准上评估了6个模型系列（Chameleon、LLaVA、Qwen-VL/Qwen2-VL，参数规模7B–30B）。SCR在POPE-Adversarial上降低幻觉4.7–6.0个百分点；CHAIR-s降低3.7–5.2个百分点（相对42–51%）；CHAIR-i降低2.7–4.4个百分点（相对44–58%）；CIDEr得分下降不超过0.8个百分点。推理仅增加43–56ms延迟，优于OPERAs、VCD和OVCD。消融实验证明注意力引导的源选择至关重要。

Conclusion: 空间信用坍缩是VLM幻觉的关键原因，而SCR通过重分配早期层的激活信用，有效缓解该问题。该方法高效、实用，适用于实时场景，并在幻觉抑制与生成质量之间取得帕累托最优。

Abstract: Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.

Abstract (中文翻译): 视觉语言模型（VLMs）经常对输入图像中不存在的对象产生幻觉。我们将这一失败归因于“空间信用坍缩”：在早期Transformer层中，激活信用集中在稀疏的视觉块上，从而抑制了上下文证据并增强了对语言先验的依赖。我们提出了“空间信用重分配”（SCR），这是一种无需训练的推理阶段干预方法，它根据低熵输入的引导，将高注意力源块的隐藏状态激活重新分配给其上下文。我们在POPE和CHAIR基准上评估了六个模型系列（包括Chameleon、LLaVA以及Qwen-VL和Qwen2-VL，参数规模涵盖7B、13B和30B）。SCR在POPE-Adversarial上将幻觉率降低了约4.7–6.0个百分点，在CHAIR-s上降低了3.7–5.2个百分点（相对降低42–51%），在CHAIR-i上降低了2.7–4.4个百分点（相对降低44–58%），同时CIDEr得分下降不超过0.8个百分点。在低熵输入上效果最为显著，符合理论框架。SCR仅带来43–56毫秒的推理开销（小模型+43–46ms，大模型+54–56ms），约为OPERAs和VCD方法的1/3–1/6，也低于OVCD（+72ms）的1.3–1.7倍，并在幻觉率和CIDEr两个指标上均帕累托优于这三种方法，适合实时应用。控制性消融实验表明，基于注意力的源块选择至关重要：若替换为均匀随机选择，幻觉率改善将从约4.7–6.0个百分点降至仅2.6–3.4个百分点，进一步证明信用坍缩是幻觉的核心驱动因素。

</details>
