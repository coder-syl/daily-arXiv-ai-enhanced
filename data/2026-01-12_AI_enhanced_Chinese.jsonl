{"id": "2601.05328", "pdf": "https://arxiv.org/pdf/2601.05328", "abs": "https://arxiv.org/abs/2601.05328", "authors": ["Fenil R. Doshi", "Thomas Fel", "Talia Konkle", "George Alvarez"], "title": "Bi-Orthogonal Factor Decomposition for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.\n  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53cc\u6b63\u4ea4\u56e0\u5b50\u5206\u89e3\uff08BFD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u6790Vision Transformer\u4e2d\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u4f4d\u7f6e\u4e0e\u5185\u5bb9\u4fe1\u606f\u4e4b\u95f4\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5e76\u63ed\u793a\u4e86DINOv2\u7b49\u6a21\u578b\u5728\u4fe1\u606f\u5904\u7406\u4e0a\u7684\u7279\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9Vision Transformer\u4e2d\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u7406\u89e3\u7f3a\u4e4f\u539f\u5219\u6027\uff0c\u5c24\u5176\u662f\u4e0d\u6e05\u695a\u6ce8\u610f\u529b\u5728token\u4e4b\u95f4\u4ea4\u6362\u7684\u662f\u4f4d\u7f6e\u4fe1\u606f\u3001\u5185\u5bb9\u4fe1\u606f\uff0c\u8fd8\u662f\u4e24\u8005\u517c\u6709\u3002", "method": "\u63d0\u51fa\u53cc\u6b63\u4ea4\u56e0\u5b50\u5206\u89e3\uff08BFD\uff09\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8eANOVA\u7684\u5206\u89e3\u5c06token\u6fc0\u6d3b\u6b63\u4ea4\u5730\u89e3\u8026\u4e3a\u4f4d\u7f6e\u548c\u5185\u5bb9\u56e0\u5b50\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u67e5\u8be2-\u952e\u4ea4\u4e92\u77e9\u9635QK^T\u8fdb\u884c\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\uff0c\u63ed\u793a\u8fd9\u4e9b\u56e0\u5b50\u5982\u4f55\u901a\u8fc7\u53cc\u6b63\u4ea4\u6a21\u6001\u8fdb\u884c\u901a\u4fe1\u3002", "result": "\u5728\u4e3b\u6d41\u89c6\u89c9\u6a21\u578b\u4e0a\u5e94\u7528BFD\u53d1\u73b0\uff1a(i) \u6ce8\u610f\u529b\u4e3b\u8981\u901a\u8fc7\u5185\u5bb9\u8fdb\u884c\uff0c\u5185\u5bb9-\u5185\u5bb9\u4ea4\u4e92\u4e3b\u5bfc\u6ce8\u610f\u529b\u80fd\u91cf\uff0cDINOv2\u6bd4\u76d1\u7763\u6a21\u578b\u66f4\u5f3a\u8c03\u5185\u5bb9-\u4f4d\u7f6e\u8026\u5408\uff1b(ii) \u6ce8\u610f\u529b\u5934\u8868\u73b0\u51fa\u4e13\u4e1a\u5316\u5206\u5de5\uff0c\u5206\u4e3a\u5185\u5bb9-\u5185\u5bb9\u3001\u5185\u5bb9-\u4f4d\u7f6e\u548c\u4f4d\u7f6e-\u4f4d\u7f6e\u64cd\u4f5c\u5668\uff1b(iii) DINOv2\u5728\u4e2d\u95f4\u5c42\u540c\u65f6\u4fdd\u7559\u4f4d\u7f6e\u7ed3\u6784\u5e76\u4e30\u5bcc\u8bed\u4e49\u5185\u5bb9\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u4f18\u7684\u6574\u4f53\u5f62\u72b6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "BFD\u65b9\u6cd5\u6709\u6548\u63ed\u793a\u4e86Vision Transformer\u4e2dtoken\u901a\u8fc7\u6ce8\u610f\u529b\u4ea4\u4e92\u65f6\u6240\u4f9d\u8d56\u7684\u4fe1\u606f\u56e0\u5b50\uff08\u4f4d\u7f6e\u6216\u8bed\u4e49\uff09\uff0c\u4e3a\u7406\u89e3\u5176\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u6d1e\u89c1\u3002", "summary_cn": "\u81ea\u6ce8\u610f\u529b\u662f\u89c6\u89c9Transformer\u7684\u6838\u5fc3\u8ba1\u7b97\u539f\u8bed\uff0c\u4f46\u6211\u4eec\u5bf9\u5176\u5728token\u4e4b\u95f4\u4ea4\u6362\u4f55\u79cd\u4fe1\u606f\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u7406\u89e3\u3002\u6ce8\u610f\u529b\u56fe\u63cf\u8ff0\u4e86\u6743\u91cd\u8d28\u91cf\u96c6\u4e2d\u7684\u4f4d\u7f6e\uff0c\u5374\u672a\u63ed\u793a\u67e5\u8be2\u4e0e\u952e\u4e4b\u95f4\u4ea4\u6362\u7684\u662f\u4f4d\u7f6e\u3001\u5185\u5bb9\uff0c\u8fd8\u662f\u4e24\u8005\u517c\u6709\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u53cc\u6b63\u4ea4\u56e0\u5b50\u5206\u89e3\uff08Bi-orthogonal Factor Decomposition, BFD\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5206\u6790\u6846\u67b6\uff1a\u9996\u5148\uff0c\u57fa\u4e8e\u65b9\u5dee\u5206\u6790\uff08ANOVA\uff09\u7684\u5206\u89e3\u65b9\u6cd5\u5c06token\u6fc0\u6d3b\u5728\u7edf\u8ba1\u4e0a\u89e3\u8026\u4e3a\u6b63\u4ea4\u7684\u4f4d\u7f6e\u56e0\u5b50\u548c\u5185\u5bb9\u56e0\u5b50\uff1b\u5176\u6b21\uff0c\u5bf9\u67e5\u8be2-\u952e\u4ea4\u4e92\u77e9\u9635QK^T\u8fdb\u884c\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\uff0c\u63ed\u793a\u51fa\u53cc\u6b63\u4ea4\u6a21\u6001\uff0c\u4ece\u800c\u9610\u660e\u8fd9\u4e9b\u56e0\u5b50\u5982\u4f55\u4ecb\u5bfc\u4fe1\u606f\u4f20\u9012\u3002\u5728\u9a8c\u8bc1\u4e86\u4f4d\u7f6e\u4e0e\u5185\u5bb9\u56e0\u5b50\u7684\u6709\u6548\u5206\u79bb\u540e\uff0c\u6211\u4eec\u5c06BFD\u5e94\u7528\u4e8e\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u5e76\u53d1\u73b0\u4e86\u4e09\u79cd\u73b0\u8c61\uff1a(i) \u6ce8\u610f\u529b\u4e3b\u8981\u901a\u8fc7\u5185\u5bb9\u8fd0\u4f5c\uff0c\u5185\u5bb9-\u5185\u5bb9\u4ea4\u4e92\u4e3b\u5bfc\u6ce8\u610f\u529b\u80fd\u91cf\uff0c\u5176\u6b21\u662f\u5185\u5bb9-\u4f4d\u7f6e\u8026\u5408\uff1bDINOv2\u6bd4\u76d1\u7763\u6a21\u578b\u5206\u914d\u66f4\u591a\u80fd\u91cf\u4e8e\u5185\u5bb9-\u4f4d\u7f6e\u4ea4\u4e92\uff0c\u5e76\u5728\u66f4\u4e30\u5bcc\u7684\u6a21\u6001\u8c31\u4e0a\u5206\u5e03\u8ba1\u7b97\u3002(ii) \u6ce8\u610f\u529b\u673a\u5236\u8868\u73b0\u51fa\u4e13\u4e1a\u5316\uff1a\u6ce8\u610f\u529b\u5934\u5206\u5316\u4e3a\u5185\u5bb9-\u5185\u5bb9\u3001\u5185\u5bb9-\u4f4d\u7f6e\u548c\u4f4d\u7f6e-\u4f4d\u7f6e\u64cd\u4f5c\u5668\uff0c\u800c\u6bcf\u4e2a\u5934\u5185\u90e8\u7684\u5947\u5f02\u6a21\u6001\u4e5f\u5448\u73b0\u51fa\u7c7b\u4f3c\u7684\u4e13\u4e1a\u5316\u3002(iii) DINOv2\u5353\u8d8a\u7684\u6574\u4f53\u5f62\u72b6\u5904\u7406\u80fd\u529b\u6e90\u4e8e\u5176\u4e2d\u95f4\u5c42\u80fd\u591f\u540c\u65f6\u4fdd\u7559\u4f4d\u7f6e\u7ed3\u6784\u5e76\u4e0a\u4e0b\u6587\u5730\u4e30\u5bcc\u8bed\u4e49\u5185\u5bb9\u3002\u603b\u4f53\u800c\u8a00\uff0cBFD\u63ed\u793a\u4e86token\u5982\u4f55\u901a\u8fc7\u6ce8\u610f\u529b\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ee5\u53ca\u54ea\u4e9b\u4fe1\u606f\u56e0\u5b50\uff08\u4f4d\u7f6e\u6216\u8bed\u4e49\uff09\u4ecb\u5bfc\u4e86\u5b83\u4eec\u7684\u901a\u4fe1\uff0c\u4ece\u800c\u4e3a\u89c6\u89c9Transformer\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.05344", "pdf": "https://arxiv.org/pdf/2601.05344", "abs": "https://arxiv.org/abs/2601.05344", "authors": ["Sagi Eppel"], "title": "Coding the Visual World: From Image to Simulation Using Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIm2Sim\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6839\u636e\u771f\u5b9e\u56fe\u50cf\u751f\u6210\u6a21\u62df\u4ee3\u7801\u5e76\u91cd\u5efa\u56fe\u50cf\uff0c\u8bc4\u4f30\u5176\u5bf9\u590d\u6742\u7cfb\u7edf\u7684\u7406\u89e3\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e3b\u6d41VLM\uff08\u5982GPT\u3001Gemini\uff09\u5177\u5907\u9ad8\u5c42\u6b21\u7684\u7cfb\u7edf\u5efa\u6a21\u80fd\u529b\uff0c\u4f46\u5728\u7ec6\u8282\u548c\u4f4e\u5c42\u7ea7\u56fe\u6848\u590d\u73b0\u65b9\u9762\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u662f\u5426\u80fd\u591f\u8bc6\u522b\u5e76\u6a21\u62df\u56fe\u50cf\u4e2d\u6240\u63cf\u7ed8\u7684\u590d\u6742\u73b0\u5b9e\u7cfb\u7edf\uff0c\u4ece\u800c\u8861\u91cf\u5176\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u6784\u5efa\u4ee3\u8868\u6027\u7cfb\u7edf\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528Im2Sim\u65b9\u6cd5\uff1a\u7ed9\u5b9a\u4e00\u5f20\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\u7684\u81ea\u7136\u56fe\u50cf\uff08\u5982\u57ce\u5e02\u3001\u4e91\u3001\u690d\u88ab\u7b49\uff09\uff0c\u8981\u6c42VLM\u63cf\u8ff0\u8be5\u7cfb\u7edf\u5e76\u7f16\u5199\u53ef\u6267\u884c\u7684\u6a21\u62df\u4ee3\u7801\uff1b\u8fd0\u884c\u4ee3\u7801\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u4e0e\u539f\u59cb\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u6d8c\u73b0\u7cfb\u7edf\uff08\u5305\u62ec\u7269\u7406\u73b0\u8c61\u3001\u690d\u88ab\u3001\u57ce\u5e02\u3001\u6750\u6599\u548c\u5730\u8d28\u6784\u9020\uff09\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u9886\u5148VLM\u80fd\u8de8\u591a\u4e2a\u62bd\u8c61\u5c42\u6b21\u548c\u5e7f\u6cdb\u9886\u57df\u7406\u89e3\u5e76\u5efa\u6a21\u591a\u7ec4\u4ef6\u7cfb\u7edf\uff0c\u4f46\u96be\u4ee5\u590d\u73b0\u56fe\u50cf\u4e2d\u7684\u7cbe\u7ec6\u7ec6\u8282\u548c\u4f4e\u5c42\u7ea7\u56fe\u6848\u6392\u5e03\u3002", "conclusion": "VLM\u5c55\u73b0\u51fa\u9ad8\u5c42\u6b21\u3001\u6df1\u5c42\u6b21\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u611f\u77e5\u56fe\u50cf\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4f53\u73b0\u51fa\u201c\u9ad8\u7406\u89e3\u3001\u4f4e\u611f\u77e5\u201d\u7684\u4e0d\u5bf9\u79f0\u7279\u6027\u3002", "summary_cn": "\u6784\u5efa\u4e16\u754c\u7684\u5fc3\u7406\u6a21\u578b\u662f\u7406\u89e3\u7684\u6838\u5fc3\u65b9\u9762\u3002\u7c7b\u4f3c\u5730\uff0c\u89c6\u89c9\u7406\u89e3\u53ef\u88ab\u89c6\u4e3a\u6784\u5efa\u56fe\u50cf\u4e2d\u6240\u63cf\u7ed8\u7cfb\u7edf\u4e4b\u4ee3\u8868\u6027\u6a21\u578b\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u5229\u7528Im2Sim\u65b9\u6cd5\uff0c\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bc6\u522b\u5e76\u6a21\u62df\u56fe\u50cf\u4e2d\u6240\u5448\u73b0\u7cfb\u7edf\u4e0e\u673a\u5236\u7684\u80fd\u529b\u3002\u5177\u4f53\u800c\u8a00\uff0cVLM\u63a5\u6536\u4e00\u5f20\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\uff08\u5982\u57ce\u5e02\u3001\u4e91\u5c42\u3001\u690d\u88ab\u7b49\uff09\u7684\u81ea\u7136\u56fe\u50cf\uff0c\u5e76\u88ab\u8981\u6c42\u63cf\u8ff0\u8be5\u7cfb\u7edf\u5e76\u7f16\u5199\u53ef\u6a21\u62df\u5e76\u751f\u6210\u8be5\u7cfb\u7edf\u7684\u4ee3\u7801\u3002\u968f\u540e\u6267\u884c\u8be5\u751f\u6210\u4ee3\u7801\u4ee5\u4ea7\u751f\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u4e0e\u539f\u59cb\u56fe\u50cf\u8fdb\u884c\u5bf9\u6bd4\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u590d\u6742\u6d8c\u73b0\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u6db5\u76d6\u7269\u7406\u7cfb\u7edf\uff08\u5982\u6ce2\u6d6a\u3001\u5149\u7ebf\u3001\u4e91\uff09\u3001\u690d\u88ab\u3001\u57ce\u5e02\u3001\u6750\u6599\u53ca\u5730\u8d28\u6784\u9020\u7b49\u3002\u901a\u8fc7\u5bf9VLM\u751f\u6210\u7684\u6a21\u578b\u4e0e\u56fe\u50cf\u8fdb\u884c\u5206\u6790\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5176\u5bf9\u56fe\u50cf\u4e2d\u7cfb\u7edf\u7684\u7406\u89e3\u7a0b\u5ea6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u9886\u5148\u7684VLM\uff08\u5982GPT\u3001Gemini\uff09\u80fd\u591f\u5728\u591a\u4e2a\u62bd\u8c61\u5c42\u6b21\u548c\u5e7f\u6cdb\u9886\u57df\u4e2d\u7406\u89e3\u5e76\u5efa\u6a21\u590d\u6742\u7684\u591a\u7ec4\u4ef6\u7cfb\u7edf\uff0c\u4f46\u5728\u590d\u73b0\u56fe\u50cf\u4e2d\u7684\u7cbe\u7ec6\u7ec6\u8282\u548c\u4f4e\u5c42\u7ea7\u56fe\u6848\u6392\u5e03\u65b9\u9762\u80fd\u529b\u6709\u9650\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u4e00\u79cd\u6709\u8da3\u7684\u4e0d\u5bf9\u79f0\u6027\uff1aVLM\u7ed3\u5408\u4e86\u9ad8\u5c42\u6b21\u3001\u6df1\u5c42\u6b21\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5374\u5bf9\u7ec6\u8282\u611f\u77e5\u6709\u9650\u3002"}}
{"id": "2601.05364", "pdf": "https://arxiv.org/pdf/2601.05364", "abs": "https://arxiv.org/abs/2601.05364", "authors": ["Sudhakar Sah", "Ravish Kumar"], "title": "STResNet & STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 1 figure", "summary": "Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u7cfb\u5217STResNet\uff08\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\uff09\u548cSTYOLO\uff08\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\uff09\uff0c\u5728\u4fdd\u6301\u4f4e\u53c2\u6570\u91cf\u548c\u9ad8\u6548\u7387\u7684\u540c\u65f6\uff0c\u5728ImageNet\u548cMS COCO\u4e0a\u5206\u522b\u53d6\u5f97\u4e86\u4f18\u4e8eMobileNetV1\u3001ShuffleNetV2\u3001YOLOv5n\u548cYOLOX Nano\u7684\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4ee5\u727a\u7272\u51c6\u786e\u7387\u4e3a\u4ee3\u4ef7\u6362\u53d6\u4f4e\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5176\u5728\u5fae\u63a7\u5236\u5668\u548c\u795e\u7ecf\u5904\u7406\u5355\u5143\u7b49\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u540c\u65f6\u517c\u987e\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u5185\u5b58\u5360\u7528\u7684\u65b0\u578b\u67b6\u6784\u3002", "method": "\u63d0\u51faSTResNet\uff08\u4eceNano\u5230Tiny\u53d8\u4f53\uff09\u548cSTYOLO\uff08Micro\u4e0eMilli\u7248\u672c\uff09\u4e24\u4e2a\u6a21\u578b\u65cf\uff0c\u8054\u5408\u4f18\u5316\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u5185\u5b58\u5360\u7528\uff0c\u5e76\u5728ImageNet 1K\u548cMS COCO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "STResNetMilli\u4ec5\u7528300\u4e07\u53c2\u6570\u8fbe\u523070.0% ImageNet Top-1\u51c6\u786e\u7387\uff0c\u4f18\u4e8eMobileNetV1\u548cShuffleNetV2\uff1bSTYOLOMicro\u548cSTYOLOMilli\u5728MS COCO\u4e0a\u5206\u522b\u8fbe\u523030.5%\u548c33.6% mAP\uff0c\u8d85\u8d8aYOLOv5n\u548cYOLOX Nano\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684STResNet\u548cSTYOLO\u6a21\u578b\u65cf\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u5185\u5b58\u5360\u7528\u7684\u826f\u597d\u5e73\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "summary_cn": "\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u7684\u6700\u65b0\u8fdb\u5c55\u663e\u8457\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u7684\u90e8\u7f72\u6548\u7387\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u67b6\u6784\u4ecd\u7136\u4ee5\u727a\u7272\u51c6\u786e\u7387\u4e3a\u4ee3\u4ef7\u6362\u53d6\u4f4e\u5ef6\u8fdf\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u57fa\u4e8e\u5fae\u63a7\u5236\u5668\u548c\u795e\u7ecf\u5904\u7406\u5355\u5143\u8bbe\u5907\u4e0a\u7684\u9002\u7528\u6027\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u6a21\u578b\u7cfb\u5217\u2014\u2014\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684STResNet\u548c\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684STYOLO\uff0c\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5bf9\u51c6\u786e\u7387\u3001\u6548\u7387\u548c\u5185\u5b58\u5360\u7528\u8fdb\u884c\u4e86\u8054\u5408\u4f18\u5316\u3002\u6240\u63d0\u51fa\u7684STResNet\u7cfb\u5217\u5305\u62ec\u4eceNano\u5230Tiny\u7b49\u591a\u4e2a\u53d8\u4f53\uff0c\u5728\u56db\u767e\u4e07\u53c2\u6570\u9884\u7b97\u5185\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684ImageNet 1K\u51c6\u786e\u7387\u3002\u5177\u4f53\u800c\u8a00\uff0cSTResNetMilli\u4ec5\u4f7f\u7528\u4e09\u767e\u4e07\u53c2\u6570\u5c31\u8fbe\u5230\u4e8670.0%\u7684Top-1\u51c6\u786e\u7387\uff0c\u5728\u76f8\u8fd1\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0b\u4f18\u4e8eMobileNetV1\u548cShuffleNetV2\u3002\u5728\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\uff0cSTYOLOMicro\u548cSTYOLOMilli\u5728MS COCO\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230\u4e8630.5%\u548c33.6%\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\uff0c\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u4e0a\u5747\u8d85\u8d8a\u4e86YOLOv5n\u548cYOLOX Nano\u3002\u6b64\u5916\uff0c\u5f53STResNetMilli\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u4e0eUltralytics\u8bad\u7ec3\u73af\u5883\u7ed3\u5408\u4f7f\u7528\u65f6\u2026\u2026"}}
{"id": "2601.05368", "pdf": "https://arxiv.org/pdf/2601.05368", "abs": "https://arxiv.org/abs/2601.05368", "authors": ["Svitlana Morkva", "Maximum Wilder-Smith", "Michael Oechsle", "Alessio Tonioni", "Marco Hutter", "Vaishakh Patil"], "title": "MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments", "categories": ["cs.CV"], "comment": null, "summary": "We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.", "AI": {"tldr": "MOSAIC-GS \u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u9ad8\u6548\u5355\u76ee\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u79cd\u51e0\u4f55\u7ebf\u7d22\u548c\u8fd0\u52a8\u7ea6\u675f\uff0c\u5728\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u4f18\u5316\u4e0e\u5b9e\u65f6\u6e32\u67d3\u3002", "motivation": "\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u56e0\u7f3a\u4e4f\u591a\u89c6\u89d2\u7ea6\u675f\u800c\u672c\u8d28\u4e0d\u9002\u5b9a\uff0c\u96be\u4ee5\u51c6\u786e\u6062\u590d\u7269\u4f53\u51e0\u4f55\u7ed3\u6784\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u3001\u5149\u6d41\u3001\u52a8\u6001\u7269\u4f53\u5206\u5272\u548c\u70b9\u8ddf\u8e2a\u7b49\u591a\u51e0\u4f55\u7ebf\u7d22\uff0c\u7ed3\u5408\u521a\u6027\u8fd0\u52a8\u7ea6\u675f\uff0c\u5728\u521d\u59cb\u5316\u9636\u6bb5\u4f30\u8ba1\u521d\u6b65\u7684\u4e09\u7ef4\u573a\u666f\u52a8\u6001\uff1b\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u9759\u6001\u4e0e\u52a8\u6001\u90e8\u5206\uff0c\u52a8\u6001\u9ad8\u65af\u57fa\u5143\u4f7f\u7528\u65f6\u95f4\u76f8\u5173\u7684 Poly-Fourier \u66f2\u7ebf\u8868\u793a\u8f68\u8ff9\u4ee5\u9ad8\u6548\u7f16\u7801\u975e\u521a\u6027\u5f62\u53d8\u3002", "result": "\u5728\u6807\u51c6\u5355\u76ee\u52a8\u6001\u573a\u666f\u57fa\u51c6\u4e0a\uff0cMOSAIC-GS \u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u5feb\u7684\u4f18\u5316\u4e0e\u6e32\u67d3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "MOSAIC-GS \u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u96be\u9898\uff0c\u5728\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86 MOSAIC-GS\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u3001\u5b8c\u5168\u663e\u5f0f\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u9ad8\u4fdd\u771f\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u8fdb\u884c\u91cd\u5efa\u3002\u7531\u4e8e\u7f3a\u4e4f\u8db3\u591f\u7684\u591a\u89c6\u89d2\u7ea6\u675f\uff0c\u5355\u76ee\u91cd\u5efa\u672c\u8d28\u4e0a\u662f\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u4f7f\u5f97\u7cbe\u786e\u6062\u590d\u7269\u4f53\u51e0\u4f55\u7ed3\u6784\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u5c24\u4e3a\u56f0\u96be\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5229\u7528\u591a\u79cd\u51e0\u4f55\u7ebf\u7d22\uff0c\u5982\u6df1\u5ea6\u3001\u5149\u6d41\u3001\u52a8\u6001\u7269\u4f53\u5206\u5272\u548c\u70b9\u8ddf\u8e2a\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u521a\u6027\u7684\u8fd0\u52a8\u7ea6\u675f\uff0c\u5728\u521d\u59cb\u5316\u9636\u6bb5\u4f30\u8ba1\u521d\u6b65\u7684\u4e09\u7ef4\u573a\u666f\u52a8\u6001\u3002\u5728\u8fdb\u884c\u5149\u5ea6\u4f18\u5316\u4e4b\u524d\u6062\u590d\u573a\u666f\u52a8\u6001\uff0c\u53ef\u51cf\u5c11\u5bf9\u4ec5\u4f9d\u8d56\u89c6\u89c9\u5916\u89c2\u8fdb\u884c\u8fd0\u52a8\u63a8\u65ad\u7684\u4f9d\u8d56\uff0c\u540e\u8005\u5728\u5355\u76ee\u8bbe\u7f6e\u4e2d\u901a\u5e38\u5177\u6709\u6b67\u4e49\u6027\u3002\u4e3a\u4e86\u5b9e\u73b0\u7d27\u51d1\u8868\u793a\u3001\u5feb\u901f\u8bad\u7ec3\u548c\u5b9e\u65f6\u6e32\u67d3\uff0c\u540c\u65f6\u652f\u6301\u975e\u521a\u6027\u5f62\u53d8\uff0c\u6211\u4eec\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u9759\u6001\u548c\u52a8\u6001\u7ec4\u4ef6\u3002\u573a\u666f\u4e2d\u6bcf\u4e2a\u52a8\u6001\u9ad8\u65af\u57fa\u5143\u90fd\u88ab\u5206\u914d\u4e00\u6761\u8f68\u8ff9\uff0c\u8be5\u8f68\u8ff9\u4ee5\u65f6\u95f4\u76f8\u5173\u7684 Poly-Fourier \u66f2\u7ebf\u8868\u793a\uff0c\u4ee5\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u8fd0\u52a8\u7f16\u7801\u3002\u6211\u4eec\u5728\u6807\u51c6\u5355\u76ee\u52a8\u6001\u573a\u666f\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86 MOSAIC-GS \u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u548c\u6e32\u67d3\u901f\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2601.05373", "pdf": "https://arxiv.org/pdf/2601.05373", "abs": "https://arxiv.org/abs/2601.05373", "authors": ["Jorge Alberto Garza-Abdala", "Gerardo Alejandro Fumagal-Gonz\u00e1lez", "Beatriz A. Bosques-Palomo", "Mario Alexis Monsivais Molina", "Daly Avedano", "Servando Cardona-Huerta", "Jos\u00e9 Gerardo Tamez-Pena"], "title": "Ensemble of radiomics and ConvNeXt for breast cancer diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted and presented at the IEEE International Symposium on Computer-Based Medical Systems (CBMS) 2025", "summary": "Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u653e\u5c04\u7ec4\u5b66\u3001\u6df1\u5ea6\u5b66\u4e60\uff08ConvNeXtV1-small\uff09\u53ca\u4e8c\u8005\u96c6\u6210\u65b9\u6cd5\u5728\u4e73\u817a\u764c\u7b5b\u67e5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u96c6\u6210\u65b9\u6cd5\u5728\u4e24\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18AUC\uff080.87\uff09\uff0c\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\uff080.83\uff09\u6216\u653e\u5c04\u7ec4\u5b66\uff080.80\uff09\u3002", "motivation": "\u65e9\u671f\u8bca\u65ad\u4e73\u817a\u764c\u5bf9\u63d0\u9ad8\u60a3\u8005\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\uff0c\u800c\u653e\u5c04\u7ec4\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u8f85\u52a9\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u65e9\u671f\u68c0\u6d4b\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7cfb\u7edf\u8bc4\u4f30\u5e76\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u5728\u771f\u5b9e\u7b5b\u67e5\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\uff08RSNA 2023\u6311\u6218\u8d5b\u6570\u636e\u548c\u58a8\u897f\u54e5TecSalud\u961f\u5217\uff09\uff0c\u5206\u522b\u8bad\u7ec3\u548c\u9a8c\u8bc1ConvNeXtV1-small\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0e\u57fa\u4e8eTecSalud\u7684\u653e\u5c04\u7ec4\u5b66\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e00\u81f4\u7684\u65b9\u6cd5\u6784\u5efa\u548c\u6821\u51c6\u96c6\u6210\u6a21\u578b\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\u53d6\u5f97\u6700\u9ad8AUC\uff080.87\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08AUC=0.83\uff09\u548c\u653e\u5c04\u7ec4\u5b66\u6a21\u578b\uff08AUC=0.80\uff09\u3002", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u653e\u5c04\u7ec4\u5b66\u7684\u96c6\u6210\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4e73\u817a\u764c\u5728\u4e73\u817aX\u7ebf\u6444\u5f71\u4e2d\u7684\u8bca\u65ad\u6027\u80fd\u3002", "summary_cn": "\u4e73\u817a\u764c\u7684\u65e9\u671f\u8bca\u65ad\u5bf9\u4e8e\u63d0\u9ad8\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002\u653e\u5c04\u7ec4\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5728\u8f85\u52a9\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u65e9\u671f\u764c\u75c7\u68c0\u6d4b\u65b9\u9762\u5df2\u5c55\u73b0\u51fa\u663e\u8457\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u653e\u5c04\u7ec4\u5b66\u3001\u6df1\u5ea6\u5b66\u4e60\u4ee5\u53ca\u96c6\u6210\u6280\u672f\u5728\u7b5b\u67e5\u6027\u4e73\u817aX\u7ebf\u6444\u5f71\u4e2d\u68c0\u6d4b\u764c\u75c7\u7684\u6027\u80fd\u3002\u7814\u7a76\u4f7f\u7528\u4e86\u4e24\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\uff1aRSNA 2023\u4e73\u817a\u764c\u68c0\u6d4b\u6311\u6218\u8d5b\u6570\u636e\u96c6\uff0811,913\u540d\u60a3\u8005\uff09\u548c\u6765\u81ea\u58a8\u897f\u54e5TecSalud\u6570\u636e\u96c6\u7684\u961f\u5217\uff0819,400\u540d\u60a3\u8005\uff09\u3002ConvNeXtV1-small\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728RSNA\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728TecSalud\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff1b\u653e\u5c04\u7ec4\u5b66\u6a21\u578b\u5219\u57fa\u4e8eTecSalud\u6570\u636e\u96c6\u5f00\u53d1\uff0c\u5e76\u91c7\u7528\u201c\u7559\u4e00\u5e74\u4ea4\u53c9\u9a8c\u8bc1\u201d\u65b9\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u3002\u96c6\u6210\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u7b56\u7565\u5bf9\u4e24\u79cd\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u878d\u5408\u4e0e\u6821\u51c6\u3002\u7ed3\u679c\u663e\u793a\uff0c\u96c6\u6210\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUC\uff090.87\uff0c\u4f18\u4e8eConvNeXtV1-small\u76840.83\u548c\u653e\u5c04\u7ec4\u5b66\u76840.80\u3002\u7efc\u4e0a\u6240\u8ff0\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u653e\u5c04\u7ec4\u5b66\u9884\u6d4b\u7684\u96c6\u6210\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u4e73\u817aX\u7ebf\u6444\u5f71\u4e2d\u4e73\u817a\u764c\u7684\u8bca\u65ad\u6548\u679c\u3002"}}
{"id": "2601.05379", "pdf": "https://arxiv.org/pdf/2601.05379", "abs": "https://arxiv.org/abs/2601.05379", "authors": ["Vladimir Frants", "Sos Agaian", "Karen Panetta"], "title": "EdgeLDR: Quaternion Low-Displacement Rank Neural Networks for Edge-Efficient Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "Deploying deep neural networks on edge devices is often limited by the memory traffic and compute cost of dense linear operators. While quaternion neural networks improve parameter efficiency by coupling multiple channels through Hamilton products, they typically retain unstructured dense weights; conversely, structured matrices enable fast computation but are usually applied in the real domain. This paper introduces EdgeLDR, a practical framework for quaternion block-circulant linear and convolutional layers that combines quaternion channel mixing with block-circulant parameter structure and enables FFT-based evaluation through the complex adjoint representation. We present reference implementations of EdgeLDR layers and compare FFT-based computation against a naive spatial-domain realization of quaternion circulant products. FFT evaluation yields large empirical speedups over the naive implementation and keeps latency stable as block size increases, making larger compression factors computationally viable. We further integrate EdgeLDR layers into compact CNN and Transformer backbones and evaluate accuracy-compression trade-offs on 32x32 RGB classification (CIFAR-10/100, SVHN) and hyperspectral image classification (Houston 2013, Pavia University), reporting parameter counts and CPU/GPU latency. The results show that EdgeLDR layers provide significant compression with competitive accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEdgeLDR\uff0c\u4e00\u79cd\u7ed3\u5408\u56db\u5143\u6570\u901a\u9053\u6df7\u5408\u4e0e\u5757\u5faa\u73af\u7ed3\u6784\u7684\u9ad8\u6548\u7ebf\u6027/\u5377\u79ef\u5c42\u6846\u67b6\uff0c\u5229\u7528FFT\u52a0\u901f\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u538b\u7f29\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53d7\u9650\u4e8e\u5bc6\u96c6\u7ebf\u6027\u7b97\u5b50\u7684\u5185\u5b58\u6d41\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u56db\u5143\u6570\u63d0\u5347\u53c2\u6570\u6548\u7387\u4f46\u4fdd\u7559\u975e\u7ed3\u6784\u5316\u6743\u91cd\uff0c\u8981\u4e48\u4f7f\u7528\u5b9e\u57df\u7ed3\u6784\u5316\u77e9\u9635\u5b9e\u73b0\u5feb\u901f\u8ba1\u7b97\uff0c\u4f46\u672a\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u63d0\u51faEdgeLDR\u6846\u67b6\uff0c\u5c06\u56db\u5143\u6570\u901a\u9053\u6df7\u5408\u4e0e\u5757\u5faa\u73af\uff08block-circulant\uff09\u53c2\u6570\u7ed3\u6784\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u590d\u4f34\u968f\u8868\u793a\u5b9e\u73b0\u57fa\u4e8eFFT\u7684\u9ad8\u6548\u8ba1\u7b97\uff1b\u63d0\u4f9b\u53c2\u8003\u5b9e\u73b0\u5e76\u5bf9\u6bd4FFT\u4e0e\u6734\u7d20\u7a7a\u95f4\u57df\u5b9e\u73b0\u7684\u6027\u80fd\u3002", "result": "FFT\u5b9e\u73b0\u76f8\u6bd4\u6734\u7d20\u65b9\u6cd5\u5e26\u6765\u663e\u8457\u52a0\u901f\uff0c\u4e14\u968f\u5757\u5c3a\u5bf8\u589e\u5927\u5ef6\u8fdf\u4fdd\u6301\u7a33\u5b9a\uff1b\u5728CIFAR-10/100\u3001SVHN\u53ca\u9ad8\u5149\u8c31\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cEdgeLDR\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u53c2\u6570\u538b\u7f29\uff0c\u5e76\u62a5\u544a\u4e86CPU/GPU\u5ef6\u8fdf\u3002", "conclusion": "EdgeLDR\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u6709\u6548\u538b\u7f29\u6a21\u578b\u5e76\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u517c\u987e\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "summary_cn": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u53d7\u9650\u4e8e\u5bc6\u96c6\u7ebf\u6027\u7b97\u5b50\u6240\u5e26\u6765\u7684\u5185\u5b58\u6d41\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u3002\u5c3d\u7ba1\u56db\u5143\u6570\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u54c8\u5bc6\u987f\u79ef\u5c06\u591a\u4e2a\u901a\u9053\u8026\u5408\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4ecd\u4fdd\u7559\u975e\u7ed3\u6784\u5316\u7684\u5bc6\u96c6\u6743\u91cd\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u7ed3\u6784\u5316\u77e9\u9635\u867d\u80fd\u5b9e\u73b0\u5feb\u901f\u8ba1\u7b97\uff0c\u4f46\u901a\u5e38\u4ec5\u5e94\u7528\u4e8e\u5b9e\u6570\u57df\u3002\u672c\u6587\u63d0\u51fa\u4e86EdgeLDR\u2014\u2014\u4e00\u79cd\u5b9e\u7528\u7684\u56db\u5143\u6570\u5757\u5faa\u73af\u7ebf\u6027\u548c\u5377\u79ef\u5c42\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u56db\u5143\u6570\u901a\u9053\u6df7\u5408\u4e0e\u5757\u5faa\u73af\u53c2\u6570\u7ed3\u6784\u76f8\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u590d\u4f34\u968f\u8868\u793a\u5b9e\u73b0\u57fa\u4e8e\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\uff08FFT\uff09\u7684\u9ad8\u6548\u8ba1\u7b97\u3002\u6211\u4eec\u63d0\u4f9b\u4e86EdgeLDR\u5c42\u7684\u53c2\u8003\u5b9e\u73b0\uff0c\u5e76\u5c06\u57fa\u4e8eFFT\u7684\u8ba1\u7b97\u65b9\u5f0f\u4e0e\u56db\u5143\u6570\u5faa\u73af\u79ef\u7684\u6734\u7d20\u7a7a\u95f4\u57df\u5b9e\u73b0\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u5b9e\u9a8c\u8868\u660e\uff0cFFT\u8bc4\u4f30\u76f8\u6bd4\u6734\u7d20\u5b9e\u73b0\u5e26\u6765\u4e86\u663e\u8457\u7684\u7ecf\u9a8c\u52a0\u901f\uff0c\u4e14\u968f\u7740\u5757\u5927\u5c0f\u589e\u52a0\uff0c\u5ef6\u8fdf\u4fdd\u6301\u7a33\u5b9a\uff0c\u4ece\u800c\u4f7f\u66f4\u5927\u538b\u7f29\u56e0\u5b50\u5728\u8ba1\u7b97\u4e0a\u53d8\u5f97\u53ef\u884c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06EdgeLDR\u5c42\u96c6\u6210\u5230\u7d27\u51d1\u7684CNN\u548cTransformer\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u5e76\u572832x32 RGB\u56fe\u50cf\u5206\u7c7b\uff08CIFAR-10/100\u3001SVHN\uff09\u548c\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff08Houston 2013\u3001Pavia University\uff09\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u7cbe\u5ea6\u4e0e\u538b\u7f29\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u540c\u65f6\u62a5\u544a\u4e86\u53c2\u6570\u91cf\u53caCPU/GPU\u5ef6\u8fdf\u3002\u7ed3\u679c\u8868\u660e\uff0cEdgeLDR\u5c42\u5728\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6a21\u578b\u538b\u7f29\u3002"}}
{"id": "2601.05394", "pdf": "https://arxiv.org/pdf/2601.05394", "abs": "https://arxiv.org/abs/2601.05394", "authors": ["Yuang Shi", "Simone Gasparini", "G\u00e9raldine Morin", "Wei Tsang Ooi"], "title": "Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation", "categories": ["cs.CV", "cs.GR", "cs.MM", "eess.IV"], "comment": null, "summary": "We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.\n  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c063D\u9ad8\u65af\u5206\u4e3a\u201c\u8349\u56fe\u9ad8\u65af\u201d\uff08\u6355\u6349\u8fb9\u7f18\u7b49\u9ad8\u9891\u7279\u5f81\uff09\u548c\u201c\u8272\u5757\u9ad8\u65af\u201d\uff08\u8868\u793a\u5e73\u6ed1\u4f4e\u9891\u533a\u57df\uff09\u7684\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5206\u5c42\u6e10\u8fdb\u5f0f\u6d41\u5f0f\u4f20\u8f93\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u7c7b\u4e0e\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7d27\u51d1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u53d7\u4f20\u7edf\u7ed8\u753b\u4e2d\u5148\u52fe\u52d2\u8f6e\u5ed3\u518d\u586b\u5145\u8272\u5f69\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u89c2\u5bdf\u52303D\u9ad8\u65af\u5728\u573a\u666f\u8868\u793a\u4e2d\u4e5f\u5177\u6709\u7c7b\u4f3c\u5206\u5de5\uff1a\u90e8\u5206\u9ad8\u65af\u64c5\u957f\u8868\u8fbe\u9ad8\u9891\u8fb9\u754c\u4fe1\u606f\uff0c\u53e6\u4e00\u4e9b\u5219\u66f4\u9002\u5408\u63cf\u8ff0\u4f4e\u9891\u5e73\u6ed1\u533a\u57df\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u5bf9\u6b64\u8fdb\u884c\u533a\u5206\uff0c\u9650\u5236\u4e86\u538b\u7f29\u6548\u7387\u4e0e\u6e10\u8fdb\u5f0f\u6e32\u67d3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5c42\u6b21\u5316\u81ea\u9002\u5e94\u5206\u7c7b\u6846\u67b6\uff0c\u76f4\u63a5\u4f5c\u7528\u4e8e3DGS\u8868\u793a\uff0c\u5229\u7528\u591a\u51c6\u5219\u5bc6\u5ea6\u805a\u7c7b\u4e0e\u8d28\u91cf\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u7ec6\u5316\uff0c\u5c06\u9ad8\u65af\u5206\u4e3aSketch Gaussians\u548cPatch Gaussians\u4e24\u7c7b\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e83D\u7ebf\u6bb5\u5148\u9a8c\u3002", "result": "\u5728\u591a\u79cd\u5ba4\u5185\u5916\u573a\u666f\u4e0a\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4\u5747\u5300\u526a\u679d\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u6a21\u578b\u5927\u5c0f\u4e0bPSNR\u63d0\u5347\u6700\u9ad81.74 dB\u3001SSIM\u63d0\u53476.7%\u3001LPIPS\u6539\u558441.4%\uff1b\u5bf9\u4e8e\u5ba4\u5185\u573a\u666f\uff0c\u4ec5\u75280.5%\u539f\u59cb\u6a21\u578b\u5927\u5c0f\u5373\u53ef\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u6784\u611f\u77e5\u9ad8\u65af\u8868\u793a\u6709\u6548\u652f\u6301\u9ad8\u6548\u5b58\u50a8\u3001\u81ea\u9002\u5e94\u6d41\u5f0f\u4f20\u8f93\u548c\u9ad8\u8d28\u91cf\u6e32\u67d3\uff0c\u9002\u7528\u4e8e\u5e26\u5bbd\u53d7\u9650\u7f51\u7edc\u4e0e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002", "summary_cn": "\u6211\u4eec\u89c2\u5bdf\u5230\u9ad8\u65af\u5206\u5e03\u5c55\u73b0\u51fa\u4e0e\u4f20\u7edf\u827a\u672f\u6280\u6cd5\u76f8\u7c7b\u4f3c\u7684\u72ec\u7279\u89d2\u8272\u548c\u7279\u6027\u2014\u2014\u6b63\u5982\u827a\u672f\u5bb6\u5148\u52fe\u52d2\u8f6e\u5ed3\u518d\u4ee5\u8272\u5f69\u586b\u5145\u5927\u9762\u79ef\u533a\u57df\u4e00\u6837\uff0c\u67d0\u4e9b\u9ad8\u65af\u6355\u6349\u8fb9\u7f18\u548c\u8f6e\u5ed3\u7b49\u9ad8\u9891\u7279\u5f81\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u8868\u793a\u66f4\u5bbd\u5e7f\u3001\u66f4\u5e73\u6ed1\u7684\u533a\u57df\uff0c\u7c7b\u4f3c\u4e8e\u7528\u4e8e\u589e\u52a0\u4f53\u79ef\u611f\u548c\u6df1\u5ea6\u7684\u7b14\u89e6\u3002\u57fa\u4e8e\u8fd9\u4e00\u89c2\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u9ad8\u65af\u5206\u4e3a\u4e24\u7c7b\uff1a(i) \u8349\u56fe\u9ad8\u65af\uff08Sketch Gaussians\uff09\uff0c\u7528\u4e8e\u8868\u793a\u9ad8\u9891\u3001\u5b9a\u4e49\u8fb9\u754c\u7684\u7279\u5f81\uff1b(ii) \u8272\u5757\u9ad8\u65af\uff08Patch Gaussians\uff09\uff0c\u7528\u4e8e\u8986\u76d6\u4f4e\u9891\u3001\u5e73\u6ed1\u7684\u533a\u57df\u3002\u8fd9\u79cd\u8bed\u4e49\u4e0a\u7684\u5206\u79bb\u81ea\u7136\u5730\u652f\u6301\u5206\u5c42\u6e10\u8fdb\u5f0f\u6d41\u5f0f\u4f20\u8f93\uff1a\u7d27\u51d1\u7684\u8349\u56fe\u9ad8\u65af\u9996\u5148\u6784\u5efa\u7ed3\u6784\u9aa8\u67b6\uff0c\u968f\u540e\u8272\u5757\u9ad8\u65af\u9010\u6b65\u7ec6\u5316\u4f53\u79ef\u7ec6\u8282\u3002  \n\u5728\u672c\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5c42\u6b21\u5316\u81ea\u9002\u5e94\u5206\u7c7b\u6846\u67b6\uff0c\u5c06\u5148\u524d\u65b9\u6cd5\u6269\u5c55\u81f3\u4efb\u610f3D\u573a\u666f\uff0c\u8be5\u6846\u67b6\u76f4\u63a5\u4f5c\u7528\u4e8e3DGS\u8868\u793a\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7ed3\u5408\u591a\u51c6\u5219\u5bc6\u5ea6\u805a\u7c7b\u4e0e\u81ea\u9002\u5e94\u7684\u8d28\u91cf\u9a71\u52a8\u7ec6\u5316\u7b56\u7565\uff0c\u65e2\u6d88\u9664\u4e86\u5bf9\u5916\u90e83D\u7ebf\u6bb5\u56fe\u5143\u7684\u4f9d\u8d56\uff0c\u53c8\u786e\u4fdd\u4e86\u53c2\u6570\u7f16\u7801\u7684\u6700\u4f18\u6548\u7387\u3002\u5728\u5305\u62ec\u4eba\u9020\u4e0e\u81ea\u7136\u73af\u5883\u5728\u5185\u7684\u591a\u6837\u5316\u573a\u666f\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u5747\u5300\u526a\u679d\u57fa\u7ebf\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u5728\u76f8\u540c\u6a21\u578b\u5927\u5c0f\u4e0bPSNR\u6700\u9ad8\u63d0\u53471.74 dB\uff0cSSIM\u63d0\u53476.7%\uff0cLPIPS\u6539\u558441.4%\u3002\u5bf9\u4e8e\u5ba4\u5185\u573a\u666f\uff0c\u4ec5\u9700\u539f\u59cb\u6a21\u578b\u5927\u5c0f\u76840.5%\u5373\u53ef\u7ef4\u6301\u826f\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u3002\u8fd9\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u8868\u793a\u65b9\u5f0f\u80fd\u591f\u9ad8\u6548\u5730\u652f\u6301\u9ad8\u4fdd\u771f3D\u5185\u5bb9\u5728\u5e26\u5bbd\u53d7\u9650\u7f51\u7edc\u548c\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b58\u50a8\u3001\u81ea\u9002\u5e94\u6d41\u5f0f\u4f20\u8f93\u4e0e\u6e32\u67d3\u3002"}}
{"id": "2601.05399", "pdf": "https://arxiv.org/pdf/2601.05399", "abs": "https://arxiv.org/abs/2601.05399", "authors": ["Zhaohui Liang", "Sivaramakrishnan Rajaraman", "Niccolo Marini", "Zhiyun Xue", "Sameer Antani"], "title": "Multi-task Cross-modal Learning for Chest X-ray Image Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u5bf9BiomedCLIP\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u63d0\u5347\u80f8\u90e8X\u5149\uff08CXR\uff09\u56fe\u50cf\u4e0e\u653e\u5c04\u5b66\u62a5\u544a\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "CLIP\u548cBiomedCLIP\u867d\u5177\u5907\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u5d4c\u5165\u80fd\u529b\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u533b\u5b66\u68c0\u7d22\u4efb\u52a1\uff08\u5982\u57fa\u4e8eCXR\u56fe\u50cf\u67e5\u8be2\u76f8\u5173\u653e\u5c04\u5b66\u62a5\u544a\uff09\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "\u4ee5BiomedCLIP\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7MLP\u6295\u5f71\u5934\uff0c\u5e76\u91c7\u7528\u5305\u542b\u4e09\u9879\u635f\u5931\u7684\u590d\u5408\u591a\u4efb\u52a1\u635f\u5931\u51fd\u6570\uff1a(1) \u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u7528\u4e8e\u533a\u5206\u6b63\u5e38\u4e0e\u5f02\u5e38CXR\uff1b(2) \u6709\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u589e\u5f3a\u7c7b\u5185\u4e00\u81f4\u6027\uff1b(3) CLIP\u635f\u5931\u4fdd\u6301\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u56fe\u50cf\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u539f\u59cbBiomedCLIP\u548c\u901a\u7528CLIP\u6a21\u578b\uff0ct-SNE\u53ef\u89c6\u5316\u4e5f\u663e\u793a\u6b63\u5e38\u4e0e\u5f02\u5e38\u6837\u672c\u8bed\u4e49\u805a\u7c7b\u66f4\u6e05\u6670\u3002", "conclusion": "\u9886\u57df\u81ea\u9002\u5e94\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u751f\u7269\u533b\u5b66\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u8bca\u65ad\u654f\u611f\u6027\u3002", "summary_cn": "CLIP \u548c BiomedCLIP \u662f\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u7684\u4ee3\u8868\uff0c\u80fd\u591f\u63d0\u4f9b\u5f3a\u5927\u7684\u8de8\u6a21\u6001\u5d4c\u5165\u80fd\u529b\uff1b\u7136\u800c\uff0c\u5b83\u4eec\u5e76\u672a\u9488\u5bf9\u7ec6\u7c92\u5ea6\u533b\u5b66\u68c0\u7d22\u4efb\u52a1\uff08\u4f8b\u5982\u4f7f\u7528\u80f8\u90e8X\u5149\uff08CXR\uff09\u56fe\u50cf\u67e5\u8be2\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\u7684\u653e\u5c04\u5b66\u62a5\u544a\uff09\u8fdb\u884c\u4f18\u5316\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u5bf9 BiomedCLIP \u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u8bc4\u4f30\u5176\u5728 CXR \u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6539\u8fdb\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u4ee5 BiomedCLIP \u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f15\u5165\u4e00\u4e2a\u8f7b\u91cf\u7ea7 MLP \u6295\u5f71\u5934\uff0c\u5e76\u91c7\u7528\u5305\u542b\u4e09\u9879\u635f\u5931\u7684\u590d\u5408\u591a\u4efb\u52a1\u635f\u5931\u51fd\u6570\uff1a(1) \u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u7528\u4e8e\u533a\u5206\u6b63\u5e38\u4e0e\u5f02\u5e38 CXR \u68c0\u67e5\uff1b(2) \u6709\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u7c7b\u5185\u4e00\u81f4\u6027\uff1b(3) CLIP \u635f\u5931\uff0c\u4ee5\u7ef4\u6301\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u9884\u8bad\u7ec3\u7684 BiomedCLIP \u548c\u901a\u7528 CLIP \u6a21\u578b\u76f8\u6bd4\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u56fe\u50cf\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u66f4\u5747\u8861\u4e14\u66f4\u5177\u4e34\u5e8a\u610f\u4e49\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0ct-SNE \u53ef\u89c6\u5316\u7ed3\u679c\u663e\u793a\u6b63\u5e38\u4e0e\u5f02\u5e38\u75c5\u4f8b\u7684\u8bed\u4e49\u805a\u7c7b\u66f4\u52a0\u6e05\u6670\uff0c\u4f53\u73b0\u51fa\u6a21\u578b\u66f4\u5f3a\u7684\u8bca\u65ad\u654f\u611f\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u9886\u57df\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u5b66\u4e60\u5728\u63a8\u52a8\u751f\u7269\u533b\u5b66\u8de8\u6a21\u6001\u68c0\u7d22\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
{"id": "2601.05432", "pdf": "https://arxiv.org/pdf/2601.05432", "abs": "https://arxiv.org/abs/2601.05432", "authors": ["Yuxiang Ji", "Yong Wang", "Ziyu Ma", "Yiming Hu", "Hailang Huang", "Xuecai Hu", "Guanhua Chen", "Liaoni Wu", "Xiangxiang Chu"], "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \\textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to \\textit{Gemini-3-Pro} with Google Search/Map grounded mode.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5730\u56fe\u63a8\u7406\u7684\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d4b\u4e88\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u201c\u5730\u56fe\u601d\u7ef4\u201d\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u4e0e\u5e76\u884c\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\uff0c\u5728\u65b0\u6784\u5efa\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6MAPBench\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u867d\u5229\u7528\u4e86\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5ffd\u7565\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u5c06\u5730\u56fe\u6574\u5408\u8fdb\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u4e3a\u6a21\u578b\u5f15\u5165\u201c\u5730\u56fe\u601d\u7ef4\u201d\u80fd\u529b\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff1a\u5148\u8fdb\u884c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee5\u63d0\u5347\u91c7\u6837\u6548\u7387\uff0c\u518d\u901a\u8fc7\u5e76\u884c\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u63a2\u7d22\u591a\u6761\u5019\u9009\u8def\u5f84\u540e\u518d\u505a\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u65b0\u63d0\u51fa\u7684MAPBench\u771f\u5b9e\u56fe\u50cf\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u5c24\u5176\u5c06Acc@500m\u4eceGemini-3-Pro\uff08\u542f\u7528Google\u641c\u7d22/\u5730\u56fe\u6a21\u5f0f\uff09\u76848.0%\u63d0\u5347\u81f322.1%\u3002", "conclusion": "\u7ed3\u5408\u5730\u56fe\u63a8\u7406\u4e0e\u667a\u80fd\u4f53\u673a\u5236\u80fd\u663e\u8457\u63d0\u5347\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\uff0c\u6240\u63d0\u65b9\u6cd5\u548cMAPBench\u57fa\u51c6\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b0\u601d\u8def\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "summary_cn": "\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u5728\u5168\u7403\u4efb\u610f\u4f4d\u7f6e\u7684\u62cd\u6444\u5730\u70b9\u3002\u73b0\u6709\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u65b9\u6cd5\u867d\u7136\u5229\u7528\u4e86\u4e16\u754c\u77e5\u8bc6\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u5374\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u4e00\u79cd\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u8d4b\u4e88\u6a21\u578b\u201c\u5730\u56fe\u601d\u7ef4\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u3002\u6211\u4eec\u4e3a\u5176\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u3002\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u667a\u80fd\u4f53\u80fd\u529b\u4ee5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u800c\u5e76\u884cTTS\u4f7f\u6a21\u578b\u5728\u505a\u51fa\u6700\u7ec8\u9884\u6d4b\u524d\u80fd\u591f\u63a2\u7d22\u591a\u6761\u5019\u9009\u8def\u5f84\uff0c\u8fd9\u5bf9\u5730\u7406\u5b9a\u4f4d\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u5728\u6700\u65b0\u4e14\u771f\u5b9e\u573a\u666f\u7684\u56fe\u50cf\u4e0a\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86MAPBench\u2014\u2014\u4e00\u4e2a\u5b8c\u5168\u7531\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6784\u6210\u7684\u7efc\u5408\u6027\u5730\u7406\u5b9a\u4f4d\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728Acc@500m\u6307\u6807\u4e0a\uff0c\u76f8\u6bd4\u542f\u7528\u4e86Google\u641c\u7d22/\u5730\u56fe\u6a21\u5f0f\u7684Gemini-3-Pro\uff0c\u4ece8.0%\u63d0\u5347\u81f322.1%\u3002"}}
{"id": "2601.05446", "pdf": "https://arxiv.org/pdf/2601.05446", "abs": "https://arxiv.org/abs/2601.05446", "authors": ["Hongyang Xie", "Hongyang He", "Victor Sanchez"], "title": "TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": "Published in BMVC 2025 see: https://bmva-archive.org.uk/bmvc/2025/assets/papers/Paper_709/paper.pdf. Conference version. 12 pages, 6 figures, 4 tables. Author-prepared version", "summary": "Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTAPM-Net\uff0c\u901a\u8fc7\u5efa\u6a21\u7ea2\u5916\u5c0f\u76ee\u6807\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5f15\u53d1\u7684\u6270\u52a8\u8f68\u8ff9\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u76ee\u6807\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5f15\u53d1\u7684\u65b9\u5411\u6027\u3001\u9010\u5c42\u6270\u52a8\u884c\u4e3a\u7684\u5efa\u6a21\u673a\u5236\uff0c\u96be\u4ee5\u6709\u6548\u533a\u5206\u4fe1\u53f7\u4e0e\u7ed3\u6784\u5316\u566a\u58f0\u3002", "method": "\u63d0\u51faTAPM-Net\u7f51\u7edc\uff0c\u5305\u542b\u6270\u52a8\u5f15\u5bfc\u8def\u5f84\u6a21\u5757\uff08PGM\uff09\u548c\u8f68\u8ff9\u611f\u77e5\u72b6\u6001\u5757\uff08TASB\uff09\u3002PGM\u6784\u5efa\u6270\u52a8\u80fd\u573a\u5e76\u63d0\u53d6\u68af\u5ea6\u8ddf\u968f\u7684\u7279\u5f81\u8f68\u8ff9\uff1bTASB\u57fa\u4e8eMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u6cbf\u8f68\u8ff9\u8fdb\u884c\u52a8\u6001\u4f20\u64ad\uff0c\u5e76\u878d\u5408\u8bcd\u7ea7\u4e0e\u53e5\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u5b9e\u73b0\u5404\u5411\u5f02\u6027\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u72b6\u6001\u8f6c\u79fb\u3002", "result": "\u5728NUAA-SIRST\u548cIRSTD-1K\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "TAPM-Net\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u76ee\u6807\u6270\u52a8\u7684\u7a7a\u95f4\u6269\u6563\u884c\u4e3a\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "summary_cn": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff08ISTD\uff09\u7531\u4e8e\u4fe1\u53f7\u5bf9\u6bd4\u5ea6\u5f31\u3001\u7a7a\u95f4\u8303\u56f4\u6709\u9650\u4ee5\u53ca\u80cc\u666f\u6742\u6ce2\u5e72\u6270\uff0c\u957f\u671f\u4ee5\u6765\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5c3d\u7ba1\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9Transformer\uff08ViT\uff09\u5728\u6027\u80fd\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u4e00\u79cd\u673a\u5236\u6765\u8ffd\u8e2a\u5c0f\u76ee\u6807\u5982\u4f55\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5f15\u53d1\u65b9\u5411\u6027\u3001\u9010\u5c42\u7684\u6270\u52a8\uff0c\u800c\u8fd9\u4e00\u673a\u5236\u5bf9\u4e8e\u5728\u7ea2\u5916\u573a\u666f\u4e2d\u533a\u5206\u771f\u5b9e\u4fe1\u53f7\u4e0e\u7ed3\u6784\u5316\u566a\u58f0\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8f68\u8ff9\u611f\u77e5Mamba\u4f20\u64ad\u7f51\u7edc\uff08TAPM-Net\uff09\uff0c\u8be5\u7f51\u7edc\u663e\u5f0f\u5efa\u6a21\u7531\u76ee\u6807\u5f15\u8d77\u7684\u7279\u5f81\u6270\u52a8\u7684\u7a7a\u95f4\u6269\u6563\u884c\u4e3a\u3002TAPM-Net\u5305\u542b\u4e24\u4e2a\u65b0\u9896\u7ec4\u4ef6\uff1a\u6270\u52a8\u5f15\u5bfc\u8def\u5f84\u6a21\u5757\uff08PGM\uff09\u548c\u8f68\u8ff9\u611f\u77e5\u72b6\u6001\u5757\uff08TASB\uff09\u3002PGM\u4ece\u591a\u5c42\u7ea7\u7279\u5f81\u4e2d\u6784\u5efa\u6270\u52a8\u80fd\u573a\uff0c\u5e76\u63d0\u53d6\u53cd\u6620\u5c40\u90e8\u54cd\u5e94\u65b9\u5411\u6027\u7684\u68af\u5ea6\u8ddf\u968f\u7279\u5f81\u8f68\u8ff9\uff1b\u8fd9\u4e9b\u8f68\u8ff9\u968f\u540e\u8f93\u5165\u5230\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u5355\u5143TASB\u4e2d\uff0c\u8be5\u5355\u5143\u6cbf\u6bcf\u6761\u8f68\u8ff9\u5efa\u6a21\u52a8\u6001\u4f20\u64ad\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u901f\u5ea6\u7ea6\u675f\u7684\u6269\u6563\u673a\u5236\u4ee5\u53ca\u6765\u81ea\u8bcd\u7ea7\u548c\u53e5\u7ea7\u5d4c\u5165\u7684\u8bed\u4e49\u5bf9\u9f50\u7279\u5f81\u878d\u5408\u3002\u4e0e\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cTAPM-Net\u80fd\u591f\u5728\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u6cbf\u7a7a\u95f4\u8f68\u8ff9\u7684\u5404\u5411\u5f02\u6027\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u72b6\u6001\u8f6c\u79fb\u3002\u5728NUAA-SIRST\u548cIRSTD-1K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTAPM-Net\u5728\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
