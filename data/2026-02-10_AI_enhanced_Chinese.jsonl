{"id": "2602.07006", "pdf": "https://arxiv.org/pdf/2602.07006", "abs": "https://arxiv.org/abs/2602.07006", "authors": ["Alokesh Manna", "Neil Spencer", "Dipak K. Dey"], "title": "Scalable spatial point process models for forensic footwear analysis", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u978b\u5370\u201c\u5076\u7136\u7279\u5f81\u201d\uff08\u5982\u78e8\u635f\u3001\u5212\u75d5\uff09\u7684\u7a00\u6709\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6cd5\u5ead\u79d1\u5b66\u4e2d\u978b\u5370\u8bc1\u636e\u7684\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u72af\u7f6a\u73b0\u573a\u63d0\u53d6\u7684\u978b\u5370\u867d\u53ef\u8bc6\u522b\u978b\u6b3e\u578b\u53f7\uff0c\u4f46\u7531\u4e8e\u540c\u6b3e\u978b\u5927\u91cf\u751f\u4ea7\uff0c\u4ec5\u9760\u578b\u53f7\u5339\u914d\u4e0d\u8db3\u4ee5\u786e\u8ba4\u552f\u4e00\u6027\u3002\u56e0\u6b64\u9700\u5206\u6790\u978b\u5e95\u4f7f\u7528\u540e\u4ea7\u751f\u7684\u72ec\u7279\u201c\u5076\u7136\u7279\u5f81\u201d\uff0c\u5e76\u91cf\u5316\u5176\u7a00\u6709\u7a0b\u5ea6\uff0c\u4ee5\u51c6\u786e\u8bc4\u4f30\u8bc1\u636e\u5f3a\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\uff1a1\uff09\u91c7\u7528\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u5d4c\u5957\u62c9\u666e\u62c9\u65af\u79ef\u5206\u8fd1\u4f3c\uff08INLA\uff09\uff0c\u5b9e\u73b0\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u978b\u5370\u6570\u636e\u7684\u9ad8\u6548\u63a8\u65ad\uff1b2\uff09\u5f15\u5165\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\uff0c\u5efa\u6a21\u978b\u5e95\u82b1\u7eb9\u4e0e\u5076\u7136\u7279\u5f81\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u978b\u5370\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u66f4\u6709\u6548\u5730\u5efa\u6a21\u5076\u7136\u7279\u5f81\u7684\u7a7a\u95f4\u5206\u5e03\u53ca\u5176\u4e0e\u978b\u5e95\u82b1\u7eb9\u7684\u5173\u7cfb\uff0c\u4e3a\u6cd5\u5ead\u79d1\u5b66\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u978b\u5370\u8bc1\u636e\u8bc4\u4f30\u5de5\u5177\u3002", "summary_cn": "\u4ece\u72af\u7f6a\u73b0\u573a\u63d0\u53d6\u7684\u978b\u5370\u8bc1\u636e\u5728\u6cd5\u5ead\u8c03\u67e5\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u901a\u8fc7\u68c0\u67e5\u978b\u5370\uff0c\u8c03\u67e5\u4eba\u5458\u53ef\u4ee5\u786e\u5b9a\u5acc\u7591\u4eba\u6240\u7a7f\u978b\u5c65\u7684\u7ec6\u8282\u3002\u7136\u800c\uff0c\u4ec5\u8bc1\u660e\u5acc\u7591\u4eba\u7684\u978b\u5b50\u4e0e\u72af\u7f6a\u73b0\u573a\u978b\u5370\u7684\u54c1\u724c\u548c\u578b\u53f7\u4e00\u81f4\u53ef\u80fd\u5e76\u4e0d\u5145\u5206\uff0c\u56e0\u4e3a\u901a\u5e38\u4f1a\u6709\u6210\u5343\u4e0a\u4e07\u53cc\u76f8\u540c\u5c3a\u7801\u3001\u54c1\u724c\u548c\u578b\u53f7\u7684\u978b\u5b50\u88ab\u5236\u9020\u51fa\u6765\uff0c\u5176\u4e2d\u4efb\u4f55\u4e00\u53cc\u90fd\u53ef\u80fd\u662f\u8be5\u978b\u5370\u7684\u6765\u6e90\u3002\u56e0\u6b64\uff0c\u8c03\u67e5\u4eba\u5458\u5e38\u7528\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\u68c0\u67e5\u978b\u5370\u662f\u5426\u5b58\u5728\u201c\u5076\u7136\u7279\u5f81\u201d\uff08accidentals\uff09\uff0c\u5373\u8d2d\u4e70\u540e\u56e0\u7a7f\u7740\u800c\u5728\u978b\u5e95\u79ef\u7d2f\u7684\u5272\u75d5\u3001\u522e\u75d5\u7b49\u7279\u5f81\u3002\u5c3d\u7ba1\u67d0\u4e9b\u7c7b\u578b\u7684\u978b\u5b50\u4e0a\u4f1a\u51fa\u73b0\u5e38\u89c1\u7684\u5076\u7136\u7279\u5f81\u6a21\u5f0f\uff0c\u4f46\u53e6\u4e00\u4e9b\u5219\u9ad8\u5ea6\u72ec\u7279\uff0c\u6709\u53ef\u80fd\u5c06\u5acc\u7591\u4eba\u7684\u978b\u5b50\u4e0e\u5176\u4ed6\u6240\u6709\u540c\u6b3e\u978b\u533a\u5206\u5f00\u6765\u3002\u56e0\u6b64\uff0c\u91cf\u5316\u67d0\u79cd\u5076\u7136\u7279\u5f81\u6a21\u5f0f\u7684\u7a00\u6709\u6027\u5bf9\u4e8e\u51c6\u786e\u8861\u91cf\u6cd5\u5ead\u8bc1\u636e\u7684\u5f3a\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u4efb\u52a1\u3002\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u6539\u8fdb\u4e3b\u8981\u4f53\u73b0\u5728\u4e24\u4e2a\u65b9\u9762\uff1a\u9996\u5148\uff0c\u6211\u4eec\u5c06\u65b9\u6cd5\u6784\u5efa\u4e3a\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\uff0c\u4ece\u800c\u501f\u52a9\u5d4c\u5957\u62c9\u666e\u62c9\u65af\u79ef\u5206\u8fd1\u4f3c\uff08INLA\uff09\u9ad8\u6548\u5730\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6807\u6ce8\u978b\u5370\u6570\u636e\u96c6\u7684\u63a8\u65ad\uff1b\u5176\u6b21\uff0c\u6211\u4eec\u5f15\u5165\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\uff0c\u4ee5\u5efa\u6a21\u978b\u5e95\u82b1\u7eb9\u4e0e\u5076\u7136\u7279\u5f81\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6211\u4eec\u5728\u4fdd\u7559\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u66f4\u4f18\uff0c\u63d0\u5347\u4e86\u6cd5\u5ead\u978b\u5370\u5206\u6790\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2602.07008", "pdf": "https://arxiv.org/pdf/2602.07008", "abs": "https://arxiv.org/abs/2602.07008", "authors": ["Ruoyu Chen", "Shangquan Sun", "Xiaoqing Guo", "Sanyi Zhang", "Kangwei Liu", "Shiming Liu", "Zhangcheng Wang", "Qunli Zhang", "Hua Zhang", "Xiaochun Cao"], "title": "Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u7ea6\u675f\u6a21\u578b\u7684\u51b3\u7b56\u4f9d\u636e\u533a\u57df\uff0c\u4f7f\u5176\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u8bc1\u636e\u533a\u57df\u4e00\u81f4\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u4ec5\u63d0\u4f9b\u7c7b\u522b\u6807\u7b7e\uff0c\u5bfc\u81f4\u6a21\u578b\u4f9d\u8d56\u6377\u5f84\u76f8\u5173\u6027\u800c\u975e\u771f\u6b63\u8bc1\u636e\u8fdb\u884c\u9884\u6d4b\uff1b\u5c3d\u7ba1\u4eba\u7c7b\u5148\u9a8c\u53ef\u7ea6\u675f\u6b64\u7c7b\u884c\u4e3a\uff0c\u4f46\u6a21\u578b\u5b66\u5230\u7684\u8868\u793a\u5e38\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u6709\u6548\u5bf9\u9f50\u3002", "method": "\u5c06\u4eba\u7c7b\u5148\u9a8c\u7f16\u7801\u4e3a\u8f93\u5165\u4e2d\u671f\u671b\u6a21\u578b\u4f9d\u8d56\u7684\u533a\u57df\uff08\u5982\u8fb9\u754c\u6846\uff09\uff0c\u5229\u7528\u9ad8\u4fdd\u771f\u7684\u5b50\u96c6\u9009\u62e9\u5f52\u56e0\u65b9\u6cd5\u5728\u8bad\u7ec3\u4e2d\u63ed\u793a\u6a21\u578b\u7684\u51b3\u7b56\u4f9d\u636e\uff0c\u5e76\u5728\u5f52\u56e0\u533a\u57df\u504f\u79bb\u5148\u9a8c\u533a\u57df\u65f6\u65bd\u52a0\u60e9\u7f5a\uff0c\u5f15\u5bfc\u6a21\u578b\u5c06\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u9884\u671f\u533a\u57df\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u57fa\u4e8eMLLM\u7684GUI\u667a\u80fd\u4f53\u70b9\u51fb\u51b3\u7b56\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u5e38\u89c4\u5206\u7c7b\u548c\u81ea\u56de\u5f52\u751f\u6210\u8bbe\u7f6e\u4e0b\uff0c\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u5747\u63d0\u5347\u4e86\u4efb\u52a1\u51c6\u786e\u7387\u548c\u51b3\u7b56\u5408\u7406\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5f52\u56e0\u7ea6\u675f\uff0c\u4f7f\u6a21\u578b\u51b3\u7b56\u4f9d\u636e\u4e0e\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4e5f\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u5408\u7406\u6027\u3002", "summary_cn": "\u53ef\u9760\u7684\u6a21\u578b\u4e0d\u4ec5\u5e94\u505a\u51fa\u6b63\u786e\u9884\u6d4b\uff0c\u8fd8\u5e94\u4ee5\u53ef\u63a5\u53d7\u7684\u8bc1\u636e\u6765\u89e3\u91ca\u5176\u51b3\u7b56\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u901a\u5e38\u4ec5\u63d0\u4f9b\u7c7b\u522b\u7ea7\u522b\u7684\u6807\u7b7e\uff0c\u4f7f\u5f97\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u6377\u5f84\u76f8\u5173\u6027\u800c\u975e\u9884\u671f\u8bc1\u636e\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\u3002\u4eba\u7c7b\u5148\u9a8c\u6709\u52a9\u4e8e\u7ea6\u675f\u6b64\u7c7b\u884c\u4e3a\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u5b66\u4e60\u5230\u7684\u8868\u793a\u5e38\u5e38\u4e0e\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u504f\u5dee\uff0c\u5c06\u6a21\u578b\u4e0e\u8fd9\u4e9b\u5148\u9a8c\u5bf9\u9f50\u4ecd\u5177\u6311\u6218\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\u3002\u6211\u4eec\u5c06\u4eba\u7c7b\u5148\u9a8c\u7f16\u7801\u4e3a\u6a21\u578b\u5e94\u4f9d\u8d56\u7684\u8f93\u5165\u533a\u57df\uff08\u4f8b\u5982\u8fb9\u754c\u6846\uff09\uff0c\u5e76\u5229\u7528\u4e00\u79cd\u9ad8\u5ea6\u4fdd\u771f\u7684\u57fa\u4e8e\u5b50\u96c6\u9009\u62e9\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u63ed\u793a\u6a21\u578b\u7684\u51b3\u7b56\u4f9d\u636e\u3002\u5f53\u5f52\u56e0\u533a\u57df\u663e\u8457\u504f\u79bb\u5148\u9a8c\u533a\u57df\u65f6\uff0c\u6211\u4eec\u5bf9\u6a21\u578b\u4f9d\u8d56\u975e\u5148\u9a8c\u8bc1\u636e\u7684\u884c\u4e3a\u65bd\u52a0\u60e9\u7f5a\uff0c\u4ece\u800c\u9f13\u52b1\u5176\u5c06\u5f52\u56e0\u8f6c\u5411\u9884\u671f\u533a\u57df\u3002\u8fd9\u901a\u8fc7\u4e00\u4e2a\u7531\u4eba\u7c7b\u5148\u9a8c\u8bf1\u5bfc\u7684\u5f52\u56e0\u7ea6\u675f\u8bad\u7ec3\u76ee\u6807\u5b9e\u73b0\u3002\u6211\u4eec\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u548c\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684GUI\u667a\u80fd\u4f53\u70b9\u51fb\u51b3\u7b56\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002\u5728\u5e38\u89c4\u5206\u7c7b\u548c\u81ea\u56de\u5f52\u751f\u6210\u8bbe\u7f6e\u4e0b\uff0c\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u59cb\u7ec8\u63d0\u5347\u4e86\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u51b3\u7b56\u7684\u5408\u7406\u6027\u3002"}}
{"id": "2602.07011", "pdf": "https://arxiv.org/pdf/2602.07011", "abs": "https://arxiv.org/abs/2602.07011", "authors": ["Zhuonan Wang", "Zhenxuan Fan", "Siwen Tan", "Yu Zhong", "Yuqian Yuan", "Haoyuan Li", "Hao Jiang", "Wenqiao Zhang", "Feifei Shao", "Hongwei Wang", "Jun Xiao"], "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "9 pages, 5 figures", "summary": "As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MAU-Set\u6570\u636e\u96c6\u548cMAU-GPT\u6a21\u578b\uff0c\u7528\u4e8e\u591a\u7c7b\u578b\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u4ea7\u54c1\u56fe\u50cf\u5206\u6790\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u8db3\u548c\u6a21\u578b\u5728\u591a\u6837\u590d\u6742\u5f02\u5e38\u6a21\u5f0f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u6784\u5efa\u4e86\u6db5\u76d6\u591a\u5de5\u4e1a\u9886\u57df\u7684MAU-Set\u6570\u636e\u96c6\u53ca\u914d\u5957\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u63d0\u51faMAU-GPT\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u578bAMoE-LoRA\u673a\u5236\u7edf\u4e00\u5f02\u5e38\u611f\u77e5\u4e0e\u901a\u7528\u4e13\u5bb6\u9002\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAU-GPT\u5728\u6240\u6709\u9886\u57df\u5747\u4e00\u81f4\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MAU-GPT\u5728\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u7684\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002", "summary_cn": "\u968f\u7740\u5de5\u4e1a\u5236\u9020\u89c4\u6a21\u7684\u6269\u5927\uff0c\u81ea\u52a8\u5316\u7ec6\u7c92\u5ea6\u4ea7\u54c1\u56fe\u50cf\u5206\u6790\u5bf9\u8d28\u91cf\u63a7\u5236\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8986\u76d6\u8303\u56f4\u6709\u9650\u4ee5\u53ca\u6a21\u578b\u5728\u591a\u6837\u4e14\u590d\u6742\u7684\u5f02\u5e38\u6a21\u5f0f\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MAU-Set\u2014\u2014\u4e00\u4e2a\u9762\u5411\u591a\u7c7b\u578b\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u7684\u7efc\u5408\u6027\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u591a\u4e2a\u5de5\u4e1a\u9886\u57df\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4ece\u4e8c\u5206\u7c7b\u5230\u590d\u6742\u63a8\u7406\u7684\u5c42\u6b21\u5316\u4efb\u52a1\u7ed3\u6784\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u652f\u6301\u516c\u5e73\u800c\u5168\u9762\u7684\u6a21\u578b\u8bc4\u4f30\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86MAU-GPT\uff0c\u4e00\u79cd\u4e13\u4e3a\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u800c\u9886\u57df\u9002\u914d\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684AMoE-LoRA\u673a\u5236\uff0c\u7edf\u4e00\u4e86\u5f02\u5e38\u611f\u77e5\u4e13\u5bb6\u4e0e\u901a\u7528\u4e13\u5bb6\u7684\u9002\u914d\u8fc7\u7a0b\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5bf9\u5404\u7c7b\u7f3a\u9677\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMAU-GPT\u5728\u6240\u6709\u9886\u57df\u5747\u6301\u7eed\u4f18\u4e8e\u4ee5\u5f80\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5728\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.07012", "pdf": "https://arxiv.org/pdf/2602.07012", "abs": "https://arxiv.org/abs/2602.07012", "authors": ["Zhonghua Wang", "Lie Ju", "Sijia Li", "Wei Feng", "Sijin Zhou", "Ming Hu", "Jianhao Xiong", "Xiaoying Tang", "Yifan Peng", "Mingquan Lin", "Yaodong Ding", "Yong Zeng", "Wenbin Wei", "Li Dong", "Zongyuan Ge"], "title": "A General Model for Retinal Segmentation and Quantification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.", "AI": {"tldr": "RetSAM \u662f\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u7f51\u819c\u5206\u5272\u4e0e\u91cf\u5316\u6846\u67b6\uff0c\u53ef\u4ece\u773c\u5e95\u56fe\u50cf\u4e2d\u7a33\u5065\u5730\u5206\u5272\u591a\u79cd\u7ed3\u6784\u548c\u75c5\u53d8\uff0c\u5e76\u751f\u621030\u591a\u4e2a\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\uff0c\u652f\u6301\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u548c\u7cfb\u7edf\u6027\u75be\u75c5\u5173\u8054\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u516c\u5f00\u7684\u591a\u6807\u7b7e\u6570\u636e\u96c6\u548c\u7edf\u4e00\u7684\u4ece\u5206\u5272\u5230\u91cf\u5316\u7684\u6d41\u7a0b\uff0c\u9650\u5236\u4e86\u89c6\u7f51\u819c\u5b9a\u91cf\u8868\u578b\u4e0e\u773c\u90e8\u53ca\u5168\u8eab\u75be\u75c5\u4e4b\u95f4\u5173\u7cfb\u7684\u5927\u89c4\u6a21\u5206\u6790\u3002", "method": "\u63d0\u51fa RetSAM \u6846\u67b6\uff0c\u5728\u8d85\u8fc720\u4e07\u5f20\u773c\u5e95\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u652f\u6301\u4e09\u7c7b\u4efb\u52a1\uff0c\u5206\u5272\u4e94\u79cd\u89e3\u5256\u7ed3\u6784\u3001\u56db\u79cd\u8868\u578b\u6a21\u5f0f\u548c20\u591a\u79cd\u75c5\u53d8\u7c7b\u578b\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a30\u591a\u4e2a\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\uff1b\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u79c1\u6709\u548c\u516c\u5f00\u6570\u636e\u3002", "result": "\u572817\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u8d8a\u7684\u5206\u5272\u6027\u80fd\uff0c\u5e73\u5747DSC\u6307\u6807\u6bd4\u5148\u524d\u6700\u4f18\u65b9\u6cd5\u9ad83.9\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u6311\u6218\u6027\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u6700\u9ad8\u63d0\u534715\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728\u4e0d\u540c\u4eba\u7fa4\u3001\u8bbe\u5907\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RetSAM \u5c06\u773c\u5e95\u56fe\u50cf\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5b9a\u91cf\u8868\u578b\uff0c\u4e3a\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u548c\u8f6c\u5316\u533b\u5b66\u63d0\u4f9b\u6709\u529b\u5de5\u5177\u3002", "summary_cn": "\u89c6\u7f51\u819c\u6210\u50cf\u5feb\u901f\u3001\u65e0\u521b\u4e14\u5e7f\u6cdb\u53ef\u7528\uff0c\u4e3a\u773c\u79d1\u548c\u5168\u8eab\u5065\u5eb7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u7ed3\u6784\u548c\u8840\u7ba1\u4fe1\u53f7\u3002\u8fd9\u79cd\u53ef\u53ca\u6027\u4e3a\u7814\u7a76\u5b9a\u91cf\u89c6\u7f51\u819c\u8868\u578b\u4e0e\u773c\u90e8\u53ca\u5168\u8eab\u75be\u75c5\u4e4b\u95f4\u7684\u5173\u7cfb\u521b\u9020\u4e86\u673a\u4f1a\u3002\u7136\u800c\uff0c\u7531\u4e8e\u516c\u5f00\u7684\u591a\u6807\u7b7e\u6570\u636e\u96c6\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7684\u4ece\u5206\u5272\u5230\u91cf\u5316\u7684\u6d41\u7a0b\uff0c\u6b64\u7c7b\u5206\u6790\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u56f0\u96be\u3002\u6211\u4eec\u63d0\u51fa\u4e86 RetSAM\u2014\u2014\u4e00\u4e2a\u7528\u4e8e\u773c\u5e95\u6210\u50cf\u7684\u901a\u7528\u89c6\u7f51\u819c\u5206\u5272\u4e0e\u91cf\u5316\u6846\u67b6\u3002\u8be5\u6846\u67b6\u80fd\u591f\u7a33\u5065\u5730\u8fdb\u884c\u591a\u76ee\u6807\u5206\u5272\u5e76\u63d0\u53d6\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\uff0c\u652f\u6301\u4e0b\u6e38\u773c\u79d1\u7814\u7a76\u548c\u773c\u7ec4\u5b66\u76f8\u5173\u6027\u5206\u6790\u3002RetSAM \u5728\u8d85\u8fc720\u4e07\u5f20\u773c\u5e95\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u4e09\u7c7b\u4efb\u52a1\uff0c\u53ef\u5206\u5272\u4e94\u79cd\u89e3\u5256\u7ed3\u6784\u3001\u56db\u79cd\u89c6\u7f51\u819c\u8868\u578b\u6a21\u5f0f\u4ee5\u53ca20\u591a\u79cd\u4e0d\u540c\u7684\u75c5\u53d8\u7c7b\u578b\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5206\u5272\u7ed3\u679c\u8f6c\u5316\u4e3a30\u591a\u4e2a\u6db5\u76d6\u7ed3\u6784\u5f62\u6001\u3001\u8840\u7ba1\u51e0\u4f55\u548c\u9000\u884c\u6027\u53d8\u5316\u7684\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\u3002\u901a\u8fc7\u7ed3\u5408\u79c1\u6709\u548c\u516c\u5f00\u773c\u5e95\u6570\u636e\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0cRetSAM \u572817\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5206\u5272\u6027\u80fd\uff0c\u5e73\u5747Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u6bd4\u4ee5\u5f80\u6700\u4f73\u65b9\u6cd5\u9ad8\u51fa3.9\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u6700\u9ad8\u63d0\u5347\u8fbe15\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728\u4e0d\u540c\u4eba\u7fa4\u3001\u6210\u50cf\u8bbe\u5907\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6240\u751f\u6210\u7684\u751f\u7269\u6807\u5fd7\u7269\u53ef\u7528\u4e8e\u7cfb\u7edf\u6027\u5173\u8054\u5206\u6790\u591a\u79cd\u4e3b\u8981\u773c\u79d1\u75be\u75c5\uff0c\u5305\u62ec\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u3001\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\u3001\u9752\u5149\u773c\u548c\u75c5\u7406\u6027\u8fd1\u89c6\u3002\u603b\u4f53\u800c\u8a00\uff0cRetSAM \u5c06\u773c\u5e95\u56fe\u50cf\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5b9a\u91cf\u8868\u578b\uff0c\u63a8\u52a8\u4e86\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u53ca\u5176\u4e34\u5e8a\u8f6c\u5316\u3002"}}
{"id": "2602.07013", "pdf": "https://arxiv.org/pdf/2602.07013", "abs": "https://arxiv.org/abs/2602.07013", "authors": ["Jiaxi Yang", "Shicheng Liu", "Yuchen Yang", "Dongwon Lee"], "title": "Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \\textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \\textbf{C}onfigurable \\textbf{R}efusal in \\textbf{VLM}s (\\textbf{CR-VLM}), a robust and efficient approach for {\\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCR-VLM\uff0c\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u64cd\u63a7\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\u673a\u5236\uff0c\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u6839\u636e\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u7075\u6d3b\u8c03\u6574\u62d2\u7edd\u884c\u4e3a\uff0c\u907f\u514d\u8fc7\u5ea6\u6216\u4e0d\u8db3\u62d2\u7edd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u62d2\u7edd\u673a\u5236\u8fc7\u4e8e\u50f5\u5316\uff0c\u91c7\u7528\u201c\u4e00\u5200\u5207\u201d\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u7ea6\u675f\uff0c\u5bfc\u81f4\u62d2\u7edd\u4e0d\u8db3\u6216\u8fc7\u5ea6\u62d2\u7edd\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faCR-VLM\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1) \u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u673a\u5236\u63d0\u53d6\u53ef\u914d\u7f6e\u62d2\u7edd\u5411\u91cf\u4ee5\u589e\u5f3a\u62d2\u7edd\u4fe1\u53f7\uff1b(2) \u5f15\u5165\u95e8\u63a7\u673a\u5236\uff0c\u5728\u4fdd\u7559\u5bf9\u5408\u6cd5\u8bf7\u6c42\u63a5\u53d7\u80fd\u529b\u7684\u540c\u65f6\u7f13\u89e3\u8fc7\u5ea6\u62d2\u7edd\uff1b(3) \u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u89c6\u89c9\u589e\u5f3a\u6a21\u5757\uff0c\u4f7f\u89c6\u89c9\u8868\u5f81\u4e0e\u62d2\u7edd\u8981\u6c42\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u591a\u79cdVLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCR-VLM\u80fd\u5b9e\u73b0\u9ad8\u6548\u3001\u6709\u6548\u4e14\u9c81\u68d2\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\u3002", "conclusion": "CR-VLM\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u4ee5\u5b9e\u73b0\u9762\u5411\u7528\u6237\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\u3002", "summary_cn": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u62d2\u7edd\u673a\u5236\u5df2\u6210\u4e3a\u786e\u4fdd\u6a21\u578b\u884c\u4e3a\u8d1f\u8d23\u4efb\u548c\u5b89\u5168\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u62d2\u7edd\u7b56\u7565\u5927\u591a\u662f\u201c\u4e00\u5200\u5207\u201d\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u7ea6\u675f\uff0c\u4ece\u800c\u5bfc\u81f4\u62d2\u7edd\u4e0d\u8db3\u6216\u8fc7\u5ea6\u62d2\u7edd\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u63a2\u8ba8\u4e86\u4e0a\u8ff0\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86**CR-VLM**\uff08Configurable Refusal in VLMs\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u64cd\u63a7\u7684\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\u65b9\u6cd5\u3002CR-VLM\u5305\u542b\u4e09\u4e2a\u96c6\u6210\u7ec4\u4ef6\uff1a(1) \u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u673a\u5236\u63d0\u53d6\u53ef\u914d\u7f6e\u62d2\u7edd\u5411\u91cf\uff0c\u4ee5\u653e\u5927\u62d2\u7edd\u4fe1\u53f7\uff1b(2) \u5f15\u5165\u95e8\u63a7\u673a\u5236\uff0c\u5728\u4fdd\u7559\u5bf9\u8303\u56f4\u5185\u67e5\u8be2\u63a5\u53d7\u80fd\u529b\u7684\u540c\u65f6\u7f13\u89e3\u8fc7\u5ea6\u62d2\u7edd\uff1b(3) \u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u89c6\u89c9\u589e\u5f3a\u6a21\u5757\uff0c\u4f7f\u89c6\u89c9\u8868\u5f81\u4e0e\u62d2\u7edd\u8981\u6c42\u5bf9\u9f50\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u591a\u79cdVLM\u4e0a\u8fdb\u884c\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0cCR-VLM\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\uff0c\u4e3aVLM\u4e2d\u5b9e\u73b0\u7528\u6237\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2602.07014", "pdf": "https://arxiv.org/pdf/2602.07014", "abs": "https://arxiv.org/abs/2602.07014", "authors": ["Qingyu Wu", "Yuxuan Han", "Haijun Li", "Zhao Xu", "Jianshan Zhao", "Xu Jin", "Longyue Wang", "Weihua Luo"], "title": "Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVectra\uff0c\u9996\u4e2a\u7528\u4e8e\u8de8\u5883\u7535\u5546\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1\uff08IIMT\uff09\u7684\u65e0\u53c2\u8003\u3001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u9a71\u52a8\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u8bc4\u5206\u7cfb\u7edf\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e09\u90e8\u5206\uff0c\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u548c\u8bc4\u5206\u6027\u80fd\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709IIMT\u7814\u7a76\u5ffd\u89c6\u4e86\u89c6\u89c9\u6e32\u67d3\u8d28\u91cf\u5bf9\u7528\u6237\u53c2\u4e0e\u7684\u5173\u952e\u4f5c\u7528\uff1b\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff08\u5982SSIM\u3001FID\uff09\uff0c\u8981\u4e48\u7f3a\u4e4f\u9886\u57df\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\uff08\u5982\u6a21\u578b\u5373\u8bc4\u59d4\u65b9\u6cd5\uff09\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u53ef\u89e3\u91ca\u6027\u3001\u65e0\u53c2\u8003\u4e14\u9002\u7528\u4e8e\u7535\u5546\u573a\u666f\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "method": "\u63d0\u51faVectra\u6846\u67b6\uff1a(1) Vectra Score\uff1a\u5c06\u89c6\u89c9\u8d28\u91cf\u5206\u89e3\u4e3a14\u4e2a\u53ef\u89e3\u91ca\u7ef4\u5ea6\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u611f\u77e5\u7684\u7f3a\u9677\u533a\u57df\u6bd4\uff08DAR\uff09\u4ee5\u51cf\u5c11\u6807\u6ce8\u6b67\u4e49\uff1b(2) Vectra Dataset\uff1a\u57fa\u4e8e110\u4e07\u771f\u5b9e\u5546\u54c1\u56fe\u6784\u5efa\uff0c\u542b2K\u8bc4\u6d4b\u57fa\u51c6\u300130K\u63a8\u7406\u6807\u6ce8\u548c3.5K\u4e13\u5bb6\u504f\u597d\u6807\u7b7e\uff1b(3) Vectra Model\uff1a\u4e00\u4e2a40\u4ebf\u53c2\u6570\u7684MLLM\uff0c\u80fd\u540c\u65f6\u8f93\u51fa\u91cf\u5316\u8bc4\u5206\u4e0e\u8bca\u65ad\u6027\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVectra\u5728\u4e0e\u4eba\u7c7b\u6392\u5e8f\u7684\u76f8\u5173\u6027\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u5176\u6a21\u578b\u5728\u8bc4\u5206\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5305\u62ecGPT-5\u548cGemini-3\u5728\u5185\u7684\u4e3b\u6d41MLLM\u3002", "conclusion": "Vectra\u4e3a\u7535\u5546IIMT\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u65e0\u9700\u53c2\u8003\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u65b0\u8303\u5f0f\uff0c\u76f8\u5173\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u5c06\u5728\u8bba\u6587\u63a5\u6536\u540e\u5f00\u6e90\u3002", "summary_cn": "\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1\uff08IIMT\uff09\u8d4b\u80fd\u8de8\u5883\u7535\u5b50\u5546\u52a1\u5546\u54c1\u5217\u8868\uff1b\u73b0\u6709\u7814\u7a76\u805a\u7126\u4e8e\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\uff0c\u800c\u89c6\u89c9\u6e32\u67d3\u8d28\u91cf\u5bf9\u7528\u6237\u53c2\u4e0e\u81f3\u5173\u91cd\u8981\u3002\u9762\u5bf9\u4fe1\u606f\u5bc6\u96c6\u7684\u5546\u54c1\u56fe\u50cf\u548c\u591a\u6a21\u6001\u7f3a\u9677\uff0c\u5f53\u524d\u57fa\u4e8e\u53c2\u8003\u7684\u65b9\u6cd5\uff08\u5982SSIM\u3001FID\uff09\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u201c\u6a21\u578b\u5373\u8bc4\u59d4\u201d\u65b9\u6cd5\u5219\u7f3a\u4e4f\u9886\u57df\u5185\u7ec6\u7c92\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\u3002\u4e3a\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Vectra\u2014\u2014\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u7535\u5546IIMT\u7684\u65e0\u53c2\u8003\u3001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u9a71\u52a8\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u3002Vectra\u5305\u542b\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\uff1a(1) Vectra Score\uff0c\u4e00\u4e2a\u591a\u7ef4\u8d28\u91cf\u5ea6\u91cf\u7cfb\u7edf\uff0c\u5c06\u89c6\u89c9\u8d28\u91cf\u5206\u89e3\u4e3a14\u4e2a\u53ef\u89e3\u91ca\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u5177\u6709\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u7f3a\u9677\u533a\u57df\u6bd4\uff08DAR\uff09\u91cf\u5316\u4ee5\u51cf\u5c11\u6807\u6ce8\u6b67\u4e49\uff1b(2) Vectra Dataset\uff0c\u901a\u8fc7\u591a\u6837\u6027\u611f\u77e5\u91c7\u6837\u4ece110\u4e07\u5f20\u771f\u5b9e\u5546\u54c1\u56fe\u50cf\u4e2d\u6784\u5efa\uff0c\u5305\u542b\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u76842K\u57fa\u51c6\u96c6\u3001\u7528\u4e8e\u6307\u4ee4\u5fae\u8c03\u768430K\u6761\u63a8\u7406\u578b\u6807\u6ce8\uff0c\u4ee5\u53ca\u7528\u4e8e\u5bf9\u9f50\u4e0e\u8bc4\u4f30\u76843.5K\u6761\u4e13\u5bb6\u504f\u597d\u6807\u7b7e\uff1b(3) Vectra Model\uff0c\u4e00\u4e2a\u62e5\u670940\u4ebf\u53c2\u6570\u7684MLLM\uff0c\u53ef\u540c\u65f6\u751f\u6210\u91cf\u5316\u8bc4\u5206\u4e0e\u8bca\u65ad\u6027\u63a8\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0cVectra\u5728\u4e0e\u4eba\u7c7b\u6392\u5e8f\u7684\u76f8\u5173\u6027\u65b9\u9762\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff0c\u4e14\u6211\u4eec\u7684\u6a21\u578b\u5728\u8bc4\u5206\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5305\u62ecGPT-5\u548cGemini-3\u5728\u5185\u7684\u9886\u5148MLLM\u3002\u76f8\u5173\u6570\u636e\u96c6\u4e0e\u6a21\u578b\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u6536\u540e\u53d1\u5e03\u3002"}}
{"id": "2602.07015", "pdf": "https://arxiv.org/pdf/2602.07015", "abs": "https://arxiv.org/abs/2602.07015", "authors": ["Subreena", "Mohammad Amzad Hossain", "Mirza Raquib", "Saydul Akbar Murad", "Farida Siddiqi Prity", "Muhammad Hanif", "Nick Rahimi"], "title": "Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6df7\u5408CNN\u67b6\u6784\uff08\u7ed3\u5408MobileNetV3-Large\u548cEfficientNetB0\uff09\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u8bc6\u522b\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u65b9\u6cd5\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u89c6\u969c\u4eba\u58eb\u5728\u8bc6\u522b\u7eb8\u5e01\u65f6\u4f9d\u8d56\u4ed6\u4eba\uff0c\u6613\u53d7\u6b3a\u8bc8\u548c\u5265\u524a\uff0c\u56e0\u6b64\u4e9f\u9700\u51c6\u786e\u3001\u53ef\u9760\u7684\u81ea\u52a8\u7eb8\u5e01\u8bc6\u522b\u6280\u672f\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u3002", "method": "\u6784\u5efa\u5305\u542b\u53d7\u63a7\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u878d\u5408\u56db\u4e2a\u989d\u5916\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u591a\u6837\u6027\uff1b\u63d0\u51fa\u7ed3\u5408MobileNetV3-Large\u4e0eEfficientNetB0\u7684\u6df7\u5408CNN\u67b6\u6784\uff0c\u540e\u63a5\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u5206\u7c7b\u5668\uff1b\u4f7f\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u53ca\u4e03\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f15\u5165LIME\u548cSHAP\u7b49\u53ef\u89e3\u91caAI\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5728\u53d7\u63a7\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.95%\u51c6\u786e\u7387\uff0c\u5728\u590d\u6742\u80cc\u666f\u4e2d\u8fbe92.84%\uff0c\u5408\u5e76\u6240\u6709\u6570\u636e\u96c6\u65f6\u8fbe94.98%\uff1b\u5e76\u901a\u8fc7\u591a\u79cd\u6307\u6807\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6df7\u5408CNN\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff0c\u5e76\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u53ef\u9760\u3001\u900f\u660e\u7684\u7eb8\u5e01\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u51c6\u786e\u7684\u8d27\u5e01\u8bc6\u522b\u5bf9\u4e8e\u8f85\u52a9\u6280\u672f\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u4f9d\u8d56\u4ed6\u4eba\u8bc6\u522b\u7eb8\u5e01\u7684\u89c6\u969c\u4eba\u58eb\u800c\u8a00\u3002\u8fd9\u79cd\u4f9d\u8d56\u4f7f\u4ed6\u4eec\u9762\u4e34\u6b3a\u8bc8\u548c\u5265\u524a\u7684\u98ce\u9669\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u53d7\u63a7\u73af\u5883\u548c\u771f\u5b9e\u573a\u666f\uff0c\u4ee5\u786e\u4fdd\u66f4\u5168\u9762\u3001\u591a\u6837\u5316\u7684\u8868\u793a\u3002\u5176\u6b21\uff0c\u4e3a\u589e\u5f3a\u6570\u636e\u96c6\u7684\u9c81\u68d2\u6027\uff0c\u6211\u4eec\u6574\u5408\u4e86\u56db\u4e2a\u989d\u5916\u7684\u6570\u636e\u96c6\uff08\u5305\u62ec\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff09\uff0c\u4ee5\u6db5\u76d6\u5404\u79cd\u590d\u6742\u60c5\u51b5\u5e76\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u514b\u670d\u73b0\u6709\u8bc6\u522b\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408CNN\u67b6\u6784\uff0c\u7ed3\u5408MobileNetV3-Large\u548cEfficientNetB0\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u968f\u540e\u91c7\u7528\u9ad8\u6548\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u5206\u7c7b\u5668\uff0c\u5728\u4fdd\u6301\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u4f7f\u7cfb\u7edf\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u5728\u53d7\u63a7\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u8fbe97.95%\uff0c\u5728\u590d\u6742\u80cc\u666f\u4e2d\u8fbe92.84%\uff0c\u5408\u5e76\u6240\u6709\u6570\u636e\u96c6\u65f6\u8fbe94.98%\u3002\u6a21\u578b\u6027\u80fd\u901a\u8fc7\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u53ca\u4e03\u9879\u6307\u6807\uff08\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001Cohen's Kappa\u3001MCC\u548cAUC\uff09\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86LIME\u548cSHAP\u7b49\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.07016", "pdf": "https://arxiv.org/pdf/2602.07016", "abs": "https://arxiv.org/abs/2602.07016", "authors": ["Mohsen Mostafa"], "title": "Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, https://www.kaggle.com/code/babydriver1233/optimized-pipeline-for-the-image-matching-challeng, https://www.kaggle.com/code/babydriver1233/integrating-lejepa-for-enhanced-image-matching", "summary": "Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.", "AI": {"tldr": "\u672c\u6587\u5728IMC2025\u6311\u6218\u80cc\u666f\u4e0b\uff0c\u63a2\u7d22\u4f7f\u7528\u53d7LeJEPA\u542f\u53d1\u7684\u9ad8\u65af\u7ea6\u675f\u56fe\u50cf\u5d4c\u5165\u8868\u793a\uff0c\u901a\u8fc7\u4e09\u4e2a\u6e10\u8fdb\u5f0f\u6d41\u7a0b\u9a8c\u8bc1\u5176\u5bf9\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\u4e2d\u573a\u666f\u53d1\u73b0\u4e0e\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u6a21\u7cca\u6761\u4ef6\u4e0b\u4f18\u4e8e\u542f\u53d1\u5f0f\u57fa\u7ebf\u3002", "motivation": "\u65e0\u7ed3\u6784\u56fe\u50cf\u96c6\u5408\u4e2d\u7684\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\u6781\u5177\u6311\u6218\uff0c\u5c24\u5176\u5f53\u56fe\u50cf\u6765\u81ea\u591a\u4e2a\u65e0\u5173\u573a\u666f\u5e76\u5305\u542b\u663e\u8457\u89c6\u89c9\u6b67\u4e49\u65f6\uff1bIMC2025\u6311\u6218\u8981\u6c42\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u540c\u65f6\u5b8c\u6210\u573a\u666f\u53d1\u73b0\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u4fc3\u4f7f\u4f5c\u8005\u63a2\u7d22\u66f4\u9c81\u68d2\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u6e10\u8fdb\u4f18\u5316\u7684\u6d41\u7a0b\uff0c\u6700\u7ec8\u91c7\u7528\u53d7LeJEPA\u542f\u53d1\u7684\u65b9\u6cd5\uff0c\u5728\u5b66\u4e60\u5230\u7684\u56fe\u50cf\u5d4c\u5165\u4e0a\u65bd\u52a0\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ea6\u675f\uff0c\u4ee5\u63d0\u5347\u805a\u7c7b\u4e00\u81f4\u6027\u548c\u59ff\u6001\u4f30\u8ba1\u9c81\u68d2\u6027\u3002", "result": "\u5728IMC2025\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u542f\u53d1\u5f0f\u57fa\u7ebf\uff0c\u9ad8\u65af\u7ea6\u675f\u5d4c\u5165\u80fd\u66f4\u597d\u5730\u5206\u79bb\u573a\u666f\u5e76\u751f\u6210\u66f4\u5408\u7406\u7684\u76f8\u673a\u59ff\u6001\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u6a21\u7cca\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7406\u8bba\u9a71\u52a8\u7684\u8868\u793a\u7ea6\u675f\uff08\u5982\u9ad8\u65af\u7ea6\u675f\uff09\u4e3a\u8fde\u63a5\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u5b9e\u9645\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6d41\u7a0b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "summary_cn": "\u4ece\u65e0\u7ed3\u6784\u56fe\u50cf\u96c6\u5408\u4e2d\u8fdb\u884c\u65e0\u76d1\u7763\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u4ecd\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u56fe\u50cf\u6765\u6e90\u4e8e\u591a\u4e2a\u65e0\u5173\u573a\u666f\u5e76\u5305\u542b\u663e\u8457\u89c6\u89c9\u6b67\u4e49\u65f6\u30022025\u5e74\u56fe\u50cf\u5339\u914d\u6311\u6218\u8d5b\uff08IMC2025\uff09\u7a81\u663e\u4e86\u8fd9\u4e9b\u56f0\u96be\uff0c\u5b83\u8981\u6c42\u5728\u5305\u542b\u79bb\u7fa4\u70b9\u548c\u6df7\u5408\u5185\u5bb9\u7684\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u540c\u65f6\u5b8c\u6210\u573a\u666f\u53d1\u73b0\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002\u672c\u6587\u7814\u7a76\u4e86\u53d7LeJEPA\uff08\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff09\u542f\u53d1\u7684\u9ad8\u65af\u7ea6\u675f\u8868\u793a\u65b9\u6cd5\u5728\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u4e2d\u7684\u5e94\u7528\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e09\u79cd\u9010\u6b65\u4f18\u5316\u7684\u6d41\u7a0b\uff0c\u6700\u7ec8\u5f62\u6210\u4e00\u79cd\u53d7LeJEPA\u542f\u53d1\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9\u6240\u5b66\u56fe\u50cf\u5d4c\u5165\u65bd\u52a0\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ea6\u675f\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5e76\u975e\u63d0\u51fa\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u800c\u662f\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u8fd9\u4e9b\u7ea6\u675f\u5728\u5b9e\u8df5\u4e2d\u5982\u4f55\u5f71\u54cd\u805a\u7c7b\u4e00\u81f4\u6027\u4e0e\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002\u5728IMC2025\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4e8e\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9ad8\u65af\u7ea6\u675f\u5d4c\u5165\u80fd\u591f\u5728\u89c6\u89c9\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u66f4\u597d\u5730\u5b9e\u73b0\u573a\u666f\u5206\u79bb\u5e76\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7684\u5408\u7406\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u57fa\u4e8e\u7406\u8bba\u52a8\u673a\u7684\u8868\u793a\u7ea6\u675f\u4e3a\u8fde\u63a5\u81ea\u76d1\u7763\u5b66\u4e60\u539f\u5219\u4e0e\u5b9e\u7528\u7684\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08Structure-from-Motion\uff09\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.07017", "pdf": "https://arxiv.org/pdf/2602.07017", "abs": "https://arxiv.org/abs/2602.07017", "authors": ["Thuraya Alzubaidi", "Sana Ammar", "Maryam Alsharqi", "Islem Rekik", "Muzammil Behzad"], "title": "XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\\% reduction in runtime, a 44.6\\% improvement in dice score, and a 96.7\\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faXAI-CLIP\uff0c\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u533a\u57df\u5b9a\u4f4d\u4e0e\u6270\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u751f\u6210\u66f4\u6e05\u6670\u3001\u89e3\u5256\u5b66\u4e0a\u66f4\u5408\u7406\u7684\u663e\u8457\u56fe\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u963b\u788d\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\uff1b\u73b0\u6709XAI\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u89e3\u91ca\u7ed3\u679c\u566a\u58f0\u5927\u6216\u89e3\u5256\u5b66\u65e0\u5173\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u8fdb\u884c\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u5b9a\u4f4d\uff0c\u5f15\u5bfc\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u7684\u533a\u57df\u611f\u77e5\u6270\u52a8\uff0c\u751f\u6210\u8fb9\u754c\u6e05\u6670\u7684\u663e\u8457\u56fe\u3002", "result": "\u5728FLARE22\u548cCHAOS\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u4f20\u7edf\u6270\u52a8\u65b9\u6cd5\uff0cXAI-CLIP\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1160%\uff0cDice\u5206\u6570\u63d0\u534744.6%\uff0c\u906e\u6321\u89e3\u91ca\u7684IoU\u63d0\u534796.7%\uff0c\u4e14\u663e\u8457\u56fe\u66f4\u5e72\u51c0\u3001\u89e3\u5256\u4e00\u81f4\u6027\u66f4\u5f3a\u3002", "conclusion": "\u5c06\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u878d\u5165\u57fa\u4e8e\u6270\u52a8\u7684XAI\u6846\u67b6\uff0c\u80fd\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u900f\u660e\u4e14\u53ef\u4e34\u5e8a\u90e8\u7f72\u7684AI\u7cfb\u7edf\u3002", "summary_cn": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u662f\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u7684\u5173\u952e\u73af\u8282\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u7cbe\u51c6\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u75be\u75c5\u76d1\u6d4b\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5377\u79ef\u67b6\u6784\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u963b\u788d\u4e34\u5e8a\u4fe1\u4efb\u4e0e\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\u3002\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\uff0c\u5305\u62ec\u57fa\u4e8e\u68af\u5ea6\u7684\u663e\u8457\u6027\u65b9\u6cd5\u548c\u57fa\u4e8e\u6270\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u5e38\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u9700\u8981\u5927\u91cf\u524d\u5411\u4f20\u64ad\uff0c\u5e76\u4e14\u5e38\u5e38\u4ea7\u751f\u566a\u58f0\u8f83\u5927\u6216\u89e3\u5256\u5b66\u4e0a\u65e0\u5173\u7684\u89e3\u91ca\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86XAI-CLIP\u2014\u2014\u4e00\u79cdROI\u5f15\u5bfc\u7684\u6270\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u6765\u5b9a\u4f4d\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u89e3\u5256\u533a\u57df\uff0c\u5e76\u5f15\u5bfc\u89e3\u91ca\u8fc7\u7a0b\u3002\u901a\u8fc7\u5c06\u8bed\u8a00\u4fe1\u606f\u5f15\u5bfc\u7684\u533a\u57df\u5b9a\u4f4d\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u76f8\u7ed3\u5408\uff0c\u5e76\u65bd\u52a0\u6709\u9488\u5bf9\u6027\u7684\u533a\u57df\u611f\u77e5\u6270\u52a8\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u751f\u6210\u66f4\u6e05\u6670\u3001\u8fb9\u754c\u611f\u77e5\u66f4\u5f3a\u7684\u663e\u8457\u56fe\u3002\u5728FLARE22\u548cCHAOS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u6270\u52a8\u65b9\u6cd5\u76f8\u6bd4\uff0cXAI-CLIP\u7684\u8fd0\u884c\u65f6\u95f4\u6700\u591a\u51cf\u5c1160%\uff0cDice\u5206\u6570\u63d0\u534744.6%\uff0c\u57fa\u4e8e\u906e\u6321\u89e3\u91ca\u7684\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u63d0\u534796.7%\u3002\u5b9a\u6027\u7ed3\u679c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u5176\u5f52\u56e0\u56fe\u66f4\u52a0\u5e72\u51c0\u3001\u89e3\u5256\u5b66\u4e00\u81f4\u6027\u66f4\u9ad8\uff0c\u4f2a\u5f71\u66f4\u5c11\u3002\u8fd9\u8868\u660e\u5c06\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u8868\u5f81\u878d\u5165\u57fa\u4e8e\u6270\u52a8\u7684XAI\u6846\u67b6\uff0c\u53ef\u663e\u8457\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7387\uff0c\u4ece\u800c\u63a8\u52a8\u900f\u660e\u4e14\u53ef\u4e34\u5e8a\u90e8\u7f72\u7684AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07019", "pdf": "https://arxiv.org/pdf/2602.07019", "abs": "https://arxiv.org/abs/2602.07019", "authors": ["Elaheh Sabziyan Varnousfaderani", "Syed A. M. Shihab", "Jonathan King"], "title": "Deep Learning Based Multi-Level Classification for Aviation Safety", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u56fe\u50cf\u8bc6\u522b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u822a\u7a7a\u9e1f\u51fb\u9632\u8303\u4e2d\u81ea\u52a8\u8bc6\u522b\u9e1f\u7c7b\u79cd\u7c7b\u3001\u7fa4\u4f53\u7c7b\u578b\u548c\u7fa4\u4f53\u89c4\u6a21\uff0c\u4ee5\u63d0\u5347\u98de\u884c\u8def\u5f84\u9884\u6d4b\u7cbe\u5ea6\u548c\u98ce\u9669\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u96f7\u8fbe\u7684\u9e1f\u51fb\u9884\u9632\u7cfb\u7edf\u65e0\u6cd5\u8bc6\u522b\u9e1f\u7c7b\u79cd\u7c7b\uff0c\u800c\u4e0d\u540c\u79cd\u7c7b\u7684\u9e1f\u5177\u6709\u4e0d\u540c\u7684\u98de\u884c\u884c\u4e3a\u548c\u9ad8\u5ea6\u504f\u597d\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u98ce\u9669\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u5e76\u5e94\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u6444\u50cf\u5934\u7cfb\u7edf\u8fdb\u884c\u89c6\u89c9\u68c0\u6d4b\uff0c\u5b9e\u73b0\u9e1f\u7c7b\u7269\u79cd\u8bc6\u522b\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e13\u7528CNN\u5206\u7c7b\u5668\u4f30\u8ba1\u9e1f\u7fa4\u7c7b\u578b\u548c\u89c4\u6a21\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u9e1f\u7c7b\u7269\u79cd\u3001\u7fa4\u4f53\u7c7b\u578b\u548c\u89c4\u6a21\uff0c\u4e3a\u7269\u79cd\u7279\u5f02\u6027\u98de\u884c\u8def\u5f84\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u5173\u952e\u8f93\u5165\uff0c\u5e76\u589e\u5f3a\u5bf9\u649e\u51fb\u4e25\u91cd\u7a0b\u5ea6\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u56fe\u50cf\u9a71\u52a8CNN\u65b9\u6cd5\u53ef\u5f25\u8865\u4f20\u7edf\u96f7\u8fbe\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u9e1f\u51fb\u98ce\u9669\u9884\u8b66\u4e0e\u822a\u7a7a\u5b89\u5168\u6c34\u5e73\u3002", "summary_cn": "\u9e1f\u51fb\u5bf9\u822a\u7a7a\u5b89\u5168\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u5e38\u5bfc\u81f4\u4eba\u5458\u4f24\u4ea1\u3001\u98de\u673a\u4e25\u91cd\u635f\u574f\u4ee5\u53ca\u5de8\u989d\u7ecf\u6d4e\u635f\u5931\u3002\u73b0\u6709\u7684\u9e1f\u51fb\u9884\u9632\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5b9e\u65f6\u63a2\u6d4b\u548c\u8ffd\u8e2a\u9e1f\u7c7b\u7684\u9e1f\u7c7b\u96f7\u8fbe\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4e00\u5927\u5c40\u9650\u5728\u4e8e\u65e0\u6cd5\u8bc6\u522b\u9e1f\u7c7b\u7269\u79cd\u2014\u2014\u800c\u8fd9\u662f\u81f3\u5173\u91cd\u8981\u7684\u56e0\u7d20\uff0c\u56e0\u4e3a\u4e0d\u540c\u7269\u79cd\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u98de\u884c\u884c\u4e3a\u548c\u9ad8\u5ea6\u504f\u597d\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u9e1f\u7c7b\u5206\u7c7b\u6846\u67b6\uff0c\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u914d\u5408\u6444\u50cf\u7cfb\u7edf\u5b9e\u73b0\u81ea\u4e3b\u89c6\u89c9\u68c0\u6d4b\u3002\u8be5CNN\u65e8\u5728\u8bc6\u522b\u9e1f\u7c7b\u7269\u79cd\uff0c\u5e76\u4e3a\u7269\u79cd\u7279\u5f02\u6027\u7684\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u5173\u952e\u8f93\u5165\uff0c\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u98de\u884c\u8def\u5f84\u9884\u6d4b\u3002\u9664\u7269\u79cd\u8bc6\u522b\u5916\uff0c\u6211\u4eec\u8fd8\u5b9e\u73b0\u4e86\u4e13\u95e8\u7684CNN\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u4f30\u8ba1\u9e1f\u7fa4\u7684\u7ec4\u6210\u7c7b\u578b\u548c\u7fa4\u4f53\u89c4\u6a21\u3002\u8fd9\u4e9b\u7279\u5f81\u4e3a\u822a\u7a7a\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8865\u5145\u4fe1\u606f\uff1a\u9e1f\u7fa4\u7c7b\u578b\u548c\u89c4\u6a21\u53ef\u63ed\u793a\u7fa4\u4f53\u98de\u884c\u884c\u4e3a\u548c\u8f68\u8ff9\u5206\u6563\u60c5\u51b5\uff1b\u800c\u7fa4\u4f53\u89c4\u6a21\u76f4\u63a5\u5173\u8054\u6f5c\u5728\u649e\u51fb\u4e25\u91cd\u7a0b\u5ea6\uff0c\u56e0\u4e3a\u591a\u53ea\u9e1f\u7684\u603b\u52a8\u80fd\u4f1a\u663e\u8457\u589e\u52a0\u6574\u4f53\u635f\u5bb3\u98ce\u9669\u3002"}}
