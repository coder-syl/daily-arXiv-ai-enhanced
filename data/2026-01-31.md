<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading](https://arxiv.org/abs/2601.20881)
*Matteo Rossi*

Main category: cs.CV

TL;DR: 本文提出了一种名为MA-LipNet的多注意力唇读网络，通过在通道、空间和时间维度上依次应用三种注意力模块（CA、JSTA、SSTA）来增强视觉特征的判别性和泛化能力，在CMLR和GRID数据集上显著降低了字符错误率和词错误率。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法因发音动作细微，导致提取的视觉特征判别性不足、泛化能力差，难以在实际场景中取得理想效果。

Method: 提出MA-LipNet，依次使用通道注意力（CA）模块抑制信息量低的通道，再结合粗粒度的联合时空注意力（JSTA）与细粒度的分离时空注意力（SSTA）模块，分别对时空维度上的无关像素和帧进行抑制，从而实现多维度视觉特征提纯。

Result: 在CMLR和GRID数据集上的实验表明，MA-LipNet显著降低了字符错误率（CER）和词错误率（WER），性能优于多种当前先进方法。

Conclusion: 多维度的视觉特征精细化处理对提升唇读系统鲁棒性和准确性至关重要，所提MA-LipNet有效验证了这一思路。

Abstract: Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\textit{Joint Spatial-Temporal Attention (JSTA)} and \textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.

Abstract (中文翻译): 唇读技术旨在从无声的唇部运动视频中解码出说话内容，在公共安全等领域具有重要应用价值。然而，由于发音动作本身较为细微，现有唇读方法常常面临特征判别能力有限和泛化性能较差的问题。为应对这些挑战，本文深入研究了从时间、空间和通道三个维度对视觉特征进行提纯的方法，提出了一种新颖的多注意力唇读网络（MA-LipNet）。该网络的核心在于依次应用三个专门设计的注意力模块：首先采用通道注意力（CA）模块自适应地重新校准通道级特征，以减轻信息量较少通道的干扰；随后引入两种不同粒度的时空注意力模块——联合时空注意力（JSTA）和分离时空注意力（SSTA），以抑制无关像素和视频帧的影响。其中，JSTA模块通过在时空维度上计算统一的权重图实现粗粒度过滤，而SSTA模块则通过分别建模时间和空间注意力进行更精细的优化。在CMLR和GRID数据集上进行的大量实验表明，MA-LipNet显著降低了字符错误率（CER）和词错误率（WER），验证了其相较于多种当前先进方法的有效性和优越性。本研究凸显了多维特征精细化对实现鲁棒视觉语音识别的重要性。

</details>


### [2] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

TL;DR: 本文提出了一种针对非马尔可夫多轮对话图像生成的训练与推理框架，通过构建强调长期历史依赖的数据集、引入基于token缓存的历史条件机制，以及改进图像重建与个性化编辑能力，显著提升了模型在多轮交互中的一致性和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多轮对话图像生成基准和训练方法本质上是马尔可夫式的，即仅依赖最近一轮的图像，忽略了更早的对话历史。然而真实场景中用户可能回溯早期状态、撤销修改或引用数轮前引入的实体，因此需要建模非马尔可夫的长期依赖关系。

Method: 作者提出了三方面方法：(i) 构建非马尔可夫多轮数据，包括强制回滚到早期视觉状态的编辑任务和跨轮次绑定名称与外观的个性化任务；(ii) 设计带token级缓存的历史条件训练与推理框架，防止多轮身份漂移；(iii) 改进高保真图像重建与可编辑个性化能力，如基于重建的DiT解码器和多阶段微调策略。

Result: 实验表明，显式训练非马尔可夫交互能显著提升多轮一致性与指令遵循能力，同时保持强大的单轮编辑与个性化性能。

Conclusion: 建模非马尔可夫长期历史对实现高质量多轮对话图像生成至关重要，所提方法有效解决了现有系统在多轮交互中的身份漂移与历史忽略问题。

Abstract: Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

Abstract (中文翻译): 对话式图像生成要求模型在多轮交互中遵循用户指令，并以交错的文本和图像作为不断累积的聊天历史为基础。尽管近期的多模态大语言模型（MLLMs）已能生成和编辑图像，但大多数现有的多轮基准和训练方法本质上是马尔可夫式的：下一输出主要依赖于最近的图像，从而允许模型采用忽略长期历史的捷径策略。本文形式化并聚焦于更具挑战性的非马尔可夫设定，其中用户可能回溯早期状态、撤销更改，或引用数轮前引入的实体。我们提出了：(i) 非马尔可夫多轮数据构建策略，包括强制检索早期视觉状态的回滚式编辑，以及跨轮次将名称与外观绑定的基于名称的多轮个性化；(ii) 一种带token级缓存的历史条件训练与推理框架，以防止多轮身份漂移；(iii) 提升高保真图像重建与可编辑个性化能力的技术，包括基于重建的DiT解码器和多阶段微调课程。我们证明，显式针对非马尔可夫交互进行训练，能在保持强大单轮编辑与个性化能力的同时，显著提升多轮一致性与指令遵循能力。

</details>


### [3] [Text controllable PET denoising](https://arxiv.org/abs/2601.20990)
*Xuehua Ye,Hongxu Yang,Adam J. Schwarz*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本引导的PET图像去噪方法，利用预训练CLIP模型与U-Net结构，在单个模型中有效提升不同计数水平下的图像质量。


<details>
  <summary>Details</summary>
Motivation: PET图像常受复杂噪声影响，降低诊断准确性；现有方法难以在不同成像条件下（如剂量、采集时间等）统一处理，因此需要一种更灵活、通用的去噪方法。

Method: 结合预训练CLIP模型的文本/图像特征与U-Net架构，构建一个文本引导的去噪模型，用于在多种计数水平下增强PET图像。

Result: 实验表明，该方法在定性和定量评估上均显著优于现有方法，能有效提升图像质量。

Conclusion: 所提方法具有良好的灵活性和泛化能力，有望满足更复杂的去噪需求或缩短PET扫描时间。

Abstract: Positron Emission Tomography (PET) imaging is a vital tool in medical diagnostics, offering detailed insights into molecular processes within the human body. However, PET images often suffer from complicated noise, which can obscure critical diagnostic information. The quality of the PET image is impacted by various factors including scanner hardware, image reconstruction, tracer properties, dose/count level, and acquisition time. In this study, we propose a novel text-guided denoising method capable of enhancing PET images across a wide range of count levels within a single model. The model utilized the features from a pretrained CLIP model with a U-Net based denoising model. Experimental results demonstrate that the proposed model leads significant improvements in both qualitative and quantitative assessments. The flexibility of the model shows the potential for helping more complicated denoising demands or reducing the acquisition time.

Abstract (中文翻译): 正电子发射断层扫描（PET）成像是医学诊断中的重要工具，能够提供人体内分子过程的详细信息。然而，PET图像常常受到复杂噪声的干扰，从而掩盖关键的诊断信息。PET图像质量受多种因素影响，包括扫描仪硬件、图像重建算法、示踪剂特性、剂量/计数水平以及采集时间。在本研究中，我们提出了一种新颖的文本引导去噪方法，能够在单一模型中对广泛计数水平下的PET图像进行增强。该模型结合了预训练CLIP模型的特征与基于U-Net的去噪网络。实验结果表明，所提出的方法在定性和定量评估方面均取得了显著改进。该模型的灵活性显示出其在应对更复杂的去噪需求或缩短采集时间方面的潜力。

</details>


### [4] [Low performing pixel correction in computed tomography with unrolled network and synthetic data training](https://arxiv.org/abs/2601.20995)
*Hongxu Yang,Levente Lippenszky,Edina Timko,Lehel Ferenczi,Gopal Avinash*

Main category: cs.CV

TL;DR: 提出一种基于合成数据的双域展开方法，无需真实临床数据即可有效校正CT探测器低性能像素引起的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的低性能像素（LPP）伪影校正方法依赖昂贵的真实临床数据集，且仅在图像域或正弦图域单独处理，忽略了CT几何前向操作中两个域之间的内在关联。

Method: 提出一种基于合成数据的双域展开方法，利用自然图像生成的合成数据建模正弦图域与图像域之间LPP的内在关联，从而在无需真实临床数据的情况下训练模型进行伪影校正。

Result: 在模拟1-2%探测器缺陷靠近等中心的实验中，所提方法显著优于当前最先进的方法。

Conclusion: 该方法无需收集真实临床数据即可有效校正LPP伪影，并可适应不同扫描仪设置，适用于基于软件的部署。

Abstract: Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.

Abstract (中文翻译): 计算机断层扫描（CT）探测器中的低性能像素（LPP）会导致重建图像中出现环状和条纹伪影，使其无法用于临床诊断。近年来，已有多种方法被提出用于校正LPP伪影，包括在图像域或正弦图域中使用有监督的深度学习方法。然而，这些方法需要专门的数据集进行训练，而此类数据的获取成本高昂。此外，现有方法仅关注图像空间或正弦图空间的校正，忽略了CT几何前向操作所带来的两个域之间的内在关联。本文提出了一种基于合成数据的双域展开方法来校正LPP伪影。具体而言，通过从自然图像生成的合成数据，利用正弦图域与图像域之间LPP的内在关联，使训练后的模型无需任何真实临床数据即可校正伪影。在模拟1–2%探测器缺陷靠近等中心的实验中，所提出的方法显著优于当前最先进的方法。结果表明，我们的解决方案无需为模型训练收集数据即可有效校正LPP伪影，并且可适应不同的扫描仪设置，适用于基于软件的应用。

</details>


### [5] [AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples](https://arxiv.org/abs/2601.21022)
*Andrea Camilloni,Chiara Micoli,Nita Mulliqi,Erik Everett Palm,Thorgerdur Palsdottir,Kelvin Szolnoky,Xiaoyi Ji,Sol Erika Boman,Andrea Discacciati,Henrik Grönberg,Lars Egevad,Tobias Nordström,Kimmo Kartasalo,Martin Eklund*

Main category: cs.CV

TL;DR: 该研究开发了一种基于AI的模型，利用诊断性前列腺活检切片预测根治性前列腺切除术后生化复发（BCR）风险，并在多个外部队列中验证了其泛化能力。结合临床变量后，模型在预后分层上优于传统CAPRA-S评分。


<details>
  <summary>Details</summary>
Motivation: 当前用于预测前列腺癌根治术后生化复发（BCR）的预后工具不够精确，亟需更可靠的方法来识别具有不良预后的高危患者。

Method: 研究基于STHLM3队列（n=676）的诊断性前列腺活检切片，采用基础模型（foundation models）和基于注意力机制的多实例学习（attention-based multiple instance learning）训练AI模型以预测个体患者的BCR风险，并在三个外部根治术队列（LEOPARD、CHIMERA、TCGA-PRAD）中评估其泛化性能。

Result: 该图像驱动模型在三个外部队列中分别实现了0.64、0.70和0.70的5年时间依赖AUC；结合临床变量后显著提升了风险分层能力，并在术后预后判断上优于指南推荐的CAPRA-S评分。

Conclusion: 基于活检训练的组织病理AI模型可在不同标本类型间泛化，辅助术前与术后决策，但其相较于简单预测模型的附加价值仍需在后续研究中审慎评估。

Abstract: Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.

Abstract (中文翻译): 根治性前列腺切除术（RP）后的生化复发（BCR）是侵袭性前列腺癌和不良预后的替代标志物，但目前的预后工具仍不够精确。我们在STHLM3队列（n = 676）的诊断性前列腺活检切片上训练了一种基于人工智能（AI）的模型，利用基础模型和基于注意力机制的多实例学习方法，以预测患者个体化的BCR风险。该模型的泛化能力在三个外部RP队列中进行了评估：LEOPARD（n = 508）、CHIMENA（n = 95）和TCGA-PRAD（n = 379）。该基于图像的方法在上述三个队列中分别达到了0.64、0.70和0.70的5年时间依赖AUC。整合临床变量后，模型提供了互补的预后价值，并实现了具有统计学意义的风险分层。与指南推荐的CAPRA-S评分相比，AI模型在术后预后判断方面带来了增量改进。这些结果表明，基于活检训练的组织病理AI模型能够跨标本类型泛化，支持术前和术后决策，但AI驱动的多模态方法相较于更简单的预测模型所增加的价值，仍需在未来研究中进行严格审视。

</details>


### [6] [BadDet+: Robust Backdoor Attacks for Object Detection](https://arxiv.org/abs/2601.21066)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: 本文提出BadDet+，一种基于惩罚机制的后门攻击框架，统一了区域误分类攻击（RMA）和目标消失攻击（ODA），在保持干净样本性能的同时，显著提升了对真实物理场景的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对目标检测的后门攻击方法存在严重缺陷，包括依赖不切实际的假设以及缺乏物理世界验证，导致其实际威胁被高估或低估。因此，亟需一种更贴近现实、具备物理鲁棒性的攻击方法以揭示目标检测模型的真实脆弱性。

Method: 作者提出了BadDet+框架，其核心是利用对数障碍惩罚（log-barrier penalty）来抑制触发样本的真实类别预测。该方法统一了RMA和ODA，并能实现位置与尺度不变性，同时增强物理世界的鲁棒性。

Result: 在真实世界基准测试中，BadDet+在保持干净数据性能的同时，相比现有RMA和ODA基线方法展现出更优的从合成到物理的迁移效果。

Conclusion: 该研究揭示了目标检测模型在后门攻击下的显著脆弱性，强调了开发专门防御机制的必要性。理论分析也证实了所提惩罚机制在特定触发特征子空间内有效，不会损害正常推理。

Abstract: Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.

Abstract (中文翻译): 后门攻击对深度学习构成了严重威胁，但与图像分类相比，其对目标检测的影响仍知之甚少。尽管已有相关攻击方法被提出，但我们发现现有基于检测的攻击方法存在关键弱点，特别是它们依赖于不切实际的假设且缺乏物理验证。为弥合这一差距，我们提出了BadDet+，一个基于惩罚机制的框架，统一了区域误分类攻击（RMA）和目标消失攻击（ODA）。其核心机制利用对数障碍惩罚来抑制触发输入的真实类别预测，从而实现（i）位置与尺度不变性，以及（ii）增强的物理鲁棒性。在真实世界基准测试中，BadDet+在保持干净性能的同时，相比现有的RMA和ODA基线方法，实现了更优的从合成到物理的迁移效果。理论分析证实，所提出的惩罚机制作用于特定触发器的特征子空间内，能够可靠地诱发攻击而不损害标准推理。这些结果凸显了目标检测中的重大漏洞，以及开发专门防御措施的必要性。

</details>


### [7] [Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization](https://arxiv.org/abs/2601.21078)
*Jiaqi Li,Guangming Wang,Shuntian Zheng,Minzhe Ni,Xiaoman Lu,Guanghui Ye,Yu Guan*

Main category: cs.CV

TL;DR: ActionVLM通过动态重加权和残差聚合策略，在时序动作定位中缓解视觉-语言模型的模态偏置，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在时序动作定位任务中过度依赖语言先验，导致模态偏置，削弱了视觉信息的作用，影响定位性能。

Method: 提出ActionVLM框架，包含：(i) 去偏重加权模块，动态评估语言相对于纯视觉预测的增益并调整其权重；(ii) 残差聚合策略，将语言作为视觉主导信号的补充而非主驱动力。

Result: 在THUMOS14数据集上，模型相较当前最优方法提升最高达3.2% mAP。

Conclusion: 通过保留视觉主导地位并自适应利用语言信息，ActionVLM有效缓解模态偏置，提升时序动作定位的准确性和鲁棒性。

Abstract: Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.

Abstract (中文翻译): 时序动作定位（TAL）需要在未剪辑视频中识别动作的边界和类别。尽管视觉-语言模型（VLMs）能够提供丰富的语义信息以补充视觉证据，但现有方法往往过度强调语言先验，牺牲了视觉性能，从而导致显著的模态偏置。我们提出了ActionVLM，一种视觉-语言聚合框架，系统性地缓解TAL中的模态偏置。我们的核心思想是在保持视觉为主导信号的同时，仅在有益时自适应地利用语言信息。为此，我们引入了：(i) 一个去偏重加权模块，用于估计语言优势——即语言相对于仅使用视觉预测所带来的增量收益，并据此动态调整语言模态的权重；(ii) 一种残差聚合策略，将语言视为补充性的细化手段，而非主要驱动因素。这种组合有效缓解了模态偏置，降低了语言先验带来的过度自信，并增强了时序推理能力。在THUMOS14上的实验表明，我们的模型相较于当前最先进的方法，mAP最高提升了3.2%。

</details>


### [8] [Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought](https://arxiv.org/abs/2601.21081)
*Yu Huo,Siyu Zhang,Kun Zeng,Haoyue Liu,Owen Lee,Junlin Chen,Yuquan Lu,Yifu Guo,Yaodong Liang,Xiaoying Tang*

Main category: cs.CV

TL;DR: 本文提出Shape-of-Thought（SoT）框架，通过在推理时无需外部引擎的连贯2D投影实现渐进式形状组装，显著提升文本到图像生成模型在组合结构约束（如数量、属性绑定和部件关系）下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的多模态模型虽具备高视觉保真度，但在处理组合结构性约束（如生成数字准确性、属性绑定和部件级关系）时仍显脆弱。为解决这一问题，作者提出一种新的视觉思维链（CoT）方法。

Method: SoT训练一个统一的多模态自回归模型，交替生成文本计划与渲染的中间状态，从而在不显式生成几何表示的前提下学习形状组装逻辑。为此，作者构建了SoT-26K数据集（基于CAD部件层次结构的大规模装配轨迹数据）和T2S-CompBench评测基准。

Result: 在SoT-26K上微调后，模型在组件数量任务上达到88.4%，在结构拓扑任务上达84.8%，比纯文本基线高出约20%。

Conclusion: SoT为透明、过程监督的组合式生成建立了新范式，显著提升了生成图像的结构性准确性。

Abstract: Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.

Abstract (中文翻译): 用于文本到图像生成的多模态模型已实现较强的视觉保真度，但在组合结构性约束下仍显脆弱——尤其是生成数值准确性、属性绑定以及部件级关系等方面。为应对这些挑战，我们提出了“思维形状”（Shape-of-Thought, SoT），这是一种视觉思维链（CoT）框架，能够在推理时无需外部引擎的情况下，通过连贯的二维投影实现渐进式形状组装。SoT训练了一个统一的多模态自回归模型，以交替生成文本计划和渲染的中间状态，帮助模型捕捉形状组装逻辑，而无需生成显式的几何表示。为支持该范式，我们引入了SoT-26K数据集——一个从基于部件的CAD层次结构中提取的大规模具身装配轨迹数据集，以及T2S-CompBench评测基准，用于评估结构完整性和轨迹忠实度。在SoT-26K上进行微调后，模型在组件数值任务上达到88.4%，在结构拓扑任务上达到84.8%，比纯文本基线高出约20%。SoT为透明、过程监督的组合式生成建立了一种新范式。代码已在https://anonymous.4open.science/r/16FE/公开，SoT-26K数据集将在论文被接收后发布。

</details>


### [9] [An AI Framework for Microanastomosis Motion Assessment](https://arxiv.org/abs/2601.21120)
*Yan Meng,Eduardo J. Torres-Rodríguez,Marcelle Altshuler,Nishanth Gowda,Arhum Naeem,Recai Yilmaz,Omar Arnaout,Daniel A. Donoho*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化系统，用于评估显微血管吻合术中的器械操作技能，整合了目标检测、跟踪、器械尖端定位和分类模块，在实验中实现了97%的检测精度和96%的mAP。


<details>
  <summary>Details</summary>
Motivation: 传统显微外科技术评估依赖专家主观判断，存在评分者间差异大、标准不统一、易受认知偏差影响及耗时等问题，亟需客观、可靠且可扩展的自动化评估方法。

Method: 该框架包含四个核心模块：(1) 基于YOLO的器械检测模块；(2) 基于DeepSORT的器械跟踪模块；(3) 利用形状描述符的器械尖端定位模块；(4) 基于专家标注数据训练的监督分类模块，用于评估器械操作熟练度。

Result: 实验结果显示，该系统在器械检测上达到97%的精确率，mAP50-95为96%，验证了其在显微吻合操作技能自动评估中的有效性。

Conclusion: 所提出的AI框架能有效实现对显微血管吻合术中器械操作技能的自动化、客观化评估，具有良好的准确性和可扩展性，有望改善当前依赖主观评价的局限。

Abstract: Proficiency in microanastomosis is a fundamental competency across multiple microsurgical disciplines. These procedures demand exceptional precision and refined technical skills, making effective, standardized assessment methods essential. Traditionally, the evaluation of microsurgical techniques has relied heavily on the subjective judgment of expert raters. They are inherently constrained by limitations such as inter-rater variability, lack of standardized evaluation criteria, susceptibility to cognitive bias, and the time-intensive nature of manual review. These shortcomings underscore the urgent need for an objective, reliable, and automated system capable of assessing microsurgical performance with consistency and scalability. To bridge this gap, we propose a novel AI framework for the automated assessment of microanastomosis instrument handling skills. The system integrates four core components: (1) an instrument detection module based on the You Only Look Once (YOLO) architecture; (2) an instrument tracking module developed from Deep Simple Online and Realtime Tracking (DeepSORT); (3) an instrument tip localization module employing shape descriptors; and (4) a supervised classification module trained on expert-labeled data to evaluate instrument handling proficiency. Experimental results demonstrate the effectiveness of the framework, achieving an instrument detection precision of 97%, with a mean Average Precision (mAP) of 96%, measured by Intersection over Union (IoU) thresholds ranging from 50% to 95% (mAP50-95).

Abstract (中文翻译): 显微血管吻合术的熟练程度是多个显微外科领域的一项基本能力。此类手术要求极高的精确度和精湛的技术，因此有效的标准化评估方法至关重要。传统上，显微外科技术的评估严重依赖专家评分者的主观判断，而这种方法本身存在评分者间差异、缺乏统一评估标准、易受认知偏差影响以及人工复核耗时等固有局限。这些缺陷凸显了开发一种客观、可靠且可自动评估显微外科操作表现的系统的迫切需求。为填补这一空白，我们提出了一种新颖的人工智能框架，用于自动评估显微血管吻合术中的器械操作技能。该系统整合了四个核心组件：（1）基于“You Only Look Once”（YOLO）架构的器械检测模块；（2）基于Deep Simple Online and Realtime Tracking（DeepSORT）开发的器械跟踪模块；（3）采用形状描述符的器械尖端定位模块；以及（4）基于专家标注数据训练的监督分类模块，用于评估器械操作熟练度。实验结果表明，该框架具有良好的效果，器械检测精确率达到97%，在IoU阈值从50%到95%范围内的平均精度（mAP50-95）达到96%。

</details>


### [10] [Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2601.21159)
*Jianzheng Wang,Huan Ni*

Main category: cs.CV

TL;DR: 提出了一种无需训练的开放词汇语义分割框架SDCI，通过双分支协同推理、交叉注意力融合和超像素结构优化，显著提升了高分辨率遥感影像的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的开放词汇语义分割方法在处理高分辨率遥感影像时，因采用单向注入和浅层后处理策略，难以同时满足几何定位和语义预测的高要求。

Method: 提出SDCI框架：1）引入交叉模型注意力融合（CAF）模块，在特征编码阶段实现双模型自注意力图互注；2）设计双向交叉图扩散精炼（BCDR）模块，通过迭代随机游走扩散提升双分支分割得分可靠性；3）结合低层超像素结构，构建基于凸优化的超像素协同预测（CSCP）机制以优化边界。

Result: 在多个遥感语义分割基准上，SDCI优于现有方法；消融实验验证了超像素结构在深度学习框架中仍具有效性。

Conclusion: 所提SDCI框架有效解决了高分辨率遥感影像开放词汇语义分割中的几何与语义挑战，证明传统超像素方法与深度学习可有效融合。

Abstract: High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.

Abstract (中文翻译): 高分辨率遥感影像具有地物密集分布和边界复杂的特点，对几何定位和语义预测提出了更高要求。现有的无需训练的开放词汇语义分割（OVSS）方法通常采用“单向注入”和“浅层后处理”策略融合CLIP与视觉基础模型（VFMs），难以满足上述需求。为此，我们提出一种面向空间正则化的双分支协同推理框架SDCI，用于无需训练的OVSS。首先，在特征编码阶段，SDCI引入交叉模型注意力融合（CAF）模块，通过将各自模型的自注意力图相互注入，引导协同推理。其次，我们设计了双向交叉图扩散精炼（BCDR）模块，利用迭代式随机游走扩散增强双分支分割得分的可靠性。最后，结合低层超像素结构，开发了一种基于凸优化的超像素协同预测（CSCP）机制，进一步优化地物边界。在多个遥感语义分割基准上的实验表明，本方法性能优于现有方法。此外，消融研究进一步证实，在深度学习框架中，利用超像素结构的传统面向对象遥感影像分析方法依然有效。

</details>
