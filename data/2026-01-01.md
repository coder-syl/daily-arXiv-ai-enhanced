<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments](https://arxiv.org/abs/2512.23786)
*Ankan Aich,Yangming Lee*

Main category: cs.CV

TL;DR: 本文提出一种结合Depth Anything V2高保真合成先验与动态向量低秩适配（DV-LORA）的方法，用于提升单目深度估计在腹腔镜手术高反光环境中的鲁棒性，并在SCARED数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督单目深度估计方法在手术内窥镜的高反光、液体环境中表现脆弱，尤其在细小器械和透明表面上易出现边界塌陷问题；同时依赖含噪真实世界伪标签训练的基础模型难以满足手术场景对几何精度的要求。

Method: 利用Depth Anything V2架构提供的高保真合成先验，通过动态向量低秩适配（DV-LORA）高效地将其迁移到医疗领域，在最小参数开销下弥合合成到真实域的差距；并设计了一种基于物理分层的评估协议以更准确衡量高反光区域的性能。

Result: 在SCARED数据集上，该方法达到98.1%的准确率（δ < 1.25），相比现有基线将平方相对误差降低超过17%，显著提升了在恶劣手术光照条件下的鲁棒性。

Conclusion: 所提方法有效解决了手术场景下单目深度估计的边界塌陷与泛化能力不足问题，为机器人手术提供了更可靠、精确的深度感知能力。

Abstract: Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.

Abstract (中文翻译): 精确的单目深度估计（MDE）对于机器人手术至关重要，但在充满反光和液体的内窥镜环境中仍然十分脆弱。现有的自监督方法通常依赖于使用含噪的真实世界伪标签训练的基础模型，在细小手术器械和透明表面上常常出现边界塌陷问题。本文通过利用Depth Anything V2架构所提供的高保真合成先验，该先验能够天然捕捉细小结构的精确几何细节，并采用动态向量低秩适配（DV-LORA）技术高效地将这些先验迁移到医学领域，在最小化参数开销的同时弥合合成域与真实域之间的差距。此外，我们在SCARED数据集上引入了一种基于物理分层的评估协议，以严格量化在常被整体指标掩盖的高反光区域中的性能表现。我们的方法建立了新的最先进水平，在准确率（< 1.25）上达到98.1%，并将平方相对误差相比现有基线降低了超过17%，展现出在恶劣手术光照条件下卓越的鲁棒性。

</details>


### [2] [Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments](https://arxiv.org/abs/2512.23819)
*Surya Rayala,Marcos Quinones-Grueiro,Naveeduddin Mohammed,Ashwin T S,Benjamin Goldberg,Randall Spain,Paige Lawton,Gautam Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的自动评估系统，用于城市作战训练中的“进入并清房”（ECR）演练，通过计算机视觉技术从训练视频中提取人体姿态、视线和轨迹数据，构建任务特定指标以量化认知、心理运动和团队协作能力，并整合到扩展的认知任务分析框架中生成综合评分，支持可扩展、客观的训练评估与复盘。


<details>
  <summary>Details</summary>
Motivation: 传统城市作战训练评估依赖昂贵侵入式传感器或主观人工观察，难以实现客观、可扩展的性能评估，尤其在认知、心理运动和团队协作等关键能力方面。

Method: 利用计算机视觉模型从训练视频中提取2D骨架、视线向量和运动轨迹，构建任务特定指标衡量心理运动流畅性、态势感知和团队协调性，并将其整合进扩展的认知任务分析（CTA）层级结构，通过加权组合生成整体表现评分。

Result: 在真实ECR演练案例中验证了该方法，提供了可操作的领域特定指标，能有效捕捉个人与团队表现，并通过Gamemaster和GIFT平台的交互式仪表板支持行动后复盘。

Conclusion: 该视频分析方法为合成训练环境提供了无需额外硬件的可扩展评估手段，尽管存在跟踪精度、真值验证等局限，未来将拓展至3D视频分析以进一步提升评估能力。

Abstract: Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.

Abstract (中文翻译): 有效的城市作战训练需要通过在逼真且受控的环境中反复练习来培养态势感知能力和肌肉记忆。其中一项关键训练科目是“进入并清房”（Enter and Clear the Room, ECR），要求参训人员进行威胁评估、协同配合并控制密闭空间。军队采用合成训练环境（Synthetic Training Environments, STEs）提供可扩展、受控的场景以支持重复训练。然而，实现自动化的绩效评估仍具挑战性，尤其是在对认知能力、心理运动技能和团队协作进行客观评价方面。传统方法通常依赖成本高昂、具有侵入性的传感器或主观的人工观察，限制了评估的可扩展性和准确性。本文提出了一种基于视频的评估流程，仅利用训练视频即可生成绩效分析结果，无需额外硬件设备。该系统利用计算机视觉模型提取2D人体骨架、视线向量和运动轨迹，并基于这些数据开发任务特定指标，用以衡量心理运动流畅性、态势感知能力和团队协调性。这些指标被整合进一个扩展的认知任务分析（Cognitive Task Analysis, CTA）层级结构中，通过加权组合生成团队协作与认知能力的总体绩效评分。我们通过真实世界ECR演练的案例研究展示了该方法的有效性，提供了可操作的、领域特定的指标以捕捉个体与团队表现。此外，我们还探讨了如何通过Gamemaster和通用智能辅导框架（Generalized Intelligent Framework for Tutoring, GIFT）中的交互式仪表板，利用这些洞察支持行动后复盘（After Action Reviews），提供直观易懂的反馈。最后，我们讨论了该方法的局限性，包括目标跟踪困难、真实数据验证以及方法的普适性问题，并指出未来工作将扩展至3D视频数据分析，以在合成训练环境中实现更具可扩展性的评估。

</details>


### [3] [Pretraining Frame Preservation in Autoregressive Video Memory Compression](https://arxiv.org/abs/2512.23851)
*Lvmin Zhang,Shengqu Cai,Muyang Li,Chong Zeng,Beijia Lu,Anyi Rao,Song Han,Gordon Wetzstein,Maneesh Agrawala*

Main category: cs.CV

TL;DR: 提出PFP神经网络结构，用于将长视频压缩为短上下文，保留任意时间位置的高频率细节，并可作为自回归视频模型的记忆编码器。


<details>
  <summary>Details</summary>
Motivation: 现有方法在压缩长视频时难以保留高频率细节，且在自回归视频建模中难以兼顾记忆长度与上下文成本。

Method: 设计PFP神经网络结构，通过显式预训练目标将20秒视频压缩至约5k长度的上下文，支持任意帧的高质量重建；预训练模型可直接微调为自回归视频模型的记忆编码器。

Result: 实验证明该方法可在低上下文成本下实现长历史记忆，并保持相对较低的保真度损失；同时通过消融实验分析不同神经架构设计的权衡。

Conclusion: PFP提供了一种有效压缩长视频并保留细节的方法，适用于需要长程记忆的自回归视频生成任务。

Abstract: We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.

Abstract (中文翻译): 我们提出了PFP，一种用于将长视频压缩为短上下文的神经网络结构，其显式的预训练目标旨在保留任意时间位置单帧的高频细节。基础模型可将一段20秒的视频压缩为长度约5k的上下文，从中可随机检索出感知质量得以保留的帧。此类预训练模型可直接微调为自回归视频模型的记忆编码器，在上下文开销较低的同时实现较长的历史记忆，并保持相对较小的保真度损失。我们通过消融实验评估了该框架，并讨论了不同神经架构设计之间的权衡。

</details>


### [4] [Lifelong Domain Adaptive 3D Human Pose Estimation](https://arxiv.org/abs/2512.23860)
*Qucheng Peng,Hongfei Xue,Pu Wang,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种面向3D人体姿态估计（3D HPE）的终身域自适应新任务，并设计了一个结合3D姿态生成器、2D姿态判别器和3D姿态估计器的GAN框架，以应对目标域非平稳性和灾难性遗忘问题，在多个数据集上验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计方法依赖于受控环境中采集的标注3D数据，难以泛化到多样化的野外场景；而当前的域自适应方法（如通用DA和无源DA）未考虑目标姿态数据集的非平稳性问题，且在持续学习新域时容易发生灾难性遗忘。

Method: 提出终身域自适应3D HPE任务，设计一个新型GAN框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器；其中3D姿态生成器融合姿态感知、时间感知和域感知知识，用于缓解域偏移、增强当前域适应能力并减轻对先前域知识的遗忘。

Result: 在多个域自适应3D HPE数据集上的大量实验表明，所提方法性能优于现有方法。

Conclusion: 该工作首次将终身域自适应引入3D HPE任务，通过创新的GAN架构和多感知生成器有效解决了非平稳目标域适应与灾难性遗忘问题，显著提升了模型在复杂真实场景下的泛化能力。

Abstract: 3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.

Abstract (中文翻译): 3D人体姿态估计（3D HPE）在人员重识别、动作识别到虚拟现实等多种应用中至关重要。然而，现有方法依赖于在受控环境中收集的标注3D数据，这使其难以泛化到多样化的野外场景。当前针对3D HPE的域自适应（DA）范式，如通用DA和无源DA，忽略了目标姿态数据集非平稳性的问题。为解决这些挑战，我们提出了一项名为“终身域自适应3D HPE”的新任务。据我们所知，这是首次将终身域自适应引入3D HPE任务。在此设定下，姿态估计器首先在源域上预训练，随后依次适应到不同的目标域；并且在适应当前目标域时，无法访问源域及所有先前的目标域。该任务的关键挑战在于既要适应当前域的姿态分布，又要保留先前域的知识，尤其是防止灾难性遗忘。为此，我们提出了一种创新的生成对抗网络（GAN）框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器，有效缓解域偏移并实现原始姿态与增强姿态的对齐。此外，我们构建了一种新颖的3D姿态生成器范式，融合姿态感知、时间感知和域感知知识，以增强对当前域的适应能力，并减轻对先前域的灾难性遗忘。大量实验在多个域自适应3D HPE数据集上验证了我们方法的优越性能。

</details>


### [5] [MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework](https://arxiv.org/abs/2512.23894)
*Krithika Iyer,Austin Tapp,Athelia Paulli,Gabrielle Dickerson,Syed Muhammad Anwar,Natasha Lepore,Marius George Linguraru*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的方法，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），从而实现颅骨分割和颅缝可视化，克服了MRI无法清晰显示骨缝的局限性。


<details>
  <summary>Details</summary>
Motivation: 儿童颅缝早闭等头颅发育异常的诊断依赖于对颅骨和颅缝的精确评估，但CT因辐射风险不适用于无明显异常的儿童，而MRI虽无辐射却难以显示颅缝和骨密度。因此，亟需一种无创且能提供类似CT骨信息的方法。

Method: 利用深度学习驱动的流程，将0.2–2岁儿童的T1加权MRI转化为合成CT（sCT），并在此基础上进行颅骨分割、生成颅缝概率热图，并从中提取颅缝分割结果。方法结合了领域特定的变分自编码器。

Result: 在内部儿科数据集上，合成CT与真实CT的结构相似度达99%，Frechet inception distance为1.01；七块颅骨的平均Dice系数为85%，颅缝Dice系数为80%；TOST检验确认sCT与真实CT在颅骨和颅缝分割上等效（p < 0.05）。

Conclusion: 该研究首次实现了从儿童MRI生成可进行颅缝分割的合成CT，在不使用电离辐射的前提下，弥合了无创颅骨评估中的关键空白，具有重要的临床应用前景。

Abstract: Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.

Abstract (中文翻译): 量化儿童正常颅骨发育和颅缝骨化过程对于诊断和治疗生长相关的头颅疾病至关重要。计算机断层扫描（CT）广泛用于评估颅骨和颅缝畸形，但其电离辐射在无明显异常的儿童中属于禁忌。磁共振成像（MRI）提供无辐射扫描且软组织对比度更优，但与CT不同，MRI无法清晰显示颅缝、估算颅骨密度或评估颅盖生长。本研究提出了一种基于深度学习的流程，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），并在此基础上预测详细的颅骨分割、生成颅缝概率热图，并从中直接获得颅缝分割结果。在本机构的儿科数据上，合成CT与真实CT相比达到了99%的结构相似度，Fréchet inception distance为1.01；七块颅骨的平均Dice系数为85%，颅缝Dice系数为80%；通过双单侧检验（TOST，p < 0.05）证实了合成CT与真实CT在颅骨和颅缝分割上的等效性。据我们所知，这是首个能在MRI衍生的合成CT上实现颅缝分割的儿科颅骨CT合成框架，尽管MRI本身对骨和颅缝的显示能力有限。通过结合鲁棒的、领域特定的变分自编码器，该方法能够从常规儿科MRI中生成视觉上难以区分的颅骨合成CT，填补了无创颅骨评估中的关键空白。

</details>


### [6] [Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale](https://arxiv.org/abs/2512.23903)
*Charith Wickrema,Eliza Mace,Hunter Brown,Heidys Cabrera,Nick Krall,Matthew O'Neill,Shivangi Sarkar,Lowell Weissman,Eric Hughes,Guido Zarrella*

Main category: cs.CV

TL;DR: 该论文研究了在超高分辨率遥感电光（EO）数据集上训练基础模型的可扩展性，发现即使在千万亿像素规模下，模型性能仍受限于数据量而非模型参数，并为未来遥感基础模型的数据采集、算力分配和优化策略提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前多模态机器学习（如生成式AI）依赖于针对非文本模态的专用编码器，但在遥感等高价值领域，缺乏类似自然图像领域的成熟扩展规律，限制了基础模型的发展。

Method: 利用超过千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练更大规模的视觉Transformer（ViT）骨干网络，观察其在拍字节级训练中的成功与失败模式，并分析跨遥感模态的领域差距问题。

Result: 实验表明，在当前规模下，模型性能仍处于数据受限状态，而非模型参数受限；同时揭示了在遥感领域扩展基础模型时的关键挑战与行为模式。

Conclusion: 研究结果为遥感基础模型的未来发展提供了实践指导，包括数据收集策略、算力预算和优化调度，强调了扩大高质量遥感数据集的重要性。

Abstract: We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.

Abstract (中文翻译): 我们探索了人工智能的扩展行为，以建立在超高分辨率电光（EO）数据集上训练基础模型的实用技术，这些数据集的规模比当前最先进的水平高出几个数量级。现代多模态机器学习（ML）应用，例如用于图像描述、搜索和推理的生成式人工智能（GenAI）系统，依赖于针对非文本模态的鲁棒且领域专用的编码器。在互联网规模数据丰富的自然图像领域，已有成熟的扩展规律可用来优化模型容量、训练算力和数据集规模的联合扩展。然而，在遥感（RS）等高价值领域，这些关系远未被充分理解。我们利用超过千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练更大规模的视觉Transformer（ViT）骨干网络，报告在拍字节级训练中观察到的成功与失败模式，并分析其对弥合其他遥感模态间领域差距的启示。我们观察到，即使在此规模下，模型性能仍符合数据受限状态，而非模型参数受限状态。这些实践洞见旨在为数据收集策略、算力预算和优化调度提供参考，以推动前沿遥感基础模型的未来发展。

</details>


### [7] [Learning to learn skill assessment for fetal ultrasound scanning](https://arxiv.org/abs/2512.23920)
*Yipei Wang,Qianye Yang,Lior Drukker,Aris T. Papageorghiou,Yipeng Hu,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出一种无需人工标注技能等级的双层优化框架，通过评估胎儿超声图像上临床任务的完成质量来自动量化操作者的超声技能水平。


<details>
  <summary>Details</summary>
Motivation: 传统超声技能评估依赖专家主观监督，耗时且缺乏客观性；现有自动化方法多采用监督学习，受限于预设的技能影响因素。

Method: 提出一种双层优化框架，包含临床任务预测器和技能预测器，两者联合优化，通过任务执行效果间接评估技能，无需人工定义技能评分。

Result: 在真实胎儿头部超声扫描视频上验证了该方法的有效性，结果表明可通过优化后的任务表现作为技能指标来预测超声操作技能。

Conclusion: 所提框架能有效实现无监督、客观的胎儿超声技能自动评估，为技能量化提供了新思路。

Abstract: Traditionally, ultrasound skill assessment has relied on expert supervision and feedback, a process known for its subjectivity and time-intensive nature. Previous works on quantitative and automated skill assessment have predominantly employed supervised learning methods, often limiting the analysis to predetermined or assumed factors considered influential in determining skill levels. In this work, we propose a novel bi-level optimisation framework that assesses fetal ultrasound skills by how well a task is performed on the acquired fetal ultrasound images, without using manually predefined skill ratings. The framework consists of a clinical task predictor and a skill predictor, which are optimised jointly by refining the two networks simultaneously. We validate the proposed method on real-world clinical ultrasound videos of scanning the fetal head. The results demonstrate the feasibility of predicting ultrasound skills by the proposed framework, which quantifies optimised task performance as a skill indicator.

Abstract (中文翻译): 传统上，超声技能评估依赖专家的监督与反馈，这一过程具有主观性强且耗时的特点。以往关于定量和自动化技能评估的研究主要采用监督学习方法，通常将分析局限于预先设定或假定的影响技能水平的因素。本文提出了一种新颖的双层优化框架，通过评估在所获取的胎儿超声图像上临床任务的完成质量来评估胎儿超声操作技能，而无需使用人工预定义的技能评分。该框架包含一个临床任务预测器和一个技能预测器，通过同时优化两个网络进行联合训练。我们在真实临床胎儿头部扫描超声视频上验证了所提方法，结果表明该框架能够通过量化优化后的任务表现作为技能指标，有效预测超声操作技能。

</details>


### [8] [MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation](https://arxiv.org/abs/2512.23936)
*Yulong Zou,Bo Liu,Cun-Jing Zheng,Yuan-ming Geng,Siyue Li,Qiankun Zuo,Shuihua Wang,Yudong Zhang,Jin Hong*

Main category: cs.CV

TL;DR: 本文提出了一种名为MGML的新框架，用于在多模态MRI数据缺失情况下提升脑肿瘤分割性能，包含元参数自适应模态融合和一致性正则化模块，在BraTS2020/2023数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床中多模态MRI数据常不完整，难以充分利用多模态信息进行病灶分割，因此如何最大化利用不完整的多模态信息成为关键挑战。

Method: 提出元引导多模态学习（MGML）框架，包括元参数化自适应模态融合（Meta-AMF）和一致性正则化模块；前者根据可用模态生成自适应软标签监督信号以促进模态融合，后者提升分割性能并增强模型鲁棒性与泛化能力，且无需修改原模型结构。

Result: 在BraTS2020和BraTS2023数据集上实验表明，该方法优于多个SOTA方法；在BraTS2020十五种缺失模态组合的平均Dice分数上，WT、TC和ET分别达到87.55、79.36和62.67。

Conclusion: 所提MGML框架能有效应对多模态MRI缺失问题，显著提升脑肿瘤分割性能，且易于集成到现有训练流程中，具有良好的实用性和推广价值。

Abstract: Leveraging multimodal information from Magnetic Resonance Imaging (MRI) plays a vital role in lesion segmentation, especially for brain tumors. However, in clinical practice, multimodal MRI data are often incomplete, making it challenging to fully utilize the available information. Therefore, maximizing the utilization of this incomplete multimodal information presents a crucial research challenge. We present a novel meta-guided multi-modal learning (MGML) framework that comprises two components: meta-parameterized adaptive modality fusion and consistency regularization module. The meta-parameterized adaptive modality fusion (Meta-AMF) enables the model to effectively integrate information from multiple modalities under varying input conditions. By generating adaptive soft-label supervision signals based on the available modalities, Meta-AMF explicitly promotes more coherent multimodal fusion. In addition, the consistency regularization module enhances segmentation performance and implicitly reinforces the robustness and generalization of the overall framework. Notably, our approach does not alter the original model architecture and can be conveniently integrated into the training pipeline for end-to-end model optimization. We conducted extensive experiments on the public BraTS2020 and BraTS2023 datasets. Compared to multiple state-of-the-art methods from previous years, our method achieved superior performance. On BraTS2020, for the average Dice scores across fifteen missing modality combinations, building upon the baseline, our method obtained scores of 87.55, 79.36, and 62.67 for the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET), respectively. We have made our source code publicly available at https://github.com/worldlikerr/MGML.

Abstract (中文翻译): 利用磁共振成像（MRI）的多模态信息在病灶分割（尤其是脑肿瘤分割）中起着至关重要的作用。然而在临床实践中，多模态MRI数据常常不完整，使得难以充分利用已有信息。因此，如何最大化利用这种不完整的多模态信息成为一个关键的研究挑战。我们提出了一种新颖的元引导多模态学习（MGML）框架，该框架包含两个组成部分：元参数化自适应模态融合模块和一致性正则化模块。元参数化自适应模态融合（Meta-AMF）使模型能够在不同输入条件下有效整合多模态信息，通过基于可用模态生成自适应的软标签监督信号，显式地促进更一致的多模态融合。此外，一致性正则化模块提升了分割性能，并隐式增强了整个框架的鲁棒性与泛化能力。值得注意的是，我们的方法无需更改原始模型架构，可方便地集成到端到端模型优化的训练流程中。我们在公开的BraTS2020和BraTS2023数据集上进行了大量实验。与往年多种先进方法相比，我们的方法取得了更优的性能。在BraTS2020数据集上，针对十五种缺失模态组合的平均Dice分数，以基线模型为基础，我们的方法在全肿瘤（WT）、肿瘤核心（TC）和增强肿瘤（ET）上分别获得了87.55、79.36和62.67的分数。我们已将源代码公开发布于https://github.com/worldlikerr/MGML。

</details>


### [9] [Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation](https://arxiv.org/abs/2512.23938)
*Hualin Ye,Bingxi Liu,Jixiang Du,Yu Qin,Ziyi Chen,Hong Zhang*

Main category: cs.CV

TL;DR: 本文提出一种新型跨视角地理定位（CVGL）系统，通过DINOv2骨干网络、多尺度通道重分配模块和基于MoE的聚合模块，在减少参数量的同时取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 跨视角地理定位中，由于视角差异显著，导致特征聚合与对齐困难，现有方法难以有效应对。

Method: 1）采用DINOv2骨干网络并结合卷积适配器进行微调；2）设计多尺度通道重分配模块以增强空间表示的多样性与稳定性；3）在特征聚合中引入混合专家（MoE）路由机制，动态选择专家子空间处理异构输入。

Result: 在University-1652和SUES-200数据集上的大量实验表明，该方法在使用更少训练参数的情况下取得了具有竞争力的性能。

Conclusion: 所提方法有效缓解了跨视角地理定位中的视角差异问题，在保持模型轻量化的同时提升了定位准确率。

Abstract: Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.

Abstract (中文翻译): 跨视角地理定位（CVGL）旨在通过将查询图像与大规模数据库中的图像进行匹配，来估计其地理位置。然而，显著的视角差异给有效的特征聚合与对齐带来了巨大挑战。为解决这些问题，我们提出了一种新颖的CVGL系统，包含三项关键改进：首先，利用DINOv2骨干网络并结合卷积适配器进行微调，以增强模型对跨视角变化的适应能力；其次，提出一个多尺度通道重分配模块，以增强空间表示的多样性和稳定性；最后，设计了一个改进的聚合模块，将混合专家（MoE）路由机制融入特征聚合过程，具体而言，该模块在交叉注意力框架中动态为键（keys）和值（values）选择专家子空间，从而实现对异构输入域的自适应处理。在University-1652和SUES-200数据集上的大量实验表明，我们的方法在使用更少训练参数的情况下取得了具有竞争力的性能。

</details>


### [10] [Kinematic-Based Assessment of Surgical Actions in Microanastomosis](https://arxiv.org/abs/2512.23942)
*Yan Meng,Daniel Donoho,Marcelle Altshuler,Omar Arnaout*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化框架，用于显微吻合术中的动作分割与技能评估，可在边缘计算设备上高效运行，并在58个专家评分视频上验证了其高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统显微外科手术评估依赖专家主观评分，存在评分者间差异大、耗时长等问题，亟需客观、可扩展的自动化评估方法。

Method: 该系统包含三个模块：(1) 基于YOLO和DeepSORT的器械尖端追踪定位模块；(2) 利用自相似矩阵进行动作边界检测与无监督聚类的动作分割模块；(3) 用于评估手术动作熟练度的有监督分类模块。

Result: 在包含58个专家评分视频的数据集上，该方法实现了92.4%的帧级动作分割准确率和85.5%的整体技能分类准确率。

Conclusion: 所提方法有望为显微外科培训提供客观、实时的反馈，推动标准化、数据驱动的训练体系，并提升高风险手术环境中的能力评估水平。

Abstract: Proficiency in microanastomosis is a critical surgical skill in neurosurgery, where the ability to precisely manipulate fine instruments is crucial to successful outcomes. These procedures require sustained attention, coordinated hand movements, and highly refined motor skills, underscoring the need for objective and systematic methods to evaluate and enhance microsurgical training. Conventional assessment approaches typically rely on expert raters supervising the procedures or reviewing surgical videos, which is an inherently subjective process prone to inter-rater variability, inconsistency, and significant time investment. These limitations highlight the necessity for automated and scalable solutions. To address this challenge, we introduce a novel AI-driven framework for automated action segmentation and performance assessment in microanastomosis procedures, designed to operate efficiently on edge computing platforms. The proposed system comprises three main components: (1) an object tip tracking and localization module based on YOLO and DeepSORT; (2) an action segmentation module leveraging self-similarity matrix for action boundary detection and unsupervised clustering; and (3) a supervised classification module designed to evaluate surgical gesture proficiency. Experimental validation on a dataset of 58 expert-rated microanastomosis videos demonstrates the effectiveness of our approach, achieving a frame-level action segmentation accuracy of 92.4% and an overall skill classification accuracy of 85.5% in replicating expert evaluations. These findings demonstrate the potential of the proposed method to provide objective, real-time feedback in microsurgical education, thereby enabling more standardized, data-driven training protocols and advancing competency assessment in high-stakes surgical environments.

Abstract (中文翻译): 显微吻合术的熟练程度是神经外科中一项关键的手术技能，精准操控精细器械的能力对成功结果至关重要。此类操作需要持续专注、协调的手部动作以及高度精细的运动技能，因此亟需客观且系统化的方法来评估和提升显微外科训练效果。传统的评估方法通常依赖专家现场监督或回看手术视频进行评分，这一过程本质上具有主观性，易受评分者间差异、不一致性和大量时间投入的影响。这些局限性凸显了对自动化、可扩展解决方案的需求。为应对这一挑战，我们提出了一种新颖的AI驱动框架，用于显微吻合术中的自动化动作分割与表现评估，该框架专为在边缘计算平台上高效运行而设计。所提出的系统包含三个主要组成部分：（1）基于YOLO和DeepSORT的器械尖端追踪与定位模块；（2）利用自相似矩阵进行动作边界检测与无监督聚类的动作分割模块；（3）用于评估手术动作熟练度的有监督分类模块。在包含58个经专家评分的显微吻合术视频数据集上的实验验证表明，该方法在复现专家评估方面表现出色，帧级动作分割准确率达到92.4%，整体技能分类准确率为85.5%。这些结果表明，所提方法在显微外科教育中具备提供客观、实时反馈的潜力，从而推动更标准化、数据驱动的训练方案，并提升高风险手术环境中能力评估的水平。

</details>
