<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合运动学计算与语言解析的自动标注流程，构建了细粒度人体动作理解数据集KPM-Bench，并提出了基于语言的MoPE算法用于提取动作属性、评估并缓解视频描述中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频描述模型在细粒度动作细节描述上存在不足，且容易产生严重幻觉，尤其在以动作为核心的视频中，对复杂肢体动态的精确刻画常被忽视。

Method: 作者开发了一个自动化标注流程，融合运动学计算与语言解析，构建了KPM-Bench数据集；同时提出MoPE算法，从文本描述中提取动作特定属性，并将其集成到GRPO后训练框架中以减少幻觉。

Result: KPM-Bench包含细粒度视频-描述对、动作理解问答对和专门用于评估动作描述幻觉的评测集；MoPE算法可独立于大模型进行精准幻觉评估，并显著提升动作相关视频描述的可靠性。

Conclusion: 通过引入KPM-Bench和MoPE方法，本文有效缓解了视频描述中的动作幻觉问题，为细粒度动作理解和可靠视频描述提供了新工具和基准。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

Abstract (中文翻译): 尽管近期取得了一些进展，视频描述模型在准确描述细粒度动作细节方面仍面临显著局限，并存在严重的幻觉问题。这些挑战在生成以动作为核心的视频描述时尤为突出，因为此类任务要求精确刻画复杂的动作和肢体动态，而这一点往往被忽视。为弥合这一差距，我们提出了一种自动标注流程，该流程融合了基于运动学的动作计算与语言解析，能够对复杂人体动作进行细致分解与描述。基于此流程，我们构建并发布了运动学解析动作基准（Kinematic Parsing Motion Benchmark, KPM-Bench），这是一个旨在促进细粒度动作理解的新型开源数据集。KPM-Bench包含：(i) 全面展现复杂动作中肢体层级动态的细粒度视频-描述对；(ii) 专注于动作理解的多样化且具挑战性的问答对；以及(iii) 一个精心策划的评测集，专门用于评估与动作描述相关的幻觉现象。此外，为系统性解决幻觉问题，我们提出了基于语言的“动作解析与提取”（Motion Parsing and Extraction, MoPE）算法，能够直接从文本描述中准确提取动作特定属性。利用MoPE，我们设计了一种精确的幻觉评估指标，该指标无需依赖大规模视觉-语言模型或纯语言模型即可运行。通过将MoPE集成到GRPO后训练框架中，我们有效缓解了幻觉问题，显著提升了以动作为核心的视频描述模型的可靠性。

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 本文提出了3D-HIW数据集和CLUTCH系统，用于实现高质量、可扩展的野外手部动作建模，支持文本到手部动作生成和手部动画描述任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于有限且昂贵的影棚采集数据，难以扩展到真实世界（in-the-wild）场景，且在动作保真度与文本-动作对齐方面表现不足。

Method: (1) 构建包含32K个3D手部动作序列及其对应文本的3D-HIW数据集，采用结合视觉语言模型与3D手部追踪器的标注流程；(2) 提出基于大语言模型（LLM）的CLUTCH系统，包含：a) 新型VQ-VAE架构SHIFT用于手部动作分词，b) 几何精炼阶段以提升动画质量。

Result: 在文本到动作和动作到文本任务上达到SOTA性能，建立了首个可扩展的野外手部动作建模基准。

Conclusion: 通过构建大规模真实场景数据集和创新的LLM驱动动画系统，显著推进了野外手部动作建模的能力，并将开源代码、数据和模型。

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

Abstract (中文翻译): 手在日常生活中扮演着核心角色，但对自然手部动作的建模仍鲜有探索。现有处理文本到手部动作生成或手部动画描述的方法依赖于动作和上下文有限的影棚采集数据集，难以低成本地扩展到“野外”场景。此外，当前模型及其训练方案难以同时保证动画保真度和文本-动作对齐。为解决这些问题，我们（1）引入了“野外3D手部”（3D-HIW）数据集，包含32K个3D手部动作序列及其对齐文本；（2）提出了基于大语言模型（LLM）的手部动画系统CLUTCH，其包含两项关键创新：(a) 一种名为SHIFT的新型VQ-VAE架构，用于对手部动作进行分词；(b) 一个几何精炼阶段，用于微调LLM。为构建3D-HIW，我们提出了一种数据标注流程，结合视觉语言模型（VLMs）和最先进的3D手部追踪器，并将其应用于涵盖广泛场景的大规模第一人称视角动作视频语料库。为充分捕捉野外动作，CLUTCH采用了SHIFT——一种部件-模态分解的VQ-VAE，从而提升了泛化能力和重建保真度。最后，为提高动画质量，我们引入了几何精炼阶段，在该阶段中，CLUTCH通过直接作用于解码后手部动作参数的重建损失进行协同监督。实验表明，该方法在文本到动作和动作到文本任务上均达到了最先进的性能，建立了首个可扩展的野外手部动作建模基准。代码、数据和模型将被公开发布。

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: 本文提出PRISM框架，通过结合边缘检测与光照解耦的自监督方法，在结肠镜场景中实现单目深度与位姿估计的SOTA性能，并揭示真实数据自监督训练和视频帧率对性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 结肠镜辅助导航中单目深度与位姿估计面临无纹理表面、复杂光照、形变及缺乏可靠体内真值数据等挑战，亟需有效利用解剖结构与光照先验提升几何学习效果。

Method: 提出PRISM（Pose-Refinement with Intrinsic Shading and edge Maps）自监督学习框架，融合基于学习的边缘检测（如DexiNed/HED）获取高频边界信息，并通过本征分解模块分离明暗（shading）与反射（reflectance），利用明暗线索辅助深度估计。

Result: 在多个真实与合成数据集上达到最先进性能；消融实验表明：(1) 真实数据上的自监督训练优于逼真模型数据的监督训练；(2) 视频帧率对模型性能至关重要，需针对数据集定制采样策略。

Conclusion: PRISM有效利用解剖与光照先验提升结肠镜场景下的几何估计能力，强调了域真实性与高质量视频采样在医学视觉任务中的重要性。

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

Abstract (中文翻译): 单目深度与位姿估计在结肠镜辅助导航的发展中起着重要作用，因其能够通过减少盲区、降低遗漏或复发性病灶的风险，以及减少检查不完整的情况，从而改善筛查效果。然而，由于存在无纹理表面、复杂的光照模式、组织形变以及缺乏带有可靠真值的体内数据集，该任务仍然具有挑战性。本文提出了PRISM（Pose-Refinement with Intrinsic Shading and edge Maps）——一种自监督学习框架，利用解剖结构和光照先验来引导几何学习。我们的方法独特地结合了边缘检测与亮度解耦以提供结构引导：具体而言，使用基于学习的边缘检测器（如DexiNed或HED）提取细薄且高频的边界以生成边缘图；同时通过一个本征分解模块将明暗（shading）与反射（reflectance）分离，使模型能够利用明暗线索进行深度估计。在多个真实和合成数据集上的实验结果展示了当前最先进的性能。我们还对训练数据选择进行了全面的消融研究，以确立结肠镜下位姿与深度估计的最佳实践。该分析得出两个实用见解：(1) 在真实世界数据上的自监督训练优于在逼真模型数据上的监督训练，凸显了域真实性相较于真值可用性的优势；(2) 视频帧率是影响模型性能的极其关键因素，需针对特定数据集进行视频帧采样以生成高质量训练数据。

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: 本文提出LGD-Net，一种无需生成虚拟IHC图像、直接从H&E切片预测HER2表达水平的新方法，通过跨模态特征幻觉和领域知识正则化，在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准的HER2免疫组化（IHC）染色流程资源消耗大、成本高、耗时长，且在许多地区不可用；而现有基于H&E切片预测HER2的方法依赖像素级虚拟染色，计算开销大且易引入伪影，影响诊断准确性。

Method: 提出Latent-Guided Dual-Stream Network（LGD-Net），不进行显式的像素级图像生成，而是通过跨模态特征幻觉将H&E形态特征映射到分子表征的潜在空间，并在训练中由IHC教师编码器引导；同时引入轻量级辅助任务（如细胞核分布和膜染色强度）对模型进行领域知识正则化。

Result: 在公开BCI数据集上的大量实验表明，LGD-Net显著优于基线方法，达到当前最优性能，且仅需单模态H&E输入即可高效推理。

Conclusion: LGD-Net提供了一种高效、准确且无需虚拟染色的HER2评分新范式，具有临床应用潜力。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

Abstract (中文翻译): 准确评估HER2表达水平对于乳腺癌的评估和靶向治疗选择至关重要。然而，标准的多步骤免疫组化（IHC）染色过程资源密集、昂贵且耗时，在许多地区常常无法获得。因此，直接从H&E切片预测HER2水平已成为一种潜在的替代方案。已有研究表明，利用从H&E图像生成的虚拟IHC图像进行自动HER2评分是有效的。然而，基于像素级的虚拟染色方法计算成本高，且容易产生重建伪影，从而导致诊断错误。为解决这些局限性，我们提出了Latent-Guided Dual-Stream Network（LGD-Net），这是一种新颖的框架，采用跨模态特征幻觉而非显式的像素级图像生成。LGD-Net在训练过程中借助教师IHC编码器的引导，学习将H&E的形态学特征直接映射到分子表征的潜在空间。为确保幻觉特征能够捕捉临床上相关的表型，我们通过轻量级的辅助正则化任务（特别是细胞核分布和膜染色强度）显式地将任务特定的领域知识融入模型训练。在公开BCI数据集上的大量实验表明，LGD-Net实现了最先进的性能，显著优于基线方法，同时仅使用单模态H&E输入即可实现高效推理。

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出一种无需额外训练的遥感图像文本引导分割方法，结合对比式和生成式视觉语言模型与SAM，在零样本设置下实现开放词汇、指代表达和推理型语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像文本引导分割方法大多依赖可训练组件，限制了泛化能力和实际应用；本文旨在探索仅使用现有基础模型、无需额外训练即可实现该任务的可能性。

Method: 提出两种策略：1）对比式方法利用CLIP作为掩码选择器，从SAM的网格提案中选出匹配文本描述的区域；2）生成式方法通过GPT-5或LoRA微调的Qwen-VL生成点击提示输入SAM，实现指代与推理分割。整体框架完全无需训练或仅需轻量LoRA微调。

Result: 在19个遥感基准数据集上（涵盖开放词汇、指代和推理任务）取得优异性能，其中对比方法在零样本开放词汇语义分割上达到SOTA，生成式方法中LoRA微调Qwen-VL效果最佳。

Conclusion: 仅依靠现有基础模型即可高效实现遥感图像的零样本文本引导分割，无需额外训练，显著提升模型通用性与实用性。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

Abstract (中文翻译): 视觉语言模型（VLMs）和视觉基础模型（VFMs）的最新进展为遥感影像的零样本文本引导分割开辟了新机遇。然而，大多数现有方法仍依赖额外的可训练组件，限制了其泛化能力和实际适用性。本文研究了在不进行额外训练的情况下，仅依靠现有基础模型能在多大程度上实现基于文本的遥感分割。我们提出了一种简单而有效的方法，将对比式和生成式VLM与Segment Anything Model（SAM）相结合，构建了一个完全无需训练或仅需轻量LoRA微调的流程。我们的对比式方法利用CLIP作为掩码选择器，从SAM的网格提案中选出与文本描述最匹配的区域，在完全零样本设置下实现了最先进的开放词汇语义分割（OVSS）。同时，我们的生成式方法通过GPT-5在零样本设置下生成点击提示，或使用LoRA微调的Qwen-VL模型生成提示来驱动SAM，实现指代表达与推理型分割，其中后者取得了最佳效果。在包括开放词汇、指代和推理任务在内的19个遥感基准上的大量实验验证了我们方法的强大能力。代码将发布于 https://github.com/josesosajs/trainfree-rs-segmentation。

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: VidEoMT 是一种仅使用编码器的视频分割模型，通过轻量级查询传播机制实现高效准确的时序建模，无需专用跟踪模块，速度提升5–10倍。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频分割方法通常依赖复杂的专用跟踪模块，带来架构复杂性和计算开销；受近期纯 ViT 在图像分割中成功启发，作者希望构建一个无需专用跟踪模块的简单高效视频分割模型。

Method: 提出 VidEoMT 模型，在仅编码器的 ViT 架构中引入轻量级查询传播机制，将前一帧的查询复用于当前帧，并结合与时间无关的学习查询进行融合，以兼顾时序一致性和内容适应性。

Result: VidEoMT 在保持竞争力精度的同时，推理速度达 160 FPS（ViT-L 骨干），比现有方法快 5–10 倍。

Conclusion: 通过在纯编码器 ViT 中设计简洁有效的查询传播与融合机制，VidEoMT 成功消除了对专用跟踪模块的依赖，在效率和性能之间取得良好平衡。

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

Abstract (中文翻译): 现有的在线视频分割模型通常将逐帧分割器与复杂的专用跟踪模块相结合。尽管有效，但这些模块引入了显著的架构复杂性和计算开销。近期研究表明，当具备足够容量并经过大规模预训练时，普通的视觉 Transformer（ViT）编码器无需专用模块即可实现精确的图像分割。受此启发，我们提出了 Video Encoder-only Mask Transformer（VidEoMT），这是一种简单的仅编码器视频分割模型，无需专用跟踪模块。为在仅编码器的 ViT 中实现时序建模，VidEoMT 引入了一种轻量级的查询传播机制，通过复用前一帧的查询来跨帧传递信息。为了在保持时序一致性的同时适应新内容，该模型采用了一种查询融合策略，将传播的查询与一组与时序无关的可学习查询相结合。因此，VidEoMT 在不增加复杂度的情况下获得了跟踪器的优势，在保持有竞争力精度的同时，速度提升5至10倍，使用 ViT-L 骨干网络时最高可达每秒160帧。代码：https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本文提出了首个面向内容视频检索（CBVR）的查询性能预测（QPP）基准VQPP，包含两个文本-视频检索数据集和两个CBVR系统，并验证了预检索预测器的有效性及其在大语言模型查询改写任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测（QPP）在文本和图像检索中已有广泛研究，但在基于内容的视频检索（CBVR）领域仍属空白。为推动该方向的发展，作者构建了首个专门针对视频检索的QPP基准。

Method: 作者构建了名为VQPP的基准，包含56K文本查询和51K视频，涵盖两个文本到视频检索数据集和两个CBVR系统，并提供官方训练、验证和测试划分。同时评估了多种预检索和后检索性能预测器，并将最佳预检索预测器作为奖励模型，通过直接偏好优化（DPO）训练大语言模型进行查询改写。

Result: 实验表明，预检索预测器能够取得具有竞争力的性能，使其可在实际检索前部署。此外，将该预测器用于LLM的查询改写任务也验证了VQPP的实际应用价值。

Conclusion: VQPP是首个面向视频检索的QPP基准，为该领域的研究提供了可复现的平台，并展示了QPP在下游任务（如查询改写）中的潜力。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

Abstract (中文翻译): 查询性能预测（QPP）是一项重要且被广泛研究的信息检索任务，具有多种应用场景，例如查询改写、查询扩展和检索系统选择等。该任务主要在文本和图像检索的背景下进行研究，而面向基于内容的视频检索（CBVR）的QPP则基本未被探索。为此，我们提出了首个视频查询性能预测（VQPP）基准，包含两个文本到视频检索数据集和两个CBVR系统。VQPP总共包含5.6万个文本查询和5.1万个视频，并提供了官方的训练集、验证集和测试集划分，以促进直接比较和可复现的结果。我们探索了多种预检索和后检索性能预测器，为未来在视频领域探索QPP建立了一个具有代表性的基准。我们的结果表明，预检索预测器能够获得具有竞争力的性能，从而在执行检索步骤之前实现相关应用。我们还通过将表现最佳的预检索预测器作为奖励模型，利用直接偏好优化（DPO）方法在查询改写任务上训练大语言模型（LLM），展示了VQPP的适用性。我们在https://github.com/AdrianLutu/VQPP发布了该基准和代码。

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 该论文指出Liu和Szirányi提出的姿态识别方法因采用帧级随机划分训练/测试集，导致数据泄露，其报告的近乎完美的准确率无法反映模型对未见个体的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估Liu和Szirányi提出的手势识别方法的有效性，特别是其评估协议是否合理，能否真正衡量模型在面对新用户时的泛化性能。

Method: 通过分析其公开的混淆矩阵、学习曲线和数据集构建方式，揭示其帧级随机划分导致同一受试者的数据同时出现在训练集和测试集中，从而造成数据泄露。

Result: 证明该方法的高准确率源于数据泄露，其评估结果不能反映模型对未见过个体的手势识别能力。

Conclusion: 强调在基于视觉的手势识别研究中，尤其是面向无人机-人类交互等需识别新用户手势的应用场景，必须采用受试者独立的数据划分方式。

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

Abstract (中文翻译): 本文对Liu和Szirányi提出的手势识别方法进行了方法论分析，特别关注其评估协议的有效性。我们指出，其所报告的近乎完美的准确率源于帧级别的随机训练-测试划分，这种划分不可避免地将同一受试者的数据样本同时混入训练集和测试集，从而导致严重的数据泄露。通过检查其公开的混淆矩阵、学习曲线和数据集构建方式，我们证明该评估方法无法衡量模型对未见过个体的泛化能力。我们的发现强调了在基于视觉的手势识别研究中采用受试者独立数据划分的重要性，尤其是在无人机-人类交互等需要可靠识别先前未见过人员所做手势的应用场景中。

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 本文提出了一种用于长视频理解的端到端框架，结合自适应视频采样器（AVS）和基于自编码器的时空压缩器（SVC），有效提取关键信息并实现高压缩率，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 长视频具有高度冗余性，现有模型在有限内存下难以高效处理大量帧并从中提取判别性信息，因此亟需一种能兼顾效率与性能的长视频理解方法。

Method: 提出一个端到端架构，包含基于信息密度的自适应视频采样器（AVS）和基于自编码器的时空视频压缩器（SVC），并与多模态大语言模型（MLLM）集成。

Result: 该框架在多个长视频理解任务和标准视频理解基准上均取得优异性能，展现出良好的通用性和有效性。

Conclusion: 所提方法能有效应对长视频冗余问题，在保持关键判别信息的同时实现高效率处理，为长视频理解提供了有力解决方案。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

Abstract (中文翻译): 随着视频骨干网络架构的最新进展以及大语言模型（LLMs）的显著成就，对时长可达数十分钟的长视频进行分析已变得可行且日益普遍。然而，视频序列固有的冗余性给当前最先进的模型带来了重大挑战，主要体现在两个方面：1）在内存限制下高效地纳入更多帧；2）从海量输入数据中提取判别性信息。本文提出了一种用于长视频理解的新型端到端框架，该框架包括一个基于信息密度的自适应视频采样器（AVS）和一个与多模态大语言模型（MLLM）集成的基于自编码器的时空视频压缩器（SVC）。所提出的系统具有两大优势：能够自适应且高效地从不同长度的视频序列中捕捉关键信息，并在保持重要判别信息的同时实现高压缩率。该框架在多种基准测试中展现出卓越性能，在长视频理解任务和标准视频理解基准上均表现突出，凸显了其在处理长时间视频序列复杂性方面的通用性与有效性。

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: 尽管视觉语言模型（VLMs）在多种视觉问答任务上表现优异，但在细粒度图像分类任务上仍显不足；研究发现更强的视觉编码器和预训练阶段对提升细粒度性能尤为关键。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视觉问答、文档理解等任务上取得显著进展，但在传统细粒度图像分类任务中表现不佳，作者旨在探究造成这种性能差异的原因。

Method: 作者在多个细粒度分类基准上评估了近期的VLMs，并通过消融实验分析不同组件（如语言模型、视觉编码器、预训练策略）对细粒度性能的影响。

Result: 实验表明，更强的语言模型对所有基准带来均衡提升，而更强的视觉编码器则显著更有利于细粒度分类；此外，在预训练阶段解冻语言模型权重对细粒度性能至关重要。

Conclusion: 提升VLMs的细粒度视觉理解能力需重点关注视觉编码器质量和预训练策略，这为未来构建更强大的视觉中心型多模态模型提供了方向。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

Abstract (中文翻译): 视觉语言模型（VLMs）在各类视觉问答基准测试中取得了显著进展，涵盖视觉推理、文档理解和多模态对话等领域。这些进步体现在基于不同基础模型、对齐架构和训练数据构建的多种VLMs中。然而，近期研究表明，这些模型在测试细粒度视觉知识的传统图像分类基准上表现滞后。我们对大量近期VLMs在细粒度分类基准上进行了测试，并识别出导致细粒度知识与其他视觉基准之间性能脱节的潜在因素。通过一系列消融实验，我们发现使用更强的语言模型能同等提升所有基准的得分，而更强的视觉编码器则对细粒度分类性能带来不成比例的更大提升。此外，我们还发现预训练阶段对细粒度性能同样至关重要，尤其是在预训练期间解冻语言模型权重时。这些发现为增强VLMs的细粒度视觉理解能力和以视觉为中心的能力铺平了道路。

</details>
