<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement](https://arxiv.org/abs/2602.10138)
*Zhihang Yi,Jian Zhao,Jiancheng Lv,Tao Wang*

Main category: cs.CV

TL;DR: 本文综述了多模态大语言模型（MLLMs）在图表理解中的应用，系统梳理了该领域的核心挑战、任务分类、数据集和方法演进，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 图表理解需要融合图形与文本信息，而当前基于MLLM的图表分析研究较为零散，缺乏系统性整理，因此亟需一份全面的综述以厘清该领域的发展脉络与关键问题。

Method: 作者通过分析图表中视觉与语言信息融合的基本挑战，对下游任务和数据集进行分类（提出规范与非规范基准的新分类法），并系统回顾从传统深度学习到最新MLLM范式的方法演进。

Result: 论文构建了MLLM图表理解领域的系统性框架，揭示了当前模型在感知与推理方面的不足，并提出了包括高级对齐技术和强化学习在内的未来研究方向。

Conclusion: 本综述为研究人员和从业者提供了MLLM如何变革图表信息融合的结构化理解，旨在推动更鲁棒、可靠系统的开发。

Abstract: Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.

Abstract (中文翻译): 图表理解是一项典型的信息融合任务，需要无缝整合图形与文本数据以提取意义。多模态大语言模型（MLLMs）的出现彻底改变了这一领域，但基于MLLM的图表分析研究仍显零散，缺乏系统性组织。本综述通过梳理该新兴领域的核心组成部分，提供了一份全面的路线图。我们首先分析了图表中融合视觉与语言信息所面临的基本挑战；随后对下游任务和数据集进行了分类，并提出了一种新颖的规范与非规范基准分类法，以凸显该领域不断扩展的范围；接着，我们全面回顾了方法论的演进历程，追溯了从经典深度学习技术到利用复杂融合策略的前沿MLLM范式的转变。通过批判性地审视当前模型的局限性，特别是其在感知与推理方面的缺陷，我们识别出若干有前景的未来方向，包括高级对齐技术以及用于认知增强的强化学习。本综述旨在为研究人员和从业者提供对MLLM如何变革图表信息融合的结构化理解，并推动构建更加鲁棒和可靠的系统。

</details>


### [2] [Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization](https://arxiv.org/abs/2602.10159)
*Tao Yu,Yujia Yang,Haopeng Jin,Junhao Gong,Xinlong Chen,Yuxuan Zhou,Shanbin Zhang,Jiabing Yang,Xinming Wang,Hongzhu Yi,Ping Nie,Kai Zou,Zhang Zhang,Yan Huang,Liang Wang,Yeshani,Ruiwen Tao,Jin Ma,Haijin Liang,Jinwen Luo*

Main category: cs.CV

TL;DR: 本文提出了RVMS-Bench，一个用于评估基于模糊记忆的真实世界视频检索的基准，并引入了模拟人类“回忆-搜索-验证”认知过程的RACLO智能体框架，发现现有MLLM在该任务上仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 传统视频检索基准局限于精确描述与封闭视频池的匹配，无法反映现实世界中用户基于模糊、多维度记忆在开放网络上进行搜索的场景。

Method: 构建了包含1440个样本的RVMS-Bench基准，样本来自真实开放网络视频，涵盖20个类别和4种时长，并采用包含全局印象、关键时刻、时间上下文和听觉记忆的分层描述框架。同时提出了RACLO智能体框架，通过溯因推理模拟人类的认知过程。

Result: 实验表明，现有的多模态大语言模型（MLLMs）在基于模糊记忆的真实世界视频检索和时刻定位任务上能力仍然不足。

Conclusion: 本工作通过提出新的基准和方法，将推动视频检索技术在真实世界非结构化场景下的鲁棒性发展。

Abstract: Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \textbf{1,440 samples} spanning \textbf{20 diverse categories} and \textbf{four duration groups}, sourced from \textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.

Abstract (中文翻译): 传统的视频检索基准侧重于将精确描述与封闭的视频池进行匹配，未能反映现实世界中以模糊、多维度记忆为特征的开放网络搜索。我们提出了\textbf{RVMS-Bench}，一个用于评估真实世界视频记忆搜索的综合系统。它包含了\textbf{1,440个样本}，涵盖\textbf{20个多样化的类别}和\textbf{四种时长分组}，均源自\textbf{真实的开放网络视频}。RVMS-Bench采用了一个分层描述框架，包括\textbf{全局印象、关键时刻、时间上下文和听觉记忆}，以模拟真实的多维搜索线索，并通过人在回路协议对所有样本进行了严格验证。我们进一步提出了\textbf{RACLO}，一个利用溯因推理来模拟人类“回忆-搜索-验证”认知过程的智能体框架，有效应对了在现实世界中通过模糊记忆搜索视频的挑战。实验表明，现有的多模态大语言模型（MLLMs）在基于模糊记忆的真实世界视频检索和时刻定位方面仍显不足。我们相信这项工作将促进视频检索技术在真实世界非结构化场景中的鲁棒性提升。

</details>


### [3] [AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems](https://arxiv.org/abs/2602.10160)
*Ishan Sahu,Somnath Hazra,Somak Aditya,Soumyajit Dey*

Main category: cs.CV

TL;DR: 本文评估了端到端自动驾驶系统在CARLA中面对黑盒对抗攻击的脆弱性，发现现有先进模型（如Transfuser和Interfuser）在三种视觉感知攻击下性能大幅下降，并提出了一种基于注意力机制的轻量级攻击检测模型AD²，有效提升了检测能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管端到端自动驾驶系统取得了显著进展，但其对抗鲁棒性尚未得到充分研究。作者旨在揭示当前主流自动驾驶代理在现实可行的黑盒对抗攻击下的脆弱性，并提出有效的防御机制以提升系统安全性。

Method: 作者在CARLA仿真环境中对两种先进自动驾驶代理（Transfuser和Interfuser）进行闭环评估，测试三种针对视觉感知管道的攻击：(i) 声波引起的物理模糊攻击，(ii) 电磁干扰导致的图像失真攻击，(iii) 在图像上添加精心设计的有界扰动以生成“幽灵物体”的数字攻击。为缓解这些威胁，作者提出一种基于注意力机制的轻量级攻击检测模型AD²，利用时空一致性进行检测。

Result: 实验表明，所测试的攻击可使自动驾驶代理的驾驶得分最多下降99%，暴露出严重安全漏洞。所提出的AD²模型在CARLA多摄像头输入下展现出优于现有方法的检测能力和计算效率。

Conclusion: 当前端到端自动驾驶系统对现实世界中的黑盒对抗攻击极为脆弱，亟需鲁棒的防御机制。本文提出的AD²模型为提升自动驾驶系统的安全性提供了一种高效可行的解决方案。

Abstract: End-to-end autonomous driving systems have achieved significant progress, yet their adversarial robustness remains largely underexplored. In this work, we conduct a closed-loop evaluation of state-of-the-art autonomous driving agents under black-box adversarial threat models in CARLA. Specifically, we consider three representative attack vectors on the visual perception pipeline: (i) a physics-based blur attack induced by acoustic waves, (ii) an electromagnetic interference attack that distorts captured images, and (iii) a digital attack that adds ghost objects as carefully crafted bounded perturbations on images. Our experiments on two advanced agents, Transfuser and Interfuser, reveal severe vulnerabilities to such attacks, with driving scores dropping by up to 99% in the worst case, raising valid safety concerns. To help mitigate such threats, we further propose a lightweight Attack Detection model for Autonomous Driving systems (AD$^2$) based on attention mechanisms that capture spatial-temporal consistency. Comprehensive experiments across multi-camera inputs on CARLA show that our detector achieves superior detection capability and computational efficiency compared to existing approaches.

Abstract (中文翻译): 端到端自动驾驶系统已取得显著进展，但其对抗鲁棒性仍鲜有探索。本研究在CARLA仿真环境中，对最先进的自动驾驶代理在黑盒对抗威胁模型下进行了闭环评估。具体而言，我们考察了视觉感知流程上的三种代表性攻击向量：(i) 由声波引起的基于物理的模糊攻击，(ii) 导致捕获图像失真的电磁干扰攻击，以及(iii) 在图像上添加精心设计的有界扰动以生成“幽灵物体”的数字攻击。我们对两种先进代理（Transfuser和Interfuser）的实验揭示了它们对此类攻击的严重脆弱性，在最坏情况下驾驶得分下降高达99%，引发了切实的安全担忧。为缓解此类威胁，我们进一步提出了一种基于注意力机制的轻量级自动驾驶攻击检测模型（AD²），该机制能够捕捉时空一致性。在CARLA上对多摄像头输入的综合实验表明，我们的检测器相比现有方法实现了更优的检测能力和计算效率。

</details>


### [4] [ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop](https://arxiv.org/abs/2602.10173)
*Clement Fuji Tsang,Anita Hu,Or Perel,Carsten Kolve,Maria Shugrina*

Main category: cs.CV

TL;DR: 本文提出了一套交互式3D高斯点阵（3DGS）选择与分割工具，结合AI驱动的2D到3D传播方法和灵活的手动编辑功能，使用户能对任意野外采集的3DGS场景进行精确的二值分割和局部编辑，无需额外优化。


<details>
  <summary>Details</summary>
Motivation: 当前从野外采集的3D高斯点阵（3DGS）中提取可用对象仍具挑战性，且可控编辑技术有限；现有方法多聚焦于自动或高层编辑，缺乏灵活、用户可控的交互式工具。

Method: 提出一种快速的AI驱动方法，将用户引导的2D选择掩码传播至3DGS，并结合灵活的手动选择与分割工具，实现对非结构化3DGS场景的任意二值分割；进一步结合自定义视频扩散模型，支持用户引导的局部编辑。

Result: 所提出的工具集在3DGS选择任务上优于当前最先进方法，并成功应用于下游的局部编辑任务，用户可直接控制AI修改的区域。

Conclusion: 该交互式选择与编辑工具适用于任意野外采集的3DGS场景，无需额外优化，显著提升了用户对3D高斯点阵表示的可控性和编辑灵活性。

Abstract: Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

Abstract (中文翻译): 3D高斯点阵（3DGS）表示正逐渐成为传统图形学的一种可行替代方案，适用于越来越多的应用场景，包括近期支持物理仿真与动画的技术。然而，从野外采集的数据中提取可用对象仍然具有挑战性，且针对该表示的可控编辑技术十分有限。不同于当前大多数聚焦于自动化方案或高层编辑的新方法，我们提出了一套以高斯点阵选择与分割为核心的交互式工具集。我们设计了一种快速的AI驱动方法，可将用户引导的2D选择掩码传播至3DGS选择中，允许用户在出现错误时进行干预，并进一步结合灵活的手动选择与分割工具，使用户能够对非结构化的3DGS场景实现几乎任意的二值分割。我们在3D高斯点阵选择任务上将本工具集与当前最先进方法进行了对比评估，并通过开发一种结合自定义视频扩散模型的用户引导局部编辑方法，展示了其在下游应用中的实用性。借助灵活的选择工具，用户可直接控制AI可修改的区域。我们的选择与编辑工具适用于任何野外采集的场景，且无需额外优化。

</details>


### [5] [When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179)
*Jiacheng Hou,Yining Sun,Ruochong Jin,Haochen Han,Fangming Liu,Wai Kin Victor Chan,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新型视觉中心越狱攻击（VJA），仅通过视觉输入即可对图像编辑模型实施攻击，并构建了安全评估基准IESBench；同时提出一种无需训练的防御方法，显著提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 随着图像编辑模型从文本驱动转向视觉提示驱动，用户意图通过视觉输入（如标记、箭头等）表达，这虽然提升了可用性，但也引入了新的安全风险——攻击面本身变为视觉形式，亟需研究此类威胁。

Method: 作者提出了Vision-Centric Jailbreak Attack (VJA)，一种纯视觉到视觉的越狱攻击方法；构建了面向安全的图像编辑模型评测基准IESBench；并设计了一种基于内省多模态推理的无需训练的防御机制。

Result: 在IESBench上的实验表明，VJA对当前先进商用模型具有高成功率：在Nano Banana Pro上达80.9%，在GPT-Image-1.5上达70.1%；所提防御方法在不使用辅助防护模型、几乎无计算开销的情况下，显著提升了弱对齐模型的安全性。

Conclusion: 该研究揭示了视觉提示图像编辑系统中的新安全漏洞，提供了评估基准与实用防御方案，为构建安全可信的现代图像编辑系统奠定基础。

Abstract: Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.

Abstract (中文翻译): 近期大型图像编辑模型的进展已将范式从文本驱动指令转向视觉提示编辑，即直接从标记、箭头和视觉-文本提示等视觉输入中推断用户意图。尽管这一范式极大地提升了可用性，但也引入了一个关键且尚未充分探索的安全风险：攻击面本身变成了视觉形式。在本研究中，我们提出了视觉中心越狱攻击（Vision-Centric Jailbreak Attack, VJA），这是首个纯粹通过视觉输入传递恶意指令的视觉到视觉越狱攻击方法。为系统性研究这一新兴威胁，我们引入了IESBench——一个面向安全的图像编辑模型基准。在IESBench上的大量实验表明，VJA能有效攻破当前最先进的商用模型，在Nano Banana Pro上攻击成功率高达80.9%，在GPT-Image-1.5上达70.1%。为缓解此漏洞，我们提出了一种基于内省多模态推理的无需训练的防御方法，该方法在不依赖辅助防护模型且计算开销可忽略的情况下，显著提升了对齐不佳模型的安全性，使其达到与商用系统相当的水平。我们的发现揭示了新的安全漏洞，并提供了基准和实用防御手段，以推动安全可信的现代图像编辑系统的发展。警告：本文包含由大型图像编辑模型生成的冒犯性图像。

</details>


### [6] [DEGMC: Denoising Diffusion Models Based on Riemannian Equivariant Group Morphological Convolutions](https://arxiv.org/abs/2602.10221)
*El Hadji S. Diop,Thierno Fall,Mohamed Daoudi*

Main category: cs.CV

TL;DR: 本文通过引入基于欧氏群等变性的几何方法和流形上的群形态卷积，改进了DDPM模型在几何特征提取与对称性建模方面的性能，在多个数据集上取得优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: 现有DDPM模型依赖的U-net架构仅具备平移等变性，难以有效提取关键几何特征并处理旋转、反射等更广泛的对称性，限制了其在复杂结构建模中的表现。

Method: 提出一种结合欧氏群（包含旋转、反射、置换）等变性的几何方法，引入黎曼流形上的群形态卷积，该卷积源自一阶Hamilton-Jacobi型偏微分方程的粘性解，并加入对流项，利用特征线法求解，以更好捕捉非线性、细薄几何结构及对称性。

Result: 在MNIST、RotoMNIST和CIFAR-10数据集上的实验表明，所提方法相比原始DDPM模型有明显性能提升。

Conclusion: 将几何先验与群等变性融入DDPM可有效增强模型对几何结构和对称性的建模能力，从而提升生成性能。

Abstract: In this work, we address two major issues in recent Denoising Diffusion Probabilistic Models (DDPM): {\bf 1)} geometric key feature extraction and {\bf 2)} network equivariance. Since the DDPM prediction network relies on the U-net architecture, which is theoretically only translation equivariant, we introduce a geometric approach combined with an equivariance property of the more general Euclidean group, which includes rotations, reflections, and permutations. We introduce the notion of group morphological convolutions in Riemannian manifolds, which are derived from the viscosity solutions of first-order Hamilton-Jacobi-type partial differential equations (PDEs) that act as morphological multiscale dilations and erosions. We add a convection term to the model and solve it using the method of characteristics. This helps us better capture nonlinearities, represent thin geometric structures, and incorporate symmetries into the learning process. Experimental results on the MNIST, RotoMNIST, and CIFAR-10 datasets show noticeable improvements compared to the baseline DDPM model.

Abstract (中文翻译): 本文针对近期去噪扩散概率模型（DDPM）中存在的两个主要问题：1）几何关键特征提取；2）网络等变性。由于DDPM预测网络依赖于U-net架构，而该架构理论上仅具有平移等变性，我们引入了一种结合更广义欧氏群（包括旋转、反射和置换）等变性质的几何方法。我们提出了黎曼流形上的群形态卷积概念，该卷积源自一类一阶Hamilton-Jacobi型偏微分方程的粘性解，可实现形态学多尺度膨胀与腐蚀操作。我们在模型中加入了对流项，并采用特征线法进行求解，从而更好地捕捉非线性特性、表达细薄几何结构，并将对称性融入学习过程。在MNIST、RotoMNIST和CIFAR-10数据集上的实验结果表明，与基线DDPM模型相比，本方法取得了显著的性能提升。

</details>


### [7] [XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability](https://arxiv.org/abs/2602.10239)
*Dominik Galus,Julia Farganus,Tymoteusz Zapala,Mikołaj Czachorowski,Piotr Borycki,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

TL;DR: 本文提出了XSPLAIN，首个专为3D Gaussian Splatting（3DGS）分类设计的原型驱动、事前可解释性框架，在不损失分类性能的前提下提供直观且可信的解释。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting虽已成为高保真3D重建的标准方法，但其在关键领域的应用受限于模型缺乏可解释性以及对Splats的分类机制不透明。现有针对点云等3D表示的可解释性方法通常依赖模糊的显著图，无法捕捉高斯基元的体素一致性。

Method: 提出XSPLAIN框架，采用体素聚合的PointNet主干网络和一种新颖的可逆正交变换，该变换在严格保持原始决策边界的同时解耦特征通道以提升可解释性。解释基于代表性训练样本，实现“这看起来像那个”的直观推理。

Result: 用户研究（N=51）表明，参与者有48.4%的时间选择XSPLAIN的解释为最佳，显著优于基线方法（p<0.001），证明了其在提供透明度和用户信任方面的有效性，且分类性能无任何下降。

Conclusion: XSPLAIN是首个面向3DGS分类的事前可解释框架，通过原型驱动的方法有效解决了现有可解释性技术的不足，为3DGS在关键领域的应用铺平了道路。

Abstract: 3D Gaussian Splatting (3DGS) has rapidly become a standard for high-fidelity 3D reconstruction, yet its adoption in multiple critical domains is hindered by the lack of interpretability of the generation models as well as classification of the Splats. While explainability methods exist for other 3D representations, like point clouds, they typically rely on ambiguous saliency maps that fail to capture the volumetric coherence of Gaussian primitives. We introduce XSPLAIN, the first ante-hoc, prototype-based interpretability framework designed specifically for 3DGS classification. Our approach leverages a voxel-aggregated PointNet backbone and a novel, invertible orthogonal transformation that disentangles feature channels for interpretability while strictly preserving the original decision boundaries. Explanations are grounded in representative training examples, enabling intuitive ``this looks like that'' reasoning without any degradation in classification performance. A rigorous user study (N=51) demonstrates a decisive preference for our approach: participants selected XSPLAIN explanations 48.4\% of the time as the best, significantly outperforming baselines $(p<0.001)$, showing that XSPLAIN provides transparency and user trust. The source code for this work is available at: https://github.com/Solvro/ml-splat-xai

Abstract (中文翻译): 3D高斯泼溅（3DGS）已迅速成为高保真3D重建的标准，但其在多个关键领域的应用却因生成模型缺乏可解释性以及对Splat的分类机制不明而受到阻碍。尽管针对点云等其他3D表示的可解释性方法已经存在，但它们通常依赖于模糊的显著图，无法捕捉高斯基元的体素一致性。我们提出了XSPLAIN，这是首个专为3DGS分类设计的事前、基于原型的可解释性框架。我们的方法利用体素聚合的PointNet主干网络和一种新颖的可逆正交变换，该变换在严格保持原始决策边界的同时解耦特征通道以实现可解释性。其解释基于代表性训练样本，能够实现直观的“这看起来像那个”式推理，且不会造成任何分类性能的下降。一项严格的用户研究（N=51）表明，参与者有48.4%的时间将XSPLAIN的解释选为最佳，显著优于基线方法（p<0.001），证明了XSPLAIN能有效提供透明度并增强用户信任。本工作的源代码已在https://github.com/Solvro/ml-splat-xai公开。

</details>
