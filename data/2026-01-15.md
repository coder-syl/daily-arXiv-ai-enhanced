<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Reading or Reasoning? Format Decoupled Reinforcement Learning for Document OCR](https://arxiv.org/abs/2601.08834)
*Yufeng Zhong,Lei Chen,Zhixiong Zeng,Xuanle Zhao,Deyang Jiang,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为格式解耦强化学习（FD-RL）的新方法，通过利用高熵模式识别格式密集型文档实例，并采用针对不同格式类型的解耦奖励机制，显著提升了OCR模型在复杂格式文本（如公式、表格）上的性能，在OmniDocBench上取得了90.41的平均分。


<details>
  <summary>Details</summary>
Motivation: 现有OCR模型在处理格式化文本（如公式、表格）时表现出显著更高的输出不确定性（熵），这表明其在格式敏感文档上的推理能力不足，需要新的方法来提升性能。

Method: 提出格式解耦强化学习（FD-RL）方法，包括基于熵的数据过滤策略以识别格式密集型实例，以及针对不同格式类型设计的格式解耦奖励机制，实现格式级别的验证而非逐词记忆。

Result: FD-RL在OmniDocBench基准上达到90.41的平均分，创下了端到端模型的新纪录，并通过全面的消融实验验证了数据、训练、过滤和奖励策略的有效性。

Conclusion: 通过利用高熵模式并引入格式解耦的强化学习框架，可以有效提升OCR模型在复杂格式文档上的表现，为未来OCR研究提供了新方向。

Abstract: Reading text from images or scanned documents via OCR models has been a longstanding focus of researchers. Intuitively, text reading is perceived as a straightforward perceptual task, and existing work primarily focuses on constructing enriched data engineering to enhance SFT capabilities. In this work, we observe that even advanced OCR models exhibit significantly higher entropy in formatted text (\emph{e.g.}, formula, table, etc.) compared to plain text, often by an order of magnitude. These statistical patterns reveal that advanced OCR models struggle with high output uncertainty when dealing with format sensitive document, suggesting that reasoning over diverse reading pathways may improve OCR performance. To address this, we propose format decoupled reinforcement learning (FD-RL), which leverages high-entropy patterns for targeted optimization. Our approach employs entropy-based data filtration strategy to identify format-intensive instances, and adopt format decoupled rewards tailored to different format types, enabling format-level validation rather than token-level memorization. FD-RL achieves an average score of 90.41 on OmniDocBench, setting a new record for end-to-end models on this highly popular benchmark. More importantly, we conduct comprehensive ablation studies over data, training, filtering, and rewarding strategies, thoroughly validating their effectiveness.

Abstract (中文翻译): 通过OCR模型从图像或扫描文档中读取文本一直是研究人员长期关注的重点。直观上，文本阅读被视为一项简单的感知任务，现有工作主要集中在构建丰富的数据工程以增强监督微调（SFT）能力。在本研究中，我们观察到，即使是先进的OCR模型，在处理格式化文本（例如公式、表格等）时，其输出熵也比普通文本高出一个数量级。这些统计模式表明，先进OCR模型在处理格式敏感文档时面临较高的输出不确定性，暗示通过推理多种阅读路径可能提升OCR性能。为此，我们提出了格式解耦强化学习（FD-RL），该方法利用高熵模式进行针对性优化。我们的方法采用基于熵的数据过滤策略来识别格式密集型实例，并针对不同格式类型设计了格式解耦的奖励机制，从而实现格式级别的验证，而非逐词记忆。FD-RL在OmniDocBench上取得了90.41的平均分，创下了该热门基准上端到端模型的新纪录。更重要的是，我们对数据、训练、过滤和奖励策略进行了全面的消融研究，充分验证了它们的有效性。

</details>


### [2] [Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models](https://arxiv.org/abs/2601.08860)
*Tarannum Mithila*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言模型和生成式图像模型在图像旋转和分布偏移下的鲁棒性与公平性问题，发现输入变换会加剧模型偏差并降低鲁棒性；为此提出结合数据增强、表征对齐和模型正则化的缓解策略，实验证明该方法在不牺牲性能的前提下显著提升鲁棒性并减少偏差放大。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLMs）和生成式图像模型虽然在多模态任务中表现优异，但其在输入变换（如图像旋转）和分布偏移下的鲁棒性与公平性尚未得到充分研究。作者旨在揭示此类变换如何导致模型预测偏差和性能下降，并提出有效缓解策略。

Method: 作者分析了图像旋转引起的扰动对模型预测、置信度校准和人口统计偏差的影响，并提出一种结合数据增强、表征对齐和模型级正则化的旋转鲁棒缓解策略。

Result: 在多个数据集上的实验表明，所提方法在不牺牲整体性能的前提下，显著提升了模型对旋转等变换的鲁棒性，并有效减少了偏差放大现象。

Conclusion: 当前多模态系统在面对输入变换时存在关键局限性，本文提出的实用缓解技术有助于构建更可靠、更公平的人工智能模型。

Abstract: Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.

Abstract (中文翻译): 视觉-语言模型（VLMs）和生成式图像模型在多模态任务中取得了卓越性能，但其在输入变换下的鲁棒性与公平性仍缺乏充分探索。本研究聚焦于图像旋转和分布偏移对前沿视觉-语言模型及生成模型中偏差传播与鲁棒性退化的影响。我们分析了由旋转引起的扰动如何影响模型的预测结果、置信度校准以及人口统计偏差模式。为应对这些问题，我们提出了结合数据增强、表征对齐和模型级正则化的旋转鲁棒缓解策略。在多个数据集上的实验结果表明，所提出的方法在不牺牲整体性能的前提下，显著提升了模型的鲁棒性，并有效减少了偏差放大。本研究揭示了当前多模态系统的关键局限，并提供了构建更可靠、更公平人工智能模型的实用缓解技术。

</details>


### [3] [R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images](https://arxiv.org/abs/2601.08867)
*Qingyu Liu,Zhongjie Ba,Jianmin Guo,Qiu Wang,Zhibo Wang,Jie Shi,Kui Ren*

Main category: cs.CV

TL;DR: 本文提出R²BD，一种高效通用的AIGC图像检测框架，通过统一重建模型G-LDM和单步残差偏置计算模块，在速度提升22倍的同时显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的AIGC图像检测方法依赖多步扩散模型，效率低下且难以泛化到GAN等其他生成范式。

Method: 提出R²BD框架，包含：(1) G-LDM统一重建模型，可模拟VAE、GAN和扩散模型的生成行为；(2) 单步残差偏置计算模块，实现一次推理即可区分真假图像。

Result: 在10个公开数据集上的实验表明，R²BD比现有重建方法快22倍以上，跨数据集评估平均准确率提升13.87%。

Conclusion: R²BD在保持高检测精度的同时大幅提升了效率，并展现出对多种生成模型的强大泛化能力。

Abstract: Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs.
  In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps.
  Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.

Abstract (中文翻译): 近年来，基于重建的方法在AIGC图像检测中受到关注。这些方法利用预训练的扩散模型对输入进行重建，并通过测量残差来区分真实图像与伪造图像。其关键优势在于减少对数据集特定伪影的依赖，并在分布偏移下提升泛化能力。然而，这类方法受限于多步反演与重建带来的显著低效性，且对扩散模型主干的依赖也限制了其对GAN等其他生成范式的泛化能力。  
本文提出了一种新颖的伪造图像检测框架R²BD，其核心包含两项设计：（1）G-LDM——一种统一的重建模型，可模拟VAE、GAN和扩散模型的生成行为，从而将检测范围扩展至以往仅限扩散模型的方法之外；（2）残差偏置计算模块，可在单次推理步骤中区分真实与伪造图像，相比现有通常需要20步以上的方法实现了显著的效率提升。  
在来自10个公开数据集的基准测试中，大量实验表明，R²BD的速度比现有基于重建的方法快22倍以上，同时取得了更优的检测准确率。在跨数据集评估中，其平均性能比当前最先进方法高出13.87%，展现出在多种生成方法下的强大效率与泛化能力。用于评估的代码和数据集已在https://github.com/QingyuLiu/RRBD公开。

</details>


### [4] [Residual Cross-Modal Fusion Networks for Audio-Visual Navigation](https://arxiv.org/abs/2601.08868)
*Yi Wang,Yinfeng Yu,Bin Ren*

Main category: cs.CV

TL;DR: 本文提出了一种跨模态残差融合网络（CRFN），通过在音频和视觉流之间引入双向残差交互，实现互补建模与细粒度对齐，在音频-视觉具身导航任务中显著优于现有方法，并展现出更强的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 音频-视觉具身导航任务的关键挑战在于如何在多模态融合过程中有效建模异构特征之间的交互，避免单一模态主导或信息退化，尤其是在跨域场景中。

Method: 提出Cross-Modal Residual Fusion Network (CRFN)，通过残差连接显式建模音频与视觉模态间的双向交互，在保持各自表征独立性的同时实现互补建模和细粒度对齐，并引入稳定化技术提升收敛性和鲁棒性。

Result: 在Replica和Matterport3D数据集上的实验表明，CRFN显著优于当前最先进的融合基线方法，并具有更强的跨域泛化能力；同时发现智能体在不同数据集中对模态的依赖存在差异。

Conclusion: CRFN有效解决了多模态融合中的信息失衡问题，提升了音频-视觉具身导航的性能与跨域适应能力，且对模态依赖差异的发现为理解具身智能体的跨模态协作机制提供了新视角。

Abstract: Audio-visual embodied navigation aims to enable an agent to autonomously localize and reach a sound source in unseen 3D environments by leveraging auditory cues. The key challenge of this task lies in effectively modeling the interaction between heterogeneous features during multimodal fusion, so as to avoid single-modality dominance or information degradation, particularly in cross-domain scenarios. To address this, we propose a Cross-Modal Residual Fusion Network, which introduces bidirectional residual interactions between audio and visual streams to achieve complementary modeling and fine-grained alignment, while maintaining the independence of their representations. Unlike conventional methods that rely on simple concatenation or attention gating, CRFN explicitly models cross-modal interactions via residual connections and incorporates stabilization techniques to improve convergence and robustness. Experiments on the Replica and Matterport3D datasets demonstrate that CRFN significantly outperforms state-of-the-art fusion baselines and achieves stronger cross-domain generalization. Notably, our experiments also reveal that agents exhibit differentiated modality dependence across different datasets. The discovery of this phenomenon provides a new perspective for understanding the cross-modal collaboration mechanism of embodied agents.

Abstract (中文翻译): 音频-视觉具身导航旨在通过利用听觉线索，使智能体能够在未见过的3D环境中自主定位并抵达声源。该任务的关键挑战在于在多模态融合过程中有效建模异构特征之间的交互，以避免单模态主导或信息退化，尤其是在跨域场景中。为此，我们提出了一种跨模态残差融合网络（Cross-Modal Residual Fusion Network, CRFN），该网络在音频与视觉流之间引入双向残差交互，以实现互补建模和细粒度对齐，同时保持各自表征的独立性。与依赖简单拼接或注意力门控的传统方法不同，CRFN通过残差连接显式建模跨模态交互，并结合稳定化技术以提升收敛性和鲁棒性。在Replica和Matterport3D数据集上的实验表明，CRFN显著优于当前最先进的融合基线方法，并展现出更强的跨域泛化能力。值得注意的是，我们的实验还揭示了智能体在不同数据集中对模态的依赖存在差异。这一现象的发现为理解具身智能体的跨模态协作机制提供了新的视角。

</details>


### [5] [ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection](https://arxiv.org/abs/2601.08873)
*Hema Hariharan Samson*

Main category: cs.CV

TL;DR: 提出ForensicFormer，一种分层多尺度框架，通过交叉注意力Transformer统一低、中、高层伪造线索，在多种未知伪造类型上达到86.8%平均准确率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像取证方法在面对跨域伪造（如AI生成图像和高级编辑工具）时失效，亟需一种通用且鲁棒的检测方法。

Method: 采用分层多尺度架构，结合交叉注意力Transformer，整合低层伪影检测、中层边界分析和高层语义推理。

Result: 在7个多样化测试集上平均准确率达86.8%，JPEG压缩下（Q=70）准确率为83%，像素级定位F1得分为0.76，各模块贡献4-10%性能提升。

Conclusion: 该方法有效融合经典图像取证与现代深度学习，为现实场景中未知篡改技术提供实用、鲁棒的通用检测方案。

Abstract: The proliferation of AI-generated imagery and sophisticated editing tools has rendered traditional forensic methods ineffective for cross-domain forgery detection. We present ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Unlike prior single-paradigm approaches, which achieve <75% accuracy on out-of-distribution datasets, our method maintains 86.8% average accuracy across seven diverse test sets, spanning traditional manipulations, GAN-generated images, and diffusion model outputs - a significant improvement over state-of-the-art universal detectors. We demonstrate superior robustness to JPEG compression (83% accuracy at Q=70 vs. 66% for baselines) and provide pixel-level forgery localization with a 0.76 F1-score. Extensive ablation studies validate that each hierarchical component contributes 4-10% accuracy improvement, and qualitative analysis reveals interpretable forensic features aligned with human expert reasoning. Our work bridges classical image forensics and modern deep learning, offering a practical solution for real-world deployment where manipulation techniques are unknown a priori.

Abstract (中文翻译): 人工智能生成图像和高级编辑工具的激增，使得传统的取证方法在跨域伪造检测中失效。我们提出了 ForensicFormer，这是一种分层多尺度框架，通过交叉注意力 Transformer 统一了低层伪影检测、中层边界分析和高层语义推理。与以往单一范式的方法相比（在分布外数据集上准确率低于75%），我们的方法在涵盖传统篡改、GAN生成图像和扩散模型输出的七个多样化测试集上，平均准确率达到86.8%，显著优于当前最先进的通用检测器。我们在 JPEG 压缩（Q=70）条件下实现了83%的准确率（基线方法为66%），并以0.76的F1分数实现了像素级伪造定位。大量消融研究验证了每个分层组件可带来4–10%的准确率提升，定性分析也揭示了与人类专家推理一致的可解释取证特征。本工作弥合了经典图像取证与现代深度学习之间的鸿沟，为现实部署中事先未知的篡改技术提供了一种实用解决方案。

</details>


### [6] [Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement](https://arxiv.org/abs/2601.08875)
*Jiahao Qin,Yiwen Wang*

Main category: cs.CV

TL;DR: SAR-Net通过将图像解耦为域不变的场景表示和域特定的外观编码，解决了跨域图像配准中因亮度恒定假设失效而导致的难题，在双向扫描显微镜数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像配准方法依赖亮度恒定假设，但在源域和目标域存在系统性强度差异（即域偏移）时，该假设不再成立，导致配准问题不适定。因此，亟需一种能应对域偏移的鲁棒配准方法。

Method: 提出SAR-Net统一框架，通过原理性的场景-外观解耦，将观测图像分解为域不变的场景表示和域特定的外观编码，并在共享潜在空间中通过重渲染而非直接强度匹配实现配准。引入场景一致性损失和域对齐损失以确保几何对应和跨域对齐。

Result: 在具有耦合域偏移和几何畸变的双向扫描显微镜数据上验证，SAR-Net取得0.885 SSIM和0.979 NCC，比最强基线提升3.1倍，且保持实时性能（77 fps）。消融实验证明两个损失项均不可或缺。

Conclusion: SAR-Net通过场景-外观解耦有效解决了域偏移下的图像配准问题，理论分析和实验结果均表明其在挑战性真实场景中的优越性和鲁棒性。

Abstract: Image registration under domain shift remains a fundamental challenge in computer vision and medical imaging: when source and target images exhibit systematic intensity differences, the brightness constancy assumption underlying conventional registration methods is violated, rendering correspondence estimation ill-posed. We propose SAR-Net, a unified framework that addresses this challenge through principled scene-appearance disentanglement. Our key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. We establish theoretical conditions under which this decomposition enables consistent cross-domain alignment (Proposition 1) and prove that our scene consistency loss provides a sufficient condition for geometric correspondence in the shared latent space (Proposition 2). Empirically, we validate SAR-Net on bidirectional scanning microscopy, where coupled domain shift and geometric distortion create a challenging real-world testbed. Our method achieves 0.885 SSIM and 0.979 NCC, representing 3.1x improvement over the strongest baseline, while maintaining real-time performance (77 fps). Ablation studies confirm that both scene consistency and domain alignment losses are necessary: removing either degrades performance by 90% SSIM or causes 223x increase in latent alignment error, respectively. Code and data are available at https://github.com/D-ST-Sword/SAR-NET.

Abstract (中文翻译): 在计算机视觉和医学影像领域，域偏移下的图像配准仍是一个根本性挑战：当源图像和目标图像表现出系统性的强度差异时，传统配准方法所依赖的亮度恒定假设被破坏，导致对应关系估计成为不适定问题。我们提出了SAR-Net，一个通过原理性的场景-外观解耦来应对这一挑战的统一框架。我们的核心观点是，观测图像可以被分解为域不变的场景表示和域特定的外观编码，从而通过重渲染而非直接强度匹配来实现配准。我们建立了该分解能够实现一致跨域对齐的理论条件（命题1），并证明了所提出的场景一致性损失为共享潜在空间中几何对应关系提供了充分条件（命题2）。我们在双向扫描显微镜数据上对SAR-Net进行了实证验证，该场景中耦合的域偏移和几何畸变构成了一个具有挑战性的真实世界测试平台。我们的方法达到了0.885的SSIM和0.979的NCC，相比最强基线提升了3.1倍，同时保持了实时性能（77 fps）。消融研究证实，场景一致性损失和域对齐损失都是必要的：移除任一损失项分别会导致SSIM下降至90%或潜在空间对齐误差增加223倍。代码和数据已在https://github.com/D-ST-Sword/SAR-NET公开。

</details>


### [7] [The Semantic Lifecycle in Embodied AI: Acquisition, Representation and Storage via Foundation Models](https://arxiv.org/abs/2601.08876)
*Shuai Chen,Hao Chen,Yuanchen Bei,Tianyang Zhao,Zhibo Zhou,Feiran Huang*

Main category: cs.CV

TL;DR: 本文提出“语义生命周期”框架，以统一视角描述基于基础模型的具身智能中语义知识的演化过程，并从获取、表示和存储三个阶段分析最新进展。


<details>
  <summary>Details</summary>
Motivation: 传统方法将语义处理视为孤立模块，难以应对复杂开放环境中的具身任务，亟需更具泛化性和鲁棒性的语义处理能力。

Method: 提出“语义生命周期”统一框架，从获取、表示和存储三阶段系统分析基础模型驱动下的具身语义研究进展。

Result: 该框架为理解具身AI中语义知识的连续流动与维护提供了整体视角，并对当前研究进行了系统性梳理与比较。

Conclusion: 基础模型显著推动了具身AI语义处理的发展，但仍存在挑战，未来研究应聚焦于语义生命周期各阶段的协同优化与实际部署。

Abstract: Semantic information in embodied AI is inherently multi-source and multi-stage, making it challenging to fully leverage for achieving stable perception-to-action loops in real-world environments. Early studies have combined manual engineering with deep neural networks, achieving notable progress in specific semantic-related embodied tasks. However, as embodied agents encounter increasingly complex environments and open-ended tasks, the demand for more generalizable and robust semantic processing capabilities has become imperative. Recent advances in foundation models (FMs) address this challenge through their cross-domain generalization abilities and rich semantic priors, reshaping the landscape of embodied AI research. In this survey, we propose the Semantic Lifecycle as a unified framework to characterize the evolution of semantic knowledge within embodied AI driven by foundation models. Departing from traditional paradigms that treat semantic processing as isolated modules or disjoint tasks, our framework offers a holistic perspective that captures the continuous flow and maintenance of semantic knowledge. Guided by this embodied semantic lifecycle, we further analyze and compare recent advances across three key stages: acquisition, representation, and storage. Finally, we summarize existing challenges and outline promising directions for future research.

Abstract (中文翻译): 具身人工智能中的语义信息本质上是多源且多阶段的，这使得在现实环境中充分挖掘其潜力以实现稳定的感知-行动闭环变得极具挑战。早期研究结合了人工工程与深度神经网络，在特定语义相关的具身任务中取得了显著进展。然而，随着具身智能体所面对的环境日益复杂、任务愈发开放，对更具泛化性和鲁棒性的语义处理能力的需求变得至关重要。近期基础模型（FMs）凭借其跨域泛化能力和丰富的语义先验，有效应对了这一挑战，重塑了具身AI的研究格局。在本综述中，我们提出了“语义生命周期”这一统一框架，用以刻画由基础模型驱动的具身AI中语义知识的演化过程。该框架突破了传统范式将语义处理视为孤立模块或割裂任务的局限，提供了一个能够捕捉语义知识连续流动与维护的整体视角。在此具身语义生命周期的指导下，我们进一步分析并比较了语义获取、表示和存储这三个关键阶段的最新研究进展。最后，我们总结了当前存在的挑战，并展望了未来有前景的研究方向。

</details>


### [8] [TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts](https://arxiv.org/abs/2601.08881)
*Yu Xu,Hongbin Yan,Juan Cao,Yiji Cheng,Tiankai Hang,Runze He,Zijin Yin,Shiyi Zhang,Yuxin Zhang,Jintao Li,Chunyu Wang,Qinglin Lu,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出一种新框架，在稀疏混合专家（MoE）模型中注入任务语义意图，通过分层任务语义标注和预测对齐正则化，使专家路由机制能根据全局任务目标进行调度，从而有效缓解图像生成与编辑任务间的干扰。


<details>
  <summary>Details</summary>
Motivation: 统一的图像生成与编辑模型在密集扩散Transformer架构中面临严重的任务干扰问题，因为共享参数空间需在相互冲突的目标（如局部编辑与主体驱动生成）之间折衷。尽管稀疏MoE是一种有前景的解决方案，但其门控网络仍是任务无关的，仅基于局部特征进行路由，无法感知全局任务意图，导致无法实现有效的专家专业化，难以解决任务干扰。

Method: 作者提出一种新框架，将语义意图注入MoE路由机制：首先设计分层任务语义标注方案，构建结构化的任务描述符（如作用范围、任务类型、保留程度）；然后引入预测对齐正则化，使内部路由决策与任务高层语义对齐，从而将门控网络从任务无关执行器转变为任务感知的调度中心。

Result: 该方法有效缓解了任务干扰，在保真度和生成质量上优于密集基线模型；分析表明，各专家自然形成了清晰且语义相关的专业化分工。

Conclusion: 通过将高层任务语义融入MoE路由机制，可显著提升统一图像生成与编辑模型的性能，并促进专家模块的语义专业化，为多任务扩散模型提供了一种有效架构。

Abstract: Unified image generation and editing models suffer from severe task interference in dense diffusion transformers architectures, where a shared parameter space must compromise between conflicting objectives (e.g., local editing v.s. subject-driven generation). While the sparse Mixture-of-Experts (MoE) paradigm is a promising solution, its gating networks remain task-agnostic, operating based on local features, unaware of global task intent. This task-agnostic nature prevents meaningful specialization and fails to resolve the underlying task interference. In this paper, we propose a novel framework to inject semantic intent into MoE routing. We introduce a Hierarchical Task Semantic Annotation scheme to create structured task descriptors (e.g., scope, type, preservation). We then design Predictive Alignment Regularization to align internal routing decisions with the task's high-level semantics. This regularization evolves the gating network from a task-agnostic executor to a dispatch center. Our model effectively mitigates task interference, outperforming dense baselines in fidelity and quality, and our analysis shows that experts naturally develop clear and semantically correlated specializations.

Abstract (中文翻译): 统一的图像生成与编辑模型在密集扩散Transformer架构中面临严重的任务干扰问题，因为共享参数空间必须在相互冲突的目标（例如局部编辑与主体驱动生成）之间做出妥协。尽管稀疏混合专家（MoE）范式是一种有前景的解决方案，但其门控网络仍然是任务无关的，仅基于局部特征进行操作，无法感知全局任务意图。这种任务无关性阻碍了有意义的专家专业化，也无法解决底层的任务干扰。本文提出一种新颖的框架，将语义意图注入MoE路由机制中。我们引入了一种分层任务语义标注方案，用于构建结构化的任务描述符（例如作用范围、任务类型、保留程度）。随后，我们设计了预测对齐正则化方法，使内部路由决策与任务的高层语义对齐。该正则化促使门控网络从任务无关的执行器演变为任务感知的调度中心。我们的模型有效缓解了任务干扰，在保真度和生成质量方面优于密集基线模型，且分析表明专家自然形成了清晰且语义相关的专业化能力。

</details>


### [9] [Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained Optimization](https://arxiv.org/abs/2601.08882)
*Thomas Snyder,H. Lexie Yang,Stefan Schnake,Steffen Schotthöfer*

Main category: cs.CV

TL;DR: 本文提出使用DLRT流形约束优化框架，在迁移学习过程中压缩基于视觉Transformer的地理空间基础模型，实现高参数压缩率的同时保持下游任务精度，优于LoRA等现有低秩方法。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署地理空间基础模型需要紧凑的架构，但现有模型参数量大且压缩常导致精度损失，限制了实际应用。

Method: 利用DLRT流形约束优化框架，在迁移学习阶段对大型视觉Transformer地理空间基础模型进行压缩，通过强制结构化低维参数化并与下游任务目标对齐。

Result: 在多个地理空间基准测试中，该方法显著减少了参数数量，同时精度损失极小，性能优于LoRA等现成低秩方法。

Conclusion: 所提方法能有效实现高性能、可部署于边缘设备的地理空间模型，在压缩与精度之间取得良好平衡。

Abstract: Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. However, their large parameter counts and the accuracy loss often induced by compression limit practical adoption. In this work, we leverage manifold-constrained optimization framework DLRT to compress large vision transformer-based geospatial foundation models during transfer learning. By enforcing structured low-dimensional parameterizations aligned with downstream objectives, this approach achieves strong compression while preserving task-specific accuracy. We show that the method outperforms of-the-shelf low-rank methods as LoRA. Experiments on diverse geospatial benchmarks confirm substantial parameter reduction with minimal accuracy loss, enabling high-performing, on-device geospatial models.

Abstract (中文翻译): 在资源受限的边缘设备上部署地理空间基础模型需要紧凑的架构，同时保持较高的下游任务性能。然而，这些模型庞大的参数量以及压缩过程中常常导致的精度损失限制了其实际应用。本文在迁移学习过程中，利用流形约束优化框架DLRT对基于视觉Transformer的大型地理空间基础模型进行压缩。通过强制实施与下游任务目标对齐的结构化低维参数化，该方法在实现强压缩的同时保留了任务特定的精度。我们证明该方法优于LoRA等现成的低秩方法。在多种地理空间基准上的实验表明，该方法能够大幅减少参数数量，同时精度损失极小，从而实现高性能的端侧地理空间模型。

</details>


### [10] [Adaptive few-shot learning for robust part quality classification in two-photon lithography](https://arxiv.org/abs/2601.08885)
*Sixian Jia,Ruo-Syuan Mei,Chenhui Shao*

Main category: cs.CV

TL;DR: 本文提出了一种用于双光子光刻（TPL）制造过程中质量控制的自适应计算机视觉框架，该框架通过新颖性检测、少样本增量学习和少样本域自适应三种方法，实现了在动态制造环境中高效更新和维护视觉模型。


<details>
  <summary>Details</summary>
Motivation: 现有用于TPL制造质量控制的计算机视觉模型通常是静态的，无法有效应对动态制造环境中的新缺陷类别、稀缺数据更新以及新零件几何形状的适应问题。

Method: 所提出的自适应CV框架基于一个尺度鲁棒的主干模型，并整合了三种关键技术：(1) 基于线性判别分析（LDA）的统计假设检验框架用于新颖性检测；(2) 一种两阶段、基于重演的策略用于少样本增量学习；(3) 一种少样本域对抗神经网络（DANN）用于少样本域自适应。

Result: 在包含半球（源域）和立方体（目标域）结构的TPL数据集上评估表明：假设检验方法以99–100%的准确率成功识别新类别批次；增量学习方法仅用K=20个样本就将新类别集成到模型中并达到92%的准确率；域自适应模型仅用K=5个样本就在目标域上达到96.19%的准确率。

Conclusion: 该框架为在不断变化的生产场景中部署和维护计算机视觉模型提供了一种鲁棒且数据高效的解决方案。

Abstract: Two-photon lithography (TPL) is an advanced additive manufacturing (AM) technique for fabricating high-precision micro-structures. While computer vision (CV) is proofed for automated quality control, existing models are often static, rendering them ineffective in dynamic manufacturing environments. These models typically cannot detect new, unseen defect classes, be efficiently updated from scarce data, or adapt to new part geometries. To address this gap, this paper presents an adaptive CV framework for the entire life-cycle of quality model maintenance. The proposed framework is built upon a same, scale-robust backbone model and integrates three key methodologies: (1) a statistical hypothesis testing framework based on Linear Discriminant Analysis (LDA) for novelty detection, (2) a two-stage, rehearsal-based strategy for few-shot incremental learning, and (3) a few-shot Domain-Adversarial Neural Network (DANN) for few-shot domain adaptation. The framework was evaluated on a TPL dataset featuring hemisphere as source domain and cube as target domain structures, with each domain categorized into good, minor damaged, and damaged quality classes. The hypothesis testing method successfully identified new class batches with 99-100% accuracy. The incremental learning method integrated a new class to 92% accuracy using only K=20 samples. The domain adaptation model bridged the severe domain gap, achieving 96.19% accuracy on the target domain using only K=5 shots. These results demonstrate a robust and data-efficient solution for deploying and maintaining CV models in evolving production scenarios.

Abstract (中文翻译): 双光子光刻（TPL）是一种用于制造高精度微结构的先进增材制造（AM）技术。尽管计算机视觉（CV）已被证明可用于自动化质量控制，但现有模型通常是静态的，使其在动态制造环境中效果不佳。这些模型通常无法检测新的、未见过的缺陷类别，难以利用稀疏数据高效更新，也无法适应新的零件几何形状。为解决这一问题，本文提出了一种适用于质量模型全生命周期维护的自适应CV框架。该框架基于一个统一且尺度鲁棒的主干模型，并整合了三种关键方法：(1) 基于线性判别分析（LDA）的统计假设检验框架用于新颖性检测；(2) 一种两阶段、基于重演的策略用于少样本增量学习；(3) 一种少样本域对抗神经网络（DANN）用于少样本域自适应。该框架在包含半球（源域）和立方体（目标域）结构的TPL数据集上进行了评估，每个域均分为良品、轻微损伤和损伤三类。实验结果表明，假设检验方法以99–100%的准确率成功识别出新类别批次；增量学习方法仅使用K=20个样本就将新类别集成至92%的准确率；域自适应模型仅用K=5个样本便在目标域上达到96.19%的准确率。这些结果证明了该框架在动态生产场景中部署和维护CV模型时具有鲁棒性和数据高效性。

</details>
