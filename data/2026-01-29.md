<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文提出一种从单目图像中恢复真实尺度3D食物模型的方法，显著提升食物体积估算精度，助力精准营养。


<details>
  <summary>Details</summary>
Motivation: 现有AI膳食评估方法难以从单目图像准确估计食物分量（“吃了多少”），因缺乏真实世界尺度信息，限制了其在精准营养中的应用。

Method: 结合大规模预训练模型提取的丰富视觉特征，估计3D重建物体的真实尺度，将单视角3D重建转换为具有物理意义的真实尺寸模型。

Result: 在两个公开数据集上的实验表明，该方法比现有技术平均体积估算误差降低近30%。

Conclusion: 该方法有效弥合了3D计算机视觉与数字健康之间的鸿沟，在精准营养领域具有重要应用潜力。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

Abstract (中文翻译): 与饮食相关的慢性疾病（如肥胖症和糖尿病）的增加，凸显了准确监测食物摄入量的必要性。尽管近年来人工智能驱动的膳食评估已取得进展，但从单目图像中恢复尺寸（份量）信息以准确回答“你吃了多少？”这一问题，因其本身是一个病态问题，仍是一项紧迫的挑战。一些3D重建方法虽在几何重建方面取得了令人印象深刻的效果，却未能恢复重建物体的关键真实世界尺度，从而限制了其在精准营养中的应用。本文通过提出一种从单目图像中恢复真实尺度3D重建物体的方法，弥合了3D计算机视觉与数字健康之间的差距。我们的方法利用在大规模数据集上训练的模型所提取的丰富视觉特征来估计重建物体的尺度。这种学习到的尺度使我们能够将单视角3D重建转换为真实、具有物理意义的模型。在两个公开可用数据集上进行的大量实验和消融研究表明，我们的方法始终优于现有技术，平均绝对体积估算误差降低了近30%，展示了其在增强精准营养领域的潜力。

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 本文提出DiSa框架，通过显式引入显著性线索解耦前景与背景特征，并结合层次化精炼模块，有效缓解了现有视觉语言模型在开放词汇语义分割中的前景偏倚和空间定位模糊问题，在六个基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（如CLIP）的开放词汇语义分割方法存在两个关键缺陷：一是对显著前景区域的偏好导致忽略背景（前景偏倚），二是缺乏精确的空间定位能力导致物体边界模糊。为解决这些问题，作者提出新方法。

Method: 提出DiSa框架，包含两个核心模块：1）显著性感知解耦模块（SDM），利用显著性线索将前景与背景特征分别建模；2）层次化精炼模块（HRM），通过多层级更新进行像素级空间上下文建模和通道级特征精炼。

Result: 在六个公开基准数据集上的大量实验表明，DiSa方法在开放词汇语义分割任务中始终优于当前最先进的方法。

Conclusion: 通过显式引入显著性信息并解耦前景与背景特征，结合层次化精炼策略，DiSa有效克服了VLM在密集预测任务中的固有局限，显著提升了开放词汇语义分割的性能。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

Abstract (中文翻译): 开放词汇语义分割旨在根据文本标签为图像中的每个像素分配语义标签。现有方法通常利用视觉-语言模型（如CLIP）进行密集预测。然而，这些在图像-文本对上预训练的视觉-语言模型倾向于关注显著的、以物体为中心的区域，在应用于分割任务时表现出两个关键局限性：（i）前景偏倚，即容易忽略背景区域；（ii）空间定位能力有限，导致物体边界模糊。为解决上述问题，我们提出了DiSa，一种新颖的显著性感知前景-背景区分解耦框架。通过在所设计的显著性感知解耦模块（SDM）中显式引入显著性线索，DiSa以分而治之的方式分别建模前景与背景的集成特征。此外，我们还提出了一个层次化精炼模块（HRM），该模块利用像素级空间上下文信息，并通过多层级更新实现通道级特征精炼。在六个基准数据集上的大量实验表明，DiSa始终优于当前最先进的方法。

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: 本文提出SSMAE框架，在标签数据稀缺但无标签数据丰富的情况下，通过联合优化掩码图像重建与分类任务，并引入基于验证的门控机制动态启用伪标签，显著提升Vision Transformer在低标签场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 在标签数据稀缺但无标签数据丰富的场景下，如何高效训练Vision Transformer（ViT）仍具挑战。现有方法在利用伪标签进行半监督学习时容易引入确认偏误，影响模型性能。

Method: 提出Semi-Supervised Masked Autoencoder（SSMAE）框架，结合有标签和无标签数据，联合优化掩码图像重建与分类任务；引入验证驱动的门控机制，仅当模型在弱增强和强增强视图下均产生高置信度且一致的预测时，才启用伪标签，以减少确认偏误。

Result: 在CIFAR-10和CIFAR-100上，SSMAE始终优于纯监督ViT和微调后的MAE，尤其在低标签比例下提升显著（如CIFAR-10使用10%标签时比ViT高出9.24%）。

Conclusion: 在数据高效的Transformer训练中，伪标签的引入时机与其生成方式同样重要；所提SSMAE有效缓解了确认偏误，显著提升了半监督ViT的性能。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

Abstract (中文翻译): 我们针对标签数据稀缺但无标签数据丰富的场景下训练Vision Transformer（ViT）所面临的挑战，提出了半监督掩码自编码器（Semi-Supervised Masked Autoencoder, SSMAE）框架。该框架利用有标签和无标签样本，联合优化掩码图像重建与分类任务，并采用动态选择的伪标签。SSMAE引入了一种基于验证的门控机制：只有当模型在同一样本的弱增强和强增强视图下均能产生可靠且高置信度的一致预测时，才激活伪标签，从而减少确认偏误。在CIFAR-10和CIFAR-100上的实验表明，SSMAE始终优于纯监督训练的ViT和微调后的MAE，尤其在低标签比例下提升显著（例如在CIFAR-10使用10%标签时，性能比ViT高出9.24%）。我们的结果表明，在数据高效的Transformer训练中，伪标签的引入时机与其生成方式同等重要。代码已开源：https://github.com/atik666/ssmae。

</details>


### [4] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 本文提出在CLIP训练中直接引入稀疏性，以同时获得可解释性和高性能的表示，优于后处理方法（如稀疏自编码器），并保留多模态能力。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然成功，但其稠密且不透明的潜在表示缺乏可解释性；现有后处理方法（如SAE）虽提升可解释性，却损害下游性能和多模态能力。

Method: 在CLIP训练过程中直接集成稀疏性约束，生成稀疏、可解释且保持性能的表示。

Result: 所提出的Sparse CLIP在保持下游任务性能的同时，实现了更强的可解释性和多模态能力，并支持语义概念对齐与可解释的视觉引导。

Conclusion: 可解释性与性能并非此消彼长，而是可以协同优化；该方法为未来多模态模型设计提供了新原则。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

Abstract (中文翻译): 对比语言-图像预训练（CLIP）已成为视觉-语言表征学习的基石，广泛应用于各类下游任务，并作为多模态大语言模型（MLLMs）默认的视觉主干网络。尽管取得成功，CLIP的稠密且不透明的潜在表征带来了显著的可解释性挑战。通常认为可解释性与性能之间存在权衡：在训练中强制引入稀疏性会降低准确率，因此近期研究转向后处理方法，如稀疏自编码器（SAEs）。然而，这些后处理方法往往导致下游性能下降，并削弱CLIP固有的多模态能力，且所学特征大多仍为单模态。我们提出一种简单而有效的方法，将稀疏性直接整合进CLIP训练过程，从而获得兼具可解释性与高性能的表征。与SAEs相比，我们的稀疏CLIP表征在保持强大下游任务性能的同时，实现了更优的可解释性，并保留了多模态能力。我们展示了多模态稀疏特征能够实现直接的语义概念对齐，并揭示了跨模态知识如何在训练中涌现的动态过程。最后，作为概念验证，我们在稀疏CLIP表征上训练了一个视觉-语言模型，实现了可解释的、基于视觉的引导能力。我们的发现挑战了“可解释性必须牺牲准确性”的传统观念，证明可解释性与性能可以协同优化，为未来模型设计提供了有前景的原则。

</details>


### [5] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 本文不聚焦于新模型开发，而是系统评估和标准化用于H&E染色图像核实例分割的公开数据集，提出统一的训练集（NucFuse-train）与测试集（NucFuse-test），为该任务建立新基准。


<details>
  <summary>Details</summary>
Motivation: 当前核实例分割研究多集中于新算法开发，且在有限、任意选取的公开数据集上进行评估，缺乏对数据集本身质量与适用性的系统分析，阻碍了模型性能的公平比较与提升。

Method: 通过文献综述收集并标准化多个公开的手动标注H&E染色图像数据集；使用两种先进模型（CNN与CNN-ViT混合架构）对各数据集进行系统评估与排序；构建统一的训练集（NucFuse-train）和测试集（NucFuse-test）以支持跨数据集公平评估与性能提升。

Result: 成功对多个公开数据集进行了性能评估与排序，验证了融合多数据集可提升分割性能，并通过外部验证证明了所提基准的有效性。

Conclusion: 本研究为H&E染色组织图像中的核实例分割任务提供了标准化的数据基准、统一的评估协议和开源实现，有助于推动该领域更可靠、可复现的研究。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

Abstract (中文翻译): 苏木精-伊红（H&E）染色图像中的细胞核实例分割在自动化组织学图像分析中扮演着重要角色，并在多种下游任务中具有广泛应用。尽管已有多种机器学习和深度学习方法被提出用于细胞核实例分割，但该领域的大多数研究集中于开发新的分割算法，并仅在数量有限且任意选取的公开数据集上进行基准测试。  
在本研究中，我们并未聚焦于模型开发，而是关注用于该任务的数据集。基于广泛的文献综述，我们识别出多个手动标注、公开可用的H&E染色图像数据集，并将其标准化为统一的输入和标注格式。我们利用两种最先进的分割模型——一种基于卷积神经网络（CNN），另一种基于CNN与视觉Transformer的混合架构——系统地评估并根据其细胞核实例分割性能对这些数据集进行排序。此外，我们提出了一个统一的测试集（NucFuse-test）用于公平的跨数据集评估，以及一个统一的训练集（NucFuse-train），通过融合多个数据集的图像以提升分割性能。  
通过评估和排序数据集、开展全面分析、生成融合数据集、进行外部验证并将我们的实现公开发布，我们为H&E染色组织学图像上的细胞核实例分割模型的训练、测试与评估提供了一个新的基准。

</details>


### [6] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的剪枝方法 SAP，通过从中层识别关键视觉块，在 ViDoRe 基准上实现超过 90% 的索引向量压缩率，同时保持检索性能，并引入 OSR 协议揭示中层语义结构锚点的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有无训练剪枝方法（如基于 EOS 注意力的方法）在高压缩率（>80%）下表现不佳，甚至不如随机选择；先前研究认为视觉 token 的重要性依赖于查询，从而质疑无训练剪枝的可行性。本文旨在探索更有效的无训练剪枝策略。

Method: 提出 Structural Anchor Pruning (SAP) 方法，从模型中间层识别关键视觉 patch 进行剪枝；同时引入 Oracle Score Retention (OSR) 协议，用于评估不同层信息对压缩效率的影响。

Result: 在 ViDoRe 基准测试中，SAP 方法在压缩索引向量超过 90% 的情况下仍保持强大的检索保真度，显著优于现有无训练剪枝方法。

Conclusion: 视觉语义结构锚点存在于模型中间层而非最终层，这为高效无训练剪枝提供了新视角；SAP 为视觉 RAG 系统提供了一种高度可扩展的解决方案。

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

Abstract (中文翻译): 近期的视觉-语言模型（例如 ColPali）能够实现细粒度的视觉文档检索（VDR），但会带来高昂的索引向量存储开销。无需训练的剪枝方法（例如基于 EOS 注意力的方法）可在不调整模型的情况下将索引向量大小减少约 60%，但在高压缩场景（>80%）下往往表现不如随机选择。先前的研究（例如 Light-ColPali）将此归因于视觉 token 的重要性本质上依赖于查询，从而质疑了无需训练剪枝的可行性。本文提出了 Structural Anchor Pruning（SAP），一种无需训练的剪枝方法，通过从模型中间层识别关键视觉块来实现高性能压缩。我们还引入了 Oracle Score Retention（OSR）协议，用于评估逐层信息如何影响压缩效率。在 ViDoRe 基准上的评估表明，SAP 在将索引向量减少超过 90% 的同时仍能保持稳健的检索保真度，为视觉 RAG 提供了一种高度可扩展的解决方案。此外，基于 OSR 的分析揭示，语义结构锚点存在于中间层，而传统剪枝方法聚焦的最终层中这些结构信号已经消散。

</details>


### [7] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 本文提出一种针对扩散式多模态大模型（如LLaDA-V）的结构化视觉token剪枝策略，在中间到后期层进行剪枝，可减少高达65%计算量，同时保留95%的任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式多模态模型（如LLaDA-V）因双向注意力机制和迭代去噪过程带来高计算开销，且其跨模态信息聚合主要发生在中后期层，导致语义对齐延迟，亟需高效推理方法。

Method: 受FastV启发，提出一种结构化token剪枝策略，在第一轮去噪的中后期层有选择地移除部分视觉token，以契合LLaDA-V的注意力聚合特性，从而降低后续所有步骤的计算量。

Result: 在多个基准上，最佳配置可减少最多65%的计算开销，同时平均保留95%的任务性能。

Conclusion: 该工作首次探索了扩散式多模态大模型中的结构化token剪枝，为LLaDA-V高效推理提供了实证基础，并展示了视觉感知剪枝在该类模型中的潜力。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

Abstract (中文翻译): 基于扩散的大型多模态模型（如LLaDA-V）在视觉-语言理解和生成方面展现出卓越能力。然而，其双向注意力机制和扩散式的迭代去噪范式引入了显著的计算开销，因为视觉token在所有层和去噪步骤中被反复处理。本文进行了深入的注意力分析，发现与自回归解码器不同，LLaDA-V主要在中后期层聚合跨模态信息，导致语义对齐延迟。受此观察启发，我们提出了一种受FastV启发的结构化token剪枝策略，在指定层有选择地移除部分视觉token，以减少FLOPs同时保留关键语义信息。据我们所知，这是首次在扩散式大型多模态模型中研究结构化token剪枝的工作。不同于FastV聚焦于浅层剪枝，我们的方法针对第一轮去噪的中后期层进行剪枝，以契合LLaDA-V延迟的注意力聚合特性，从而维持输出质量；并且由于在第一轮就进行剪枝，可降低所有后续步骤的计算量。我们的框架为LLaDA-V的高效推理提供了实证基础，并突显了视觉感知剪枝在扩散式多模态模型中的潜力。在多个基准测试中，我们的最佳配置最多可减少65%的计算成本，同时平均保留95%的任务性能。

</details>


### [8] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle 是一种轻量级的图像和视频风格迁移模型，基于 Qwen-Image-Edit 构建，通过课程持续学习框架在高质量与合成数据集上训练，在保持内容一致性的前提下实现对未见风格的泛化能力，并在风格相似性、内容一致性和美学质量方面达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 扩散 Transformer（DiT）在内容保留风格迁移任务中面临内容与风格特征在内部表示中纠缠的问题，导致难以同时兼顾风格表达与内容保真。因此，需要一种有效方法来解耦或协调这两者，以实现高质量的风格迁移。

Method: TeleStyle 基于 Qwen-Image-Edit 模型，利用其强大的内容保留和风格定制能力；构建了一个包含精选真实风格样本和大量野外风格合成三元组的混合数据集；采用课程持续学习（Curriculum Continual Learning）策略进行训练；并引入视频到视频风格迁移模块以提升时序一致性。

Result: TeleStyle 在风格相似性、内容一致性和美学质量三项核心指标上均达到当前最优（SOTA）性能，且能有效泛化至未见过的风格，同时保持高精度的内容保真。

Conclusion: TeleStyle 通过结合高质量数据、课程持续学习策略和视频时序优化模块，成功解决了 DiT 在内容保留风格迁移中的关键挑战，为图像与视频风格迁移提供了高效且实用的解决方案。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

Abstract (中文翻译): 内容保留的风格迁移旨在根据内容参考和风格参考生成具有指定风格的输出，但由于扩散 Transformer（DiT）在其内部表示中存在内容与风格特征的固有纠缠，这一任务仍具挑战性。在本技术报告中，我们提出了 TeleStyle——一种轻量而高效的图像与视频风格迁移模型。该模型基于 Qwen-Image-Edit 构建，充分利用其在内容保留和风格定制方面的强大能力。为支持有效训练，我们整理了一个高质量的特定风格数据集，并进一步利用数千种多样化的野外风格类别合成了三元组数据。我们引入了一种课程持续学习（Curriculum Continual Learning）框架，在由干净（精选）和噪声（合成）三元组组成的混合数据集上训练 TeleStyle，使其能够在不牺牲内容保真度的前提下泛化到未见过的风格。此外，我们还设计了一个视频到视频的风格迁移模块，以增强时间一致性和视觉质量。TeleStyle 在风格相似性、内容一致性和美学质量三项核心评估指标上均达到了当前最优水平。代码和预训练模型已开源：https://github.com/Tele-AI/TeleStyle。

</details>


### [9] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: 本文研究了利用计算机视觉模型和大语言多模态模型（LLMs）自动评估船体生物污损严重程度（LoF等级），发现两者各有优势，混合方法最具前景。


<details>
  <summary>Details</summary>
Motivation: 传统依赖潜水员的船体生物污损检测方法存在安全风险且难以规模化，亟需自动化、可扩展的替代方案。

Method: 评估了卷积神经网络、基于Transformer的分割模型和零样本大语言多模态模型（LLMs）在新西兰初级产业部提供的专家标注数据集上的表现；LLMs通过结构化提示和检索进行引导。

Result: 计算机视觉模型在LoF极端等级上准确率高，但在中间等级表现不佳；LLMs无需训练即达到有竞争力的性能，并提供可解释输出。

Conclusion: 不同方法具有互补优势，结合分割覆盖度与LLM推理的混合方法是实现可扩展、可解释生物污损评估的有前景路径。

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

Abstract (中文翻译): 船体上的海洋生物污损带来了重大的生态、经济和生物安全风险。传统的调查方法依赖潜水员检查，既危险又难以规模化。本研究探讨了使用自定义计算机视觉模型和大型多模态语言模型（LLMs）对生物污损严重程度（按污损等级LoF量表）进行自动分类。研究在来自新西兰初级产业部的专家标注数据集上，评估了卷积神经网络、基于Transformer的分割模型以及零样本LLMs的性能。结果表明，计算机视觉模型在LoF量表的极端类别上表现出高准确率，但由于数据集不平衡和图像构图问题，在中间等级上表现不佳。而通过结构化提示和检索引导的LLMs，无需训练即可取得具有竞争力的性能，并能提供可解释的输出。研究结果展示了不同方法之间的互补优势，表明融合分割覆盖率与LLM推理的混合方法，是实现可扩展且可解释的生物污损评估的一条有前景的途径。

</details>


### [10] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO improves human preference alignment in text-to-image generation by introducing dense rewards that evaluate each denoising step, addressing the sparse reward problem of prior GRPO methods.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO-based methods suffer from sparse reward issues where a single terminal reward is applied uniformly across all denoising steps, leading to misalignment between global feedback and fine-grained step contributions.

Method: DenseGRPO introduces two key components: (1) step-wise reward prediction using an ODE-based approach to assign dense rewards based on intermediate clean images; (2) a reward-aware exploration scheme that adaptively adjusts stochasticity injection in the SDE sampler according to timestep-specific noise intensity.

Result: Extensive experiments on standard benchmarks show DenseGRPO outperforms existing methods, validating the importance of dense rewards for effective alignment in flow matching models.

Conclusion: DenseGRPO effectively addresses the sparse reward problem in GRPO-based alignment by leveraging dense, step-wise rewards and adaptive exploration, significantly improving text-to-image generation aligned with human preferences.

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

Abstract (中文翻译): 基于流匹配模型的近期GRPO方法在文本到图像生成中展现出显著的人类偏好对齐改进。然而，这些方法仍面临稀疏奖励问题：整个去噪轨迹的终端奖励被应用于所有中间步骤，导致全局反馈信号与各中间去噪步骤的实际细粒度贡献不匹配。为解决此问题，我们提出了\textbf{DenseGRPO}，一种通过密集奖励对齐人类偏好的新框架，该框架评估每个去噪步骤的细粒度贡献。具体而言，我们的方法包含两个关键组件：（1）通过基于ODE的方法，在中间干净图像上应用奖励模型，预测每一步的奖励增益作为密集奖励，从而确保反馈信号与各步骤贡献的一致性，促进有效训练；（2）基于所估计的密集奖励，揭示了现有GRPO方法中均匀探索策略与随时间变化的噪声强度之间的不匹配缺陷，导致探索空间不合理。因此，我们提出了一种奖励感知方案，通过在SDE采样器中自适应调整特定时间步的随机性注入，校准探索空间，确保在所有时间步上都具有合适的探索范围。在多个标准基准上的大量实验验证了所提DenseGRPO的有效性，并突显了有效密集奖励在流匹配模型对齐中的关键作用。

</details>
