{"id": "2602.04994", "pdf": "https://arxiv.org/pdf/2602.04994", "abs": "https://arxiv.org/abs/2602.04994", "authors": ["Zhuosen Bao", "Xia Du", "Zheng Lin", "Jizhe Zhou", "Zihan Fang", "Jiening Wu", "Yuxin Zhang", "Zhe Chen", "Chi-man Pun", "Wei Ni", "Jun Luo"], "title": "SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 8 figures", "summary": "With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.", "AI": {"tldr": "SIDeR\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u89e3\u8026\u7684\u65e0\u9650\u5236\u4eba\u8138\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bed\u4e49\u5f15\u5bfc\u91cd\u7ec4\uff0c\u5728\u4fdd\u6301\u673a\u5668\u53ef\u8bc6\u522b\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u540c\u65f6\u751f\u6210\u89c6\u89c9\u4e0a\u533f\u540d\u4f46\u81ea\u7136\u7684\u4eba\u8138\u56fe\u50cf\uff0c\u5e76\u652f\u6301\u5bc6\u7801\u6388\u6743\u4e0b\u7684\u539f\u59cb\u56fe\u50cf\u6062\u590d\u3002", "motivation": "\u968f\u7740\u4eba\u8138\u8bc6\u522b\u6280\u672f\u6df1\u5ea6\u878d\u5165\u5728\u7ebf\u94f6\u884c\u3001\u8eab\u4efd\u9a8c\u8bc1\u7b49\u7f51\u7edc\u670d\u52a1\uff0c\u5982\u4f55\u5728\u56fe\u50cf\u5b58\u50a8\u4e0e\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u6709\u6548\u5c06\u8eab\u4efd\u4fe1\u606f\u4e0e\u89c6\u89c9\u8868\u5f81\u89e3\u8026\uff0c\u6210\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5173\u952e\u6311\u6218\u3002", "method": "SIDeR\u5c06\u4eba\u8138\u56fe\u50cf\u5206\u89e3\u4e3a\u673a\u5668\u53ef\u8bc6\u522b\u7684\u8eab\u4efd\u7279\u5f81\u5411\u91cf\u548c\u89c6\u89c9\u53ef\u611f\u77e5\u7684\u8bed\u4e49\u5916\u89c2\u6210\u5206\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u5f15\u5bfc\u91cd\u7ec4\u673a\u5236\u751f\u6210\u5bf9\u6297\u6027\u533f\u540d\u4eba\u8138\uff1b\u5f15\u5165\u52a8\u91cf\u9a71\u52a8\u7684\u65e0\u9650\u5236\u6270\u52a8\u4f18\u5316\u548c\u8bed\u4e49-\u89c6\u89c9\u5e73\u8861\u56e0\u5b50\uff0c\u5408\u6210\u591a\u6837\u4e14\u81ea\u7136\u7684\u5bf9\u6297\u6837\u672c\uff1b\u540c\u65f6\u652f\u6301\u901a\u8fc7\u6b63\u786e\u5bc6\u7801\u6062\u590d\u539f\u59cb\u56fe\u50cf\u3002", "result": "\u5728CelebA-HQ\u548cFFHQ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSIDeR\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u8fbe99%\uff0c\u5728\u57fa\u4e8ePSNR\u7684\u56fe\u50cf\u6062\u590d\u8d28\u91cf\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534741.28%\u3002", "conclusion": "SIDeR\u5728\u4fdd\u969c\u4eba\u8138\u56fe\u50cf\u89c6\u89c9\u533f\u540d\u6027\u7684\u540c\u65f6\u7ef4\u6301\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u53ef\u9006\u6062\u590d\uff0c\u4e3a\u4eba\u8138\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65b0\u8303\u5f0f\u3002", "summary_cn": "\u968f\u7740\u4eba\u8138\u8bc6\u522b\u6280\u672f\u6df1\u5ea6\u878d\u5165\u5728\u7ebf\u94f6\u884c\u3001\u8eab\u4efd\u9a8c\u8bc1\u7b49\u8054\u7f51\u670d\u52a1\uff0c\u5728\u56fe\u50cf\u5b58\u50a8\u4e0e\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u8eab\u4efd\u4fe1\u606f\u4e0e\u89c6\u89c9\u8868\u5f81\u7684\u6709\u6548\u89e3\u8026\u5df2\u6210\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5173\u952e\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SIDeR\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u89e3\u8026\u9a71\u52a8\u7684\u65e0\u9650\u5236\u4eba\u8138\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\u3002SIDeR\u5c06\u4eba\u8138\u56fe\u50cf\u5206\u89e3\u4e3a\u673a\u5668\u53ef\u8bc6\u522b\u7684\u8eab\u4efd\u7279\u5f81\u5411\u91cf\u548c\u89c6\u89c9\u53ef\u611f\u77e5\u7684\u8bed\u4e49\u5916\u89c2\u6210\u5206\uff0c\u901a\u8fc7\u5728\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bed\u4e49\u5f15\u5bfc\u7684\u91cd\u7ec4\uff0c\u751f\u6210\u89c6\u89c9\u4e0a\u533f\u540d\u4f46\u4fdd\u6301\u673a\u5668\u7ea7\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u5bf9\u6297\u4eba\u8138\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u52a8\u91cf\u9a71\u52a8\u7684\u65e0\u9650\u5236\u6270\u52a8\u4f18\u5316\u7b56\u7565\u4e0e\u8bed\u4e49-\u89c6\u89c9\u5e73\u8861\u56e0\u5b50\uff0c\u80fd\u591f\u5408\u6210\u591a\u79cd\u89c6\u89c9\u591a\u6837\u4e14\u9ad8\u5ea6\u81ea\u7136\u7684\u5bf9\u6297\u6837\u672c\u3002\u6b64\u5916\uff0c\u5bf9\u4e8e\u6388\u6743\u8bbf\u95ee\uff0c\u5f53\u63d0\u4f9b\u6b63\u786e\u5bc6\u7801\u65f6\uff0c\u53d7\u4fdd\u62a4\u56fe\u50cf\u53ef\u88ab\u8fd8\u539f\u4e3a\u539f\u59cb\u5f62\u5f0f\u3002\u5728CelebA-HQ\u548cFFHQ\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSIDeR\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u653b\u51fb\u6210\u529f\u7387\u8fbe\u523099%\uff0c\u5728\u57fa\u4e8ePSNR\u7684\u6062\u590d\u8d28\u91cf\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa41.28%\u3002"}}
{"id": "2602.05037", "pdf": "https://arxiv.org/pdf/2602.05037", "abs": "https://arxiv.org/abs/2602.05037", "authors": ["Bishoy Galoaa", "Xiangyu Bai", "Utsav Nandi", "Sai Siddhartha Vivek Dhir Rangoju", "Somaieh Amraee", "Sarah Ostadabbas"], "title": "UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\\% reduction in identity switches and 12\\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\\% MOTA on SportsMOT.", "AI": {"tldr": "UniTrack \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u56fe\u8bba\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u53ef\u5fae\u5b66\u4e60\u7edf\u4e00\u4f18\u5316\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u68c0\u6d4b\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u5347\u5404\u7c7b MOT \u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u539f\u6709\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u8ddf\u8e2a\u67b6\u6784\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u90e8\u7f72\u7075\u6d3b\u6027\u3002\u4f5c\u8005\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u3001\u53ef\u63d2\u62d4\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u76f4\u63a5\u4f18\u5316\u8ddf\u8e2a\u4efb\u52a1\u7684\u6838\u5fc3\u6307\u6807\uff0c\u540c\u65f6\u517c\u5bb9\u73b0\u6709\u7cfb\u7edf\u3002", "method": "UniTrack \u5c06\u68c0\u6d4b\u7cbe\u5ea6\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u6574\u5408\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u7aef\u5230\u7aef\u53ef\u5fae\u7684\u56fe\u8bba\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u53ef\u5fae\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u4f7f\u7f51\u7edc\u80fd\u5b66\u4e60\u8de8\u5e27\u7684\u8fd0\u52a8\u8fde\u7eed\u6027\u548c\u8eab\u4efd\u5173\u7cfb\u7684\u6574\u4f53\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41 MOT \u6a21\u578b\uff08\u5982 Trackformer\u3001MOTR\u3001FairMOT\u3001ByteTrack\u3001GTR\u3001MOTE\uff09\u548c\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u9a8c\u8bc1\uff0cUniTrack \u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u6700\u591a\u51cf\u5c11 53% \u7684 ID \u5207\u6362\uff0cIDF1 \u63d0\u5347\u8fbe 12%\uff0c\u5176\u4e2d GTR \u5728 SportsMOT \u4e0a MOTA \u63d0\u9ad8 9.7%\u3002", "conclusion": "UniTrack \u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u3001\u5373\u63d2\u5373\u7528\u7684\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u76ee\u6807\u8ddf\u8e2a\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86 UniTrack\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u56fe\u8bba\u635f\u5931\u51fd\u6570\uff0c\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u53ef\u5fae\u5b66\u4e60\u76f4\u63a5\u4f18\u5316\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u7279\u5b9a\u76ee\u6807\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347 MOT \u6027\u80fd\u3002\u4e0e\u4ee5\u5f80\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u8ddf\u8e2a\u67b6\u6784\u7684\u57fa\u4e8e\u56fe\u7684 MOT \u65b9\u6cd5\u4e0d\u540c\uff0cUniTrack \u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u5c06\u68c0\u6d4b\u7cbe\u5ea6\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u6574\u5408\u5230\u5355\u4e00\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709 MOT \u7cfb\u7edf\u4e2d\u800c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u3002\u901a\u8fc7\u53ef\u5fae\u56fe\u8868\u793a\u5b66\u4e60\uff0cUniTrack \u4f7f\u7f51\u7edc\u80fd\u591f\u5b66\u4e60\u8de8\u5e27\u7684\u8fd0\u52a8\u8fde\u7eed\u6027\u548c\u8eab\u4efd\u5173\u7cfb\u7684\u6574\u4f53\u8868\u793a\u3002\u6211\u4eec\u5728\u591a\u79cd\u8ddf\u8e2a\u6a21\u578b\u548c\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86 UniTrack\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u6240\u6709\u6d4b\u8bd5\u67b6\u6784\u548c\u6570\u636e\u96c6\uff08\u5305\u62ec Trackformer\u3001MOTR\u3001FairMOT\u3001ByteTrack\u3001GTR \u548c MOTE\uff09\u4e0a\u5747\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4e0a\uff0c\u8eab\u4efd\u5207\u6362\u6700\u591a\u51cf\u5c11\u4e86 53%\uff0cIDF1 \u6307\u6807\u63d0\u5347\u4e86 12%\uff0c\u5176\u4e2d GTR \u5728 SportsMOT \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 9.7% \u7684 MOTA \u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2602.05049", "pdf": "https://arxiv.org/pdf/2602.05049", "abs": "https://arxiv.org/abs/2602.05049", "authors": ["Yiye Chen", "Yanan Jian", "Xiaoyi Dong", "Shuxin Cao", "Jing Wu", "Patricio Vela", "Benjamin E. Lundell", "Dongdong Chen"], "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "In submission. Project website: https://vista-vla.github.io/", "summary": "Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u89c6\u89c9\u6761\u4ef6\u4f9d\u8d56\u6765\u6539\u5584Vision-Language-Action\uff08VLA\uff09\u6a21\u578b\u4e2d\u7684\u89c6\u89c9-\u52a8\u4f5c\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u4e0d\u4fee\u6539\u67b6\u6784\u6216\u589e\u52a0\u6570\u636e\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6269\u5c55\u5230\u52a8\u4f5c\u7a7a\u95f4\u7684\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9-\u52a8\u4f5c\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5373\u52a8\u4f5c\u9884\u6d4b\u5bf9\u5f53\u524d\u89c6\u89c9\u72b6\u6001\u4f9d\u8d56\u8f83\u5f31\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u53ef\u9760\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u6210\u529f\u6267\u884c\u8f68\u8ff9\u6bd4\u5931\u8d25\u8f68\u8ff9\u5177\u6709\u66f4\u5f3a\u7684\u89c6\u89c9\u4f9d\u8d56\u6027\uff0c\u56e0\u6b64\u5e0c\u671b\u663e\u5f0f\u589e\u5f3aVLA\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u6761\u4ef6\u4f5c\u7528\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u6b65\uff1a\u9996\u5148\u5728\u8f68\u8ff9\u8ddf\u968f\u4ee3\u7406\u4efb\u52a1\u4e0a\u901a\u8fc7\u504f\u597d\u4f18\u5316\u4f7f\u52a8\u4f5c\u9884\u6d4b\u4e0e\u89c6\u89c9\u8f93\u5165\u5bf9\u9f50\uff1b\u7136\u540e\u5728\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u84b8\u998f\u5c06\u589e\u5f3a\u7684\u5bf9\u9f50\u80fd\u529b\u8fc1\u79fb\u5230\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u79bb\u6563OpenVLA\u6a21\u578b\u4e0a\u540c\u65f6\u63d0\u5347\u4e86\u89c6\u89c9\u6761\u4ef6\u6027\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684OpenVLA-OFT\u8bbe\u7f6e\u4e0b\u4e5f\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "\u663e\u5f0f\u5f3a\u5316VLA\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u6761\u4ef6\u4f9d\u8d56\u53ef\u6709\u6548\u7f13\u89e3\u89c6\u89c9-\u52a8\u4f5c\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u7b56\u7565\u53ef\u9760\u6027\uff0c\u4e14\u65e0\u9700\u67b6\u6784\u6539\u52a8\u6216\u989d\u5916\u6570\u636e\u6536\u96c6\u3002", "summary_cn": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5df2\u5728\u591a\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6269\u5c55\u5230\u52a8\u4f5c\u7a7a\u95f4\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9-\u52a8\u4f5c\u9519\u4f4d\u95ee\u9898\uff0c\u5373\u52a8\u4f5c\u9884\u6d4b\u5bf9\u5f53\u524d\u89c6\u89c9\u72b6\u6001\u7684\u4f9d\u8d56\u8f83\u5f31\uff0c\u4ece\u800c\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u52a8\u4f5c\u8f93\u51fa\u3002\u672c\u6587\u4ece\u89c6\u89c9\u6761\u4ef6\u4f9d\u8d56\u7684\u89d2\u5ea6\u7814\u7a76VLA\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\uff0c\u6210\u529f\u7684\u6267\u884c\u8f68\u8ff9\u59cb\u7ec8\u6bd4\u5931\u8d25\u8f68\u8ff9\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u89c6\u89c9\u4f9d\u8d56\u6027\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u5f0f\u589e\u5f3aVLA\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u6761\u4ef6\u4f5c\u7528\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5728\u8f68\u8ff9\u8ddf\u968f\u4ee3\u7406\u4efb\u52a1\u4e0a\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5b9e\u73b0\u52a8\u4f5c\u9884\u6d4b\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u5bf9\u9f50\uff0c\u7136\u540e\u5728\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u84b8\u998f\u5c06\u8fd9\u79cd\u589e\u5f3a\u7684\u5bf9\u9f50\u80fd\u529b\u8fc1\u79fb\u5230\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u3002\u5728\u4e0d\u5f15\u5165\u67b6\u6784\u4fee\u6539\u6216\u989d\u5916\u6570\u636e\u6536\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u79bb\u6563OpenVLA\u6a21\u578b\u7684\u89c6\u89c9\u6761\u4ef6\u6027\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5728\u6269\u5c55\u5230\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684OpenVLA-OFT\u8bbe\u7f6e\u65f6\u4e5f\u6301\u7eed\u5e26\u6765\u6027\u80fd\u589e\u76ca\u3002\u9879\u76ee\u7f51\u7ad9\uff1ahttps://vista-vla.github.io/\u3002"}}
{"id": "2602.05078", "pdf": "https://arxiv.org/pdf/2602.05078", "abs": "https://arxiv.org/abs/2602.05078", "authors": ["Gautham Vinod", "Fengqing Zhu"], "title": "Food Portion Estimation: From Pixels to Calories", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "comment": null, "summary": "Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u4e2d\u7528\u4e8e\u7cbe\u786e\u4f30\u7b97\u98df\u7269\u4efd\u91cf\u7684\u4e0d\u540c\u7b56\u7565\uff0c\u91cd\u70b9\u89e3\u51b3\u4ece2D\u56fe\u50cf\u4f30\u8ba13D\u98df\u7269\u5c3a\u5bf8\u7684\u6311\u6218\u3002", "motivation": "\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u5728\u6162\u6027\u75c5\u548c\u80a5\u80d6\u9632\u63a7\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5176\u6838\u5fc3\u96be\u70b9\u5728\u4e8e\u4ece\u4e8c\u7ef4\u56fe\u50cf\u51c6\u786e\u4f30\u8ba1\u98df\u7269\u7684\u4e09\u7ef4\u4f53\u79ef\u3002", "method": "\u7efc\u8ff0\u4e86\u591a\u79cd\u514b\u670d\u8be5\u9650\u5236\u7684\u7b56\u7565\uff0c\u5305\u62ec\u4f7f\u7528\u6df1\u5ea6\u56fe\u3001\u591a\u89c6\u89d2\u8f93\u5165\u3001\u6a21\u677f\u5339\u914d\u7b49\u8f85\u52a9\u4fe1\u606f\uff0c\u4ee5\u53ca\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u5355\u76ee\u56fe\u50cf\u6216\u8f85\u52a9\u8f93\u5165\u8fdb\u884c\u4efd\u91cf\u9884\u6d4b\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u5f53\u524d\u7528\u4e8e\u63d0\u5347\u98df\u7269\u4efd\u91cf\u4f30\u8ba1\u51c6\u786e\u6027\u7684\u4e3b\u8981\u6280\u672f\u8def\u5f84\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u6311\u6218\uff0c\u878d\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u591a\u6a21\u6001\u8f93\u5165\u7684\u65b9\u6cd5\u5c55\u73b0\u51fa\u6539\u5584\u56fe\u50cf\u57fa\u996e\u98df\u8bc4\u4f30\u7cbe\u5ea6\u7684\u6f5c\u529b\u3002", "summary_cn": "\u4f9d\u8d56\u56fe\u50cf\u8fdb\u884c\u996e\u98df\u8bc4\u4f30\u662f\u4e00\u79cd\u91cd\u8981\u7b56\u7565\uff0c\u80fd\u591f\u51c6\u786e\u4e14\u4fbf\u6377\u5730\u76d1\u6d4b\u4e2a\u4f53\u5065\u5eb7\u72b6\u51b5\uff0c\u4ece\u800c\u5728\u6162\u6027\u75be\u75c5\u548c\u80a5\u80d6\u7684\u9884\u9632\u4e0e\u7167\u62a4\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u9762\u4e34\u4e00\u4e2a\u6838\u5fc3\u96be\u9898\uff1a\u4ece\u4e8c\u7ef4\u56fe\u50cf\u8f93\u5165\u4e2d\u4f30\u7b97\u98df\u7269\u7684\u4e09\u7ef4\u5c3a\u5bf8\u3002\u4e3a\u514b\u670d\u8fd9\u4e00\u5173\u952e\u9650\u5236\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u591a\u79cd\u7b56\u7565\uff0c\u4f8b\u5982\u4f7f\u7528\u6df1\u5ea6\u56fe\u3001\u591a\u89c6\u89d2\u8f93\u5165\u7b49\u8f85\u52a9\u4fe1\u606f\uff0c\u6216\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff08\u5982\u6a21\u677f\u5339\u914d\uff09\u3002\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4e5f\u6709\u52a9\u4e8e\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u53ef\u901a\u8fc7\u5355\u76ee\u56fe\u50cf\u6216\u7ed3\u5408\u56fe\u50cf\u4e0e\u8f85\u52a9\u8f93\u5165\uff0c\u7cbe\u786e\u9884\u6d4b\u56fe\u50cf\u4e2d\u7684\u98df\u7269\u4efd\u91cf\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u7528\u4e8e\u5b9e\u73b0\u51c6\u786e\u4efd\u91cf\u4f30\u7b97\u7684\u5404\u79cd\u7b56\u7565\u3002"}}
{"id": "2602.05096", "pdf": "https://arxiv.org/pdf/2602.05096", "abs": "https://arxiv.org/abs/2602.05096", "authors": ["Joseph D. Janizek", "Sonnet Xu", "Junayd Lateef", "Roxana Daneshjou"], "title": "Visual concept ranking uncovers medical shortcuts used by large multimodal models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u89c6\u89c9\u6982\u5ff5\u6392\u5e8f\uff08VCR\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e2d\u7684\u5173\u952e\u89c6\u89c9\u6982\u5ff5\uff0c\u5e76\u63ed\u793a\u5176\u5728\u533b\u7597\u4efb\u52a1\uff08\u5982\u76ae\u80a4\u764c\u5206\u7c7b\uff09\u4e2d\u5bf9\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u8868\u73b0\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63ed\u793a\u6a21\u578b\u5728\u5904\u7406\u533b\u5b66\u56fe\u50cf\u65f6\u7684\u6f5c\u5728\u7f3a\u9677\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u4eba\u7fa4\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u65b9\u9762\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u53ef\u9a8c\u8bc1\u7684\u5ba1\u8ba1\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u201c\u89c6\u89c9\u6982\u5ff5\u6392\u5e8f\u201d\uff08Visual Concept Ranking, VCR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790LMM\u5728\u533b\u5b66\u4efb\u52a1\u63d0\u793a\u4e0b\u7684\u884c\u4e3a\uff0c\u8bc6\u522b\u5176\u4f9d\u8d56\u7684\u5173\u952e\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u751f\u6210\u53ef\u624b\u52a8\u9a8c\u8bc1\u7684\u5047\u8bbe\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLMM\u5728\u4f7f\u7528\u793a\u4f8b\u63d0\u793a\u8fdb\u884c\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u65f6\uff0c\u5728\u4e0d\u540c\u4eba\u53e3\u4e9a\u7fa4\u4e4b\u95f4\u5b58\u5728\u610f\u5916\u7684\u6027\u80fd\u5dee\u8ddd\uff1bVCR\u6210\u529f\u8bc6\u522b\u51fa\u6a21\u578b\u6240\u4f9d\u8d56\u7684\u89c6\u89c9\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u5e72\u9884\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "VCR\u4e3a\u5ba1\u8ba1\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63ed\u793a\u6a21\u578b\u504f\u5dee\u5e76\u63d0\u5347\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u3002", "summary_cn": "\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u80fd\u591f\u63ed\u793a\u6a21\u578b\u7f3a\u9677\u7684\u5ba1\u8ba1\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e2d\u91cd\u8981\u89c6\u89c9\u6982\u5ff5\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u8be5\u65b9\u6cd5\u7814\u7a76\u8fd9\u4e9b\u6a21\u578b\u5728\u9762\u5bf9\u533b\u7597\u4efb\u52a1\u63d0\u793a\u65f6\u6240\u8868\u73b0\u51fa\u7684\u884c\u4e3a\u3002\u6211\u4eec\u4e3b\u8981\u805a\u7126\u4e8e\u4ece\u4e34\u5e8a\u76ae\u80a4\u79d1\u56fe\u50cf\u4e2d\u5206\u7c7b\u6076\u6027\u76ae\u80a4\u75c5\u53d8\u7684\u4efb\u52a1\uff0c\u5e76\u8f85\u4ee5\u80f8\u90e8X\u5149\u7247\u548c\u81ea\u7136\u56fe\u50cf\u7684\u8865\u5145\u5b9e\u9a8c\u3002\u5728\u5c55\u793aLMM\u5728\u4f7f\u7528\u793a\u4f8b\u63d0\u793a\u65f6\u5728\u4e0d\u540c\u4eba\u53e3\u4e9a\u7fa4\u4e4b\u95f4\u5b58\u5728\u610f\u5916\u7684\u6027\u80fd\u5dee\u8ddd\u540e\uff0c\u6211\u4eec\u5c06\u6240\u63d0\u51fa\u7684\u201c\u89c6\u89c9\u6982\u5ff5\u6392\u5e8f\u201d\uff08Visual Concept Ranking, VCR\uff09\u65b9\u6cd5\u5e94\u7528\u4e8e\u8fd9\u4e9b\u6a21\u578b\u548c\u63d0\u793a\u3002VCR\u80fd\u591f\u751f\u6210\u4e0e\u4e0d\u540c\u89c6\u89c9\u7279\u5f81\u4f9d\u8d56\u76f8\u5173\u7684\u5047\u8bbe\uff0c\u968f\u540e\u6211\u4eec\u901a\u8fc7\u4eba\u5de5\u5e72\u9884\u5bf9\u8fd9\u4e9b\u5047\u8bbe\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2602.05126", "pdf": "https://arxiv.org/pdf/2602.05126", "abs": "https://arxiv.org/abs/2602.05126", "authors": ["Weiyi Qin", "Yingci Liu-Swetz", "Shiwei Tan", "Hao Wang"], "title": "CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology", "categories": ["cs.CV"], "comment": null, "summary": "Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.", "AI": {"tldr": "\u63d0\u51faCLEAR-HPV\u6846\u67b6\uff0c\u5728\u65e0\u9700\u6982\u5ff5\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5728MIL\u6a21\u578b\u4e2d\u81ea\u52a8\u53d1\u73b0\u53ef\u89e3\u91ca\u7684\u5f62\u6001\u5b66\u6982\u5ff5\uff08\u5982\u89d2\u5316\u3001\u57fa\u5e95\u6837\u3001\u95f4\u8d28\uff09\uff0c\u5c06\u9ad8\u7ef4\u7279\u5f81\u538b\u7f29\u4e3a\u4ec510\u4e2a\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u65b9\u6cd5\u867d\u80fd\u6709\u6548\u9884\u6d4bHPV\u72b6\u6001\uff0c\u4f46\u7f3a\u4e4f\u5f62\u6001\u5b66\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u7406\u89e3\u548c\u5e94\u7528\u3002", "method": "\u63d0\u51faCLEAR-HPV\u6846\u67b6\uff0c\u5728\u6ce8\u610f\u529b\u52a0\u6743\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u81ea\u52a8\u53d1\u73b0\u5f62\u6001\u5b66\u6982\u5ff5\uff0c\u751f\u6210\u7a7a\u95f4\u6982\u5ff5\u56fe\uff0c\u5e76\u7528\u7d27\u51d1\u7684\u6982\u5ff5\u6bd4\u4f8b\u5411\u91cf\u8868\u793a\u6574\u5f20\u5207\u7247\uff0c\u65e0\u9700\u8bad\u7ec3\u65f6\u63d0\u4f9b\u6982\u5ff5\u6807\u7b7e\u3002", "result": "CLEAR-HPV\u5728TCGA-HNSCC\u3001TCGA-CESC\u548cCPTAC-HNSCC\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5b9a\uff0c\u5c061536\u7ef4\u7684\u9ad8\u7ef4\u7279\u5f81\u538b\u7f29\u81f3\u4ec510\u4e2a\u53ef\u89e3\u91ca\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u59cbMIL\u5d4c\u5165\u7684\u9884\u6d4b\u4fe1\u606f\u3002", "conclusion": "CLEAR-HPV\u4e3a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684MIL\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u4e0e\u9aa8\u5e72\u7f51\u7edc\u65e0\u5173\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5168\u5207\u7247\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4e2d\u6982\u5ff5\u7ea7\u522b\u7684\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u8868\u793a\u3002", "summary_cn": "\u4eba\u4e73\u5934\u7624\u75c5\u6bd2\uff08HPV\uff09\u72b6\u6001\u662f\u5934\u9888\u764c\u548c\u5bab\u9888\u764c\u9884\u540e\u53ca\u6cbb\u7597\u53cd\u5e94\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\u3002\u5c3d\u7ba1\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5728HPV\u76f8\u5173\u5168\u5207\u7247\u75c5\u7406\u56fe\u50cf\u7684\u5207\u7247\u7ea7\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5f62\u6001\u5b66\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CLEAR-HPV\uff08Concept-Level Explainable Attention-guided Representation for HPV\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u91cd\u6784MIL\u6f5c\u5728\u7a7a\u95f4\uff0c\u65e0\u9700\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u6982\u5ff5\u6807\u7b7e\u5373\u53ef\u5b9e\u73b0\u6982\u5ff5\u53d1\u73b0\u3002CLEAR-HPV\u5728\u6ce8\u610f\u529b\u52a0\u6743\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u81ea\u52a8\u8bc6\u522b\u51fa\u89d2\u5316\u3001\u57fa\u5e95\u6837\u548c\u95f4\u8d28\u7b49\u5f62\u6001\u5b66\u6982\u5ff5\uff0c\u751f\u6210\u7a7a\u95f4\u6982\u5ff5\u56fe\uff0c\u5e76\u4f7f\u7528\u7d27\u51d1\u7684\u6982\u5ff5\u6bd4\u4f8b\u5411\u91cf\u8868\u793a\u6bcf\u5f20\u5207\u7247\u3002CLEAR-HPV\u7684\u6982\u5ff5\u6bd4\u4f8b\u5411\u91cf\u5728\u4fdd\u7559\u539f\u59cbMIL\u5d4c\u5165\u9884\u6d4b\u4fe1\u606f\u7684\u540c\u65f6\uff0c\u5c06\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\uff08\u4f8b\u59821536\u7ef4\uff09\u538b\u7f29\u81f3\u4ec510\u4e2a\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u3002CLEAR-HPV\u5728TCGA-HNSCC\u3001TCGA-CESC\u548cCPTAC-HNSCC\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5168\u5207\u7247\u75c5\u7406\u56fe\u50cf\u7684\u6ce8\u610f\u529b\u578bMIL\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u4e0e\u9aa8\u5e72\u7f51\u7edc\u65e0\u5173\u7684\u7d27\u51d1\u4e14\u6982\u5ff5\u7ea7\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002"}}
{"id": "2602.05132", "pdf": "https://arxiv.org/pdf/2602.05132", "abs": "https://arxiv.org/abs/2602.05132", "authors": ["Jia Li", "Wenjie Zhao", "Shijian Deng", "Bolin Lai", "Yuheng Wu", "RUijia Chen", "Jon E. Froehlich", "Yuhang Zhao", "Yapeng Tian"], "title": "ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faARGaze\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u673a\u5236\u7684\u5728\u7ebf\u7b2c\u4e00\u4eba\u79f0\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f53\u524d\u89c6\u89c9\u7279\u5f81\u548c\u8fd1\u671f\u89c6\u7ebf\u5386\u53f2\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7b2c\u4e00\u4eba\u79f0\uff08\u81ea\u6211\u4e2d\u5fc3\uff09\u89c6\u7ebf\u4f30\u8ba1\u7f3a\u4e4f\u660e\u786e\u7684\u5934\u90e8\u6216\u773c\u90e8\u4fe1\u53f7\uff0c\u9700\u4f9d\u8d56\u7a00\u758f\u3001\u95f4\u63a5\u7684\u7ebf\u7d22\uff08\u5982\u624b-\u7269\u4ea4\u4e92\uff09\u63a8\u65ad\u6ce8\u610f\u529b\uff1b\u540c\u65f6\uff0c\u76ee\u6807\u5bfc\u5411\u6d3b\u52a8\u4e2d\u89c6\u7ebf\u5177\u6709\u5f3a\u65f6\u95f4\u8fde\u7eed\u6027\uff0c\u53ef\u4f5c\u4e3a\u9884\u6d4b\u5148\u9a8c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u8fd9\u4e00\u7279\u6027\u3002", "method": "\u63d0\u51faARGaze\u6a21\u578b\uff0c\u5c06\u89c6\u7ebf\u4f30\u8ba1\u91cd\u6784\u4e3a\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\uff1a\u4f7f\u7528Transformer\u89e3\u7801\u5668\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u4ee5\uff08i\uff09\u5f53\u524d\u89c6\u89c9\u7279\u5f81\u548c\uff08ii\uff09\u56fa\u5b9a\u957f\u5ea6\u7684\u8fd1\u671f\u89c6\u7ebf\u76ee\u6807\u4f30\u8ba1\u7a97\u53e3\uff08Gaze Context Window\uff09\u4e3a\u6761\u4ef6\uff0c\u8fdb\u884c\u56e0\u679c\u3001\u6d41\u5f0f\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u6211\u4e2d\u5fc3\u89c6\u7ebf\u4f30\u8ba1\u57fa\u51c6\u7684\u5728\u7ebf\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5229\u7528\u6709\u9650\u5386\u53f2\u7684\u81ea\u56de\u5f52\u5efa\u6a21\u5bf9\u9c81\u68d2\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "ARGaze\u6709\u6548\u5229\u7528\u89c6\u7ebf\u7684\u65f6\u95f4\u8fde\u7eed\u6027\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u5f0f\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u5728\u7ebf\u7b2c\u4e00\u4eba\u79f0\u89c6\u7ebf\u4f30\u8ba1\uff0c\u4e3aAR\u4e0e\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff1b\u4f5c\u8005\u5c06\u5f00\u6e90\u4ee3\u7801\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "summary_cn": "\u5728\u7ebf\u81ea\u6211\u4e2d\u5fc3\u89c6\u7ebf\u4f30\u8ba1\u65e8\u5728\u4ec5\u4f7f\u7528\u8fc7\u53bb\u548c\u5f53\u524d\u5e27\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\uff0c\u9884\u6d4b\u4f69\u6234\u6444\u50cf\u5934\u8005\u5f53\u524d\u6ce8\u89c6\u7684\u4f4d\u7f6e\uff0c\u8fd9\u9879\u4efb\u52a1\u5bf9\u589e\u5f3a\u73b0\u5b9e\u548c\u8f85\u52a9\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002\u4e0e\u7b2c\u4e09\u4eba\u79f0\u89c6\u7ebf\u4f30\u8ba1\u4e0d\u540c\uff0c\u8be5\u573a\u666f\u7f3a\u4e4f\u660e\u786e\u7684\u5934\u90e8\u6216\u773c\u90e8\u4fe1\u53f7\uff0c\u8981\u6c42\u6a21\u578b\u4ece\u624b-\u7269\u4ea4\u4e92\u548c\u663e\u8457\u573a\u666f\u5185\u5bb9\u7b49\u7a00\u758f\u3001\u95f4\u63a5\u7ebf\u7d22\u4e2d\u63a8\u65ad\u5f53\u524d\u89c6\u89c9\u6ce8\u610f\u529b\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5728\u76ee\u6807\u5bfc\u5411\u6d3b\u52a8\u4e2d\uff0c\u89c6\u7ebf\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u65f6\u95f4\u8fde\u7eed\u6027\uff1a\u4e86\u89e3\u7528\u6237\u6700\u8fd1\u7684\u6ce8\u89c6\u4f4d\u7f6e\u53ef\u4e3a\u9884\u6d4b\u4e0b\u4e00\u6b65\u6ce8\u89c6\u63d0\u4f9b\u6709\u529b\u5148\u9a8c\u3002\u53d7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6761\u4ef6\u81ea\u56de\u5f52\u89e3\u7801\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51faARGaze\u65b9\u6cd5\uff0c\u5c06\u89c6\u7ebf\u4f30\u8ba1\u91cd\u6784\u4e3a\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\uff1a\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\uff0cTransformer\u89e3\u7801\u5668\u901a\u8fc7\u7ed3\u5408\uff08i\uff09\u5f53\u524d\u89c6\u89c9\u7279\u5f81\u548c\uff08ii\uff09\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\u8fd1\u671f\u89c6\u7ebf\u76ee\u6807\u4f30\u8ba1\u7a97\u53e3\uff08Gaze Context Window\uff09\u6765\u9884\u6d4b\u5f53\u524d\u89c6\u7ebf\u3002\u8be5\u8bbe\u8ba1\u4fdd\u8bc1\u4e86\u56e0\u679c\u6027\uff0c\u5e76\u652f\u6301\u8d44\u6e90\u53d7\u9650\u7684\u6d41\u5f0f\u63a8\u7406\u3002\u6211\u4eec\u5728\u591a\u4e2a\u81ea\u6211\u4e2d\u5fc3\u89c6\u7ebf\u57fa\u51c6\u7684\u5728\u7ebf\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5927\u91cf\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5229\u7528\u6709\u9650\u89c6\u7ebf\u5386\u53f2\u7684\u81ea\u56de\u5f52\u5efa\u6a21\u5bf9\u9c81\u68d2\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u6e90\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2602.05159", "pdf": "https://arxiv.org/pdf/2602.05159", "abs": "https://arxiv.org/abs/2602.05159", "authors": ["Wenhui Cui", "Ziyi Kou", "Chuan Qin", "Ergys Ristani", "Li Guan"], "title": "AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves", "categories": ["cs.CV"], "comment": "Accepted by ICASSP 2026", "summary": "Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u90e8\u8ffd\u8e2a\u6a21\u578b\u5728\u6234\u624b\u5957\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u88f8\u624b\u6a21\u578b\u56e0\u5916\u89c2\u5dee\u5f02\u5728\u624b\u5957\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u63d0\u51faAirGlove\u65b9\u6cd5\uff0c\u5229\u7528\u5df2\u6709\u624b\u5957\u6570\u636e\u63d0\u5347\u5bf9\u65b0\u7c7b\u578b\u624b\u5957\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u624b\u5957\u8ffd\u8e2a\u65b9\u6cd5\u53d7\u9650\u4e8e\u4fe1\u53f7\u8d28\u91cf\u548c\u6821\u51c6\u7cbe\u5ea6\uff0c\u800c\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u867d\u5728\u88f8\u624b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0d\u540c\u5916\u89c2\u7684\u624b\u5957\u4e0a\u6548\u679c\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u63a2\u7d22\u89c6\u89c9\u6a21\u578b\u5728\u6234\u624b\u5957\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u5e76\u63d0\u5347\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5bf9\u73b0\u6709\u89c6\u89c9\u624b\u90e8\u8ffd\u8e2a\u6a21\u578b\u5728\u624b\u5957\u6570\u636e\u4e0a\u7684\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff1b\u968f\u540e\u63d0\u51faAirGlove\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5df2\u6709\u7684\u591a\u79cd\u4f20\u611f\u624b\u5957\u6570\u636e\uff0c\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u624b\u5957\u8868\u5f81\uff0c\u4ece\u800c\u5728\u4ec5\u6709\u5c11\u91cf\u65b0\u624b\u5957\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u88f8\u624b\u6a21\u578b\u5728\u624b\u5957\u4e0a\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff1b\u800cAirGlove\u5728\u591a\u79cd\u4f20\u611f\u624b\u5957\u4e0a\u5747\u80fd\u6709\u6548\u6cdb\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u89c9\u624b\u90e8\u8ffd\u8e2a\u6a21\u578b\u5728\u624b\u5957\u573a\u666f\u4e2d\u9762\u4e34\u5916\u89c2\u5dee\u5f02\u5e26\u6765\u7684\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u5f15\u5165\u5982AirGlove\u8fd9\u6837\u7684\u6cdb\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u6709\u9650\u6570\u636e\u4e0b\u6709\u6548\u63d0\u5347\u5bf9\u65b0\u7c7b\u578b\u624b\u5957\u7684\u8ffd\u8e2a\u6027\u80fd\u3002", "summary_cn": "\u4f20\u611f\u624b\u5957\u5df2\u6210\u4e3a\u9065\u64cd\u4f5c\u548c\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u7684\u91cd\u8981\u5de5\u5177\uff0c\u56e0\u5176\u80fd\u591f\u63d0\u4f9b\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u548c\u89e6\u89c9\u53cd\u9988\u7b49\u4e30\u5bcc\u4fe1\u53f7\u3002\u4e00\u79cd\u5e38\u89c1\u7684\u8ffd\u8e2a\u6234\u624b\u5957\u624b\u90e8\u7684\u65b9\u6cd5\u662f\u76f4\u63a5\u4f7f\u7528\u4f20\u611f\u5668\u4fe1\u53f7\uff08\u5982\u89d2\u901f\u5ea6\u3001\u91cd\u529b\u65b9\u5411\uff09\u6765\u4f30\u8ba1\u4e09\u7ef4\u624b\u90e8\u59ff\u6001\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u8ffd\u8e2a\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u53d7\u9650\uff0c\u56e0\u4e3a\u5176\u7cbe\u5ea6\u5e38\u53d7\u4f20\u611f\u5668\u4fe1\u53f7\u8d28\u91cf\u548c\u6821\u51c6\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u8fd1\u671f\u57fa\u4e8e\u89c6\u89c9\u7684\u65b9\u6cd5\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5728\u4eba\u7c7b\u88f8\u624b\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u5177\u6709\u663e\u8457\u4e0d\u540c\u89c6\u89c9\u5916\u89c2\u7684\u6234\u624b\u5957\u624b\u90e8\u4e0a\u7684\u8868\u73b0\u4ecd\u7f3a\u4e4f\u6df1\u5165\u63a2\u7d22\u3002\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u90e8\u8ffd\u8e2a\u6a21\u578b\u5728\u6234\u624b\u5957\u624b\u90e8\u4e0a\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u96f6\u6837\u672c\u548c\u5fae\u8c03\u4e24\u79cd\u8bbe\u7f6e\u3002\u5206\u6790\u8868\u660e\uff0c\u7531\u4e8e\u88f8\u624b\u4e0e\u624b\u5957\u8bbe\u8ba1\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5916\u89c2\u5dee\u5f02\uff0c\u73b0\u6709\u88f8\u624b\u6a21\u578b\u5728\u4f20\u611f\u624b\u5957\u4e0a\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AirGlove\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5df2\u6709\u7684\u624b\u5957\u6570\u636e\uff0c\u5c06\u6240\u5b66\u7684\u624b\u5957\u8868\u5f81\u6cdb\u5316\u81f3\u4ec5\u6709\u5c11\u91cf\u6570\u636e\u7684\u65b0\u7c7b\u578b\u624b\u5957\u4e0a\u3002\u5728\u591a\u79cd\u4f20\u611f\u624b\u5957\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAirGlove\u80fd\u6709\u6548\u5c06\u624b\u90e8\u59ff\u6001\u6a21\u578b\u6cdb\u5316\u81f3\u65b0\u624b\u5957\u8bbe\u8ba1\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6848\u3002"}}
{"id": "2602.05162", "pdf": "https://arxiv.org/pdf/2602.05162", "abs": "https://arxiv.org/abs/2602.05162", "authors": ["Anay Majee", "Rishabh Iyer"], "title": "SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 7 tables, 10 figures", "summary": "Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSHaSaM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u6a21\u786c\u6837\u672c\u6316\u6398\u63d0\u5347\u6a21\u578b\u516c\u5e73\u6027\u4e0e\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u4e2d\u6613\u4ece\u6807\u6ce8\u6570\u636e\u7ee7\u627f\u793e\u4f1a\u548c\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u5bfc\u81f4\u5bf9\u654f\u611f\u5c5e\u6027\uff08\u5982\u79cd\u65cf\u3001\u5e74\u9f84\u3001\u6027\u522b\uff09\u7684\u4e0d\u516c\u5e73\u9884\u6d4b\uff1b\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u53cd\u800c\u52a0\u5267\u4e86\u4e0d\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51faSHaSaM\uff08Submodular Hard Sample Mining\uff09\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1aSHaSaM-MINE\u5229\u7528\u5b50\u6a21\u5b50\u96c6\u9009\u62e9\u7b56\u7565\u6316\u6398\u96be\u5206\u6b63\u8d1f\u6837\u672c\u4ee5\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\uff1bSHaSaM-LEARN\u57fa\u4e8e\u5b50\u6a21\u6761\u4ef6\u4e92\u4fe1\u606f\u8bbe\u8ba1\u7ec4\u5408\u635f\u5931\u51fd\u6570\uff0c\u5728\u6700\u5927\u5316\u76ee\u6807\u7c7b\u522b\u51b3\u7b56\u8fb9\u754c\u7684\u540c\u65f6\u6700\u5c0f\u5316\u654f\u611f\u5c5e\u6027\u5f71\u54cd\u3002", "result": "\u5728CelebA\u548cUTKFace\u6570\u636e\u96c6\u4e0a\uff0cSHaSaM\u5728\u66f4\u5c11\u8bad\u7ec3\u8f6e\u6b21\u5185\u5b9e\u73b0SOTA\u6548\u679c\uff0c\u516c\u5e73\u6027\uff08Equalized Odds\uff09\u63d0\u5347\u6700\u591a2.7\u70b9\uff0c\u51c6\u786e\u7387\u63d0\u9ad83.5%\u3002", "conclusion": "SHaSaM\u901a\u8fc7\u7edf\u4e00\u7684\u5b50\u6a21\u4f18\u5316\u6846\u67b6\u6709\u6548\u6291\u5236\u6a21\u578b\u5b66\u4e60\u654f\u611f\u5c5e\u6027\u76f8\u5173\u7279\u5f81\uff0c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u3002", "summary_cn": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e38\u5e38\u4ece\u6807\u6ce8\u6570\u636e\u4e2d\u7ee7\u627f\u793e\u4f1a\u548c\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u5c24\u5176\u5728\u5b58\u5728\u79cd\u65cf\u3001\u5e74\u9f84\u3001\u6027\u522b\u7b49\u654f\u611f\u5c5e\u6027\u7684\u60c5\u51b5\u4e0b\u66f4\u4e3a\u4e25\u91cd\u3002\u73b0\u6709\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5c5e\u6027\u7ec4\u4e4b\u95f4\u56fa\u6709\u6570\u636e\u4e0d\u5e73\u8861\u7684\u5f71\u54cd\uff0c\u65e0\u610f\u4e2d\u5f3a\u5316\u4e86\u5bf9\u654f\u611f\u5c5e\u6027\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u52a0\u5267\u4e86\u4e0d\u516c\u5e73\u6027\u5e76\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SHaSaM\uff08Submodular Hard Sample Mining\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u9762\u5411\u516c\u5e73\u6027\u7684\u8868\u5f81\u5b66\u4e60\u5efa\u6a21\u4e3a\u5b50\u6a21\u786c\u6837\u672c\u6316\u6398\u95ee\u9898\u3002\u6211\u4eec\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u5305\u62ec\uff1aSHaSaM-MINE\uff0c\u5f15\u5165\u5b50\u6a21\u5b50\u96c6\u9009\u62e9\u7b56\u7565\u6765\u6316\u6398\u96be\u5206\u7684\u6b63\u8d1f\u6837\u672c\uff0c\u6709\u6548\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\uff1b\u4ee5\u53caSHaSaM-LEARN\uff0c\u57fa\u4e8e\u5b50\u6a21\u6761\u4ef6\u4e92\u4fe1\u606f\u6784\u5efa\u4e00\u7c7b\u7ec4\u5408\u635f\u5931\u51fd\u6570\uff0c\u5728\u6700\u5927\u5316\u76ee\u6807\u7c7b\u522b\u95f4\u51b3\u7b56\u8fb9\u754c\u7684\u540c\u65f6\u6700\u5c0f\u5316\u654f\u611f\u5c5e\u6027\u7684\u5f71\u54cd\u3002\u8fd9\u4e00\u7edf\u4e00\u6846\u67b6\u9650\u5236\u6a21\u578b\u5b66\u4e60\u4e0e\u654f\u611f\u5c5e\u6027\u76f8\u5173\u7684\u7279\u5f81\uff0c\u4ece\u800c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u3002\u5728CelebA\u548cUTKFace\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSHaSaM\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7ed3\u679c\uff0c\u5728\u66f4\u5c11\u8bad\u7ec3\u8f6e\u6b21\u5185\uff0c\u6a21\u578b\u516c\u5e73\u6027\uff08Equalized Odds\uff09\u6700\u591a\u63d0\u53472.7\u70b9\uff0c\u51c6\u786e\u7387\u63d0\u9ad83.5%\u3002"}}
{"id": "2602.05163", "pdf": "https://arxiv.org/pdf/2602.05163", "abs": "https://arxiv.org/abs/2602.05163", "authors": ["Andreas Mentzelopoulos", "Keith Ellenbogen"], "title": "LOBSTgER-enhance: an underwater image enhancement pipeline", "categories": ["cs.CV"], "comment": "12 pages, 30 figures, work done as part of LOBSTgER", "summary": "Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.\n  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u4fee\u590d\u6c34\u4e0b\u6444\u5f71\u4e2d\u7684\u5bf9\u6bd4\u5ea6\u4e0b\u964d\u3001\u6a21\u7cca\u548c\u989c\u8272\u5931\u771f\u95ee\u9898\uff0c\u5728\u5c0f\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u89c6\u89c9\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6c34\u4e0b\u6444\u5f71\u5b58\u5728\u56fa\u6709\u6311\u6218\uff0c\u5982\u5bf9\u6bd4\u5ea6\u964d\u4f4e\u3001\u7a7a\u95f4\u6a21\u7cca\u548c\u6ce2\u957f\u76f8\u5173\u7684\u8272\u5f69\u5931\u771f\uff0c\u4e25\u91cd\u5f71\u54cd\u6d77\u6d0b\u751f\u7269\u7684\u89c6\u89c9\u8868\u73b0\u529b\uff1b\u6444\u5f71\u5e08\u901a\u5e38\u9700\u4f9d\u8d56\u7e41\u91cd\u7684\u540e\u671f\u5904\u7406\u6765\u6821\u6b63\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u81ea\u52a8\u5316\u4e14\u9ad8\u6548\u7684\u4fee\u590d\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5408\u6210\u9000\u5316\u6d41\u7a0b\u6765\u6a21\u62df\u6c34\u4e0b\u56fe\u50cf\u9000\u5316\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u7ba1\u9053\u5b66\u4e60\u9006\u8f6c\u8fd9\u4e9b\u9000\u5316\u6548\u679c\uff1b\u6a21\u578b\u5728\u7ea62.5k\u5f20\u9ad8\u8d28\u91cf\u6c34\u4e0b\u610f\u8bc6\u6444\u5f71\u56fe\u50cf\u4e0a\u4ece\u5934\u8bad\u7ec3\uff0c\u53c2\u6570\u91cf\u7ea6\u4e3a1100\u4e07\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210512x768\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u8868\u73b0\u51fa\u9ad8\u611f\u77e5\u4e00\u81f4\u6027\u4e0e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u9a71\u52a8\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u6c34\u4e0b\u6444\u5f71\u7684\u5178\u578b\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316\u9ad8\u8d28\u91cf\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6848\u3002", "summary_cn": "\u6c34\u4e0b\u6444\u5f71\u9762\u4e34\u663e\u8457\u7684\u56fa\u6709\u6311\u6218\uff0c\u5305\u62ec\u5bf9\u6bd4\u5ea6\u964d\u4f4e\u3001\u7a7a\u95f4\u6a21\u7cca\u4ee5\u53ca\u4e0e\u6ce2\u957f\u76f8\u5173\u7684\u8272\u5f69\u5931\u771f\u3002\u8fd9\u4e9b\u6548\u5e94\u4f1a\u63a9\u76d6\u6d77\u6d0b\u751f\u7269\u7684\u751f\u52a8\u6027\uff0c\u5c24\u5176\u5bf9\u6ce8\u91cd\u751f\u6001\u610f\u8bc6\u7684\u6444\u5f71\u5e08\u800c\u8a00\uff0c\u4ed6\u4eec\u5e38\u5e38\u9700\u8981\u4f9d\u8d56\u7e41\u91cd\u7684\u540e\u671f\u5904\u7406\u6d41\u7a0b\u6765\u6821\u6b63\u8fd9\u4e9b\u5931\u771f\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5408\u6210\u9000\u5316\u6d41\u7a0b\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u5b66\u4e60\u9006\u8f6c\u5176\u5f71\u54cd\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u6c34\u4e0b\u56fe\u50cf\u9000\u5316\u7684\u81ea\u52a8\u4fee\u590d\u3002\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u5747\u57fa\u4e8eKeith Ellenbogen\u63d0\u4f9b\u7684\u5c0f\u89c4\u6a21\u9ad8\u8d28\u91cf\u610f\u8bc6\u6444\u5f71\u56fe\u50cf\u6570\u636e\u96c6\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ece\u5934\u8bad\u7ec3\u7ea62500\u5f20\u56fe\u50cf\u540e\uff0c\u4f7f\u7528\u7ea61100\u4e07\u53c2\u6570\u7684\u6a21\u578b\u5373\u53ef\u5408\u6210512\u00d7768\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u5e76\u5c55\u73b0\u51fa\u9ad8\u5ea6\u7684\u611f\u77e5\u4e00\u81f4\u6027\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
