<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration](https://arxiv.org/abs/2602.11214)
*Manuel Hetzel,Kerim Turacan,Hannes Reichert,Konrad Doll,Bernhard Sick*

Main category: cs.CV

TL;DR: 提出DD-MDN模型，结合去噪扩散与混合密度网络，在短观测条件下实现高精度、校准良好的人类轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了人类轨迹预测中的不确定性建模、校准能力以及在短观测窗口下的鲁棒性，而这些对路径规划和避障等下游任务至关重要。

Method: 采用端到端概率模型DD-MDN，结合少样本去噪扩散主干网络和双混合密度网络，学习自校准的驻留区域和按概率排序的锚点路径，无需预设锚点或终点。

Result: 在ETH/UCY、SDD、inD和IMPTC数据集上达到SOTA性能，在短观测条件下表现出色，并具备可靠的不确定性建模能力。

Conclusion: DD-MDN有效解决了人类轨迹预测中不确定性校准与短观测鲁棒性问题，为实际应用提供了更可靠的支持。

Abstract: Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.

Abstract (中文翻译): 人类轨迹预测（HTF）通过过去轨迹和环境上下文预测未来人类运动，广泛应用于自动驾驶、智能监控和人机交互。尽管以往工作聚焦于预测准确性、社会交互建模和多样性，但对不确定性建模、校准能力以及短观测窗口下的预测关注较少，而这些对于路径规划和碰撞规避等下游任务至关重要。本文提出DD-MDN，一种端到端的概率型HTF模型，兼具高位置精度、校准良好的不确定性和对短观测的鲁棒性。该方法利用少样本去噪扩散主干网络和双混合密度网络，学习自校准的驻留区域和按概率排序的锚点路径，从而生成多样化的轨迹假设，无需预设锚点或终点。在ETH/UCY、SDD、inD和IMPTC数据集上的实验表明，该方法在预测精度、短观测鲁棒性和不确定性建模方面均达到当前最优水平。代码已开源：https://github.com/kav-institute/ddmdn。

</details>


### [2] [ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning](https://arxiv.org/abs/2602.11236)
*Yandan Yang,Shuang Zeng,Tong Lin,Xinyuan Chang,Dekang Qi,Junjin Xiao,Haoyun Liu,Ronghan Chen,Yuzhi Chen,Dongjie Huo,Feng Xiong,Xing Wei,Zhiheng Ma,Mu Xu*

Main category: cs.CV

TL;DR: ABot-M0 是一个统一框架，通过系统化数据处理、动作流形学习和模块化感知，实现跨多种机器人形态的通用具身智能。


<details>
  <summary>Details</summary>
Motivation: 当前机器人领域缺乏统一的数据表示、训练目标和模型架构，阻碍了“一脑多形”范式的发展；亟需一种能整合异构数据并提升跨平台泛化能力的方法。

Method: 提出 ABot-M0 框架：1）构建包含600万轨迹的 UniACT-dataset 数据集；2）基于动作流形假设（Action Manifold Hypothesis）引入 Action Manifold Learning (AML)，使用 DiT 骨干网络直接预测低维连续动作；3）采用双流机制融合 VLM 语义与几何先验，结合可插拔3D模块增强空间理解。

Result: 实验表明各组件具有独立性和叠加增益，显著提升动作预测效率、策略稳定性及跨任务/平台的泛化能力。

Conclusion: ABot-M0 为通用具身智能提供了可复现、模块化且高效的学习框架，推动“一脑多形”机器人系统的发展。

Abstract: Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.

Abstract (中文翻译): 在机器人领域，构建适用于多种硬件平台的通用具身智能体（即“一脑多形”范式）仍是一大核心挑战。当前进展受限于数据碎片化、表示不一致以及训练目标错位等问题。本文提出 ABot-M0 框架，通过构建系统化的数据整理流程，并联合优化模型架构与训练策略，实现从异构原始数据到统一高效表示的端到端转换。我们从六个公开数据集中清洗、标准化并平衡样本，构建了大规模 UniACT-dataset 数据集，包含超过600万条轨迹和9500小时的数据，涵盖多样化的机器人形态与任务场景。统一预训练显著提升了跨平台和跨任务的知识迁移与泛化能力，支持通用具身智能的发展。为提升动作预测的效率与稳定性，我们提出“动作流形假设”：有效的机器人动作并非分布于完整的高维空间，而是位于由物理规律和任务约束所决定的低维光滑流形上。基于此，我们引入动作流形学习（Action Manifold Learning, AML），利用 DiT 骨干网络直接预测干净、连续的动作序列，将学习目标从去噪转变为向可行流形投影，从而提升解码速度与策略稳定性。此外，ABot-M0 通过双流机制支持模块化感知，将视觉语言模型（VLM）的语义信息与几何先验相结合，并集成如 VGGT 和 Qwen-Image-Edit 等可插拔的3D模块以处理多视角输入，在不修改主干网络的前提下增强空间理解能力，缓解标准 VLM 在三维推理中的局限性。实验表明，各组件可独立运行并带来叠加性能增益。我们将开源全部代码与数据处理流程，以促进可复现性与后续研究。

</details>


### [3] [Toward Reliable Tea Leaf Disease Diagnosis Using Deep Learning Model: Enhancing Robustness With Explainable AI and Adversarial Training](https://arxiv.org/abs/2602.11239)
*Samanta Ghosh,Jannatul Adan Mahi,Shayan Abrar,Md Parvez Mia,Asaduzzaman Rayhan,Abdul Awal Yasir,Asaduzzaman Hridoy*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动化茶树叶病害分类方法，利用茶LeafBD数据集（含5278张高分辨率图像，7类标签），结合对抗训练与可解释AI技术，EfficientNetB3模型达到了93%的准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国经济高度依赖茶叶产业，但茶树易受多种叶部病害影响，导致产量和品质下降；传统人工检测效率低、易出错，因此亟需一种高效、自动化的病害识别方法。

Method: 研究构建了一个包含数据预处理、划分、对抗训练、数据增强、模型训练、评估及可解释性分析的完整流程；采用DenseNet201和EfficientNetB3进行分类，并引入对抗训练提升鲁棒性，使用Grad-CAM进行可视化解释。

Result: 实验结果显示，EfficientNetB3在茶LeafBD数据集上达到93%的分类准确率，DenseNet201为91%，验证了所提方法在茶树叶病害识别中的有效性。

Conclusion: 该研究提出的深度学习框架能高效、准确地识别茶树叶病害，结合可解释AI技术增强了模型可信度，为智能农业管理提供了实用解决方案。

Abstract: Tea is a valuable asset for the economy of Bangladesh. So, tea cultivation plays an important role to boost the economy. These valuable plants are vulnerable to various kinds of leaf infections which may cause less production and low quality. It is not so easy to detect these diseases manually. It may take time and there could be some errors in the detection.Therefore, the purpose of the study is to develop an automated deep learning model for tea leaf disease classification based on the teaLeafBD dataset so that anyone can detect the diseases more easily and efficiently. There are 5,278 high-resolution images in this dataset. The images are classified into seven categories. Six of them represents various diseases and the rest one represents healthy leaves. The proposed pipeline contains data preprocessing, data splitting, adversarial training, augmentation, model training, evaluation, and comprehension made possible with Explainable AI strategies. DenseNet201 and EfficientNetB3 were employed to perform the classification task. To prepare the model more robustly, we applied adversarial training so it can operate effectively even with noisy or disturbed inputs. In addition, Grad-CAM visualization was executed to analyze the model's predictions by identifying the most influential regions of each image. Our experimental outcomes revealed that EfficientNetB3 achieved the highest classification accuracy of 93%, while DenseNet201 reached 91%. The outcomes prove that the effectiveness of the proposed approach can accurately detect tea leaf diseases and provide a practical solution for advanced agricultural management.

Abstract (中文翻译): 茶叶是孟加拉国经济的重要资产，因此茶叶种植在推动经济发展中发挥着关键作用。然而，这些宝贵的植物容易受到多种叶部病害的侵袭，可能导致产量减少和品质下降。人工检测这些病害并不容易，既耗时又可能出错。因此，本研究旨在基于teaLeafBD数据集开发一种自动化的深度学习模型，以更轻松、高效地检测茶树叶病害。该数据集包含5,278张高分辨率图像，分为七类：其中六类代表不同病害，一类代表健康叶片。所提出的流程包括数据预处理、数据划分、对抗训练、数据增强、模型训练、评估以及通过可解释人工智能（Explainable AI）策略实现的模型理解。研究采用了DenseNet201和EfficientNetB3进行分类任务。为增强模型鲁棒性，应用了对抗训练，使其即使在面对噪声或扰动输入时也能有效运行。此外，还通过Grad-CAM可视化技术分析模型预测，识别每张图像中最具影响力的区域。实验结果表明，EfficientNetB3取得了93%的最高分类准确率，而DenseNet201达到了91%。结果证明，所提出的方法能够准确检测茶树叶病害，为现代农业管理提供了切实可行的解决方案。

</details>


### [4] [Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration](https://arxiv.org/abs/2602.11241)
*Jinghan He,Junfeng Fang,Feng Xiong,Zijun Yao,Fei Shen,Haiyun Guo,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出Active-Zero框架，通过三个协同演化的智能体（Searcher、Questioner、Solver）实现视觉语言模型的主动探索式自学习，在多个基准上显著优于现有自博弈方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的自博弈方法依赖于静态图像集合的被动交互，导致对初始数据集高度依赖且学习效率低下；模型无法主动获取与其当前能力相匹配的视觉数据，造成计算资源浪费。

Method: 提出Active-Zero框架，包含三个协同演化的智能体：Searcher根据模型能力前沿从开放世界图像库中检索图像，Questioner生成校准后的推理任务，Solver则通过准确率奖励进行优化，形成闭环自引导课程。

Result: 在Qwen2.5-VL-7B-Instruct模型上，Active-Zero在12个基准测试中推理任务平均准确率达53.97%（提升5.7%），通用理解达59.77%（提升3.9%），持续优于现有自博弈基线。

Conclusion: 主动探索是构建可扩展、自适应的自演化视觉语言系统的关键要素。

Abstract: Self-play has enabled large language models to autonomously improve through self-generated challenges. However, existing self-play methods for vision-language models rely on passive interaction with static image collections, resulting in strong dependence on initial datasets and inefficient learning. Without the ability to actively seek visual data tailored to their evolving capabilities, agents waste computational effort on samples that are either trivial or beyond their current skill level. To address these limitations, we propose Active-Zero, a framework that shifts from passive interaction to active exploration of visual environments. Active-Zero employs three co-evolving agents: a Searcher that retrieves images from open-world repositories based on the model's capability frontier, a Questioner that synthesizes calibrated reasoning tasks, and a Solver refined through accuracy rewards. This closed loop enables self-scaffolding auto-curricula where the model autonomously constructs its learning trajectory. On Qwen2.5-VL-7B-Instruct across 12 benchmarks, Active-Zero achieves 53.97 average accuracy on reasoning tasks (5.7% improvement) and 59.77 on general understanding (3.9% improvement), consistently outperforming existing self-play baselines. These results highlight active exploration as a key ingredient for scalable and adaptive self-evolving vision-language systems.

Abstract (中文翻译): 自博弈使大语言模型能够通过自我生成的挑战实现自主提升。然而，现有的视觉语言模型自博弈方法依赖于与静态图像集合的被动交互，导致对初始数据集的高度依赖和低效的学习过程。由于无法主动寻求与其不断演进能力相匹配的视觉数据，智能体将大量计算资源浪费在过于简单或超出当前能力水平的样本上。为解决这些问题，我们提出了Active-Zero框架，将交互方式从被动转向对视觉环境的主动探索。Active-Zero采用三个协同演化的智能体：Searcher根据模型的能力前沿从开放世界图像库中检索图像，Questioner合成经过校准的推理任务，而Solver则通过准确率奖励进行优化。这一闭环机制实现了自引导的自动课程，使模型能够自主构建其学习轨迹。在Qwen2.5-VL-7B-Instruct模型上，Active-Zero在12个基准测试中推理任务平均准确率达到53.97%（提升5.7%），通用理解达到59.77%（提升3.9%），持续优于现有的自博弈基线方法。这些结果表明，主动探索是构建可扩展、自适应的自演化视觉语言系统的关键要素。

</details>


### [5] [ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems](https://arxiv.org/abs/2602.11242)
*Yitong Wang,Yue Yao*

Main category: cs.CV

TL;DR: ReTracing 是一个多智能体具身表演艺术项目，通过考古学方法揭示生成式AI如何通过编舞动作编码社会文化偏见，并探讨“在也能移动、思考并留下痕迹的AI中，何以为人”这一问题。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能如何塑造、限制并生成人体动作，揭示生成式系统在编舞中隐含的社会文化偏见，并反思人类在具备行动与思考能力的AI环境中的身份。

Method: 从科幻小说中提取描述人机交互的句子，利用大语言模型（LLMs）为每段文字生成“应做”与“不应做”的成对提示；再用基于扩散的文本到视频模型将这些提示转化为人类表演者的编舞指导和四足机器人的运动指令。人类与机器人在镜面地板上执行动作，由多相机运动捕捉系统记录，并重建为3D点云与运动轨迹，形成数字动作档案。

Result: 成功构建了一个由AI生成、人类与机器人共同演绎的具身动作档案，可视化了生成模型如何通过动作编码社会文化规范与偏见。

Conclusion: ReTracing 不仅是一种新型的跨媒介艺术实践，更提供了一种批判性方法，用于审视生成式AI在身体动作层面所承载的文化预设，并引发对人类在AI时代主体性的深层思考。

Abstract: We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts "what to do" and "what not to do" for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?

Abstract (中文翻译): 我们提出了 ReTracing，这是一个多智能体具身表演艺术项目，采用考古学方法考察人工智能如何塑造、限制并生成身体动作。该项目从科幻小说中提取描述人机交互的句子，利用大语言模型（LLMs）为每段摘录生成“应做”与“不应做”的成对提示。随后，基于扩散的文本到视频模型将这些提示转化为人类表演者的编舞指南和四足机器人的电机指令。人类与机器人在镜面地板上执行这些动作，由多相机运动追踪系统捕捉，并重建为三维点云与运动轨迹，形成一个动作痕迹的数字档案。通过这一过程，ReTracing 揭示了生成式系统如何通过编排的动作编码社会文化偏见。在人工智能、人类与机器人沉浸式互动中，ReTracing 直面当今一个关键问题：在同样能够移动、思考并留下痕迹的AI之中，何以为人？

</details>


### [6] [Stress Tests REVEAL Fragile Temporal and Visual Grounding in Video-Language Models](https://arxiv.org/abs/2602.11244)
*Sethuraman T,Savya Khosla,Aditi Tiwari,Vidya Ganesh,Rakshana Jayaprakash,Aditya Jain,Vignesh Srinivasakumar,Onkar Kishor Susladkar,Srinidhi Sunkara,Aditya Shanmugham,Rakesh Vaideeswaran,Abbaas Alif Mohamed Nishar,Simon Jenni,Derek Hoiem*

Main category: cs.CV

TL;DR: 该论文发现当前视频-语言模型（VidLMs）在处理视频内容、时序和运动方面存在严重缺陷，并提出了名为REVEAL的诊断基准，通过五项压力测试揭示这些弱点。


<details>
  <summary>Details</summary>
Motivation: 探究当前视频-语言模型是否真正理解视频中的内容、时间顺序和运动，因为现有模型可能依赖语言捷径或忽略关键视觉信息。

Method: 提出REVEAL诊断基准，包含五项受控压力测试：时间预期偏差、仅依赖语言的捷径、视频谄媚（video sycophancy）、对摄像机运动的敏感性、以及对时空遮挡的鲁棒性；同时提供自动生成诊断样本的数据管道。

Result: 主流开源和闭源VidLM在所有测试中表现不佳：将倒放视频描述为正向、忽略视频内容作答、认同错误陈述、难以处理基本摄像机运动、在简单时空遮挡下无法整合时间信息；而人类在这些任务上轻松成功。

Conclusion: 当前VidLM并未真正稳健地理解视频内容与时序动态，需更严格的评估基准；作者将公开REVEAL基准和代码以推动后续研究。

Abstract: This work investigates a fundamental question: Do Video-Language Models (VidLMs) robustly account for video content, temporal sequence, and motion? Our investigation shows that, surprisingly, they often do not. We introduce REVEAL{}, a diagnostic benchmark that probes fundamental weaknesses of contemporary VidLMs through five controlled stress tests; assessing temporal expectation bias, reliance on language-only shortcuts, video sycophancy, camera motion sensitivity, and robustness to spatiotemporal occlusion. We test leading open- and closed-source VidLMs and find that these models confidently describe reversed scenes as forward, answer questions while neglecting video content, agree with false claims, struggle with basic camera motion, and fail to aggregate temporal information amidst simple spatiotemporal masking. Humans, on the other hand, succeed at these tasks with ease. Alongside our benchmark, we provide a data pipeline that automatically generates diagnostic examples for our stress tests, enabling broader and more scalable evaluation. We will release our benchmark and code to support future research.

Abstract (中文翻译): 本研究探讨了一个基本问题：视频-语言模型（VidLMs）是否能够稳健地理解视频内容、时间顺序和运动？我们的研究发现，令人惊讶的是，它们通常并不能。我们提出了REVEAL{}诊断基准，通过五项受控压力测试来揭示当前VidLMs的根本弱点，包括时间预期偏差、对纯语言捷径的依赖、视频谄媚（即盲目附和用户输入）、对摄像机运动的敏感性，以及在时空遮挡下的鲁棒性。我们测试了主流的开源和闭源VidLMs，发现这些模型会自信地将倒放场景描述为正向播放，在回答问题时忽略视频内容，认同错误陈述，难以应对基本的摄像机运动，并且在简单的时空遮挡下无法有效整合时间信息。相比之下，人类能轻松完成这些任务。除基准外，我们还提供了一个可自动生成诊断样本的数据管道，以支持更广泛、可扩展的评估。我们将公开发布该基准和代码，以促进未来研究。

</details>


### [7] [Advancing Digital Twin Generation Through a Novel Simulation Framework and Quantitative Benchmarking](https://arxiv.org/abs/2602.11314)
*Jacob Rubinstein,Avi Donaty,Don Engel*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过高质量3D模型和程序化生成的相机位姿合成图像，用于可重复、可量化的数字孪生重建评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于摄影测量法生成数字孪生的方法存在多种设计选择，但其效果多依赖主观定性评价，缺乏可量化、可重复的评估手段。

Method: 构建一个新流程：利用高质量3D模型与程序化生成的虚拟相机位姿合成2D图像，并以此作为输入进行三维重建，从而将重建结果与已知的真实（ground-truth）参数进行定量比较。

Result: 该方法支持大量可重复实验，能对重建算法在视角和对象上的估计精度进行系统性、量化评估。

Conclusion: 所提出的合成图像生成流程为数字孪生和三维重建研究提供了可靠的基准测试平台，有助于推动该领域向更客观、可量化的方向发展。

Abstract: The generation of 3D models from real-world objects has often been accomplished through photogrammetry, i.e., by taking 2D photos from a variety of perspectives and then triangulating matched point-based features to create a textured mesh. Many design choices exist within this framework for the generation of digital twins, and differences between such approaches are largely judged qualitatively. Here, we present and test a novel pipeline for generating synthetic images from high-quality 3D models and programmatically generated camera poses. This enables a wide variety of repeatable, quantifiable experiments which can compare ground-truth knowledge of virtual camera parameters and of virtual objects against the reconstructed estimations of those perspectives and subjects.

Abstract (中文翻译): 从现实世界物体生成3D模型通常通过摄影测量法实现，即从多个视角拍摄2D照片，然后通过三角化匹配的点特征来构建带纹理的网格。在此框架内，生成数字孪生存在多种设计选择，而不同方法之间的差异主要依靠定性判断。本文提出并测试了一种新颖的流程，该流程利用高质量3D模型和程序化生成的相机位姿来生成合成图像。这使得能够开展大量可重复且可量化的实验，将虚拟相机参数和虚拟物体的已知真实信息与其重建后的估计结果进行对比。

</details>


### [8] [Selective Prior Synchronization via SYNC Loss](https://arxiv.org/abs/2602.11316)
*Ishan Mishra,Jiajie Li,Deepak Mishra,Jinjun Xiong*

Main category: cs.CV

TL;DR: 本文提出SYNC loss，将后验（post-hoc）方法中的softmax响应整合到先验（ad-hoc）方法SelectiveNet的训练过程中，通过利用选择先验（selective prior）提升模型在不确定情况下的选择性预测性能，在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测方法分为两类：一类是修改网络结构或目标函数的ad-hoc方法（如SelectiveNet），另一类是基于模型输出概率进行分析的post-hoc方法（如softmax响应）。作者发现post-hoc方法隐含生成的选择先验（selective prior）仅用于推理阶段，而未在训练中加以利用，限制了模型性能。

Method: 提出SYNC loss，将post-hoc方法（softmax响应）产生的选择先验融入ad-hoc方法（SelectiveNet）的训练过程，使模型在训练阶段也能利用不确定性信息，从而提升选择性预测能力。

Result: 在CIFAR-100、ImageNet-100和Stanford Cars等多个数据集上，所提方法不仅提升了模型泛化能力，还在选择性预测任务上超越了先前方法，达到新的SOTA水平。

Conclusion: 将选择先验引入训练阶段能有效提升选择性预测性能，SYNC loss成功融合了ad-hoc与post-hoc方法的优势，为不确定性下的可靠预测提供了新思路。

Abstract: Prediction under uncertainty is a critical requirement for the deep neural network to succeed responsibly. This paper focuses on selective prediction, which allows DNNs to make informed decisions about when to predict or abstain based on the uncertainty level of their predictions. Current methods are either ad-hoc such as SelectiveNet, focusing on how to modify the network architecture or objective function, or post-hoc such as softmax response, achieving selective prediction through analyzing the model's probabilistic outputs. We observe that post-hoc methods implicitly generate uncertainty information, termed the selective prior, which has traditionally been used only during inference. We argue that the selective prior provided by the selection mechanism is equally vital during the training stage. Therefore, we propose the SYNC loss which introduces a novel integration of ad-hoc and post-hoc method. Specifically, our approach incorporates the softmax response into the training process of SelectiveNet, enhancing its selective prediction capabilities by examining the selective prior. Evaluated across various datasets, including CIFAR-100, ImageNet-100, and Stanford Cars, our method not only enhances the model's generalization capabilities but also surpasses previous works in selective prediction performance, and sets new benchmarks for state-of-the-art performance.

Abstract (中文翻译): 在不确定性下进行预测是深度神经网络实现负责任决策的关键要求。本文聚焦于选择性预测，即允许深度神经网络根据其预测的不确定性水平，自主决定是否进行预测或放弃预测。当前的方法可分为两类：一类是临时性的（ad-hoc）方法，如SelectiveNet，侧重于修改网络架构或目标函数；另一类是事后分析型（post-hoc）方法，如softmax响应，通过对模型概率输出的分析实现选择性预测。我们观察到，post-hoc方法隐式地生成了一种称为“选择先验”（selective prior）的不确定性信息，传统上仅在推理阶段使用。我们认为，由选择机制提供的选择先验在训练阶段同样至关重要。因此，我们提出了SYNC loss，这是一种新颖的ad-hoc与post-hoc方法的融合策略。具体而言，我们的方法将softmax响应整合进SelectiveNet的训练过程中，通过考察选择先验来增强其选择性预测能力。在包括CIFAR-100、ImageNet-100和Stanford Cars在内的多个数据集上的实验表明，该方法不仅提升了模型的泛化能力，还在选择性预测性能上超越了以往工作，树立了新的最先进（state-of-the-art）基准。

</details>


### [9] [MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors](https://arxiv.org/abs/2602.11323)
*Arda Alniak,Sinan Kalkan,Mustafa Mert Ankarali,Afsar Saranli,Abdullah Aydin Alatan*

Main category: cs.CV

TL;DR: 本文提出一种将基于Vision Transformer的深度估计先验融入VINS-Mono后端的新框架，在保持边缘设备实时性的同时，显著提升单目视觉惯性里程计在低纹理环境下的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统单目视觉惯性里程计（VIO）在低纹理环境中因缺乏足够视觉特征而难以准确估计位姿；虽然基于ViT的密集深度估计模型能提供几何一致的深度信息，但其计算开销大，难以部署于边缘设备。

Method: 将学习到的深度先验直接集成到VINS-Mono优化后端中，引入仿射不变深度一致性约束和成对序数约束，并通过基于方差的门控机制显式过滤不稳定的深度伪影。

Result: 在TartanGround和M3ED数据集上的实验表明，该方法在挑战性场景中有效防止轨迹发散，绝对轨迹误差（ATE）最多降低28.3%。

Conclusion: 所提方法在严格遵守边缘设备计算限制的前提下，实现了对度量尺度的稳健恢复，显著提升了VIO系统在低纹理环境中的性能。

Abstract: Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.

Abstract (中文翻译): 传统的单目视觉惯性里程计（VIO）系统在低纹理环境中表现不佳，因为稀疏的视觉特征不足以进行精确的位姿估计。为解决此问题，密集单目深度估计（MDE）被广泛探索作为补充信息源。尽管近期基于视觉Transformer（ViT）的复杂基础模型能够提供密集且几何一致的深度信息，但其高昂的计算需求通常使其无法在边缘设备上实现实时部署。我们的工作通过将学习到的深度先验直接集成到VINS-Mono的优化后端中，弥合了这一差距。我们提出了一种新颖的框架，该框架施加仿射不变的深度一致性约束和成对序数约束，并通过基于方差的门控机制显式地滤除不稳定的伪影。该方法在严格遵守边缘设备计算限制的同时，稳健地恢复了度量尺度。在TartanGround和M3ED数据集上的大量实验表明，我们的方法在具有挑战性的场景中有效防止了轨迹发散，并显著提升了精度，绝对轨迹误差（ATE）最多降低了28.3%。代码将公开发布。

</details>


### [10] [Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content](https://arxiv.org/abs/2602.11339)
*Evgeney Bogatyrev,Khaled Abud,Ivan Molodetskikh,Nikita Alutis,Dmitry Vatolin*

Main category: cs.CV

TL;DR: 该论文提出了一个面向真实流媒体场景的超分辨率数据集StreamSR，并引入了一个高效实时超分模型EfRLFN，该模型在视觉质量和运行效率上均有提升；同时展示了在新数据集上微调可显著提升现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有实时超分辨率方法在压缩视频内容上表现不佳，且常用数据集无法准确反映流媒体视频特性，导致当前基准缺乏现实相关性。

Method: 构建了来自YouTube的真实流媒体超分辨率数据集StreamSR；提出EfRLFN模型，整合高效通道注意力机制和双曲正切激活函数，优化架构并设计复合损失函数；对11个SOTA模型进行基准测试，并在新数据集上微调以评估泛化能力。

Result: EfRLFN在视觉质量和运行效率上优于现有方法；在StreamSR上微调其他模型能带来显著且可泛化的性能提升。

Conclusion: 面向真实流媒体场景的数据集对超分辨率研究至关重要，所提EfRLFN模型有效平衡了效率与质量，且新数据集有助于提升模型在实际应用中的表现。

Abstract: Recent advancements in real-time super-resolution have enabled higher-quality video streaming, yet existing methods struggle with the unique challenges of compressed video content. Commonly used datasets do not accurately reflect the characteristics of streaming media, limiting the relevance of current benchmarks. To address this gap, we introduce a comprehensive dataset - StreamSR - sourced from YouTube, covering a wide range of video genres and resolutions representative of real-world streaming scenarios. We benchmark 11 state-of-the-art real-time super-resolution models to evaluate their performance for the streaming use-case.
  Furthermore, we propose EfRLFN, an efficient real-time model that integrates Efficient Channel Attention and a hyperbolic tangent activation function - a novel design choice in the context of real-time super-resolution. We extensively optimized the architecture to maximize efficiency and designed a composite loss function that improves training convergence. EfRLFN combines the strengths of existing architectures while improving both visual quality and runtime performance.
  Finally, we show that fine-tuning other models on our dataset results in significant performance gains that generalize well across various standard benchmarks. We made the dataset, the code, and the benchmark available at https://github.com/EvgeneyBogatyrev/EfRLFN.

Abstract (中文翻译): 近期实时超分辨率技术的进步使得高质量视频流媒体成为可能，但现有方法在处理压缩视频内容时仍面临独特挑战。常用数据集无法准确反映流媒体视频的特性，限制了当前基准的相关性。为填补这一空白，我们提出了一个全面的数据集——StreamSR，其源自YouTube，涵盖多种视频类型和分辨率，代表了真实世界的流媒体场景。我们对11种最先进的实时超分辨率模型进行了基准测试，以评估它们在流媒体应用场景中的性能。此外，我们提出了EfRLFN——一种高效的实时模型，它融合了高效通道注意力机制和双曲正切激活函数（这在实时超分辨率领域是一种新颖的设计）。我们对架构进行了大量优化以最大化效率，并设计了一种复合损失函数以改善训练收敛性。EfRLFN结合了现有架构的优点，在视觉质量和运行性能方面均有所提升。最后，我们证明在我们的数据集上微调其他模型可带来显著的性能提升，且这种提升在多个标准基准上具有良好的泛化能力。我们已在https://github.com/EvgeneyBogatyrev/EfRLFN公开了数据集、代码和基准。

</details>
