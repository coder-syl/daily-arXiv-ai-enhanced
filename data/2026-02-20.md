<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 本文提出一种基于高斯泼溅（Gaussian Splatting, GS）的数字孪生方法，用于高效、精确地实现土木基础设施三维损伤可视化。


<details>
  <summary>Details</summary>
Motivation: 传统基于二维图像的损伤识别难以满足对土木基础设施进行精准三维损伤可视化的需求；而现有三维重建方法中，GS在场景表示、渲染质量和处理无纹理区域方面优于传统摄影测量和NeRF，因此值得探索其在数字孪生中的应用。

Method: 1）利用GS进行三维重建，将二维损伤分割结果映射到三维空间并减少分割误差；2）设计多尺度重建策略，在效率与损伤细节之间取得平衡；3）支持随时间演化的损伤更新数字孪生模型。

Result: 在开源合成地震后检测数据集上的实验表明，该方法能有效实现全面的三维损伤可视化。

Conclusion: 所提出的GS驱动数字孪生方法为土木基础设施损伤的三维可视化提供了一种高效且有前景的技术路径。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

Abstract (中文翻译): 近期土木基础设施检测领域的进展凸显了在数字孪生中实现精确三维损伤可视化的重要性，超越了传统的基于二维图像的损伤识别方法。与传统的摄影测量三维重建技术相比，神经辐射场（NeRF）和高斯泼溅（GS）等现代方法在场景表示、渲染质量以及处理无特征区域方面表现更优。其中，GS因其高效性脱颖而出，它利用离散的各向异性三维高斯分布来表示辐射场，不同于NeRF的连续隐式模型。本研究提出了一种面向高效三维损伤可视化的GS驱动数字孪生方法。该方法的主要贡献包括：1）利用基于GS的三维重建将二维损伤分割结果可视化，并减少分割误差；2）开发多尺度重建策略，以平衡计算效率与损伤细节保留；3）支持随时间演化的损伤对数字孪生模型进行动态更新。在用于震后检测的开源合成数据集上的实验验证表明，所提出的方法为土木基础设施数字孪生中的全面三维损伤可视化提供了有前景的解决方案。

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: 本文提出了一个大规模多维视频质量评估数据集UltraVQA，并引入了一种新的后训练优化方法Analytic Score Optimization（ASO），以提升多维度视频质量评分的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统视频质量评估（VQA）主要依赖单一平均意见分数（MOS），难以全面反映用户生成内容（UGC）的多方面质量。为克服这一局限，作者旨在构建一个包含多维细粒度标注的大规模数据集，并开发一种能有效利用这些丰富标注的新方法，从而更准确、可解释地评估视频质量。

Method: 作者构建了UltraVQA数据集，其中每个视频在五个质量维度（运动质量、运动幅度、美学质量、内容质量和清晰度）上由多名人工评分员打分，并附有基于集体判断生成的GPT解释。在此基础上，提出Analytic Score Optimization（ASO）方法，将质量评估重构为一个正则化的决策过程，通过闭式解来捕捉人类评分的序数特性，确保与人类排序偏好对齐。

Result: 实验表明，所提ASO方法在质量预测任务中优于大多数基线模型（包括闭源API和开源模型），并显著降低了平均绝对误差（MAE）。

Conclusion: 该研究强调了多维、可解释标注以及基于强化对齐策略在推动视频质量评估领域发展中的重要性，为未来VQA研究提供了新的数据基础和方法论。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

Abstract (中文翻译): 视频质量评估（VQA）正从单一的平均意见分数（MOS）向对视频内容更丰富、多方面的评估演进。本文提出了一个大规模多维VQA数据集UltraVQA，该数据集涵盖了多样化的用户生成内容（UGC），并在五个关键质量维度上进行了标注：运动质量、运动幅度、美学质量、内容质量和清晰度。数据集中每个视频均由三名以上人工评分员在这些维度上进行打分，并配有细粒度的子属性标签，同时还附有基于集体人工判断由GPT生成的解释性理由。为了更好地利用这些丰富的标注信息并改进离散质量评分的评估，我们引入了分析性评分优化（Analytic Score Optimization, ASO），这是一种为多维VQA设计的、具有理论依据的后训练目标。通过将质量评估重新构架为一个正则化的决策过程，我们得到了一个闭式解，该解能够自然地捕捉人类评分的序数特性，确保与人类的排序偏好保持一致。实验结果表明，我们的方法在质量预测任务中优于包括闭源API和开源模型在内的大多数基线方法，同时降低了预测的平均绝对误差（MAE）。本工作突显了多维、可解释标注以及基于强化对齐策略在推进视频质量评估领域发展中的重要性。

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: 本文提出DODO，首个基于块离散扩散机制的视觉语言模型（VLM），用于光学字符识别（OCR），在保持接近SOTA准确率的同时，实现最高3倍于自回归模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有OCR方法多依赖自回归解码，处理长文档时计算开销大、速度慢；而尽管OCR任务具有高度确定性，适合并行解码，但现有掩码扩散模型因结构不稳定，无法满足OCR对精确匹配的严格要求。

Method: 提出DODO模型，采用块离散扩散机制，将文本生成分解为多个块，以缓解全局扩散中的同步错误，从而在保证准确性的同时实现高效并行解码。

Result: DODO在OCR任务上达到接近最先进水平的准确率，同时相比自回归基线模型，推理速度提升最高达3倍。

Conclusion: 块离散扩散机制能有效解决OCR中扩散模型的稳定性问题，在保持高精度的同时显著提升推理效率，为未来高效OCR系统提供新方向。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

Abstract (中文翻译): 光学字符识别（OCR）是信息数字化的一项基础任务，是连接视觉数据与文本理解的关键桥梁。尽管现代视觉语言模型（VLM）在此领域已取得高准确率，但它们主要依赖自回归解码方式，对于长文档而言，由于每个生成的词元都需要依次进行前向传播，导致计算成本高昂且速度缓慢。我们发现一个关键机遇可突破这一瓶颈：与开放生成任务不同，OCR是一项高度确定性的任务，其视觉输入严格决定了唯一的输出序列，理论上可通过扩散模型实现高效的并行解码。然而，我们指出，现有的掩码扩散模型未能发挥这一潜力；这些模型引入的结构性不稳定性在图像描述等灵活任务中影响较小，但在OCR这种要求严格精确匹配的任务中却是灾难性的。为弥合这一差距，我们提出了DODO——首个采用块离散扩散机制的视觉语言模型，成功释放了扩散模型在OCR中的加速潜力。通过将生成过程分解为多个块，DODO有效缓解了全局扩散中的同步错误。实验表明，该方法在达到接近最先进准确率的同时，推理速度相较自回归基线模型最高提升3倍。

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出StereoAdapter-2，通过引入基于选择性状态空间模型的ConvSS2D算子替代传统ConvGRU，实现单步高效长距离视差传播，并构建大规模合成水下双目数据集UW-StereoDepth-80K，在零样本设置下显著提升水下深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 水下立体深度估计受波长相关光衰减、散射和折射引起的严重域偏移影响；现有方法使用基于GRU的迭代优化策略，但其顺序门控和局部卷积核需多次迭代才能传播长距离视差信息，在大视差和无纹理区域表现受限。

Method: 提出StereoAdapter-2框架：1）用新型ConvSS2D算子（基于选择性状态空间模型）替代ConvGRU更新器，采用四向扫描策略对齐极线几何并捕获垂直结构一致性，以线性复杂度实现单步长距离空间传播；2）构建大规模合成水下双目数据集UW-StereoDepth-80K，通过两阶段生成流程结合语义感知风格迁移与几何一致的新视角合成；3）结合StereoAdapter中的动态LoRA自适应机制。

Result: 在零样本设置下，TartanAir-UW上提升17%，SQUID上提升7.2%，并在BlueROV2平台上验证了实际鲁棒性。

Conclusion: 所提方法有效解决了水下立体深度估计中的长距离视差传播难题，结合新数据集和自适应机制，在多个基准上达到SOTA性能，具备良好的实际应用潜力。

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

Abstract (中文翻译): 立体深度估计是水下机器人感知的基础，但却受到由波长相关光衰减、散射和折射引起的严重域偏移影响。近期方法利用单目基础模型结合基于GRU的迭代优化策略进行水下适应；然而，GRU中的顺序门控机制和局部卷积核需要多次迭代才能实现长距离视差传播，在大视差和无纹理的水下区域性能受限。本文提出StereoAdapter-2，用一种基于选择性状态空间模型的新型ConvSS2D算子替代传统的ConvGRU更新器。该算子采用四向扫描策略，自然地与极线几何对齐，同时捕捉垂直结构一致性，从而在单次更新步骤中以线性计算复杂度实现高效的长距离空间传播。此外，我们构建了UW-StereoDepth-80K——一个大规模合成水下立体数据集，通过结合语义感知风格迁移和几何一致的新视角合成的两阶段生成流程，涵盖多样化的基线、衰减系数和散射参数。结合StereoAdapter中继承的动态LoRA自适应机制，我们的框架在水下基准测试中实现了最先进的零样本性能，在TartanAir-UW上提升17%，在SQUID上提升7.2%，并在BlueROV2平台上的真实场景验证中展示了方法的鲁棒性。

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SemCovNet的新模型，用于解决视觉模型中语义覆盖不平衡（SCI）问题，通过引入语义描述符图、注意力调制模块和对齐损失，显著提升了模型的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在语义覆盖不平衡（SCI）问题，即语义表示呈长尾分布，导致模型在学习和推理稀有但有意义的语义时表现不佳。这种偏差不同于传统的类别不平衡，发生在语义层面，影响模型的公平性与泛化能力。

Method: 作者提出了SemCovNet模型，包含三个核心组件：语义描述符图（SDM）用于学习语义表示；描述符注意力调制模块（DAM）动态加权视觉与概念特征；描述符-视觉对齐损失（DVA）用于对齐视觉特征与语义描述。同时引入覆盖差异指数（CDI）来量化语义公平性。

Result: 在多个数据集上的实验表明，SemCovNet显著降低了CDI指标，提高了模型的可靠性和语义公平性。

Conclusion: 该工作首次将语义覆盖不平衡（SCI）定义为一种可度量且可纠正的偏差，为推动语义公平性和可解释视觉学习奠定了基础。

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

Abstract (中文翻译): 现代视觉模型越来越依赖于超越类别标签的丰富语义表示，包括描述性概念和上下文属性。然而，现有数据集中存在一种此前被忽视的偏差——语义覆盖不平衡（Semantic Coverage Imbalance, SCI），其源于语义表示的长尾分布。与类别不平衡不同，SCI发生在语义层面，影响模型对稀有但有意义语义的学习与推理能力。为缓解SCI问题，本文提出了一种新型模型——语义覆盖感知网络（Semantic Coverage-Aware Network, SemCovNet），该模型显式地学习以纠正语义覆盖差异。SemCovNet整合了用于学习语义表示的语义描述符图（Semantic Descriptor Map, SDM）、动态加权视觉与概念特征的描述符注意力调制模块（Descriptor Attention Modulation, DAM），以及用于对齐视觉特征与描述符语义的描述符-视觉对齐损失（Descriptor-Visual Alignment, DVA）。我们通过覆盖差异指数（Coverage Disparity Index, CDI）来量化语义公平性，该指标衡量覆盖度与误差之间的对齐程度。在多个数据集上的大量实验表明，SemCovNet显著提升了模型的可靠性，并大幅降低了CDI，实现了更公平、更均衡的性能表现。本研究将SCI确立为一种可度量且可纠正的偏差，为推进语义公平性与可解释视觉学习奠定了基础。

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual 是一种在超大规模社交媒体数据上训练的统一视觉模型，在图像和视频理解任务中达到 SOTA 性能，兼具高准确率与计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在处理大规模、多模态（图像与视频）社交媒体数据时面临语义多样性不足、标签噪声高以及计算效率低等问题，亟需一种可扩展、鲁棒且高效的统一架构。

Method: 提出三阶段训练流程：自监督 MAE、半监督 hashtag 分类和 CLIP 式对比学习；基于 Vision Transformer 并引入高效 token 重组（EViT）；使用 LLM 作为文本编码器（LLM2CLIP）提升跨模态能力；训练数据包括 150 亿图像-文本对和 100 亿视频-hashtag 对，并采用平衡与去噪策略进行数据清洗。

Result: 在 ImageNet、Kinetics、HMDB51 和 MSCOCO 等多个基准上达到 SOTA；模型对域偏移和对抗扰动具有强鲁棒性；LLM2CLIP 显著提升检索性能和泛化能力。

Conclusion: Xray-Visual 为可扩展的多模态视觉模型设立了新基准，在保持高精度的同时实现优异的计算效率，适用于真实世界的大规模应用场景。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

Abstract (中文翻译): 我们提出了 Xray-Visual，这是一种用于大规模图像和视频理解的统一视觉模型架构，基于工业级规模的社交媒体数据进行训练。我们的模型利用了来自 Facebook 和 Instagram 的超过 150 亿条经过筛选的图像-文本对以及 100 亿条视频-hashtag 对，并通过包含平衡策略和噪声抑制机制的稳健数据清洗流程，以最大化语义多样性并最小化标签噪声。我们引入了一个三阶段训练流程，结合自监督的 MAE、半监督的 hashtag 分类以及 CLIP 风格的对比学习，以联合优化图像和视频模态。该架构基于 Vision Transformer 主干网络，并引入高效 token 重组（EViT）以提升计算效率。大量实验表明，Xray-Visual 在多个多样化基准上均达到当前最优性能，包括用于图像分类的 ImageNet、用于视频理解的 Kinetics 和 HMDB51，以及用于跨模态检索的 MSCOCO。该模型对域偏移和对抗扰动表现出强大的鲁棒性。我们进一步证明，将大语言模型作为文本编码器（LLM2CLIP）可显著提升检索性能和泛化能力，尤其是在真实世界环境中。Xray-Visual 为可扩展的多模态视觉模型设立了新的基准，同时保持了卓越的准确率和计算效率。

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 本文提出HSI-SC-NeRF，一种基于固定相机的多通道神经辐射场框架，用于高通量的农产品高光谱三维重建，解决了传统方法在自动化表型系统中难以集成的问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）和3D重建技术对农业可持续发展和育种至关重要，但现有方法在大规模融合这两种模态时存在硬件复杂、与自动化系统不兼容、依赖移动相机等问题，限制了其在室内农业环境中的通量和可重复性。

Method: 作者开发了HSI-SC-NeRF框架：使用固定相机配合旋转样品台，在特制聚四氟乙烯成像腔内获取多视角高光谱数据；通过ArUco标记估计物体姿态并转换至相机坐标系；采用两阶段训练策略，先进行几何初始化再进行辐射度优化，并利用复合光谱损失联合优化所有高光谱波段的重建。

Result: 在三种农产品样本上的实验表明，该方法在可见光与近红外波段均实现了高空间重建精度和良好的光谱保真度。

Conclusion: HSI-SC-NeRF适用于集成到自动化农业工作流中，为高通量农产品产后检测提供了有效解决方案。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

Abstract (中文翻译): 高光谱成像（HSI）和三维重建技术的进步使得对农产品品质和植物表型进行准确、高通量的表征成为可能，这对推动农业可持续发展和育种计划至关重要。HSI能够捕捉农产品详细的生化特征，而三维几何数据则显著提升了形态学分析能力。然而，在大规模应用中融合这两种模态仍具挑战性，因为传统方法通常涉及复杂的硬件设置，难以与自动化表型系统兼容。近期神经辐射场（NeRF）的发展虽提供了计算高效的三维重建手段，但通常依赖移动相机设置，限制了其在标准室内农业环境中的通量和可重复性。为应对这些挑战，本文提出了HSI-SC-NeRF——一种面向农产品产后检测的固定相机多通道NeRF框架，用于高通量高光谱三维重建。该方法利用固定相机在特制的聚四氟乙烯成像腔内（提供漫射均匀照明）捕获旋转物体的多视角高光谱数据；通过ArUco标定标记估计物体姿态，并通过模拟姿态变换将其转换至相机参考坐标系，从而支持在固定相机数据上进行标准NeRF训练；同时采用多通道NeRF形式，结合复合光谱损失函数对所有高光谱波段进行联合优化，并辅以两阶段训练协议，将几何初始化与辐射度精炼解耦。在三种农产品样本上的实验表明，该方法在可见光与近红外光谱范围内均实现了高空间重建精度和优异的光谱保真度，验证了HSI-SC-NeRF适用于集成到自动化农业工作流程中。

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 提出动态分词策略，在推理时根据内容复杂度和去噪时间步动态调整图像/视频生成中的patch大小，显著提升速度而不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: DiT模型在图像和视频生成中计算开销大，主要源于固定patch大小的分词方式，无法适应不同去噪阶段对细节建模的需求。

Method: 在推理过程中，根据去噪时间步和内容复杂度动态调整patch大小：早期使用较大patch建模全局结构，后期使用较小patch细化局部细节。

Result: 在FLUX-1.Dev和Wan 2.1上分别实现最高3.52倍和3.2倍加速，同时保持生成质量和提示一致性。

Conclusion: 动态分词是一种高效且有效的推理优化策略，可在不牺牲生成质量的前提下大幅降低DiT的计算成本。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

Abstract (中文翻译): 扩散变换器（DiTs）在图像和视频生成任务中取得了最先进的性能，但其成功是以高昂的计算开销为代价的。这种低效性主要源于固定的分词过程——在整个去噪阶段始终使用固定大小的图像块（patch），而忽略了内容复杂度的变化。我们提出了一种动态分词策略，这是一种高效的测试时方法，可根据内容复杂度和去噪时间步动态调整图像块的大小。我们的核心洞察是：在去噪早期阶段，只需使用较粗（较大）的图像块来建模全局结构；而在后期阶段，则需要更细（更小）的图像块以精炼局部细节。在推理过程中，该方法在图像和视频生成的不同去噪步骤中动态重新分配图像块大小，在显著降低计算成本的同时，保持了感知层面的生成质量。大量实验验证了该方法的有效性：在FLUX-1.Dev和Wan 2.1模型上，分别实现了最高3.52倍和3.2倍的加速，且未牺牲生成质量与提示遵循能力。

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: PRIMO是一种监督式潜在变量填补模型，用于处理多模态学习中模态缺失的问题，通过建模缺失模态与观测模态之间的关系，在训练和推理阶段均能有效利用不完整数据，并量化每个样本中缺失模态对预测的影响。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多模态大语言模型（MLLMs）假设在训练和推理时所有模态都可用，但实际场景中模态常因缺失、异步采集或仅部分样本具备而变得不完整。因此，亟需一种能有效处理模态缺失并利用不完整数据的方法。

Method: 提出PRIMO模型，通过引入一个潜在变量来建模缺失模态与其观测模态在预测任务中的关系。在推理阶段，从学习到的缺失模态分布中采样多次，以获得边缘预测分布并分析每个样本中缺失模态对预测的影响。使用基于方差的指标量化模态在实例层面的预测影响。

Result: 在合成XOR数据集、Audio-Vision MNIST和MIMIC-III（用于死亡率和ICD-9预测）上的实验表明：当某一模态完全缺失时，PRIMO性能与单模态基线相当；当所有模态都可用时，其性能与多模态基线相当。此外，PRIMO能可视化不同缺失模态补全方式下产生的合理标签集合。

Conclusion: PRIMO提供了一种有效且可解释的多模态学习框架，能够在模态不完整的情况下充分利用所有训练样本，并在实例级别量化缺失模态对预测结果的影响，具有良好的鲁棒性和实用性。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

Abstract (中文翻译): 尽管多模态大语言模型（MLLMs）近期取得了显著成功，但现有方法通常假设在训练和推理过程中所有模态均可用。然而在实践中，多模态数据常常是不完整的，因为某些模态可能缺失、异步采集，或仅在部分样本中存在。本文提出PRIMO，一种监督式的潜在变量填补模型，用于量化多模态学习场景中任意缺失模态对预测的影响。PRIMO能够利用所有可用的训练样本，无论其模态是否完整。具体而言，该模型通过一个潜在变量对缺失模态进行建模，该变量捕捉了缺失模态与观测模态在预测任务中的关联。在推理阶段，我们从所学习的缺失模态分布中多次采样，既用于获得边缘预测分布（以进行预测），也用于分析每个样本中缺失模态对预测结果的影响。我们在合成XOR数据集、Audio-Vision MNIST以及MIMIC-III（用于死亡率和ICD-9预测）上评估了PRIMO。在所有数据集上，当某一模态完全缺失时，PRIMO的性能与单模态基线相当；当所有模态都可用时，其性能与多模态基线相当。PRIMO通过基于预测方差的指标，在实例级别量化模态的预测影响，并可视化展示了不同缺失模态补全方式所产生的合理标签集合。

</details>


### [10] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像块的框架，用于在人机协作绘画中进行空间作者归属，通过15幅抽象画作的法证案例研究，实现了88.8%的图像块级准确率，并利用条件香农熵量化了风格重叠，验证了模型能有效识别混合作者身份。


<details>
  <summary>Details</summary>
Motivation: 随着具身智能（agentic AI）越来越多地参与创意生产，明确记录作者身份对艺术家、收藏家和法律场景变得至关重要。然而，在人机协作创作中，作者归属存在固有模糊性，亟需有效方法进行量化和识别。

Method: 采用基于图像块（patch-based）的作者归属框架，利用普通平板扫描仪获取数据，并通过“留一画作交叉验证”（leave-one-painting-out cross-validation）评估模型性能；同时引入条件香农熵来衡量协作作品中的风格重叠程度，并与纹理特征和预训练特征基线方法进行对比。

Result: 该方法在图像块级别达到88.8%的准确率（按多数投票在画作级别为86.7%），优于纹理和预训练特征基线（68.0%-84.7%）；混合标注区域的条件香农熵比纯人类或纯机器人作品高64%（p=0.003），表明模型能识别混合作者身份而非分类失败。

Conclusion: 虽然所训练模型仅适用于特定的人机配对，但该方法为数据稀缺环境下的人机创意协作提供了样本高效的作者归属范式，未来有望推广至更广泛的人机协作绘画场景。

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

Abstract (中文翻译): 随着具身智能（agentic AI）越来越多地参与创意生产，明确记录作者身份对艺术家、收藏家和法律场景变得至关重要。我们提出了一种基于图像块的框架，用于在人机协作绘画实践中进行空间作者归属，并通过一位人类艺术家与一个机器人系统共同创作的15幅抽象画作的法证案例研究加以验证。该方法利用普通平板扫描仪采集数据，并采用“留一画作交叉验证”策略，在图像块级别达到了88.8%的准确率（通过多数投票在画作级别为86.7%），优于基于纹理和预训练特征的基线方法（68.0%–84.7%）。对于作者身份本就模糊的协作艺术品，我们使用条件香农熵来量化风格重叠程度；人工标注的混合区域比纯人类或纯机器人创作的作品表现出高64%的不确定性（p=0.003），表明模型识别的是混合作者身份，而非分类失败。尽管训练所得模型仅适用于该特定的人机组合，但该方法为人机协作创意流程中数据稀缺情况下的高效样本作者归属提供了方法论基础，未来有望扩展至任意人机协作绘画的作者归属任务。

</details>
