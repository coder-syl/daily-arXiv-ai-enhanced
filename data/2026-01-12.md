<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Bi-Orthogonal Factor Decomposition for Vision Transformers](https://arxiv.org/abs/2601.05328)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 本文提出双正交因子分解（BFD）方法，用于解析Vision Transformer中自注意力机制在位置与内容信息之间的交互方式，并揭示了DINOv2等模型在信息处理上的特性。


<details>
  <summary>Details</summary>
Motivation: 当前对Vision Transformer中自注意力机制的理解缺乏原则性，尤其是不清楚注意力在token之间交换的是位置信息、内容信息，还是两者兼有。

Method: 提出双正交因子分解（BFD）框架：第一阶段使用基于ANOVA的分解将token激活正交地解耦为位置和内容因子；第二阶段对查询-键交互矩阵QK^T进行奇异值分解（SVD），揭示这些因子如何通过双正交模态进行通信。

Result: 在主流视觉模型上应用BFD发现：(i) 注意力主要通过内容进行，内容-内容交互主导注意力能量，DINOv2比监督模型更强调内容-位置耦合；(ii) 注意力头表现出专业化分工，分为内容-内容、内容-位置和位置-位置操作器；(iii) DINOv2在中间层同时保留位置结构并丰富语义内容，从而实现更优的整体形状处理能力。

Conclusion: BFD方法有效揭示了Vision Transformer中token通过注意力交互时所依赖的信息因子（位置或语义），为理解其工作机制提供了实用洞见。

Abstract: Self-attention is the central computational primitive of Vision Transformers, yet we lack a principled understanding of what information attention mechanisms exchange between tokens. Attention maps describe where weight mass concentrates; they do not reveal whether queries and keys trade position, content, or both. We introduce Bi-orthogonal Factor Decomposition (BFD), a two-stage analytical framework: first, an ANOVA-based decomposition statistically disentangles token activations into orthogonal positional and content factors; second, SVD of the query-key interaction matrix QK^T exposes bi-orthogonal modes that reveal how these factors mediate communication. After validating proper isolation of position and content, we apply BFD to state-of-the-art vision models and uncover three phenomena.(i) Attention operates primarily through content. Content-content interactions dominate attention energy, followed by content-position coupling. DINOv2 allocates more energy to content-position than supervised models and distributes computation across a richer mode spectrum. (ii) Attention mechanisms exhibit specialization: heads differentiate into content-content, content-position, and position-position operators, while singular modes within heads show analogous specialization. (iii) DINOv2's superior holistic shape processing emerges from intermediate layers that simultaneously preserve positional structure while contextually enriching semantic content.
  Overall, BFD exposes how tokens interact through attention and which informational factors - positional or semantic - mediate their communication, yielding practical insights into vision transformer mechanisms.

Abstract (中文翻译): 自注意力是视觉Transformer的核心计算原语，但我们对其在token之间交换何种信息缺乏原则性的理解。注意力图描述了权重质量集中的位置，却未揭示查询与键之间交换的是位置、内容，还是两者兼有。我们提出了双正交因子分解（Bi-orthogonal Factor Decomposition, BFD），这是一个两阶段分析框架：首先，基于方差分析（ANOVA）的分解方法将token激活在统计上解耦为正交的位置因子和内容因子；其次，对查询-键交互矩阵QK^T进行奇异值分解（SVD），揭示出双正交模态，从而阐明这些因子如何介导信息传递。在验证了位置与内容因子的有效分离后，我们将BFD应用于最先进的视觉模型，并发现了三种现象：(i) 注意力主要通过内容运作，内容-内容交互主导注意力能量，其次是内容-位置耦合；DINOv2比监督模型分配更多能量于内容-位置交互，并在更丰富的模态谱上分布计算。(ii) 注意力机制表现出专业化：注意力头分化为内容-内容、内容-位置和位置-位置操作器，而每个头内部的奇异模态也呈现出类似的专业化。(iii) DINOv2卓越的整体形状处理能力源于其中间层能够同时保留位置结构并上下文地丰富语义内容。总体而言，BFD揭示了token如何通过注意力进行交互，以及哪些信息因子（位置或语义）介导了它们的通信，从而为视觉Transformer机制提供了实用的见解。

</details>


### [2] [Coding the Visual World: From Image to Simulation Using Vision Language Models](https://arxiv.org/abs/2601.05344)
*Sagi Eppel*

Main category: cs.CV

TL;DR: 本文提出Im2Sim方法，通过让视觉语言模型（VLM）根据真实图像生成模拟代码并重建图像，评估其对复杂系统的理解能力。结果表明，主流VLM（如GPT、Gemini）具备高层次的系统建模能力，但在细节和低层级图案复现方面表现有限。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLM）是否能够识别并模拟图像中所描绘的复杂现实系统，从而衡量其视觉理解能力，特别是构建代表性系统模型的能力。

Method: 采用Im2Sim方法：给定一张真实世界系统的自然图像（如城市、云、植被等），要求VLM描述该系统并编写可执行的模拟代码；运行代码生成合成图像，并与原始图像进行比较。

Result: 在多种复杂涌现系统（包括物理现象、植被、城市、材料和地质构造）上的实验显示，领先VLM能跨多个抽象层次和广泛领域理解并建模多组件系统，但难以复现图像中的精细细节和低层级图案排布。

Conclusion: VLM展现出高层次、深层次的视觉理解能力，但在感知图像细节方面存在局限，体现出“高理解、低感知”的不对称特性。

Abstract: The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) demonstrate the capacity to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.

Abstract (中文翻译): 构建世界的心理模型是理解的核心方面。类似地，视觉理解可被视为构建图像中所描绘系统之代表性模型的能力。本研究利用Im2Sim方法，探索视觉语言模型（VLM）识别并模拟图像中所呈现系统与机制的能力。具体而言，VLM接收一张真实世界系统（如城市、云层、植被等）的自然图像，并被要求描述该系统并编写可模拟并生成该系统的代码。随后执行该生成代码以产生合成图像，并与原始图像进行对比。该方法在多种复杂涌现系统上进行了测试，涵盖物理系统（如波浪、光线、云）、植被、城市、材料及地质构造等。通过对VLM生成的模型与图像进行分析，我们评估了其对图像中系统的理解程度。结果表明，当前领先的VLM（如GPT、Gemini）能够在多个抽象层次和广泛领域中理解并建模复杂的多组件系统，但在复现图像中的精细细节和低层级图案排布方面能力有限。这些发现揭示了一种有趣的不对称性：VLM结合了高层次、深层次的视觉理解能力，却对细节感知有限。

</details>


### [3] [STResNet & STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs](https://arxiv.org/abs/2601.05364)
*Sudhakar Sah,Ravish Kumar*

Main category: cs.CV

TL;DR: 本文提出了两个新的轻量级模型系列STResNet（用于图像分类）和STYOLO（用于目标检测），在保持低参数量和高效率的同时，在ImageNet和MS COCO上分别取得了优于MobileNetV1、ShuffleNetV2、YOLOv5n和YOLOX Nano的精度。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级神经网络通常以牺牲准确率为代价换取低延迟，限制了其在微控制器和神经处理单元等资源受限设备上的应用。因此，亟需同时兼顾精度、效率和内存占用的新型架构。

Method: 提出STResNet（从Nano到Tiny变体）和STYOLO（Micro与Milli版本）两个模型族，联合优化精度、效率和内存占用，并在ImageNet 1K和MS COCO数据集上进行评估。

Result: STResNetMilli仅用300万参数达到70.0% ImageNet Top-1准确率，优于MobileNetV1和ShuffleNetV2；STYOLOMicro和STYOLOMilli在MS COCO上分别达到30.5%和33.6% mAP，超越YOLOv5n和YOLOX Nano。

Conclusion: 所提出的STResNet和STYOLO模型族在资源受限平台上实现了精度、效率和内存占用的良好平衡，显著优于现有轻量级模型，适用于边缘设备部署。

Abstract: Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.

Abstract (中文翻译): 轻量级神经网络的最新进展显著提高了深度学习模型在边缘硬件上的部署效率。然而，大多数现有架构仍然以牺牲准确率为代价换取低延迟，这限制了它们在基于微控制器和神经处理单元设备上的适用性。在本研究中，我们提出了两个新的模型系列——用于图像分类的STResNet和用于目标检测的STYOLO，它们在资源受限平台上对准确率、效率和内存占用进行了联合优化。所提出的STResNet系列包括从Nano到Tiny等多个变体，在四百万参数预算内实现了具有竞争力的ImageNet 1K准确率。具体而言，STResNetMilli仅使用三百万参数就达到了70.0%的Top-1准确率，在相近计算复杂度下优于MobileNetV1和ShuffleNetV2。在目标检测方面，STYOLOMicro和STYOLOMilli在MS COCO数据集上分别达到了30.5%和33.6%的平均精度均值（mAP），在准确率和效率上均超越了YOLOv5n和YOLOX Nano。此外，当STResNetMilli作为骨干网络与Ultralytics训练环境结合使用时……

</details>


### [4] [MOSAIC-GS: Monocular Scene Reconstruction via Advanced Initialization for Complex Dynamic Environments](https://arxiv.org/abs/2601.05368)
*Svitlana Morkva,Maximum Wilder-Smith,Michael Oechsle,Alessio Tonioni,Marco Hutter,Vaishakh Patil*

Main category: cs.CV

TL;DR: MOSAIC-GS 是一种基于高斯泼溅的高效单目动态场景重建方法，通过融合多种几何线索和运动约束，在保证高质量重建的同时实现快速优化与实时渲染。


<details>
  <summary>Details</summary>
Motivation: 单目视频重建因缺乏多视角约束而本质不适定，难以准确恢复物体几何结构和时间一致性。

Method: 利用深度、光流、动态物体分割和点跟踪等多几何线索，结合刚性运动约束，在初始化阶段估计初步的三维场景动态；将场景分解为静态与动态部分，动态高斯基元使用时间相关的 Poly-Fourier 曲线表示轨迹以高效编码非刚性形变。

Result: 在标准单目动态场景基准上，MOSAIC-GS 相比现有方法实现了显著更快的优化与渲染速度，同时保持与最先进方法相当的重建质量。

Conclusion: MOSAIC-GS 有效解决了单目动态场景重建中的几何模糊性和时间一致性难题，在效率与质量之间取得了良好平衡。

Abstract: We present MOSAIC-GS, a novel, fully explicit, and computationally efficient approach for high-fidelity dynamic scene reconstruction from monocular videos using Gaussian Splatting. Monocular reconstruction is inherently ill-posed due to the lack of sufficient multiview constraints, making accurate recovery of object geometry and temporal coherence particularly challenging. To address this, we leverage multiple geometric cues, such as depth, optical flow, dynamic object segmentation, and point tracking. Combined with rigidity-based motion constraints, these cues allow us to estimate preliminary 3D scene dynamics during an initialization stage. Recovering scene dynamics prior to the photometric optimization reduces reliance on motion inference from visual appearance alone, which is often ambiguous in monocular settings. To enable compact representations, fast training, and real-time rendering while supporting non-rigid deformations, the scene is decomposed into static and dynamic components. Each Gaussian in the dynamic part of the scene is assigned a trajectory represented as time-dependent Poly-Fourier curve for parameter-efficient motion encoding. We demonstrate that MOSAIC-GS achieves substantially faster optimization and rendering compared to existing methods, while maintaining reconstruction quality on par with state-of-the-art approaches across standard monocular dynamic scene benchmarks.

Abstract (中文翻译): 我们提出了 MOSAIC-GS，这是一种新颖、完全显式且计算高效的高保真动态场景重建方法，利用高斯泼溅技术从单目视频中进行重建。由于缺乏足够的多视角约束，单目重建本质上是不适定问题，使得精确恢复物体几何结构和时间一致性尤为困难。为此，我们利用多种几何线索，如深度、光流、动态物体分割和点跟踪，并结合基于刚性的运动约束，在初始化阶段估计初步的三维场景动态。在进行光度优化之前恢复场景动态，可减少对仅依赖视觉外观进行运动推断的依赖，后者在单目设置中通常具有歧义性。为了实现紧凑表示、快速训练和实时渲染，同时支持非刚性形变，我们将场景分解为静态和动态组件。场景中每个动态高斯基元都被分配一条轨迹，该轨迹以时间相关的 Poly-Fourier 曲线表示，以实现参数高效的运动编码。我们在标准单目动态场景基准上验证了 MOSAIC-GS 相比现有方法在优化和渲染速度上显著提升，同时保持与最先进方法相当的重建质量。

</details>


### [5] [Ensemble of radiomics and ConvNeXt for breast cancer diagnosis](https://arxiv.org/abs/2601.05373)
*Jorge Alberto Garza-Abdala,Gerardo Alejandro Fumagal-González,Beatriz A. Bosques-Palomo,Mario Alexis Monsivais Molina,Daly Avedano,Servando Cardona-Huerta,José Gerardo Tamez-Pena*

Main category: cs.CV

TL;DR: 本文比较了放射组学、深度学习（ConvNeXtV1-small）及二者集成方法在乳腺癌筛查中的表现，发现集成方法在两个独立数据集上取得了最优AUC（0.87），优于单独使用深度学习（0.83）或放射组学（0.80）。


<details>
  <summary>Details</summary>
Motivation: 早期诊断乳腺癌对提高患者生存率至关重要，而放射组学和深度学习在辅助放射科医生进行早期检测方面展现出巨大潜力。因此，有必要系统评估并比较这些方法在真实筛查场景中的性能。

Method: 研究使用两个独立数据集（RSNA 2023挑战赛数据和墨西哥TecSalud队列），分别训练和验证ConvNeXtV1-small深度学习模型与基于TecSalud的放射组学模型，并采用一致的方法构建和校准集成模型。

Result: 集成方法在跨数据集验证中取得最高AUC（0.87），显著优于单独的深度学习模型（AUC=0.83）和放射组学模型（AUC=0.80）。

Conclusion: 结合深度学习与放射组学的集成方法能显著提升乳腺癌在乳腺X线摄影中的诊断性能。

Abstract: Early diagnosis of breast cancer is crucial for improving survival rates. Radiomics and deep learning (DL) have shown significant potential in assisting radiologists with early cancer detection. This paper aims to critically assess the performance of radiomics, DL, and ensemble techniques in detecting cancer from screening mammograms. Two independent datasets were used: the RSNA 2023 Breast Cancer Detection Challenge (11,913 patients) and a Mexican cohort from the TecSalud dataset (19,400 patients). The ConvNeXtV1-small DL model was trained on the RSNA dataset and validated on the TecSalud dataset, while radiomics models were developed using the TecSalud dataset and validated with a leave-one-year-out approach. The ensemble method consistently combined and calibrated predictions using the same methodology. Results showed that the ensemble approach achieved the highest area under the curve (AUC) of 0.87, compared to 0.83 for ConvNeXtV1-small and 0.80 for radiomics. In conclusion, ensemble methods combining DL and radiomics predictions significantly enhance breast cancer diagnosis from mammograms.

Abstract (中文翻译): 乳腺癌的早期诊断对于提高生存率至关重要。放射组学和深度学习（DL）在辅助放射科医生进行早期癌症检测方面已展现出显著潜力。本文旨在系统评估放射组学、深度学习以及集成技术在筛查性乳腺X线摄影中检测癌症的性能。研究使用了两个独立数据集：RSNA 2023乳腺癌检测挑战赛数据集（11,913名患者）和来自墨西哥TecSalud数据集的队列（19,400名患者）。ConvNeXtV1-small深度学习模型在RSNA数据集上训练，并在TecSalud数据集上验证；放射组学模型则基于TecSalud数据集开发，并采用“留一年交叉验证”方法进行验证。集成方法采用统一策略对两种模型的预测结果进行融合与校准。结果显示，集成方法取得了最高的曲线下面积（AUC）0.87，优于ConvNeXtV1-small的0.83和放射组学的0.80。综上所述，结合深度学习与放射组学预测的集成方法可显著提升乳腺X线摄影中乳腺癌的诊断效果。

</details>


### [6] [EdgeLDR: Quaternion Low-Displacement Rank Neural Networks for Edge-Efficient Deep Learning](https://arxiv.org/abs/2601.05379)
*Vladimir Frants,Sos Agaian,Karen Panetta*

Main category: cs.CV

TL;DR: 本文提出EdgeLDR，一种结合四元数通道混合与块循环结构的高效线性/卷积层框架，利用FFT加速计算，在保持精度的同时显著压缩模型参数。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署深度神经网络受限于密集线性算子的内存流量和计算开销。现有方法要么使用四元数提升参数效率但保留非结构化权重，要么使用实域结构化矩阵实现快速计算，但未结合两者优势。

Method: 提出EdgeLDR框架，将四元数通道混合与块循环（block-circulant）参数结构结合，并通过复伴随表示实现基于FFT的高效计算；提供参考实现并对比FFT与朴素空间域实现的性能。

Result: FFT实现相比朴素方法带来显著加速，且随块尺寸增大延迟保持稳定；在CIFAR-10/100、SVHN及高光谱图像数据集上，EdgeLDR在保持竞争力精度的同时实现显著参数压缩，并报告了CPU/GPU延迟。

Conclusion: EdgeLDR是一种实用且高效的框架，能有效压缩模型并在边缘设备上实现低延迟推理，兼顾精度与计算效率。

Abstract: Deploying deep neural networks on edge devices is often limited by the memory traffic and compute cost of dense linear operators. While quaternion neural networks improve parameter efficiency by coupling multiple channels through Hamilton products, they typically retain unstructured dense weights; conversely, structured matrices enable fast computation but are usually applied in the real domain. This paper introduces EdgeLDR, a practical framework for quaternion block-circulant linear and convolutional layers that combines quaternion channel mixing with block-circulant parameter structure and enables FFT-based evaluation through the complex adjoint representation. We present reference implementations of EdgeLDR layers and compare FFT-based computation against a naive spatial-domain realization of quaternion circulant products. FFT evaluation yields large empirical speedups over the naive implementation and keeps latency stable as block size increases, making larger compression factors computationally viable. We further integrate EdgeLDR layers into compact CNN and Transformer backbones and evaluate accuracy-compression trade-offs on 32x32 RGB classification (CIFAR-10/100, SVHN) and hyperspectral image classification (Houston 2013, Pavia University), reporting parameter counts and CPU/GPU latency. The results show that EdgeLDR layers provide significant compression with competitive accuracy.

Abstract (中文翻译): 在边缘设备上部署深度神经网络通常受限于密集线性算子所带来的内存流量和计算成本。尽管四元数神经网络通过哈密顿积将多个通道耦合，从而提高了参数效率，但它们通常仍保留非结构化的密集权重；另一方面，结构化矩阵虽能实现快速计算，但通常仅应用于实数域。本文提出了EdgeLDR——一种实用的四元数块循环线性和卷积层框架，该框架将四元数通道混合与块循环参数结构相结合，并通过复伴随表示实现基于快速傅里叶变换（FFT）的高效计算。我们提供了EdgeLDR层的参考实现，并将基于FFT的计算方式与四元数循环积的朴素空间域实现进行了对比。实验表明，FFT评估相比朴素实现带来了显著的经验加速，且随着块大小增加，延迟保持稳定，从而使更大压缩因子在计算上变得可行。我们进一步将EdgeLDR层集成到紧凑的CNN和Transformer骨干网络中，并在32x32 RGB图像分类（CIFAR-10/100、SVHN）和高光谱图像分类（Houston 2013、Pavia University）任务上评估了精度与压缩率之间的权衡，同时报告了参数量及CPU/GPU延迟。结果表明，EdgeLDR层在保持有竞争力的精度的同时，实现了显著的模型压缩。

</details>


### [7] [Sketch&Patch++: Efficient Structure-Aware 3D Gaussian Representation](https://arxiv.org/abs/2601.05394)
*Yuang Shi,Simone Gasparini,Géraldine Morin,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: 本文提出一种将3D高斯分为“草图高斯”（捕捉边缘等高频特征）和“色块高斯”（表示平滑低频区域）的混合表示方法，实现分层渐进式流式传输，并通过自适应聚类与优化策略，在保持模型紧凑的同时显著提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 受传统绘画中先勾勒轮廓再填充色彩的启发，作者观察到3D高斯在场景表示中也具有类似分工：部分高斯擅长表达高频边界信息，另一些则更适合描述低频平滑区域。现有方法未对此进行区分，限制了压缩效率与渐进式渲染能力。

Method: 提出一种层次化自适应分类框架，直接作用于3DGS表示，利用多准则密度聚类与质量驱动的自适应细化，将高斯分为Sketch Gaussians和Patch Gaussians两类，无需依赖外部3D线段先验。

Result: 在多种室内外场景上评估表明，相比均匀剪枝基线，该方法在相同模型大小下PSNR提升最高1.74 dB、SSIM提升6.7%、LPIPS改善41.4%；对于室内场景，仅用0.5%原始模型大小即可保持视觉质量。

Conclusion: 所提出的结构感知高斯表示有效支持高效存储、自适应流式传输和高质量渲染，适用于带宽受限网络与资源受限设备。

Abstract: We observe that Gaussians exhibit distinct roles and characteristics analogous to traditional artistic techniques -- like how artists first sketch outlines before filling in broader areas with color, some Gaussians capture high-frequency features such as edges and contours, while others represent broader, smoother regions analogous to brush strokes that add volume and depth. Based on this observation, we propose a hybrid representation that categorizes Gaussians into (i) Sketch Gaussians, which represent high-frequency, boundary-defining features, and (ii) Patch Gaussians, which cover low-frequency, smooth regions. This semantic separation naturally enables layered progressive streaming, where the compact Sketch Gaussians establish the structural skeleton before Patch Gaussians incrementally refine volumetric detail.
  In this work, we extend our previous method to arbitrary 3D scenes by proposing a novel hierarchical adaptive categorization framework that operates directly on the 3DGS representation. Our approach employs multi-criteria density-based clustering, combined with adaptive quality-driven refinement. This method eliminates dependency on external 3D line primitives while ensuring optimal parametric encoding effectiveness. Our comprehensive evaluation across diverse scenes, including both man-made and natural environments, demonstrates that our method achieves up to 1.74 dB improvement in PSNR, 6.7% in SSIM, and 41.4% in LPIPS at equivalent model sizes compared to uniform pruning baselines. For indoor scenes, our method can maintain visual quality with only 0.5\% of the original model size. This structure-aware representation enables efficient storage, adaptive streaming, and rendering of high-fidelity 3D content across bandwidth-constrained networks and resource-limited devices.

Abstract (中文翻译): 我们观察到高斯分布展现出与传统艺术技法相类似的独特角色和特性——正如艺术家先勾勒轮廓再以色彩填充大面积区域一样，某些高斯捕捉边缘和轮廓等高频特征，而另一些则表示更宽广、更平滑的区域，类似于用于增加体积感和深度的笔触。基于这一观察，我们提出一种混合表示方法，将高斯分为两类：(i) 草图高斯（Sketch Gaussians），用于表示高频、定义边界的特征；(ii) 色块高斯（Patch Gaussians），用于覆盖低频、平滑的区域。这种语义上的分离自然地支持分层渐进式流式传输：紧凑的草图高斯首先构建结构骨架，随后色块高斯逐步细化体积细节。  
在本工作中，我们通过提出一种新颖的层次化自适应分类框架，将先前方法扩展至任意3D场景，该框架直接作用于3DGS表示。我们的方法结合多准则密度聚类与自适应的质量驱动细化策略，既消除了对外部3D线段图元的依赖，又确保了参数编码的最优效率。在包括人造与自然环境在内的多样化场景上的全面评估表明，与均匀剪枝基线相比，本方法在相同模型大小下PSNR最高提升1.74 dB，SSIM提升6.7%，LPIPS改善41.4%。对于室内场景，仅需原始模型大小的0.5%即可维持良好的视觉质量。这种结构感知的表示方式能够高效地支持高保真3D内容在带宽受限网络和资源受限设备上的存储、自适应流式传输与渲染。

</details>


### [8] [Multi-task Cross-modal Learning for Chest X-ray Image Retrieval](https://arxiv.org/abs/2601.05399)
*Zhaohui Liang,Sivaramakrishnan Rajaraman,Niccolo Marini,Zhiyun Xue,Sameer Antani*

Main category: cs.CV

TL;DR: 本文提出一种多任务学习框架对BiomedCLIP进行微调，以提升胸部X光（CXR）图像与放射学报告之间的跨模态检索性能。


<details>
  <summary>Details</summary>
Motivation: CLIP和BiomedCLIP虽具备强大的跨模态嵌入能力，但在细粒度医学检索任务（如基于CXR图像查询相关放射学报告）中表现不足，因此需要针对性优化。

Method: 以BiomedCLIP为骨干网络，引入轻量级MLP投影头，并采用包含三项损失的复合多任务损失函数：(1) 二元交叉熵损失用于区分正常与异常CXR；(2) 有监督对比损失增强类内一致性；(3) CLIP损失保持跨模态对齐。

Result: 微调后的模型在图像到文本和文本到图像检索任务中均优于原始BiomedCLIP和通用CLIP模型，t-SNE可视化也显示正常与异常样本语义聚类更清晰。

Conclusion: 领域自适应的多任务学习能有效提升生物医学跨模态检索的临床相关性和诊断敏感性。

Abstract: CLIP and BiomedCLIP are examples of vision-language foundation models and offer strong cross-modal embeddings; however, they are not optimized for fine-grained medical retrieval tasks, such as retrieving clinically relevant radiology reports using chest X-ray (CXR) image queries. To address this shortcoming, we propose a multi-task learning framework to fine-tune BiomedCLIP and evaluate improvements to CXR image-text retrieval. Using BiomedCLIP as the backbone, we incorporate a lightweight MLP projector head trained with a multi-task composite loss function that includes: (1) a binary cross-entropy loss to distinguish normal from abnormal CXR studies, (2) a supervised contrastive loss to reinforce intra-class consistency, and (3) a CLIP loss to maintain cross-modal alignment. Experimental results demonstrate that the fine-tuned model achieves more balanced and clinically meaningful performance across both image-to-text and text-to-image retrieval tasks compared to the pretrained BiomedCLIP and general-purpose CLIP models. Furthermore, t-SNE visualizations reveal clearer semantic clustering of normal and abnormal cases, demonstrating the model's enhanced diagnostic sensitivity. These findings highlight the value of domain-adaptive, multi-task learning for advancing cross-modal retrieval in biomedical applications.

Abstract (中文翻译): CLIP 和 BiomedCLIP 是视觉-语言基础模型的代表，能够提供强大的跨模态嵌入能力；然而，它们并未针对细粒度医学检索任务（例如使用胸部X光（CXR）图像查询具有临床相关性的放射学报告）进行优化。为解决这一问题，我们提出一种多任务学习框架对 BiomedCLIP 进行微调，并评估其在 CXR 图像-文本检索任务中的改进效果。该方法以 BiomedCLIP 为骨干网络，引入一个轻量级 MLP 投影头，并采用包含三项损失的复合多任务损失函数：(1) 二元交叉熵损失，用于区分正常与异常 CXR 检查；(2) 有监督对比损失，以增强类内一致性；(3) CLIP 损失，以维持跨模态对齐。实验结果表明，与预训练的 BiomedCLIP 和通用 CLIP 模型相比，微调后的模型在图像到文本和文本到图像检索任务中均实现了更均衡且更具临床意义的性能。此外，t-SNE 可视化结果显示正常与异常病例的语义聚类更加清晰，体现出模型更强的诊断敏感性。这些发现凸显了领域自适应多任务学习在推动生物医学跨模态检索方面的价值。

</details>


### [9] [Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization](https://arxiv.org/abs/2601.05432)
*Yuxiang Ji,Yong Wang,Ziyu Ma,Yiming Hu,Hailang Huang,Xuecai Hu,Guanhua Chen,Liaoni Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出了一种结合地图推理的图像地理定位方法，通过赋予大视觉语言模型“地图思维”能力，并采用强化学习与并行测试时缩放策略，在新构建的真实世界基准MAPBench上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在图像地理定位任务中虽利用了世界知识和推理能力，但忽略了人类常用的策略——使用地图。因此，作者希望将地图整合进模型推理过程以提升性能。

Method: 作者为模型引入“地图思维”能力，将其建模为一个“地图中的智能体”循环，并采用两阶段优化方案：先进行智能体强化学习（RL）以提升采样效率，再通过并行测试时缩放（TTS）探索多条候选路径后再做最终预测。

Result: 在新提出的MAPBench真实图像基准上，该方法在多数指标上优于现有开源和闭源模型，尤其将Acc@500m从Gemini-3-Pro（启用Google搜索/地图模式）的8.0%提升至22.1%。

Conclusion: 结合地图推理与智能体机制能显著提升图像地理定位性能，所提方法和MAPBench基准为该领域提供了有效的新思路和评估工具。

Abstract: The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model \textit{Thinking with Map} ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\% to 22.1\% compared to \textit{Gemini-3-Pro} with Google Search/Map grounded mode.

Abstract (中文翻译): 图像地理定位任务旨在利用视觉线索预测图像在全球任意位置的拍摄地点。现有的大视觉语言模型（LVLM）方法虽然利用了世界知识、思维链推理和智能体能力，却忽视了人类常用的一种策略——使用地图。在本研究中，我们首先赋予模型“地图思维”能力，并将其形式化为一个“地图中的智能体”循环。我们为其设计了一个两阶段优化方案，包括智能体强化学习（RL）和并行测试时缩放（TTS）。强化学习增强了模型的智能体能力以提高采样效率，而并行TTS使模型在做出最终预测前能够探索多条候选路径，这对地理定位至关重要。为了在最新且真实场景的图像上评估我们的方法，我们进一步提出了MAPBench——一个完全由真实世界图像构成的综合性地理定位训练与评估基准。实验结果表明，我们的方法在大多数指标上优于现有的开源和闭源模型，特别是在Acc@500m指标上，相比启用了Google搜索/地图模式的Gemini-3-Pro，从8.0%提升至22.1%。

</details>


### [10] [TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection](https://arxiv.org/abs/2601.05446)
*Hongyang Xie,Hongyang He,Victor Sanchez*

Main category: cs.CV

TL;DR: 本文提出TAPM-Net，通过建模红外小目标在特征空间中引发的扰动轨迹，实现更优的小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测方法缺乏对目标在特征空间中引发的方向性、逐层扰动行为的建模机制，难以有效区分信号与结构化噪声。

Method: 提出TAPM-Net网络，包含扰动引导路径模块（PGM）和轨迹感知状态块（TASB）。PGM构建扰动能场并提取梯度跟随的特征轨迹；TASB基于Mamba状态空间模型，沿轨迹进行动态传播，并融合词级与句级语义特征，实现各向异性、上下文敏感的状态转移。

Result: 在NUAA-SIRST和IRSTD-1K数据集上达到红外小目标检测的最先进性能。

Conclusion: TAPM-Net通过显式建模目标扰动的空间扩散行为，在保持低计算成本的同时提升了检测精度，为红外小目标检测提供了新思路。

Abstract: Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.

Abstract (中文翻译): 红外小目标检测（ISTD）由于信号对比度弱、空间范围有限以及背景杂波干扰，长期以来是一项具有挑战性的任务。尽管卷积神经网络（CNN）和视觉Transformer（ViT）在性能上有所提升，但现有模型缺乏一种机制来追踪小目标如何在特征空间中引发方向性、逐层的扰动，而这一机制对于在红外场景中区分真实信号与结构化噪声至关重要。为解决这一局限性，我们提出了轨迹感知Mamba传播网络（TAPM-Net），该网络显式建模由目标引起的特征扰动的空间扩散行为。TAPM-Net包含两个新颖组件：扰动引导路径模块（PGM）和轨迹感知状态块（TASB）。PGM从多层级特征中构建扰动能场，并提取反映局部响应方向性的梯度跟随特征轨迹；这些轨迹随后输入到基于Mamba的状态空间单元TASB中，该单元沿每条轨迹建模动态传播过程，并结合速度约束的扩散机制以及来自词级和句级嵌入的语义对齐特征融合。与现有基于注意力的方法不同，TAPM-Net能够在保持全局一致性的同时，以较低的计算开销实现沿空间轨迹的各向异性、上下文敏感的状态转移。在NUAA-SIRST和IRSTD-1K数据集上的实验表明，TAPM-Net在红外小目标检测任务中达到了当前最先进的性能。

</details>
