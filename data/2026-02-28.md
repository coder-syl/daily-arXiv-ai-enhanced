<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

TL;DR: 本文提出在下游任务模型训练中引入新的鲁棒性损失，以减少病理基础模型对技术变异的敏感性，在不重新训练基础模型的前提下提升模型的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前组织病理学中的基础模型不仅学习了生物学相关的特征，还捕获了前处理和扫描仪特异性等技术变异，导致基于这些特征训练的下游任务模型存在偏差，影响其在真实临床场景中的泛化能力。

Method: 在下游任务特定模型的训练过程中引入新颖的鲁棒性损失函数，并利用包含6155名患者、27,042张全切片图像（WSI）的大规模实验设置，从8个流行的计算病理基础模型中提取特征，训练数千个模型进行评估。

Result: 所提方法显著提升了模型对技术变异的鲁棒性，同时通过聚焦于生物学相关特征，也提高了预测准确性。

Conclusion: 该方法无需重新训练基础模型，即可有效缓解计算病理基础模型的鲁棒性问题，推动在常规临床实践中部署可靠的计算病理模型。

Abstract: Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

Abstract (中文翻译): 组织病理学中的基础模型有望促进高性能且可泛化的深度学习系统的发展。然而，当前的模型不仅捕捉了生物学相关的特征，还包含了前处理和扫描仪特异性等技术变异，从而导致基于这些特征训练的下游任务模型产生偏差。本文提出在下游任务特定模型的训练过程中引入新颖的鲁棒性损失，以降低对技术变异的敏感性。我们设计了一个包含6155名患者、共计27,042张全切片图像（WSI）的综合性实验框架，从8个流行的计算病理基础模型中提取特征，训练了数千个模型。除了显著提升模型的鲁棒性外，我们还发现，通过聚焦于生物学相关特征，预测准确性也得到了提高。本方法在无需重新训练基础模型的前提下，成功缓解了计算病理基础模型的鲁棒性问题，为在常规临床实践中应用可靠的计算病理模型提供了可行路径。

</details>


### [2] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

TL;DR: MNAS-Unet is a new medical image segmentation framework that integrates Monte Carlo Tree Search (MCTS) with Neural Architecture Search (NAS), achieving higher accuracy, faster search (54% less budget), and a lightweight model (0.6M parameters).


<details>
  <summary>Details</summary>
Motivation: Existing NAS methods for medical image segmentation often suffer from high computational costs and suboptimal architecture exploration efficiency; the authors aim to improve both search efficiency and segmentation performance under practical resource constraints.

Method: The proposed MNAS-Unet uses Monte Carlo Tree Search (MCTS) to dynamically explore promising network architectures during NAS. It also introduces optimized DownSC and UpSC units for efficient model adjustment.

Result: MNAS-Unet achieves superior segmentation accuracy on PROMISE12, Ultrasound Nerve, and CHAOS datasets compared to NAS-Unet and other SOTA models. It reduces architecture search epochs from 300 to 139 (54% less), uses only 0.6M parameters, and consumes less GPU memory.

Conclusion: MNAS-Unet effectively balances search efficiency and segmentation accuracy, offering a practical and lightweight solution for medical image segmentation under limited computational resources.

Abstract: This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

Abstract (中文翻译): 本文提出了一种新颖的医学图像分割框架MNAS-Unet，该框架结合了蒙特卡洛树搜索（MCTS）与神经架构搜索（NAS）。MNAS-Unet通过MCTS动态探索有前景的网络架构，显著提升了架构搜索的效率和准确性。同时，该方法优化了DownSC和UpSC单元结构，实现了快速而精确的模型调整。实验结果表明，MNAS-Unet在PROMISE12、超声神经和CHAOS等多个医学图像数据集上的分割精度优于NAS-Unet及其他先进模型。此外，与NAS-Unet相比，在相同搜索设置下，MNAS-Unet将架构搜索预算减少了54%（提前在139个epoch停止，而非300个epoch），并构建了一个仅含0.6M参数的轻量级模型，同时降低了GPU内存消耗，进一步提升了其实用性。这些结果表明，MNAS-Unet在实际资源限制下，能够在保持竞争力的分割精度的同时显著提高搜索效率。

</details>


### [3] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

TL;DR: AeroDGS is a physics-guided 4D Gaussian splatting method for monocular UAV videos that addresses depth ambiguity and unstable motion estimation in dynamic aerial scenes by integrating geometric lifting and physical priors.


<details>
  <summary>Details</summary>
Motivation: Existing 4D reconstruction methods struggle with monocular aerial videos due to challenges like single-view capture, large spatial coverage, small object footprints, and significant motion disparities, leading to depth ambiguity and unreliable motion estimation.

Method: AeroDGS employs a Monocular Geometry Lifting module to reconstruct static and dynamic geometry from a single aerial sequence and a Physics-Guided Optimization module that enforces ground-support, upright-stability, and trajectory-smoothness priors to produce physically plausible motion.

Result: Experiments on both synthetic and real-world UAV datasets show that AeroDGS surpasses current state-of-the-art methods in reconstruction fidelity for dynamic aerial environments.

Conclusion: By combining geometric reasoning with physical constraints, AeroDGS effectively resolves the ill-posed nature of monocular dynamic aerial reconstruction and sets a new benchmark in this challenging domain.

Abstract: Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

Abstract (中文翻译): 近期在4D场景重建方面的进展显著提升了多个领域中的动态建模能力。然而，现有方法在航拍条件下仍存在局限性，尤其是在单视角拍摄、大空间范围、以及空间占比较小但运动差异较大的动态物体场景中。这些挑战导致严重的深度模糊和不稳定的运动估计，使得单目航拍重建本质上是一个病态问题。为此，我们提出了AeroDGS——一种面向单目无人机视频的物理引导4D高斯泼溅框架。AeroDGS引入了一个单目几何提升模块，可从单段航拍序列中重建可靠的静态与动态几何结构，为动态估计提供稳健基础。为进一步解决单目模糊问题，我们提出了一个物理引导优化模块，融合了可微分的地面支撑、直立稳定性及轨迹平滑性先验，将模糊的图像线索转化为物理一致的运动。该框架联合优化静态背景与动态实体，实现稳定几何结构与连贯时间演化的统一。此外，我们构建了一个涵盖多种飞行高度和运动条件的真实世界无人机数据集，用于评估动态航拍重建性能。在合成与真实无人机场景上的实验表明，AeroDGS优于当前最先进的方法，在动态航拍环境中实现了更优的重建保真度。

</details>


### [4] [Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention](https://arxiv.org/abs/2602.22381)
*Zhengkang Fan,Chengkun Sun,Russell Terry,Jie Xu,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了一种无需手动分割的深度学习框架，通过引入器官聚焦注意力（OFA）损失函数，在不依赖肿瘤区域分割的情况下实现肾肿瘤恶性程度预测，性能优于传统基于分割的方法。


<details>
  <summary>Details</summary>
Motivation: 现有影像学方法在术前预测肾肿瘤恶性程度时准确性不足，而依赖手动分割的传统深度学习方法虽能提升性能，但存在耗时、昂贵且依赖专家知识的问题。

Method: 开发了一种基于器官聚焦注意力（OFA）损失函数的深度学习框架，使图像块仅关注其他器官区域，从而在部署阶段无需对3D肾CT图像进行分割即可进行恶性预测。

Result: 在UF IDR私有数据集上达到AUC 0.685、F1-score 0.872；在KiTS21公开数据集上达到AUC 0.760、F1-score 0.852，优于依赖分割裁剪的传统模型。

Conclusion: 该方法在无需显式分割的情况下提升了恶性预测准确性，为肾癌诊断提供了一种更高效、可靠的临床决策支持工具。

Abstract: Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.

Abstract (中文翻译): 准确预测肾肿瘤的恶性程度对于指导临床决策和优化治疗策略至关重要。然而，现有的影像学方法在手术干预前尚无法可靠地预测恶性程度。尽管深度学习在利用3D CT图像进行恶性预测方面展现出潜力，但传统方法通常依赖人工分割来分离肿瘤区域并减少噪声，从而提升预测性能。然而，人工分割过程费时、成本高，且依赖专家知识。本研究开发了一种深度学习框架，采用器官聚焦注意力（Organ Focused Attention, OFA）损失函数，调整图像块的注意力机制，使其仅关注其他器官区域。因此，在部署阶段无需对3D肾CT图像进行分割即可实现恶性预测。所提框架在佛罗里达大学综合数据仓库（UF Integrated Data Repository, IDR）的私有数据集上取得了AUC 0.685和F1分数0.872，在公开可用的KiTS21数据集上取得了AUC 0.760和F1分数0.852。这些结果优于依赖基于分割裁剪以减少噪声的传统模型，表明该框架在无需显式分割输入的情况下仍能提升预测准确性。研究结果表明，该方法为肾癌恶性程度预测提供了一种更高效、可靠的手段，有助于提升肾癌诊断中的临床决策能力。

</details>


### [5] [Vision Transformers Need More Than Registers](https://arxiv.org/abs/2602.22394)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: ViTs often rely on background patches as shortcuts due to global attention and coarse supervision, causing artifacts; the paper proposes selective patch integration into the CLS token to mitigate this and boost performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Artifacts in Vision Transformers are prevalent but poorly understood; the authors aim to uncover their root cause and propose a remedy.

Method: The authors perform systematic analysis to identify lazy aggregation behavior and propose selectively integrating relevant patch features into the CLS token to suppress background-dominated shortcuts.

Result: The proposed method consistently improves performance across 12 benchmarks under label-, text-, and self-supervised settings.

Conclusion: The work reveals that ViT artifacts stem from lazy use of background patches and offers a new perspective on improving ViT representation by controlling feature aggregation.

Abstract: Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.

Abstract (中文翻译): 视觉Transformer（ViT）在大规模数据上预训练后，可为多种下游任务提供通用表征。然而，在不同监督范式和下游任务中，ViT中的伪影现象普遍存在。通过对ViT中伪影的系统分析，我们发现其根本机制尚未得到充分阐明。本文通过系统性分析得出结论：这些伪影源于一种“惰性聚合”行为——ViT在全局注意力机制和粗粒度语义监督的驱动下，利用语义无关的背景图像块作为捷径来表征全局语义。我们的解决方案选择性地将图像块特征整合到CLS token中，减少以背景为主导的捷径影响，并在标签监督、文本监督和自监督三种设定下的12个基准测试中持续提升性能。我们希望本研究能为理解ViT的行为提供新的视角。

</details>


### [6] [CLIP Is Shortsighted: Paying Attention Beyond the First Sentence](https://arxiv.org/abs/2602.22419)
*Marc-Antoine Lavoie,Anas Mahmoud,Aldo Zaimi,Arsene Fansi Tchango,Steven L. Waslander*

Main category: cs.CV

TL;DR: CLIP模型因训练数据中长文本描述常以简短摘要开头，导致注意力集中在开头而忽略后续细节。本文提出DeBias-CLIP，在训练时移除摘要句并采用句子子采样与文本填充策略，使监督信号均匀分布，从而提升长文本检索性能且不增加参数。


<details>
  <summary>Details</summary>
Motivation: CLIP在预训练中主要使用图像与短标题配对的数据，使其偏向于学习显著物体的简单描述，难以对复杂场景和密集描述进行细粒度对齐。虽有研究通过微调长标题数据缓解此问题，但发现这些长标题（无论人工或LLM生成）通常以一句总结开头，造成模型依赖该“捷径”，削弱对全文的对齐能力。

Method: 提出DeBias-CLIP方法：在训练过程中移除长标题中的首句摘要，并结合句子子采样（sentence sub-sampling）和文本token填充（text token padding），使对比学习的监督信号均匀覆盖所有文本位置，避免模型过度关注开头部分。

Result: DeBias-CLIP在长文本检索任务上达到SOTA，同时提升短文本检索性能，对句子顺序扰动更鲁棒，且可作为Long-CLIP的即插即用替代方案，无需额外可训练参数。

Conclusion: 通过消除长标题中首句摘要带来的训练偏差，DeBias-CLIP有效提升了CLIP模型对长文本描述的细粒度对齐能力，为多模态表示学习提供了一种简单高效的改进策略。

Abstract: CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.

Abstract (中文翻译): CLIP模型通过在互联网规模数据上进行图像-文本对比学习，学习到可迁移的多模态特征，广泛应用于零样本分类、多模态检索、文本到图像扩散模型以及作为大型视觉-语言模型中的图像编码器。然而，CLIP的预训练主要依赖于配有简短标题的图像，这使得模型倾向于编码显著物体的简单描述，导致在复杂场景和密集描述上的对齐较为粗糙。尽管近期工作通过在小规模长标题数据集上微调来缓解这一问题，但我们发现一个普遍存在的偏差：无论是人工撰写还是大语言模型生成的长标题，通常都以一句总结性语句开头，随后才是详细描述。我们证明这种结构在训练中形成了一种“捷径”，使模型注意力集中在开头句子和早期token上，削弱了对标题其余部分的对齐能力。为解决此问题，我们提出了DeBias-CLIP方法：在训练时移除摘要句，并采用句子子采样和文本token填充策略，将监督信号均匀分布到所有token位置。DeBias-CLIP在长文本检索任务上达到当前最优性能，同时提升了短文本检索效果，并对句子顺序的排列变化更加鲁棒。该方法可作为Long-CLIP的即插即用替代方案，且无需引入额外的可训练参数。

</details>


### [7] [SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read](https://arxiv.org/abs/2602.22426)
*Yibo Peng,Peng Xia,Ding Zhong,Kaide Zeng,Siwei Han,Yiyang Zhou,Jiaqi Liu,Ruiyi Zhang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文揭示了多模态大语言模型（MLLMs）存在“模态惰性”问题，即在图像中嵌入文本时，模型倾向于依赖提示中的文本捷径而非真正读取图像中的文字。为此，作者提出可视化问题（VQ）设置进行诊断，并设计了一种名为SimpleOCR的即插即用训练策略，通过将训练样本转换为VQ格式并引入随机样式，强制模型利用视觉文本提取能力。该方法在多个OOD基准上显著提升性能，且数据效率极高。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）发展迅速，但其视觉基础机制仍不明确：模型是否真正“阅读”图像中的嵌入文本，还是仅依赖文本提示中的参数化捷径？这一问题亟待解决，以确保模型具备真实的视觉理解能力。

Method: 作者提出Visualized-Question（VQ）设置，将文本查询直接渲染到图像上，强制模型进行视觉交互；并在此基础上提出SimpleOCR训练策略，通过将训练样本转换为VQ格式并加入随机样式，破坏文本捷径，迫使模型激活和优化其视觉文本提取路径。

Result: 在Qwen2.5-VL上的实验表明，尽管模型具备强大的OCR能力，在VQ设置下性能却下降高达12.7%。而采用SimpleOCR后，在四个OOD基准上平均提升5.4%，优于基于原始图像的GRPO方法2.7%；且仅需8.5K样本（比近期RL方法少30倍）即可实现更优性能，并可与NoisyRollout等高级RL策略无缝结合。

Conclusion: MLLMs普遍存在“模态惰性”问题，而SimpleOCR作为一种即插即用、高效且无需架构修改的训练策略，能有效缓解该问题，显著提升模型对图像中文本的真实理解能力，并具有良好的兼容性和数据效率。

Abstract: Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.

Abstract (中文翻译): 尽管多模态大语言模型（MLLMs）取得了快速进展，但关于其视觉基础机制的一个关键问题仍未得到解答：这些模型究竟是真正“阅读”图像中嵌入的文本，还是仅仅依赖文本提示中的参数化捷径？在本研究中，我们通过引入“可视化问题”（Visualized-Question, VQ）设置来诊断这一问题，即将文本查询直接渲染到图像上，从而在结构上强制模型进行视觉交互。我们在Qwen2.5-VL上的诊断实验揭示了一个惊人的能力-利用差距：尽管模型具备强大的OCR能力，在VQ设置下其性能却最多下降12.7%，暴露出深层次的“模态惰性”。为弥合这一差距，我们提出了SimpleOCR——一种即插即用的训练策略，通过对学习过程施加结构性约束来解决问题。该方法将训练样本转换为VQ格式并引入随机样式，有效消除基于文本的捷径，迫使模型激活并优化其视觉文本提取通路。实验证明，SimpleOCR在无需修改模型架构的情况下带来稳健的性能提升：在四个代表性OOD基准上，其性能超越基线模型5.4%，优于基于原始图像的GRPO方法2.7%；同时展现出极高的数据效率，仅用8.5K样本（比近期基于强化学习的方法少30倍）即可实现更优性能。此外，其即插即用特性使其能够与NoisyRollout等先进强化学习策略无缝集成，实现互补性提升。代码已开源：https://github.com/aiming-lab/SimpleOCR。

</details>


### [8] [Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge](https://arxiv.org/abs/2602.22455)
*Giuseppe Lando,Rosario Forte,Antonino Furnari*

Main category: cs.CV

TL;DR: 该论文研究在边缘设备上使用多模态大语言模型（MLLMs）实现实时在线情景记忆问答的可行性，提出双线程流水线架构，在消费级和企业级GPU上均取得接近云端方案的准确率，同时保障隐私与低延迟。


<details>
  <summary>Details</summary>
Motivation: 云端卸载虽常见，但存在隐私泄露和延迟问题，尤其对于可穿戴助手而言。因此，作者探索在边缘设备上部署MLLM以实现高效、隐私保护的情景记忆问答。

Method: 提出一个包含两个异步线程的问答流水线：Descriptor Thread持续将视频流转换为轻量级文本记忆；QA Thread基于该文本记忆进行推理回答问题，并在资源受限条件下评估MLLM性能。

Result: 在QAEgo4D-Closed基准测试中，消费级8GB GPU上达到51.76%准确率，TTFT为0.41秒；本地企业级服务器达54.40%准确率，TTFT为0.88秒；而云端方案为56.00%。边缘方案表现接近云端。

Conclusion: 边缘部署MLLM用于情景记忆问答具有实际可行性，在准确率、延迟和隐私保护方面展现出良好平衡，为可穿戴智能助理提供了一种有前景的解决方案。

Abstract: We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.

Abstract (中文翻译): 我们研究了使用多模态大语言模型（MLLMs）进行实时在线情景记忆问答的可行性。尽管云端卸载较为常见，但对于可穿戴助手而言，它会引发隐私和延迟问题，因此我们探索在边缘设备上的实现方式。我们在问答流水线中引入了流式处理约束，该流水线由两个异步线程组成：一个描述符线程（Descriptor Thread）持续将视频转换为轻量级文本记忆，另一个问答线程（QA Thread）则基于该文本记忆进行推理以回答查询。在QAEgo4D-Closed基准上的实验分析了在严格资源限制下MLLM的性能，结果表明其表现甚至可与云端方案相媲美。具体而言，在消费级8GB GPU上运行的端到端配置实现了51.76%的准确率，首字生成时间（TTFT）为0.41秒；扩展至本地企业级服务器后，准确率达到54.40%，TTFT为0.88秒；相比之下，云端方案的准确率为56.00%。这些具有竞争力的结果凸显了基于边缘的方案在隐私保护型情景记忆检索中的潜力。

</details>


### [9] [MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation](https://arxiv.org/abs/2602.22462)
*Raiyan Jahangir,Nafiz Imtiaz Khan,Amritanand Sudheerkumar,Vladimir Filkov*

Main category: cs.CV

TL;DR: 本文提出MammoWise，一个本地多模型管道，利用开源视觉语言模型（VLM）生成乳腺X线摄影报告并执行多任务分类，支持零样本、少样本和思维链提示，并可选结合多模态检索增强生成（RAG），在保持报告质量的同时提升BI-RADS、密度和关键发现的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线筛查工作量大、时效性强且文档繁重，放射科医生需将细微视觉发现转化为一致的BI-RADS评估、乳腺密度分类和结构化报告。现有基于VLM的图像到文本报告方法多依赖封闭云系统或紧耦合架构，限制了隐私性、可复现性和适应性。

Method: 构建名为MammoWise的本地多模型流水线，整合Ollama托管的开源VLM与乳腺X线数据集，支持零样本、少样本及思维链提示，并可选使用向量数据库实现多模态RAG以提供病例上下文。在VinDr-Mammo和DMID数据集上评估MedGemma、LLaVA-Med和Qwen2.5-VL模型，采用BERTScore和ROUGE-L评估报告质量，并测试BI-RADS分类、乳腺密度和关键发现识别性能。进一步对MedGemma进行参数高效微调（QLoRA）。

Result: 报告生成效果稳定，少样本提示和RAG可进一步提升质量；分类任务可行但受模型和数据集影响较大。经QLoRA微调后，MedGemma在BI-RADS、乳腺密度和钙化分类上的准确率分别达0.7545、0.8840和0.9341，同时保持良好报告质量。

Conclusion: MammoWise为在本地部署开源VLM用于乳腺X线报告提供了一个实用、可扩展且可复现的统一框架，兼顾隐私保护与临床实用性。

Abstract: Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

Abstract (中文翻译): 乳腺X线筛查具有高工作量、强时效性和繁重的文档要求。放射科医生必须将细微的视觉发现转化为一致的BI-RADS评估、乳腺密度分类和结构化叙述性报告。尽管近期的视觉语言模型（VLM）已实现图像到文本的报告生成，但许多方法依赖封闭的云系统或高度耦合的架构，限制了隐私性、可复现性和适应性。我们提出了MammoWise——一个本地多模型流水线，将开源VLM转化为乳腺X线报告生成器和多任务分类器。MammoWise支持任意由Ollama托管的VLM和乳腺X线数据集，并支持零样本、少样本和思维链提示，还可选结合基于向量数据库的多模态检索增强生成（RAG）以提供病例特定上下文。我们在VinDr-Mammo和DMID数据集上评估了MedGemma、LLaVA-Med和Qwen2.5-VL模型，评估指标包括报告质量（BERTScore、ROUGE-L）、BI-RADS分类、乳腺密度和关键发现识别。结果表明，报告生成效果始终稳健，且在少样本提示和RAG下进一步提升；分类任务可行但对模型和数据集选择敏感。通过对MedGemma进行参数高效微调（QLoRA），其可靠性显著提高，在BI-RADS、乳腺密度和钙化分类上的准确率分别达到0.7545、0.8840和0.9341，同时保持了良好的报告质量。MammoWise为在统一且可复现的工作流中本地部署VLM用于乳腺X线报告提供了一个实用且可扩展的框架。

</details>


### [10] [Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models](https://arxiv.org/abs/2602.22469)
*Niamul Hassan Samin,Md Arifur Rahman,Abdullah Ibne Hanif,Juena Ahmed Noshin,Md Ashikur Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的推理时干预方法——空间信用重分配（SCR），通过将早期Transformer层中集中在稀疏视觉块上的激活重新分配到其上下文，有效缓解视觉语言模型（VLMs）中的幻觉问题，在多个基准上显著降低幻觉率，同时几乎不影响生成质量，并具有较低的推理开销。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）常在输入图像中不存在的对象上产生幻觉。作者发现这一问题源于“空间信用坍缩”：在早期Transformer层中，激活信用过度集中在稀疏的视觉块上，抑制了上下文证据，导致模型更依赖语言先验。

Method: 提出Spatial Credit Redistribution (SCR)方法，在推理阶段无需训练地将高注意力源块的隐藏状态激活重新分配给其上下文，该过程由低熵输入引导，并依赖注意力机制选择源块。

Result: 在POPE和CHAIR基准上对六个模型家族（Chameleon、LLaVA、Qwen等）进行评估，SCR在POPE-Adversarial上降低幻觉4.7–6.0个百分点，在CHAIR-s和CHAIR-i上分别降低3.7–5.2和2.7–4.4个百分点（相对降幅达42–58%），CIDEr指标下降不超过0.8个百分点。推理延迟仅增加43–56毫秒，优于现有方法OPER A、VCD和OVCD。消融实验证明注意力引导的源块选择至关重要。

Conclusion: 空间信用坍缩是VLM幻觉的关键原因，而SCR通过简单有效的推理时干预显著缓解该问题，在幻觉控制与生成质量之间取得优异平衡，适用于实时应用。

Abstract: Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.

Abstract (中文翻译): 视觉语言模型（VLMs）经常对输入图像中不存在的对象产生幻觉。我们将这一失败归因于“空间信用坍缩”：在早期Transformer层中，激活信用集中在稀疏的视觉块上，从而抑制了上下文证据并增加了对语言先验的依赖。我们提出了“空间信用重分配”（SCR），这是一种无需训练的推理时干预方法，它根据低熵输入的引导，将高注意力源块的隐藏状态激活重新分配给其上下文。我们在POPE和CHAIR基准上评估了六个模型系列（包括Chameleon、LLaVA和Qwen，涵盖Qwen-VL和Qwen2-VL），模型规模涵盖7B、13B和30B。SCR在POPE-Adversarial上将幻觉率降低了约4.7–6.0个百分点，在CHAIR-s上降低了3.7–5.2个百分点（相对降低42–51%），在CHAIR-i上降低了2.7–4.4个百分点（相对降低44–58%），同时CIDEr指标的下降不超过0.8个百分点。改进效果在低熵输入上最为显著，与理论框架一致。SCR仅带来43–56毫秒的推理开销（小模型：+43–46毫秒；大模型：+54–56毫秒），约为OPER A和VCD方法的1/3–1/6，也比OVCD（+72毫秒）低1.3–1.7倍，同时在幻觉率和CIDEr指标上均优于这三种方法，使其适用于实时场景。受控消融实验进一步证实，基于注意力的源块选择至关重要：若替换为均匀随机选择，幻觉率的改善将从约4.7–6.0个百分点降至仅2.6–3.4个百分点，表明信用坍缩确实是关键驱动因素。

</details>
