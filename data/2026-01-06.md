<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements](https://arxiv.org/abs/2601.00812)
*Takashi Ushio,Kazuhiro Onishi,Hideyoshi Yanagisawa*

Main category: cs.CV

TL;DR: 该研究基于自由能原理，仅利用广告视频的场景级表达特征（无需生理信号或主观评分）量化了“愉悦度”、“惊讶”和“习惯化”三种情绪，并识别出三种典型情绪模式，在多个数据集和参数设置下验证了方法的稳健性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统情绪估计方法依赖外部信息（如生理信号或主观评分），缺乏可解释性；本研究旨在建立一种不依赖此类信息、基于自由能原理的情绪估计方法，以更好地理解广告视频对观众情绪的影响。

Method: 基于自由能（FE）原理，利用Kullback-Leibler散度（KLD）、贝叶斯惊讶（BS）和不确定性（UN）三个指标，从1,059个15秒食品广告视频的场景级表达特征中量化“愉悦度”、“惊讶”和“习惯化”情绪。

Result: 实验表明：KLD反映与品牌呈现相关的“愉悦度”，BS捕捉由信息复杂性引起的“惊讶”，UN反映由元素类型、空间布局不确定性及元素数量/变异性引发的“惊讶”；研究还识别出三种典型情绪模式，并在不同超参数和六类日本广告视频中验证了方法的稳健性和泛化性。

Conclusion: 该方法为可解释的情绪估计提供了新途径，未来可通过整合更多表达元素并结合主观评分进一步验证，有望支持更具吸引力的广告视频创作技术的发展。

Abstract: Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified "pleasantness," "surprise," and "habituation" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected "pleasantness" associated with brand presentation, BS has captured "surprise" arising from informational complexity, and UN has reflected "surprise" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.

Abstract (中文翻译): 在观看广告视频过程中的情绪反应被认为是理解媒体效果的关键因素，因为它们会影响注意力、记忆和购买意愿。为了建立一种无需依赖生理信号或主观评分等外部信息的可解释情绪估计方法论基础，本研究基于自由能（FE）原理，仅从广告视频的场景级表达特征出发，对“愉悦度”、“惊讶”和“习惯化”进行了量化。自由能原理为感知、学习和行为提供了一个统一的解释框架，其中Kullback-Leibler散度（KLD）用于捕捉预测误差，贝叶斯惊讶（BS）用于捕捉信念更新，不确定性（UN）则反映先验模糊性，三者共同构成了自由能的核心组成部分。通过对1,059个15秒食品广告视频的实验分析，结果表明：KLD反映了与品牌呈现相关的“愉悦度”，BS捕捉了由信息复杂性引发的“惊讶”，而UN则反映了由元素类型和空间布局的不确定性，以及所呈现元素的多样性和数量所驱动的“惊讶”。本研究还识别出三种典型的情绪模式，即不确定刺激型、持续高情绪型和瞬时峰值衰减型，证明了所提方法的有效性。在九种超参数设置下的稳健性测试以及六类日本广告视频（涵盖三种类型和两种时长）的泛化性验证均表明这些趋势具有稳定性。未来可通过整合更广泛的表达元素，并通过主观评分进一步验证该方法，最终为开发支持创作更具吸引力广告视频的技术提供指导。

</details>


### [2] [Can Generative Models Actually Forge Realistic Identity Documents?](https://arxiv.org/abs/2601.00829)
*Alexander Vinogradov*

Main category: cs.CV

TL;DR: 当前开源扩散模型虽能模仿证件外观，但无法达到法证级别的真实性，难以绕过人工或自动验证系统。


<details>
  <summary>Details</summary>
Motivation: 公众担忧生成式图像模型可能被滥用于伪造身份证件，因此需评估其实际风险。

Method: 评估多个公开可用的文本到图像和图像到图像生成模型（如Stable Diffusion、Qwen、Flux等）在伪造身份证件方面的表现。

Result: 这些模型可模拟证件表面外观，但无法复现结构与法证层面的真实性。

Conclusion: 当前生成模型在制作具有法证真实性的身份证件深度伪造方面的能力被高估，应加强机器学习从业者与证件鉴伪专家的合作以进行更现实的风险评估。

Abstract: Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.

Abstract (中文翻译): 生成式图像模型最近在图像逼真度方面取得了显著进展，引发了公众对其可能被滥用于文件伪造的担忧。本文探讨了当前开源且公开可用的基于扩散的生成模型是否能够生成足以绕过人工或自动化验证系统的身份证件伪造品。我们使用多种公开可用的生成模型系列（包括 Stable Diffusion、Qwen、Flux、Nano-Banana 等）评估了文本到图像和图像到图像的生成流程。研究结果表明，尽管当前的生成模型可以模拟证件的表面美学特征，但无法再现其结构和法证层面的真实性。因此，生成式身份证件深度伪造达到法证级真实性的风险可能被高估，这突显了机器学习从业者与证件鉴伪专家之间合作开展现实风险评估的重要性。

</details>


### [3] [Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs](https://arxiv.org/abs/2601.00837)
*Agniv Roy Choudhury*

Main category: cs.CV

TL;DR: 本研究比较了从头训练的CNN与基于迁移学习（ResNet50、DenseNet121、EfficientNet-B0）的模型在儿童肺炎X光诊断中的表现，发现微调后的ResNet50效果最佳，准确率达99.43%，表明迁移学习结合微调可作为资源匮乏地区有效的筛查工具。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎是五岁以下儿童死亡的主要原因，每年导致超70万例死亡。胸部X光诊断受限于放射科医生数量不足和判读差异，亟需可靠的自动化辅助诊断方法。

Method: 使用5,216张儿童胸片数据集（80/10/10划分训练/验证/测试），训练并评估七种模型（包括从头训练CNN和三种迁移学习架构的冻结骨干与微调版本），采用准确率、F1分数和AUC指标，并利用Grad-CAM提供可解释性分析。

Result: 微调版ResNet50表现最优：准确率99.43%、F1分数99.61%、AUC 99.93%，仅3例误判；微调策略平均比冻结骨干高5.5个百分点；Grad-CAM显示模型关注临床相关肺部区域。

Conclusion: 迁移学习结合微调显著优于从头训练的CNN，在儿童肺炎检测中达到近乎完美的准确率，具备在资源有限环境中作为筛查工具的潜力；未来需在多中心及成人数据集上进一步验证。

Abstract: Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.
  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.
  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.
  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\% accuracy, 99.61\% F1-score, and 99.93\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.
  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.
  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.

Abstract (中文翻译): 肺炎是五岁以下儿童死亡的主要原因，每年造成超过70万例死亡。胸部X光片的准确诊断受限于放射科医生的可获得性及其判读的变异性。本研究旨在比较从头训练的卷积神经网络（CNN）与采用迁移学习（ResNet50、DenseNet121、EfficientNet-B0）的模型在儿童肺炎检测中的性能，并评估冻结骨干网络与微调两种训练策略。研究使用包含5,216张儿童胸部X光片的数据集，按80/10/10比例划分为训练集、验证集和测试集。共训练并评估了七种模型，评价指标包括准确率、F1分数和AUC，并通过Grad-CAM可视化提供模型可解释性。结果显示，微调后的ResNet50表现最佳，准确率达99.43%，F1分数为99.61%，AUC达99.93%，仅出现3例误分类；微调策略平均比冻结骨干网络高出5.5个百分点；Grad-CAM确认模型预测依据的是临床上相关的肺部区域。结论指出，采用微调的迁移学习方法在儿童肺炎检测任务中显著优于从头训练的CNN，展现出接近完美的准确性，该系统在资源有限地区具有作为筛查工具的巨大潜力。未来工作应在多中心及成人数据集上进一步验证这些发现。

</details>


### [4] [Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS](https://arxiv.org/abs/2601.00839)
*Zahid Ullah,Muhammad Hilal,Eunsoo Lee,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: 本文在CAMUS数据集上对U-Net、Attention U-Net和TransUNet进行了标准化对比实验，强调了数据预处理（如NIfTI vs PNG）、自监督预训练和GPT辅助伪标签对分割性能的影响，并提供了可复现的基准与实用建议。


<details>
  <summary>Details</summary>
Motivation: 现有综述缺乏将心脏超声图像分割的深度学习进展与统一、可复现的实验基准相结合的工作。

Method: 在CAMUS数据集上，使用相同的训练划分、损失函数和评估标准，对比U-Net、Attention U-Net和TransUNet三种架构；探索多种预处理方式（原生NIfTI、16位PNG、GPT辅助伪标签）及自监督预训练（SSL）的影响。

Result: 原生NIfTI数据训练的U-Net达到94%平均Dice；PNG流程为91%；Attention U-Net在小或低对比区域略有改善；TransUNet结合SSL在困难帧上泛化能力最强；经置信度过滤的伪标签提升了模型鲁棒性。

Conclusion: 研究建立了标准化、可复现的分割基准，提供了超声数据预处理的最佳实践，并展望了自监督学习与GPT驱动的多模态标注流程在快速标注与数据集构建中的潜力。

Abstract: Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.

Abstract (中文翻译): 多篇综述论文总结了心脏影像与深度学习的进展，但很少有工作将这些综述与统一且可复现的实验基准联系起来。本研究结合了对心脏超声分割文献的聚焦综述，并在“用于多结构超声分割的心脏采集（CAMUS）”超声心动图数据集上，对三种具有影响力的架构——U-Net、Attention U-Net和TransUNet——进行了受控比较。我们的基准测试涵盖了多种预处理路径，包括原生NIfTI体数据、16位PNG导出、GPT辅助的多边形伪标签，以及在数千帧未标注动态图像上的自监督预训练（SSL）。在使用相同的训练划分、损失函数和评估标准下，直接在NIfTI数据（保留原始动态范围）上训练的普通U-Net达到了94%的平均Dice系数，而16位PNG流程在类似条件下达到了91%。Attention U-Net在小目标或低对比度区域提供了适度改进，减少了边界泄漏；而TransUNet凭借其建模全局空间上下文的能力，在困难帧上展现出最强的泛化性能，尤其是在使用SSL初始化时。经过置信度过滤的伪标签扩展了训练集并提升了模型鲁棒性。总体而言，我们的贡献有三方面：一是在标准化的CAMUS预处理与评估条件下，对U-Net、Attention U-Net和TransUNet进行了统一、公平的基准测试；二是就超声数据准备过程中如何保持强度保真度、分辨率一致性和对齐提供了实用指导；三是展望了可扩展的自监督方法以及新兴的多模态GPT驱动标注流程在快速标注、质量保证和针对性数据集构建方面的前景。

</details>


### [5] [Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge](https://arxiv.org/abs/2601.00854)
*Igor Lodin,Sergii Filatov,Vira Filatova,Dmytro Filatov*

Main category: cs.CV

TL;DR: 提出MCLSC方法，在资源受限的边缘设备上通过两个潜在语义画布（静态层和动态层）实现高效的视觉情境感知，利用运动补偿与异步触发分割显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现实时、高效的视觉情境感知，避免对每一帧都执行高成本的全景分割。

Method: 采用Motion-Compensated Latent Semantic Canvases（MCLSC）框架，维护两个潜在语义画布：缓慢累积的静态层和快速更新的动态层；基于视频流稳定后的基准坐标系，并仅在检测到运动时异步触发Mask2Former全景分割，同时通过运动补偿保持语义记忆的一致性。

Result: 在480p预录视频上，相比逐帧分割，原型系统将分割调用次数减少30倍以上，端到端平均处理时间降低20倍以上，同时保持静态/动态语义叠加的一致性。

Conclusion: MCLSC能显著降低边缘设备上的计算负载，同时维持高质量的语义情境感知能力，适用于资源受限场景。

Abstract: We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.

Abstract (中文翻译): 我们提出了运动补偿潜在语义画布（MCLSC），用于在资源受限的边缘设备上实现视觉情境感知。其核心思想是在一个由视频流稳定得到的基准坐标系中，维护两个潜在语义画布——一个缓慢累积的静态层和一个快速更新的动态层。高开销的全景分割模型（Mask2Former）以异步方式运行，并受运动触发：仅当检测到运动表明有新信息出现时才执行推理，而通过稳定化和运动补偿机制维持潜在语义记忆的一致坐标系统。在480p预录视频片段上的实验表明，与朴素的逐帧分割方法相比，我们的原型系统将分割调用次数减少了30倍以上，端到端平均处理时间降低了20倍以上，同时仍能保持连贯的静态/动态语义叠加效果。

</details>


### [6] [VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.00879)
*Zahid Ullah,Jihie Kim*

Main category: cs.CV

TL;DR: 提出VLOrdinalFormer，一种结合视觉与语言的序数学习框架，用于膝骨关节炎（KOA）自动分级，在KL1和KL2等早期阶段显著提升性能，并具备良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 膝骨关节炎（KOA）是全球致残主因之一，其严重程度常依据Kellgren-Lawrence（KL）分级系统评估。然而，KL1与KL2等早期阶段在X光片上差异细微，放射科医生间判读一致性差，亟需自动化、可靠且可解释的分级方法。

Method: 提出VLOrdinalFormer框架：以ViT-L/16为主干网络，结合CORAL序数回归方法，并引入CLIP驱动的语义对齐模块，将关节间隙狭窄、骨赘形成和软骨下硬化等临床文本概念融入模型；采用分层五折交叉验证、类别感知重加权及测试时增强配合全局阈值优化以提升鲁棒性。

Result: 在OAI kneeKL224数据集上，VLOrdinalFormer在宏F1分数和总体准确率上优于CNN和ViT基线模型，尤其在KL1和KL2分级上性能显著提升，同时未牺牲轻度或重度病例的分类精度；Grad-CAM与CLIP相似图显示模型关注区域具有临床相关性。

Conclusion: VLOrdinalFormer展示了视觉-语言对齐的序数Transformer在KOA自动分级和疾病进展评估中的潜力，可作为常规放射实践中可靠且可解释的辅助工具。

Abstract: Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we propose VLOrdinalFormer, a vision language guided ordinal learning framework for fully automated KOA grading from knee radiographs. The proposed method combines a ViT L16 backbone with CORAL based ordinal regression and a Contrastive Language Image Pretraining (CLIP) driven semantic alignment module, allowing the model to incorporate clinically meaningful textual concepts related to joint space narrowing, osteophyte formation, and subchondral sclerosis. To improve robustness and mitigate overfitting, we employ stratified five fold cross validation, class aware re weighting to emphasize challenging intermediate grades, and test time augmentation with global threshold optimization. Experiments conducted on the publicly available OAI kneeKL224 dataset demonstrate that VLOrdinalFormer achieves state of the art performance, outperforming CNN and ViT baselines in terms of macro F1 score and overall accuracy. Notably, the proposed framework yields substantial performance gains for KL1 and KL2 without compromising classification accuracy for mild or severe cases. In addition, interpretability analyses using Grad CAM and CLIP similarity maps confirm that the model consistently attends to clinically relevant anatomical regions. These results highlight the potential of vision language aligned ordinal transformers as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice.

Abstract (中文翻译): 膝骨关节炎（KOA）是全球致残的主要原因之一，使用Kellgren-Lawrence（KL）分级系统进行准确的严重程度评估对于临床决策至关重要。然而，早期病变阶段（尤其是KL1和KL2）在X光片上的影像学差异非常细微，常常导致放射科医生之间存在较大的判读差异。为应对这些挑战，我们提出了VLOrdinalFormer——一种基于视觉-语言引导的序数学习框架，用于从膝关节X光片中实现全自动KOA分级。该方法结合了ViT-L/16骨干网络、基于CORAL的序数回归以及由对比语言-图像预训练（CLIP）驱动的语义对齐模块，使模型能够整合与关节间隙狭窄、骨赘形成和软骨下硬化相关的具有临床意义的文本概念。为提高模型鲁棒性并缓解过拟合，我们采用了分层五折交叉验证、针对困难中间等级的类别感知重加权策略，以及结合全局阈值优化的测试时增强技术。在公开可用的OAI kneeKL224数据集上的实验表明，VLOrdinalFormer实现了当前最优性能，在宏F1分数和整体准确率方面均优于CNN和ViT基线模型。值得注意的是，所提框架在KL1和KL2分级上带来了显著的性能提升，同时并未降低对轻度或重度病例的分类准确性。此外，利用Grad-CAM和CLIP相似性图进行的可解释性分析证实，模型始终聚焦于具有临床相关性的解剖区域。这些结果凸显了视觉-语言对齐的序数Transformer作为可靠且可解释工具在常规放射实践中用于KOA分级和疾病进展评估的巨大潜力。

</details>


### [7] [VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition](https://arxiv.org/abs/2601.00887)
*Hongbo Jin,Kuanwei Lin,Wenhao Zhang,Yichen Jin,Ge Li*

Main category: cs.CV

TL;DR: 本文提出VideoCuRL框架，通过将视频理解任务的难度分解为视觉时间感知负荷和认知推理深度两个正交维度，设计了基于2D课程学习的强化学习方法，并引入动态稀疏KL散度与结构化回访机制，显著提升了VideoLLM在推理与感知任务上的性能，同时避免了生成式课程带来的高推理开销。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在视频大语言模型（VideoLLMs）训练中多采用随机打乱或基于标量难度指标的简单课程策略，难以有效区分视频理解中的两类正交挑战：视觉时间感知负荷与认知推理深度。

Method: 提出VideoCuRL框架，利用无需训练的代理指标（光流与关键帧熵衡量视觉复杂度，校准惊奇度衡量认知复杂度）将数据映射到二维课程网格，并采用能力感知的对角波前策略调度训练；同时引入动态稀疏KL散度和结构化回访机制以稳定训练过程。

Result: 在VSI-Bench上推理任务提升+2.5，在VideoMME上感知任务提升+2.9，显著优于现有RL基线；且消除了生成式课程带来的高昂推理开销。

Conclusion: VideoCuRL通过解耦视频理解中的两类难度维度并设计高效的课程学习策略，为视频大语言模型提供了一种可扩展、鲁棒的后训练方案。

Abstract: Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.

Abstract (中文翻译): 强化学习（RL）对于赋予视频大语言模型（VideoLLMs）复杂的时空推理能力至关重要。然而，当前的RL范式主要依赖于随机数据打乱或基于标量难度指标的简单课程策略。我们认为，标量指标无法解耦视频理解中的两类正交挑战：视觉时间感知负荷与认知推理深度。为此，我们提出了VideoCuRL，一种新颖的框架，将难度分解为这两个维度。我们采用高效且无需训练的代理指标——光流和关键帧熵用于衡量视觉复杂度，校准惊奇度（Calibrated Surprisal）用于衡量认知复杂度，从而将数据映射到一个二维课程网格上。随后，一种能力感知的对角波前（Diagonal Wavefront）策略从基础对齐逐步调度至复杂推理的训练过程。此外，我们引入了动态稀疏KL散度（Dynamic Sparse KL）和结构化回访（Structured Revisiting）机制，以防止训练过程中出现奖励崩溃和灾难性遗忘。大量实验表明，VideoCuRL在推理任务（VSI-Bench上+2.5）和感知任务（VideoMME上+2.9）上均显著超越了强大的RL基线方法。值得注意的是，VideoCuRL消除了基于生成式课程所带来的高昂推理开销，为鲁棒的视频后训练提供了一种可扩展的解决方案。

</details>


### [8] [Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study](https://arxiv.org/abs/2601.00888)
*Happy Gery Pangestu,Andi Prademon Yunus,Siti Khomsah*

Main category: cs.CV

TL;DR: 该研究系统比较了五种CNN主干网络在印尼蜡染风格迁移任务中的表现，发现ResNet在保持结构相似性的同时显著优于VGG模型的计算效率，适合资源受限环境下的实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于VGG的神经风格迁移方法虽具强风格表现力，但计算和内存开销大，难以在资源受限环境中部署，因此需探索更高效的替代架构。

Method: 对VGG16、VGG19、Inception V3、ResNet50和ResNet101五种CNN主干进行系统性对比，基于245组受控实验，结合定量指标（如SSIM、LPIPS、FLOPs）、定性评估与统计分析（ANOVA），考察结构保留、风格表现与计算效率之间的权衡。

Result: 主干网络选择在结构相似性上无显著差异（SSIM ANOVA p=0.83）；ResNet比VGG快5-6倍收敛，感知相似性相当（LPIPS≈0.53），且FLOPs减少超16倍（0.63 vs 10.12 GFLOPs）；定性上，VGG生成更密集的绘画纹理，ResNet更稳定保留几何结构与蜡染笔触，Inception V3表现居中但噪声较多。

Conclusion: 在神经风格迁移中，应将架构选择重点从追求最大风格强度转向兼顾效率与结构保留，ResNet主干是面向工业级可扩展蜡染生成的实用基础。

Abstract: Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.

Abstract (中文翻译): 神经风格迁移（NST）为印尼蜡染图案的数字保存与生成式探索提供了计算框架；然而，现有方法主要依赖VGG架构，其强大的风格表现力以高昂的计算和内存开销为代价，限制了在资源受限环境中的实际部署。本研究对五种广泛使用的CNN主干网络（VGG16、VGG19、Inception V3、ResNet50和ResNet101）进行了系统性对比分析，基于245组受控实验，结合定量指标、定性评估和统计分析，考察结构保留、风格表现与计算效率之间的权衡。结果表明，主干网络的选择在结构相似性方面未产生统计显著差异（SSIM的ANOVA检验p=0.83），说明各模型在结构保留能力上相当，但风格质量并不等同。在此背景下，基于ResNet的架构比VGG模型收敛速度快约5–6倍，同时保持相近的感知相似性（LPIPS = 0.53），且所需FLOPs减少超过16倍（0.63 vs 10.12 GFLOPs）。定性分析揭示了稳定的风格权衡：VGG生成更密集的绘画式纹理，ResNet倾向于几何稳定性并更好地保留蜡染笔触，风格化程度较温和，而Inception V3则表现出介于两者之间但噪声更多的行为。这些发现将NST中的架构选择重心从最大化风格强度转向注重效率与结构保留的实际部署，凸显了基于ResNet的主干网络作为可扩展、面向工业应用的蜡染生成实用基础的潜力。

</details>


### [9] [CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis](https://arxiv.org/abs/2601.00897)
*Sai Teja Erukude,Jane Mascarenhas,Lior Shamir*

Main category: cs.CV

TL;DR: 本文提出CornViT，一种三阶段卷积视觉Transformer（CvT）框架，用于自动化玉米籽粒分级。该方法在纯度、形态和胚朝向三个任务上显著优于ResNet和DenseNet，并配套发布数据集与Web应用。


<details>
  <summary>Details</summary>
Motivation: 当前玉米籽粒分级仍主要依赖人工检查，缺乏高效、自动化的解决方案。为提升种子认证、定向播种和育种中的分级准确性，亟需引入能模拟人类分析师分层推理能力的智能视觉系统。

Method: 提出CornViT三阶段CvT-13分类器：第一阶段区分纯与非纯籽粒；第二阶段对纯籽粒分为扁平与圆形；第三阶段判断纯扁平籽粒的胚朝向（上/下）。基于公开图像集构建并发布三个阶段专用数据集，并采用ImageNet-22k预训练CvT-13模型进行头部微调。

Result: 在测试集上，CornViT在纯度、形态和胚朝向任务分别达到93.76%、94.11%和91.12%准确率，显著优于ResNet-50（76.56–81.02%）和DenseNet-121（86.56–89.38%）。同时开发了基于Flask的Web应用以支持阶段化推理和可视化输出。

Conclusion: CornViT结合卷积增强的自注意力机制，在玉米籽粒质量评估任务中表现出优越性能，其框架、数据集和Web工具共同构成了一套可部署的自动化解决方案，有助于推动种子质量工作流程的智能化。

Abstract: Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.

Abstract (中文翻译): 准确的玉米籽粒分级对于种子认证、定向播种和育种至关重要，但目前仍主要依靠人工检查。本研究提出了CornViT——一种三阶段卷积视觉Transformer（CvT）框架，用于模拟人类种子分析师的分层推理过程，实现单粒籽粒评估。该框架包含三个依次运行的CvT-13分类器，处理384×384的RGB图像：第一阶段区分纯籽粒与非纯籽粒；第二阶段将纯籽粒分为扁平型和圆型；第三阶段判断纯扁平籽粒的胚朝向（向上或向下）。研究团队从一个公开的玉米种子图像集中手动重新标注并筛选图像，构建了三个阶段专用的数据集：纯度数据集含7265粒，形态数据集含3859粒纯籽粒，胚朝向数据集含1960粒纯扁平籽粒，所有数据均已作为基准发布。通过对ImageNet-22k预训练的CvT-13主干网络仅微调分类头，模型在测试集上分别取得了93.76%（纯度）、94.11%（形态）和91.12%（胚朝向）的准确率。在相同训练条件下，ResNet-50的准确率仅为76.56%至81.02%，DenseNet-121为86.56%至89.38%。结果表明，卷积增强的自注意力机制在籽粒分析中具有明显优势。为便于应用，作者还开发了一个基于Flask的Web应用程序，支持分阶段推理并通过浏览器界面提供可解释的输出。CornViT框架、精选数据集和Web应用共同构成了一个可在种子质量工作流程中部署的自动化玉米籽粒质量评估解决方案。源代码和数据均已公开。

</details>


### [10] [Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems](https://arxiv.org/abs/2601.00905)
*Eliot Park,Abhi Kumar,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 本文研究了最新视觉-语言模型（GPT-4o、GPT-4o-mini 和 Claude 3.5）在判断日常物品可回收性方面的应用，评估其在匹配物品与正确回收箱、考虑地域规则、污染/损坏及多材料组成等复杂场景下的表现，发现模型虽有显著进步但仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 公众普遍难以准确判断物品的可回收性及其正确处置方式，亟需智能工具辅助提升回收效率和环保实践。

Method: 利用精选的图像数据集，评估GPT-4o、GPT-4o-mini和Claude 3.5等视觉-语言模型在将物品匹配到合适回收箱的能力，包括判断物品是否能放入回收箱，并测试模型在三种挑战性场景下的表现：(i) 根据地区回收指南调整预测；(ii) 考虑污染或结构损坏；(iii) 处理多材料物品。

Result: 这些模型在上下文理解方面相比以往版本有显著提升，尤其在处理复杂回收情境时表现更佳，但在某些具体任务上仍存在不足。

Conclusion: 持续优化具备上下文感知能力的模型对于改善公众回收行为和推动环境可持续发展至关重要。

Abstract: While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.

Abstract (中文翻译): 尽管高效回收的重要性广受认可，但普通公众在准确判断物品的可回收性及其正确处置方式方面仍面临复杂挑战。本研究探索了前沿视觉-语言模型（GPT-4o、GPT-4o-mini 和 Claude 3.5）在预测常见废弃物品可回收性方面的应用。通过一个精心整理的图像数据集，我们评估了这些模型将物品匹配至适当回收箱的能力，包括判断物品是否能物理放入可用回收箱。此外，我们还考察了模型在若干具有挑战性的场景中的表现：(i) 根据特定地区的回收指南调整预测；(ii) 考虑物品的污染或结构损坏情况；(iii) 处理由多种材料组成的物品。研究结果表明，相较于早期版本，这些模型在上下文理解方面取得了显著进步，但也揭示了其尚存的不足之处。持续改进具备上下文感知能力的模型，对于提升公众回收实践和促进环境可持续发展至关重要。

</details>
