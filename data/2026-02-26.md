<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [StoryTailor:A Zero-Shot Pipeline for Action-Rich Multi-Subject Visual Narratives](https://arxiv.org/abs/2602.21273)
*Jinghao Hu,Yuhe Zhang,GuoHua Geng,Kang Li,Han Zhang*

Main category: cs.CV

TL;DR: StoryTailor is a zero-shot pipeline that generates temporally coherent, multi-frame visual narratives with preserved subject identity and background continuity using only a single RTX 4090 GPU.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to simultaneously maintain action faithfulness to text, subject identity consistency, and background continuity across frames without fine-tuning.

Method: StoryTailor employs three modules: Gaussian-Centered Attention (GCA) for subject focus and grounding-box handling, Action-Boost Singular Value Reweighting (AB-SVR) to enhance action-related text embeddings, and Selective Forgetting Cache (SFC) to manage background continuity by retaining useful cues and discarding irrelevant history.

Result: Experiments show CLIP-T scores improve by 10–15%, DreamSim scores are lower than strong baselines, and CLIP-I remains competitive; inference is faster than FluxKontext under matched settings, with qualitative improvements in scene stability and expressiveness.

Conclusion: StoryTailor effectively addresses the triad of challenges in zero-shot narrative image generation, achieving high-quality, coherent multi-frame outputs on consumer-grade hardware.

Abstract: Generating multi-frame, action-rich visual narratives without fine-tuning faces a threefold tension: action text faithfulness, subject identity fidelity, and cross-frame background continuity. We propose StoryTailor, a zero-shot pipeline that runs on a single RTX 4090 (24 GB) and produces temporally coherent, identity-preserving image sequences from a long narrative prompt, per-subject references, and grounding boxes. Three synergistic modules drive the system: Gaussian-Centered Attention (GCA) to dynamically focus on each subject core and ease grounding-box overlaps; Action-Boost Singular Value Reweighting (AB-SVR) to amplify action-related directions in the text embedding space; and Selective Forgetting Cache (SFC) that retains transferable background cues, forgets nonessential history, and selectively surfaces retained cues to build cross-scene semantic ties. Compared with baseline methods, experiments show that CLIP-T improves by up to 10-15%, with DreamSim lower than strong baselines, while CLIP-I stays in a visually acceptable, competitive range. With matched resolution and steps on a 24 GB GPU, inference is faster than FluxKontext. Qualitatively, StoryTailor delivers expressive interactions and evolving yet stable scenes.

Abstract (中文翻译): 在无需微调的情况下生成多帧、动作丰富的视觉叙事面临三重挑战：对动作文本的忠实度、主体身份的一致性，以及跨帧背景的连续性。我们提出了 StoryTailor，一种零样本流水线，仅需单块 RTX 4090（24 GB）显卡，即可根据长篇叙事提示、每个主体的参考图像和定位框，生成时间上连贯且身份保持一致的图像序列。该系统由三个协同模块驱动：高斯中心注意力（GCA）动态聚焦于每个主体核心并缓解定位框重叠；动作增强奇异值重加权（AB-SVR）在文本嵌入空间中放大与动作相关的信息方向；选择性遗忘缓存（SFC）保留可迁移的背景线索，遗忘非必要历史，并有选择地调用保留线索以构建跨场景的语义关联。与基线方法相比，实验表明 CLIP-T 指标提升高达 10–15%，DreamSim 表现低于强基线，而 CLIP-I 保持在视觉上可接受且具竞争力的水平。在相同分辨率和步数下，StoryTailor 在 24 GB GPU 上的推理速度优于 FluxKontext。定性来看，StoryTailor 能生成富有表现力的交互和既演变又稳定的场景。

</details>


### [2] [HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles](https://arxiv.org/abs/2602.21333)
*Yifan Wang,Francesco Pittaluga,Zaid Tasneem,Chenyu You,Manmohan Chandraker,Ziyu Jiang*

Main category: cs.CV

TL;DR: HorizonForge 是一个统一框架，通过可编辑的高斯点云与网格表示和噪声感知视频扩散渲染，实现高保真、可控的驾驶场景生成，并在新基准 HorizonSuite 上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时实现驾驶场景生成中的照片级真实感和精确控制，限制了自动驾驶仿真系统的可扩展性和实用性。

Method: 提出 HorizonForge 框架：1）将场景重建为可编辑的高斯点云（Gaussian Splats）和网格（Meshes）；2）支持细粒度3D操作和语言驱动的车辆插入；3）通过噪声感知视频扩散过程渲染编辑结果，确保时空一致性，无需逐轨迹优化；4）引入新基准 HorizonSuite 用于系统评估。

Result: 实验表明，高斯-网格表示比其他3D表示具有更高保真度，视频扩散中的时间先验对一致合成至关重要。HorizonForge 在用户偏好上提升83.4%，FID指标提升25.19%，显著优于当前最优方法。

Conclusion: HorizonForge 为可控、高保真的自动驾驶仿真提供了一个简单而强大的新范式，结合可编辑3D表示与时空一致的扩散渲染，在质量和可控性上取得突破。

Abstract: Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce HorizonForge, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose HorizonSuite, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these findings, HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method. Project page: https://horizonforge.github.io/ .

Abstract (中文翻译): 可控驾驶场景生成对于实现逼真且可扩展的自动驾驶仿真至关重要，但现有方法难以同时实现照片级真实感和精确控制。我们提出了 HorizonForge，这是一个统一框架，将场景重建为可编辑的高斯点云和网格，从而支持细粒度的三维操控以及基于语言指令的车辆插入。编辑结果通过一种噪声感知的视频扩散过程进行渲染，该过程强制保证空间和时间一致性，能够在单次前馈过程中生成多样化的场景变体，而无需针对每条轨迹进行优化。为了标准化评估，我们进一步提出了 HorizonSuite，这是一个涵盖自车和他车层面编辑任务（如轨迹修改和物体操控）的综合性基准。大量实验表明，高斯-网格表示相比其他三维表示能显著提升保真度，而视频扩散中的时间先验对于连贯合成至关重要。结合这些发现，HorizonForge 建立了一种简单而强大的范式，用于实现照片级真实感且可控的驾驶仿真，在用户偏好上比第二优的最先进方法提升了83.4%，FID指标提升了25.19%。项目页面：https://horizonforge.github.io/。

</details>


### [3] [Scaling View Synthesis Transformers](https://arxiv.org/abs/2602.21341)
*Evan Kim,Hyunwoo Ryu,Thomas W. Mitchel,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文系统研究了视图合成Transformer的缩放规律，提出了一种计算高效的编码器-解码器架构SVSM，在多个计算量级上优于纯解码器模型，并以更少训练计算量达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有几何无关的视图合成Transformer虽在新视角合成（NVS）任务中表现优异，但其随计算资源增加的缩放规律尚不明确，且此前对编码器-解码器架构的评估存在偏差。

Method: 作者系统分析了不同计算预算下的视图合成Transformer缩放行为，提出名为Scalable View Synthesis Model (SVSM)的编码器-解码器架构，并通过控制变量实验验证其计算效率与性能优势。

Result: SVSM在多个计算级别上与纯解码器模型具有相当的缩放效率，实现了更优的性能-计算帕累托前沿，并在真实世界NVS基准上以显著减少的训练计算量超越先前SOTA。

Conclusion: 编码器-解码器架构在合理设计下可成为计算最优的NVS模型，先前对其不利的结论源于次优架构选择和不公平的计算预算比较。

Abstract: Geometry-free view synthesis transformers have recently achieved state-of-the-art performance in Novel View Synthesis (NVS), outperforming traditional approaches that rely on explicit geometry modeling. Yet the factors governing their scaling with compute remain unclear. We present a systematic study of scaling laws for view synthesis transformers and derive design principles for training compute-optimal NVS models. Contrary to prior findings, we show that encoder-decoder architectures can be compute-optimal; we trace earlier negative results to suboptimal architectural choices and comparisons across unequal training compute budgets. Across several compute levels, we demonstrate that our encoder-decoder architecture, which we call the Scalable View Synthesis Model (SVSM), scales as effectively as decoder-only models, achieves a superior performance-compute Pareto frontier, and surpasses the previous state-of-the-art on real-world NVS benchmarks with substantially reduced training compute.

Abstract (中文翻译): 几何无关的视图合成Transformer最近在新视角合成（NVS）任务中取得了最先进的性能，超越了依赖显式几何建模的传统方法。然而，其性能随计算资源增加的缩放规律仍不清楚。我们对视图合成Transformer的缩放规律进行了系统性研究，并推导出训练计算最优NVS模型的设计原则。与以往的研究结论相反，我们证明编码器-解码器架构可以是计算最优的；我们将早期的负面结果归因于次优的架构选择以及在不等训练计算预算下的不公平比较。在多个计算量级上，我们提出的编码器-解码器架构（称为可扩展视图合成模型，SVSM）展现出与纯解码器模型相当的缩放效率，实现了更优的性能-计算帕累托前沿，并在真实世界NVS基准测试中以显著减少的训练计算量超越了先前的最先进方法。

</details>


### [4] [Towards Controllable Video Synthesis of Routine and Rare OR Events](https://arxiv.org/abs/2602.21365)
*Dominik Schneider,Lalithkumar Seenivasan,Sampath Rapuri,Vishalroshan Anil,Aiza Maksutova,Yiqing Shen,Jan Emily Mangulabnan,Hao Ding,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的手术室视频合成框架，通过几何抽象和条件控制生成罕见且安全关键事件的逼真视频，并利用合成数据训练AI模型以检测无菌区违规临近事件，验证了其在提升环境智能系统性能方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于伦理和操作上的困难，难以收集包含罕见、安全关键或非典型事件的大规模手术室工作流数据集，这限制了用于检测、理解和缓解此类事件的环境智能系统的发展。

Method: 提出一个手术室视频扩散框架，整合了几何抽象模块、条件控制模块和微调后的扩散模型：首先将手术室场景转换为抽象几何表示，再以此为条件引导合成过程，最终生成逼真的手术室事件视频；并基于此框架构建了一个合成数据集，用于训练和验证无菌区违规临近事件的检测模型。

Result: 在常规手术室事件视频合成任务中，该方法优于现成的视频扩散基线，在域内和域外数据集上均取得更低的FVD/LPIPS和更高的SSIM/PSNR；定性结果展示了其可控合成反事实事件的能力；基于合成数据训练的AI模型在检测临近安全关键事件时达到70.13%的召回率；消融实验量化了关键设计选择带来的性能提升。

Conclusion: 该方法能够从抽象几何表示中可控地合成常规和罕见的手术室事件视频，不仅成功生成了罕见且安全关键的场景，还展示了其在支持环境智能模型开发方面的应用潜力。

Abstract: Purpose: Curating large-scale datasets of operating room (OR) workflow, encompassing rare, safety-critical, or atypical events, remains operationally and ethically challenging. This data bottleneck complicates the development of ambient intelligence for detecting, understanding, and mitigating rare or safety-critical events in the OR.
  Methods: This work presents an OR video diffusion framework that enables controlled synthesis of rare and safety-critical events. The framework integrates a geometric abstraction module, a conditioning module, and a fine-tuned diffusion model to first transform OR scenes into abstract geometric representations, then condition the synthesis process, and finally generate realistic OR event videos. Using this framework, we also curate a synthetic dataset to train and validate AI models for detecting near-misses of sterile-field violations.
  Results: In synthesizing routine OR events, our method outperforms off-the-shelf video diffusion baselines, achieving lower FVD/LPIPS and higher SSIM/PSNR in both in- and out-of-domain datasets. Through qualitative results, we illustrate its ability for controlled video synthesis of counterfactual events. An AI model trained and validated on the generated synthetic data achieved a RECALL of 70.13% in detecting near safety-critical events. Finally, we conduct an ablation study to quantify performance gains from key design choices.
  Conclusion: Our solution enables controlled synthesis of routine and rare OR events from abstract geometric representations. Beyond demonstrating its capability to generate rare and safety-critical scenarios, we show its potential to support the development of ambient intelligence models.

Abstract (中文翻译): 目的：整理涵盖罕见、安全关键或非典型事件的大规模手术室（OR）工作流数据集，在操作和伦理层面仍具有挑战性。这一数据瓶颈使得开发用于检测、理解和缓解手术室中罕见或安全关键事件的环境智能系统变得复杂。  
方法：本研究提出了一种手术室视频扩散框架，可实现对罕见及安全关键事件的可控合成。该框架整合了几何抽象模块、条件控制模块和微调后的扩散模型，首先将手术室场景转化为抽象几何表示，然后以此作为条件引导合成过程，最终生成逼真的手术室事件视频。利用该框架，我们还构建了一个合成数据集，用于训练和验证检测无菌区违规临近事件的AI模型。  
结果：在合成常规手术室事件方面，我们的方法优于现成的视频扩散基线模型，在域内和域外数据集上均取得了更低的FVD/LPIPS值以及更高的SSIM/PSNR值。通过定性结果，我们展示了该方法在可控合成反事实事件方面的能力。在所生成的合成数据上训练和验证的AI模型，在检测临近安全关键事件时达到了70.13%的召回率。最后，我们进行了消融实验，以量化关键设计选择所带来的性能提升。  
结论：我们的解决方案能够从抽象几何表示中可控地合成常规和罕见的手术室事件。除了展示其生成罕见且安全关键场景的能力外，我们还证明了其在支持环境智能模型开发方面的潜力。

</details>


### [5] [Momentum Memory for Knowledge Distillation in Computational Pathology](https://arxiv.org/abs/2602.21395)
*Yongxin Guo,Hao Lu,Onur C. Koyun,Zhengjie Zhu,Muhammet Fatih Demir,Metin Nafi Gurcan*

Main category: cs.CV

TL;DR: 提出了一种名为MoMKD的新方法，通过动量更新的记忆机制和梯度解耦，在仅使用组织病理学图像进行推理的情况下，显著提升了跨模态知识蒸馏的性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习（整合基因组学与组织病理学）在癌症诊断中潜力巨大，但临床应用受限于配对数据稀缺；现有知识蒸馏方法因依赖批次内局部对齐而表现不稳定。

Method: 提出Momentum Memory Knowledge Distillation (MoMKD)，利用动量更新的记忆库跨批次聚合基因组与病理信息，并解耦基因组与病理分支的梯度，防止模态主导和模态差距问题。

Result: 在TCGA-BRCA基准（HER2、PR、ODX分类）及独立内部测试集上，MoMKD优于当前最先进的MIL和多模态KD方法，在仅用组织病理图像推理时表现出更强的性能与泛化能力。

Conclusion: MoMKD为计算病理学提供了一个稳健且可泛化的跨模态知识蒸馏范式，有助于推动多模态模型在临床中的实际应用。

Abstract: Multimodal learning that integrates genomics and histopathology has shown strong potential in cancer diagnosis, yet its clinical translation is hindered by the limited availability of paired histology-genomics data. Knowledge distillation (KD) offers a practical solution by transferring genomic supervision into histopathology models, enabling accurate inference using histology alone. However, existing KD methods rely on batch-local alignment, which introduces instability due to limited within-batch comparisons and ultimately degrades performance.
  To address these limitations, we propose Momentum Memory Knowledge Distillation (MoMKD), a cross-modal distillation framework driven by a momentum-updated memory. This memory aggregates genomic and histopathology information across batches, effectively enlarging the supervisory context available to each mini-batch. Furthermore, we decouple the gradients of the genomics and histology branches, preventing genomic signals from dominating histology feature learning during training and eliminating the modality-gap issue at inference time.
  Extensive experiments on the TCGA-BRCA benchmark (HER2, PR, and ODX classification tasks) and an independent in-house testing dataset demonstrate that MoMKD consistently outperforms state-of-the-art MIL and multimodal KD baselines, delivering strong performance and generalization under histology-only inference. Overall, MoMKD establishes a robust and generalizable knowledge distillation paradigm for computational pathology.

Abstract (中文翻译): 整合基因组学与组织病理学的多模态学习在癌症诊断中展现出强大潜力，但其临床转化受限于配对组织病理-基因组数据的稀缺性。知识蒸馏（KD）通过将基因组监督信号迁移到组织病理模型中，提供了一种实用解决方案，使得仅使用组织病理图像即可实现准确推断。然而，现有KD方法依赖于批次内的局部对齐，由于批次内样本对比有限，导致训练不稳定并最终损害性能。为解决这些问题，我们提出了动量记忆知识蒸馏（Momentum Memory Knowledge Distillation, MoMKD），这是一种基于动量更新记忆库的跨模态蒸馏框架。该记忆库存储并聚合跨批次的基因组与组织病理信息，有效扩大了每个小批次可用的监督上下文。此外，我们解耦了基因组分支与组织病理分支的梯度，防止训练过程中基因组信号主导组织病理特征的学习，并在推理阶段消除模态差距问题。在TCGA-BRCA基准（HER2、PR和ODX分类任务）以及一个独立的内部测试数据集上的大量实验表明，MoMKD始终优于当前最先进的MIL和多模态KD基线方法，在仅使用组织病理图像进行推理时展现出卓越的性能与泛化能力。总体而言，MoMKD为计算病理学建立了一个稳健且可泛化的知识蒸馏范式。

</details>


### [6] [MMLoP: Multi-Modal Low-Rank Prompting for Efficient Vision-Language Adaptation](https://arxiv.org/abs/2602.21397)
*Sajjad Ghiasvand,Haniyeh Ehsani Oskouie,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: MMLoP 是一种仅需 11.5K 可训练参数的多模态低秩提示方法，在保持高参数效率的同时，通过低秩分解、一致性损失、均匀漂移校正和共享上投影机制，在多个数据集上实现了优于大多数现有方法的准确率-效率平衡。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多模态提示方法虽然性能优越，但需要数百万可训练参数，丧失了提示调优原本具有的参数高效优势。因此，亟需一种既能实现深度多模态提示，又能保持极低参数量的方法。

Method: 提出 MMLoP 框架：1）在视觉和文本编码器的每一层使用低秩分解参数化提示；2）引入自调节一致性损失，将提示后的表示锚定到冻结的零样本 CLIP 特征；3）采用均匀漂移校正以消除提示调优引起的全局嵌入偏移；4）设计共享上投影机制，通过公共低秩因子耦合视觉与文本提示，促进跨模态对齐。

Result: 在三个基准和11个多样化数据集上的实验表明，MMLoP 以仅 11.5K 参数显著优于多数现有方法（包括参数量高出几个数量级的方法），并在基础到新类泛化任务中达到 79.70% 的调和平均准确率。

Conclusion: MMLoP 成功实现了深度多模态提示学习中的高准确性与高参数效率的统一，为资源受限场景下的视觉-语言模型适配提供了有效方案。

Abstract: Prompt learning has become a dominant paradigm for adapting vision-language models (VLMs) such as CLIP to downstream tasks without modifying pretrained weights. While extending prompts to both vision and text encoders across multiple transformer layers significantly boosts performance, it dramatically increases the number of trainable parameters, with state-of-the-art methods requiring millions of parameters and abandoning the parameter efficiency that makes prompt tuning attractive. In this work, we propose \textbf{MMLoP} (\textbf{M}ulti-\textbf{M}odal \textbf{Lo}w-Rank \textbf{P}rompting), a framework that achieves deep multi-modal prompting with only \textbf{11.5K trainable parameters}, comparable to early text-only methods like CoOp. MMLoP parameterizes vision and text prompts at each transformer layer through a low-rank factorization, which serves as an implicit regularizer against overfitting on few-shot training data. To further close the accuracy gap with state-of-the-art methods, we introduce three complementary components: a self-regulating consistency loss that anchors prompted representations to frozen zero-shot CLIP features at both the feature and logit levels, a uniform drift correction that removes the global embedding shift induced by prompt tuning to preserve class-discriminative structure, and a shared up-projection that couples vision and text prompts through a common low-rank factor to enforce cross-modal alignment. Extensive experiments across three benchmarks and 11 diverse datasets demonstrate that MMLoP achieves a highly favorable accuracy-efficiency tradeoff, outperforming the majority of existing methods including those with orders of magnitude more parameters, while achieving a harmonic mean of 79.70\% on base-to-novel generalization.

Abstract (中文翻译): 提示学习已成为适应如 CLIP 等视觉-语言模型（VLMs）到下游任务的主流范式，无需修改预训练权重。尽管将提示扩展到视觉和文本编码器的多个 Transformer 层能显著提升性能，但这会大幅增加可训练参数数量，当前最先进的方法需要数百万参数，放弃了使提示调优具有吸引力的参数效率。本文提出 MMLoP（多模态低秩提示），该框架仅需 11.5K 个可训练参数即可实现深度多模态提示，与早期仅文本的方法（如 CoOp）相当。MMLoP 通过对每个 Transformer 层的视觉和文本提示进行低秩分解来参数化，这种分解可作为针对少样本训练数据过拟合的隐式正则化器。为进一步缩小与最先进方法的精度差距，我们引入三个互补组件：一种自调节一致性损失，在特征和 logits 层面将提示后的表示锚定到冻结的零样本 CLIP 特征；一种均匀漂移校正，消除提示调优引起的全局嵌入偏移以保留类别判别结构；以及一种共享上投影，通过公共低秩因子耦合视觉和文本提示以强制跨模态对齐。在三个基准和11个多样化数据集上的大量实验表明，MMLoP 实现了极具优势的精度-效率权衡，优于大多数现有方法（包括参数量高出数个数量级的方法），并在基础到新类泛化任务中达到 79.70% 的调和平均准确率。

</details>


### [7] [FlowFixer: Towards Detail-Preserving Subject-Driven Generation](https://arxiv.org/abs/2602.21402)
*Jinyoung Jun,Won-Dong Jang,Wenbin Ouyang,Raghudeep Gadde,Jungbeom Lee*

Main category: cs.CV

TL;DR: FlowFixer 是一个用于主体驱动生成（SDG）的细化框架，通过图像到图像的直接转换恢复因尺度和视角变化而丢失的细节，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在主体驱动生成中，由于主体的尺度和视角变化，生成图像常会丢失精细细节；同时，基于语言提示的方法存在语义模糊问题。因此，需要一种更可靠、能保留细节的细化机制。

Method: FlowFixer 采用直接的图像到图像翻译方式，从视觉参考图出发，避免语言提示的歧义；引入一步去噪方案自动生成自监督训练数据，该数据在保留全局结构的同时去除高频细节，模拟真实 SDG 错误；并提出基于关键点匹配的评估指标以衡量细节保真度。

Result: 实验表明，FlowFixer 在定性和定量评估中均优于当前最先进的 SDG 方法，在高保真主体驱动生成任务中树立了新基准。

Conclusion: FlowFixer 有效解决了 SDG 中细节丢失的问题，通过图像级监督和新的评估指标显著提升了生成质量，为高保真图像生成提供了新思路。

Abstract: We present FlowFixer, a refinement framework for subject-driven generation (SDG) that restores fine details lost during generation caused by changes in scale and perspective of a subject. FlowFixer proposes direct image-to-image translation from visual references, avoiding ambiguities in language prompts. To enable image-to-image training, we introduce a one-step denoising scheme to generate self-supervised training data, which automatically removes high-frequency details while preserving global structure, effectively simulating real-world SDG errors. We further propose a keypoint matching-based metric to properly assess fidelity in details beyond semantic similarities usually measured by CLIP or DINO. Experimental results demonstrate that FlowFixer outperforms state-of-the-art SDG methods in both qualitative and quantitative evaluations, setting a new benchmark for high-fidelity subject-driven generation.

Abstract (中文翻译): 我们提出了 FlowFixer，这是一个用于主体驱动生成（SDG）的细化框架，旨在恢复因主体尺度和视角变化而在生成过程中丢失的精细细节。FlowFixer 提出直接从视觉参考图进行图像到图像的翻译，避免了语言提示所带来的语义歧义。为了实现图像到图像的训练，我们引入了一种一步去噪方案，用于生成自监督训练数据，该方案在自动去除高频细节的同时保留全局结构，有效模拟了现实世界中的 SDG 错误。我们还提出了一种基于关键点匹配的度量指标，以更准确地评估细节保真度，超越了通常由 CLIP 或 DINO 衡量的语义相似性。实验结果表明，FlowFixer 在定性和定量评估中均优于当前最先进的 SDG 方法，为高保真主体驱动生成设立了新的基准。

</details>


### [8] [Exploring Vision-Language Models for Open-Vocabulary Zero-Shot Action Segmentation](https://arxiv.org/abs/2602.21406)
*Asim Unmesh,Kaki Ramesh,Mayank Patel,Rahul Jain,Karthik Ramani*

Main category: cs.CV

TL;DR: 本文提出开放词汇零样本时序动作分割（OVTAS）任务，并设计了一种无需训练的基于视觉语言模型（VLM）的分割流程，在标准基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序动作分割方法受限于封闭词汇和固定标签集，而现实中动作种类繁多、标注成本高，难以构建全面数据集，因此需要探索开放词汇、零样本的解决方案。

Method: 提出一种无需训练的OVTAS流程，包括帧-动作嵌入相似性（FAES）用于匹配视频帧与候选动作标签，以及相似性矩阵时序分割（SMTS）用于增强时间一致性。

Result: 在标准基准上的实验表明，所提方法在无任务特定监督的情况下仍能取得优异性能；同时对14种不同VLM进行了系统评估，首次全面分析其在开放词汇动作分割中的适用性。

Conclusion: 视觉语言模型具备强大的零样本能力，可有效支持开放词汇的结构化时序理解，为未来时序动作分割研究提供新方向。

Abstract: Temporal Action Segmentation (TAS) requires dividing videos into action segments, yet the vast space of activities and alternative breakdowns makes collecting comprehensive datasets infeasible. Existing methods remain limited to closed vocabularies and fixed label sets. In this work, we explore the largely unexplored problem of Open-Vocabulary Zero-Shot Temporal Action Segmentation (OVTAS) by leveraging the strong zero-shot capabilities of Vision-Language Models (VLMs). We introduce a training-free pipeline that follows a segmentation-by-classification design: Frame-Action Embedding Similarity (FAES) matches video frames to candidate action labels, and Similarity-Matrix Temporal Segmentation (SMTS) enforces temporal consistency. Beyond proposing OVTAS, we present a systematic study across 14 diverse VLMs, providing the first broad analysis of their suitability for open-vocabulary action segmentation. Experiments on standard benchmarks show that OVTAS achieves strong results without task-specific supervision, underscoring the potential of VLMs for structured temporal understanding.

Abstract (中文翻译): 时序动作分割（TAS）要求将视频划分为若干动作片段，然而由于活动空间庞大且存在多种可能的划分方式，收集全面的数据集是不可行的。现有方法仍局限于封闭词汇表和固定的标签集合。在本研究中，我们通过利用视觉语言模型（VLM）强大的零样本能力，探索了开放词汇零样本时序动作分割（OVTAS）这一鲜有研究的问题。我们提出了一种无需训练的流程，采用“先分类后分割”的设计：帧-动作嵌入相似性（FAES）用于将视频帧与候选动作标签进行匹配，而相似性矩阵时序分割（SMTS）则用于增强时间一致性。除了提出OVTAS任务外，我们还对14种不同的VLM进行了系统性研究，首次全面分析了它们在开放词汇动作分割中的适用性。在标准基准上的实验表明，OVTAS在无需任务特定监督的情况下取得了优异的结果，凸显了VLM在结构化时序理解方面的潜力。

</details>


### [9] [WildSVG: Towards Reliable SVG Generation Under Real-Word Conditions](https://arxiv.org/abs/2602.21416)
*Marco Terral,Haotian Zhang,Tianyang Zhang,Meng Lin,Xiaoqing Xie,Haoran Dai,Darsh Kaushik,Pai Peng,Nicklas Scharpff,David Vazquez,Joan Rodriguez*

Main category: cs.CV

TL;DR: 本文提出SVG提取任务，旨在从自然图像中提取可缩放矢量图形（SVG），并发布WildSVG基准（包含Natural和Synthetic两个子集）以评估现有模型在真实场景中的表现，发现当前方法仍远未达标，但迭代优化策略显示出潜力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在干净渲染图或文本描述生成SVG方面表现良好，但在真实自然图像中因噪声、杂乱和域偏移而效果不佳；且缺乏合适的评测基准来推动该方向的研究。

Method: 构建WildSVG基准，包括Natural WildSVG（真实图像与对应SVG标注）和Synthetic WildSVG（将复杂SVG渲染嵌入真实场景以模拟困难条件）；并在该基准上评测当前最先进的多模态模型。

Result: 现有模型在真实场景下的SVG提取性能远未达到实用水平，但采用迭代优化的方法展现出改进潜力。

Conclusion: WildSVG是首个面向SVG提取任务的系统性评测基准，揭示了当前方法的不足，并指出迭代精炼等方向可能推动未来进展。

Abstract: We introduce the task of SVG extraction, which consists in translating specific visual inputs from an image into scalable vector graphics. Existing multimodal models achieve strong results when generating SVGs from clean renderings or textual descriptions, but they fall short in real-world scenarios where natural images introduce noise, clutter, and domain shifts. A central challenge in this direction is the lack of suitable benchmarks. To address this need, we introduce the WildSVG Benchmark, formed by two complementary datasets: Natural WildSVG, built from real images containing company logos paired with their SVG annotations, and Synthetic WildSVG, which blends complex SVG renderings into real scenes to simulate difficult conditions. Together, these resources provide the first foundation for systematic benchmarking SVG extraction. We benchmark state-of-the-art multimodal models and find that current approaches perform well below what is needed for reliable SVG extraction in real scenarios. Nonetheless, iterative refinement methods point to a promising path forward, and model capabilities are steadily improving

Abstract (中文翻译): 我们提出了SVG提取任务，即从图像中将特定视觉内容转换为可缩放矢量图形（SVG）。现有的一些多模态模型在从干净渲染图或文本描述生成SVG方面取得了良好效果，但在真实世界场景中，由于自然图像存在噪声、杂乱背景和域偏移等问题，其表现明显不足。该方向的一个核心挑战在于缺乏合适的评测基准。为此，我们提出了WildSVG基准，它由两个互补的数据集组成：Natural WildSVG，由包含公司Logo的真实图像及其对应的SVG标注构成；以及Synthetic WildSVG，通过将复杂的SVG渲染图融合到真实场景中，以模拟更具挑战性的条件。这两个数据集共同构成了首个用于系统性评测SVG提取任务的基础资源。我们在该基准上对当前最先进的多模态模型进行了评估，发现它们在真实场景中的表现距离可靠应用仍有较大差距。尽管如此，迭代优化方法显示出良好的前景，模型能力也在稳步提升。

</details>


### [10] [ECHOSAT: Estimating Canopy Height Over Space And Time](https://arxiv.org/abs/2602.21421)
*Jan Pauls,Karsten Schrödter,Sven Ligensa,Martin Schwartz,Berkant Turan,Max Zimmer,Sassan Saatchi,Sebastian Pokutta,Philippe Ciais,Fabian Gieseke*

Main category: cs.CV

TL;DR: ECHOSAT 是一个基于多源卫星数据和视觉Transformer模型构建的全球10米分辨率、多年份一致的树木高度动态地图，能准确捕捉森林生长与扰动事件。


<details>
  <summary>Details</summary>
Motivation: 现有全球树高图仅提供静态快照，无法反映森林随时间变化的动态特征，而这对精准碳核算至关重要。

Method: 利用多传感器卫星数据训练专用视觉Transformer模型，进行像素级时间序列回归，并引入自监督“生长损失”约束预测结果符合自然树木生长规律（如逐年增高或因火灾等扰动导致骤降）。

Result: 该模型在单年预测任务中优于现有最先进方法，并首次提供了可量化全球尺度树木生长与扰动的多年份高度图。

Conclusion: ECHOSAT有望推动全球碳监测与森林扰动评估工作，相关数据已公开发布。

Abstract: Forest monitoring is critical for climate change mitigation. However, existing global tree height maps provide only static snapshots and do not capture temporal forest dynamics, which are essential for accurate carbon accounting. We introduce ECHOSAT, a global and temporally consistent tree height map at 10 m resolution spanning multiple years. To this end, we resort to multi-sensor satellite data to train a specialized vision transformer model, which performs pixel-level temporal regression. A self-supervised growth loss regularizes the predictions to follow growth curves that are in line with natural tree development, including gradual height increases over time, but also abrupt declines due to forest loss events such as fires. Our experimental evaluation shows that our model improves state-of-the-art accuracies in the context of single-year predictions. We also provide the first global-scale height map that accurately quantifies tree growth and disturbances over time. We expect ECHOSAT to advance global efforts in carbon monitoring and disturbance assessment. The maps can be accessed at https://github.com/ai4forest/echosat.

Abstract (中文翻译): 森林监测对减缓气候变化至关重要。然而，现有的全球树高图仅提供静态快照，无法捕捉对精确碳核算至关重要的森林时间动态变化。我们提出了 ECHOSAT——一个分辨率为10米、覆盖多年份的全球一致性树高动态地图。为此，我们利用多源卫星数据训练了一个专用的视觉Transformer模型，以实现像素级的时间序列回归。通过引入一种自监督的“生长损失”函数，使模型预测结果遵循符合自然树木生长规律的变化曲线，包括随时间逐渐增高，以及因火灾等森林损失事件引起的骤然下降。实验评估表明，该模型在单年预测任务中的精度优于当前最先进的方法。我们还首次提供了可在全球尺度上准确量化树木生长与扰动事件的多年份树高图。我们期望 ECHOSAT 能够推动全球碳监测与扰动评估工作。相关地图可通过 https://github.com/ai4forest/echosat 获取。

</details>
