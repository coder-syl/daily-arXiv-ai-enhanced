<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [V-Agent: An Interactive Video Search System Using Vision-Language Models](https://arxiv.org/abs/2512.16925)
*SunYoung Park,Jong-Hyeon Lee,Youngjune Kim,Daegyu Sung,Younghyun Yu,Young-rok Cha,Jeongho Ju*

Main category: cs.CV

TL;DR: V-Agent 是一个基于多智能体的视频检索平台，结合微调后的视觉语言模型（VLM）与图像-文本检索模型，在 MultiVENT 2.0 基准上实现了零样本最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的视频检索系统在处理多模态内容（如视觉和语音）时存在局限性，难以实现上下文感知的精准检索。为解决这一问题，作者提出 V-Agent 平台，旨在通过融合视觉与语音信息提升视频搜索能力。

Method: V-Agent 利用少量视频偏好数据对视觉语言模型（VLM）进行微调，并结合图像-文本检索模型生成的检索向量。该系统包含三个协同工作的智能体：路由智能体、搜索智能体和聊天智能体。其中，搜索智能体使用 VLM 对视频帧和 ASR 语音转录分别编码至共享的多模态表示空间，并引入重排序模块以优化检索结果。

Result: 所提方法在 MultiVENT 2.0 基准测试中取得了零样本设置下的最先进性能。

Conclusion: V-Agent 展示了在学术研究和实际应用中处理多模态视频检索任务的强大潜力，特别是在支持用户与系统交互式对话方面具有显著优势。

Abstract: We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.

Abstract (中文翻译): 我们提出了 V-Agent，这是一个新颖的多智能体平台，专为高级视频搜索和用户与系统之间的交互式对话而设计。通过对视觉语言模型（VLM）使用小型视频偏好数据集进行微调，并结合图像-文本检索模型提供的检索向量，我们克服了传统基于文本的检索系统在多模态场景中的局限性。基于 VLM 的检索模型能够将来自自动语音识别（ASR）模块的视频帧和音频转录分别嵌入到共享的多模态表示空间中，使 V-Agent 能够同时理解视觉和语音内容，从而实现上下文感知的视频搜索。该系统由三个智能体组成——路由智能体、搜索智能体和聊天智能体——它们协同工作，通过优化搜索结果并与用户沟通来满足用户意图。搜索智能体结合 VLM 检索模型和额外的重排序模块，进一步提升了视频检索质量。我们提出的框架在 MultiVENT 2.0 基准测试中展示了最先进的零样本性能，凸显了其在学术研究和实际应用中的潜力。

</details>


### [2] [Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content](https://arxiv.org/abs/2512.16947)
*Reza Chandra,Adang Suhendra,Lintang Yuniar Banowosari,Prihandoko*

Main category: cs.CV

TL;DR: 该研究比较了CNN和VGG-16模型在快速识别含色情图像网站中的性能，结果表明CNN模型（在50个epoch、学习率0.001下）准确率达94.87%，优于VGG-16。


<details>
  <summary>Details</summary>
Motivation: 印尼政府虽封锁了大量含色情内容的网站，但公众仍可通过VPN访问，因此亟需一种能快速识别此类内容的技术手段。

Method: 采用深度学习方法，分别构建并全面评估卷积神经网络（CNN）与VGG-16模型，以识别疑似包含色情图像的网站。

Result: 在第八次实验中，CNN模型在epoch为50、学习率为0.001时取得最佳性能，准确率达94.87%，优于VGG-16模型。

Conclusion: CNN模型在快速且准确地检测色情内容方面比VGG-16模型更有效。

Abstract: In 2020, a total of 59,741 websites were blocked by the Indonesian government due to containing negative content, including pornography, with 14,266 websites falling into this category. However, these blocked websites could still be accessed by the public using virtual private networks (VPNs). This prompted the research idea to quickly identify pornographic content. This study aims to develop a system capable of identifying websites suspected of containing pornographic image content, using a deep learning approach with convolutional neural network (CNN) and visual geometry group 16 (VGG-16) model. The two models were then explored comprehensively and holistically to determine which model was most effective in detecting pornographic content quickly. Based on the findings of the comparison between testing the CNN and VGG-16 models, research results showed that the best test results were obtained in the eighth experiment using the CNN model at an epoch value level of 50 and a learning rate of 0.001 of 0.9487 or 94.87%. This can be interpreted that the CNN model is more effective in detecting pornographic content quickly and accurately compared to using the VGG-16 model.

Abstract (中文翻译): 2020年，印度尼西亚政府共屏蔽了59,741个含有负面内容（包括色情内容）的网站，其中14,266个属于色情类别。然而，公众仍可通过虚拟私人网络（VPN）访问这些被屏蔽的网站。这促使研究人员提出快速识别色情内容的想法。本研究旨在开发一种能够识别疑似包含色情图像内容网站的系统，采用深度学习方法，使用卷积神经网络（CNN）和视觉几何组16（VGG-16）模型。随后对这两种模型进行了全面而整体的探索，以确定哪种模型在快速检测色情内容方面更为有效。根据对CNN和VGG-16模型的对比测试结果，研究发现，在第八次实验中，CNN模型在epoch值为50、学习率为0.001时取得了最佳测试结果，准确率达到0.9487（即94.87%）。这表明，与VGG-16模型相比，CNN模型在快速且准确地检测色情内容方面更为有效。

</details>


### [3] [AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals](https://arxiv.org/abs/2512.16948)
*Qi Xu,Shuai Gong,Xuming Ran,Haihua Luo,Yangfan Hu*

Main category: cs.CV

TL;DR: AVM is a modular, structure-preserving deep learning framework that separates stable visual encoding from condition-specific adaptation using a frozen Vision Transformer encoder and independent modulation subnetworks, achieving better generalization and interpretability across stimuli, subjects, and datasets in mouse V1 neural response prediction.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models struggle to disentangle stable visual encoding from condition-specific neural adaptation, limiting their generalization across different stimuli and individuals.

Method: The Adaptive Visual Model (AVM) uses a frozen Vision Transformer encoder for consistent visual features and adds independently trained modular subnetworks to modulate responses based on stimulus content and subject identity, preserving core architecture while enabling condition-aware adaptation.

Result: AVM outperforms the state-of-the-art V1T model by ~2% in predictive correlation on two large-scale mouse V1 datasets and achieves a 9.1% improvement in explained variance (FEVE) in cross-dataset adaptation, showing strong generalization, interpretable modulation, and high efficiency.

Conclusion: AVM offers a unified, scalable framework for adaptive neural modeling under structural constraints, with implications for both neuroscience and biologically inspired AI.

Abstract: While deep learning models have shown strong performance in simulating neural responses, they often fail to clearly separate stable visual encoding from condition-specific adaptation, which limits their ability to generalize across stimuli and individuals. We introduce the Adaptive Visual Model (AVM), a structure-preserving framework that enables condition-aware adaptation through modular subnetworks, without modifying the core representation. AVM keeps a Vision Transformer-based encoder frozen to capture consistent visual features, while independently trained modulation paths account for neural response variations driven by stimulus content and subject identity. We evaluate AVM in three experimental settings, including stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all of which involve structured changes in inputs and individuals. Across two large-scale mouse V1 datasets, AVM outperforms the state-of-the-art V1T model by approximately 2% in predictive correlation, demonstrating robust generalization, interpretable condition-wise modulation, and high architectural efficiency. Specifically, AVM achieves a 9.1% improvement in explained variance (FEVE) under the cross-dataset adaptation setting. These results suggest that AVM provides a unified framework for adaptive neural modeling across biological and experimental conditions, offering a scalable solution under structural constraints. Its design may inform future approaches to cortical modeling in both neuroscience and biologically inspired AI systems.

Abstract (中文翻译): 尽管深度学习模型在模拟神经反应方面表现出色，但它们通常难以清晰地区分稳定的视觉编码与特定条件下的适应性，这限制了其在不同刺激和个体间的泛化能力。我们提出了自适应视觉模型（AVM），这是一种结构保持的框架，通过模块化的子网络实现对条件感知的适应，而无需修改核心表征。AVM将基于Vision Transformer的编码器冻结以捕捉一致的视觉特征，同时使用独立训练的调制路径来解释由刺激内容和个体身份引起的神经反应变化。我们在三种实验设置中评估了AVM，包括刺激层面的变化、跨个体泛化和跨数据集适应，这些设置均涉及输入和个体的结构性变化。在两个大规模小鼠V1数据集上，AVM在预测相关性上比当前最先进的V1T模型高出约2%，展现出强大的泛化能力、可解释的条件调制机制以及高架构效率。特别是在跨数据集适应场景下，AVM在解释方差（FEVE）上提升了9.1%。这些结果表明，AVM为在生物和实验条件下进行自适应神经建模提供了一个统一框架，在结构约束下具有良好的可扩展性，其设计可能为未来神经科学和类脑人工智能系统中的皮层建模方法提供启发。

</details>


### [4] [Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections](https://arxiv.org/abs/2512.16950)
*Adrian Straker,Paul Magdon,Marco Zullich,Maximilian Freudenberg,Christoph Kleinn,Johannes Breidenbach,Stefano Puliti,Nils Nölke*

Main category: cs.CV

TL;DR: 本文提出一种结合Finer-CAM与TLS点云投影分段的新方法，用于解析深度学习模型在树种分类中所依赖的结构特征。研究发现模型主要利用树冠特征进行分类，在部分树种中茎干和细枝也起重要作用，并揭示了模型判别逻辑与人类专家认知的一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于TLS和深度学习的树种分类方法取得了高精度，但其决策过程缺乏可解释性，限制了对模型可靠性、偏差及数据局限性的理解。

Method: 作者将Finer-CAM（类激活映射）解释方法与TLS投影中的结构特征分段相结合，系统评估驱动树种分类的关键特征；使用2,445棵欧洲七种树木的TLS数据训练并交叉验证五个YOLOv8模型。

Result: 模型平均准确率达96%（标准差0.24%）；630张显著图分析表明，模型主要依赖树冠特征进行分类，银桦、山毛榉、英国栎和挪威云杉尤为明显，而欧洲白蜡、苏格兰松和花旗松则更多依赖茎干特征；细枝结构对模型决策有显著贡献；模型对相似树种的判断与人类专家一致。

Conclusion: 为提升模型可信度并识别潜在偏差与局限，需深入理解树种分类模型的决策机制；所提方法有助于揭示模型依赖的结构特征，增强可解释性。

Abstract: Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.

Abstract (中文翻译): 树种分类几十年来一直是森林遥感领域的核心研究方向。新型传感器和分类方法（如地基激光扫描TLS和深度学习）虽已达到最先进的精度，但其决策过程仍不透明。诸如Finer-CAM（类激活映射）等方法可以突出显示TLS投影中对目标树种分类起关键作用、而在外观相似的对比树种中不常见的特征。本文提出一种新方法，将Finer-CAM解释结果与代表树木结构特征的TLS投影分段相联系，以系统评估哪些特征驱动了树种区分。研究利用来自七种欧洲树种共2,445棵树的TLS数据，通过交叉验证训练并验证了五个YOLOv8模型，平均准确率达到96%（标准差为0.24%）。对630张显著图的分析表明，模型主要依赖TLS投影中的树冠特征进行树种分类。这一现象在银桦、欧洲山毛榉、英国栎和挪威云杉中尤为显著，而欧洲白蜡、苏格兰松和花旗松的分类则更常依赖茎干特征。特别是细枝的表征对模型决策具有重要贡献。此外，模型认为彼此相似的树种，也与人类专家的认知一致。研究结果进一步强调，有必要加强对树种分类模型决策过程的理解，以揭示数据集和模型的局限性与偏差，并增强对模型预测结果的信心。

</details>


### [5] [Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories](https://arxiv.org/abs/2512.16954)
*Chayan Jain,Rishant Sharma,Archit Garg,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: 该论文提出了一种分阶段的视频生成方法，通过先用大语言模型生成剧本，再用文本到图像模型创建一致的角色视觉锚点，最后合成各场景视频，显著提升了角色一致性，并揭示了当前模型在印度与西方主题生成中的文化偏见。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频AI在生成长篇、连贯且角色一致的视频故事方面存在显著挑战。

Method: 提出一种类电影制作的多阶段流程：首先利用大语言模型生成详细剧本，再用文本到图像模型为每个角色生成一致的视觉锚点，最后由视频生成模型逐场景合成视频。

Result: 实验表明，移除视觉锚定机制会导致角色一致性评分从7.99骤降至0.55；此外，研究还发现现有模型在印度与西方主题视频生成中存在文化偏见，表现为角色一致性和动态程度的差异。

Conclusion: 多阶段分解和视觉先验对保持角色身份至关重要，同时需关注并解决视频生成模型中的文化偏见问题。

Abstract: Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.

Abstract (中文翻译): 当前文本到视频AI在生成长篇、连贯且角色一致的视频故事方面面临重大挑战。我们提出了一种类电影制作的视频生成方法。该方法不采用一步到位的方式生成视频，而是首先利用大语言模型生成详细的制作剧本，再以此剧本指导文本到图像模型为每个角色创建一致的视觉形象，这些视觉形象作为锚点，引导视频生成模型逐个合成每个场景。我们的基线对比验证了这种多阶段分解的必要性：具体而言，若移除视觉锚定机制，角色一致性评分将从7.99急剧下降至0.55，证实了视觉先验对于身份保持的重要性。此外，我们还分析了当前模型中的文化差异，揭示了印度主题与西方主题生成在主体一致性和动态程度方面存在明显偏见。

</details>


### [6] [InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression](https://arxiv.org/abs/2512.16975)
*Haotian Ye,Qiyuan He,Jiaqi Han,Puheng Li,Jiaojiao Fan,Zekun Hao,Fitsum Reda,Yogesh Balaji,Huayu Chen,Sheng Liu,Angela Yao,James Zou,Stefano Ermon,Haoxiang Wang,Ming-Yu Liu*

Main category: cs.CV

TL;DR: 本文提出InfoTok，一种基于信息论的自适应视频分词框架，通过按信息丰富度分配token，在保持性能的同时显著提升压缩率。


<details>
  <summary>Details</summary>
Motivation: 当前视频分词器采用固定压缩率，无法应对视频内容复杂性和信息密度变化，导致冗余或信息丢失。

Method: 基于香农信息论，提出一种新的ELBO（证据下界）算法，并构建基于Transformer的自适应压缩器实现自适应分词。

Result: 在保持性能不变的情况下节省20%的token，实现2.3倍压缩率，优于现有启发式自适应方法。

Conclusion: InfoTok通过依据信息丰富度分配token，实现了更紧凑且准确的视频表示，为未来研究提供新思路。

Abstract: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.

Abstract (中文翻译): 准确而高效的离散视频分词对于长视频序列处理至关重要。然而，视频固有的复杂性和可变的信息密度对当前的分词器构成了显著瓶颈——这些分词器以固定速率刚性地压缩所有内容，导致冗余或信息丢失。受香农信息论的启发，本文提出了InfoTok，一个用于自适应视频分词的原则性框架。我们严格证明了现有的与数据无关的训练方法在表示长度上是次优的，并提出了一种新颖的基于证据下界（ELBO）的算法，该算法接近理论最优。利用这一框架，我们开发了一种基于Transformer的自适应压缩器，从而实现自适应分词。实验证明，该方法在不影响性能的前提下节省了20%的token，并实现了2.3倍的压缩率，同时优于以往基于启发式的自适应方法。通过根据信息丰富度分配token，InfoTok实现了更紧凑且准确的视频表示，为未来研究提供了有价值的见解。

</details>


### [7] [Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video](https://arxiv.org/abs/2512.16977)
*Hao Li,Daiwei Lu,Xing Yao,Nicholas Kavoussi,Ipek Oguz*

Main category: cs.CV

TL;DR: 本文提出Endo-SemiS，一种用于内窥镜视频帧分割的半监督框架，在标注数据有限的情况下仍能实现可靠分割。该方法结合交叉监督、不确定性引导的伪标签、联合伪标签监督和互学习策略，并引入利用时空信息的校正网络，在两个临床任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频帧的像素级标注成本高昂且耗时，限制了深度学习模型在临床中的应用。因此，亟需一种能在有限标注条件下有效利用大量未标注数据的半监督分割方法。

Method: Endo-SemiS框架采用四种策略：(1) 两个独立网络间的交叉监督；(2) 基于不确定性的高置信度伪标签生成；(3) 聚合双网络可靠像素的联合伪标签监督；(4) 特征与图像层面的互学习以降低方差。此外，还引入一个利用内窥镜视频时空信息的独立校正网络。

Result: 在输尿管镜肾结石激光碎石术和结肠镜息肉筛查两个临床数据集上，Endo-SemiS在标注数据有限的情况下，性能显著优于当前最先进的分割方法。

Conclusion: Endo-SemiS通过有效利用未标注数据和时空上下文信息，为内窥镜视频分割提供了一种高效且可靠的半监督解决方案，在临床应用中展现出巨大潜力。

Abstract: In this paper, we present Endo-SemiS, a semi-supervised segmentation framework for providing reliable segmentation of endoscopic video frames with limited annotation. EndoSemiS uses 4 strategies to improve performance by effectively utilizing all available data, particularly unlabeled data: (1) Cross-supervision between two individual networks that supervise each other; (2) Uncertainty-guided pseudo-labels from unlabeled data, which are generated by selecting high-confidence regions to improve their quality; (3) Joint pseudolabel supervision, which aggregates reliable pixels from the pseudo-labels of both networks to provide accurate supervision for unlabeled data; and (4) Mutual learning, where both networks learn from each other at the feature and image levels, reducing variance and guiding them toward a consistent solution. Additionally, a separate corrective network that utilizes spatiotemporal information from endoscopy video to improve segmentation performance. Endo-SemiS is evaluated on two clinical applications: kidney stone laser lithotomy from ureteroscopy and polyp screening from colonoscopy. Compared to state-of-the-art segmentation methods, Endo-SemiS substantially achieves superior results on both datasets with limited labeled data. The code is publicly available at https://github.com/MedICL-VU/Endo-SemiS

Abstract (中文翻译): 本文提出了Endo-SemiS，一种用于内窥镜视频帧分割的半监督框架，能够在标注数据有限的情况下提供可靠的分割结果。Endo-SemiS通过四种策略有效利用所有可用数据（尤其是未标注数据）来提升性能：(1) 两个独立网络之间的交叉监督，彼此相互监督；(2) 从未标注数据中生成基于不确定性的伪标签，通过选择高置信度区域来提高其质量；(3) 联合伪标签监督，聚合来自两个网络伪标签中的可靠像素，为未标注数据提供准确的监督信号；(4) 互学习机制，使两个网络在特征层面和图像层面相互学习，从而降低方差并引导它们达成一致的解决方案。此外，还引入了一个独立的校正网络，利用内窥镜视频中的时空信息来进一步提升分割性能。Endo-SemiS在两个临床应用场景中进行了评估：输尿管镜下的肾结石激光碎石术和结肠镜下的息肉筛查。与当前最先进的分割方法相比，Endo-SemiS在标注数据有限的情况下，在两个数据集上均取得了显著更优的结果。代码已公开发布于https://github.com/MedICL-VU/Endo-SemiS。

</details>


### [8] [A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos](https://arxiv.org/abs/2512.16978)
*Mohammed Irfan Kurpath,Jaseel Muhammad Kaithakkodan,Jinxing Zhou,Sahal Shaji Mullappilly,Mohammad Almansoori,Noor Ahsan,Beknur Kalmakhanbet,Sambal Shikhar,Rishabh Lalla,Jean Lahoud,Mariette Awad,Fahad Shahbaz Khan,Salman Khan,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出了LongShOTBench，一个用于评估多模态大语言模型（MLLMs）在长视频理解中表现的诊断性基准，涵盖开放式问题、多轮对话、多模态推理和智能体工具使用，并引入配套系统LongShOTAgent；实验显示当前SOTA模型在此任务上表现不佳，突显该任务的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解基准要么侧重时间长度，要么侧重多模态丰富性，很少兼顾两者；且多依赖单一准确率指标，难以揭示模型失败原因。

Method: 构建LongShOTBench基准：通过可扩展、人工验证的流程生成包含参考答案和分级评分标准的样本，涵盖视频、音频、语音的多模态推理任务及智能体工具调用；同时开发LongShOTAgent系统，通过预处理、搜索和迭代优化分析长视频。

Result: 在LongShOTBench上，Gemini-2.5-Flash得分为52.95%，开源模型低于30%，LongShOTAgent达到44.66%，表明当前MLLM在真实场景长视频理解上仍有显著差距。

Conclusion: LongShOTBench为评估和改进多模态大语言模型在长视频理解任务上提供了实用、可复现的基础。

Abstract: Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.

Abstract (中文翻译): 长格式多模态视频理解需要将视觉、语音和环境音频信息整合起来，并进行连贯的长距离推理。现有的基准测试要么强调时间长度，要么强调多模态丰富性，但很少同时兼顾两者；尽管部分基准引入了开放式问题和高级评估指标，但大多仍依赖单一分数的准确率，难以揭示模型的具体失败模式。我们提出了LongShOTBench——一个诊断性基准，包含意图驱动的开放式问题、单轮与多轮对话，以及需要跨视频、音频和语音进行多模态推理和智能体工具调用的任务。每个样本均配有参考答案和分级评分标准，以实现可解释、可追溯的评估。LongShOTBench通过一个可扩展且经人工验证的流程构建，确保覆盖全面且结果可复现，所有样本均经过人工校验与修正。此外，我们还提出了LongShOTAgent，一种通过预处理、搜索和迭代优化来分析长视频的智能体系统。在LongShOTBench上，当前最先进的多模态大语言模型（MLLMs）表现存在显著差距：Gemini-2.5-Flash得分为52.95%，开源模型得分均低于30%，而LongShOTAgent达到44.66%。这些结果凸显了现实世界中长视频理解任务的难度。LongShOTBench为评估和提升MLLM提供了实用且可复现的基础。所有资源已在GitHub公开：https://github.com/mbzuai-oryx/longshot。

</details>


### [9] [4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation](https://arxiv.org/abs/2512.17012)
*Chiao-An Yang,Ryo Hachiuma,Sifei Liu,Subhashree Radhakrishnan,Raymond A. Yeh,Yu-Chiang Frank Wang,Min-Hung Chen*

Main category: cs.CV

TL;DR: 本文提出4D-RGPT模型、P4D训练框架和R4D-Bench评测基准，以提升多模态大语言模型在4D（3D+时间）视频理解和区域级问答上的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在处理3D结构和时间动态方面能力有限，主要受限于较弱的4D感知和时间理解能力；现有3D/4D视频问答（VQA）基准侧重静态场景且缺乏区域级提示。

Method: 提出：(a) 4D-RGPT，一种专用于从视频输入中捕捉4D表征并增强时间感知的MLLM；(b) 感知4D蒸馏（P4D），一种将冻结专家模型中的4D表征迁移至4D-RGPT的训练框架；(c) R4D-Bench，一个结合自动与人工验证构建的、支持区域级提示的深度感知动态场景评测基准。

Result: 4D-RGPT在现有4D VQA基准和新提出的R4D-Bench上均取得显著性能提升。

Conclusion: 所提方法有效增强了MLLM对4D动态场景的理解能力，并通过新基准推动了该领域的评估标准。

Abstract: Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.

Abstract (中文翻译): 尽管多模态大语言模型（MLLMs）取得了进展，但其在3D结构和时间动态方面的推理能力仍然有限，受限于较弱的4D感知和时间理解能力。现有的3D和4D视频问答（VQA）基准也侧重于静态场景，且缺乏区域级别的提示机制。我们通过以下方式解决这些问题：(a) 提出4D-RGPT，一种专门设计用于从视频输入中捕捉4D表征并增强时间感知的MLLM；(b) 提出感知4D蒸馏（P4D），一种训练框架，可将冻结专家模型中的4D表征迁移到4D-RGPT中，以实现全面的4D感知；(c) 构建R4D-Bench，一个面向具有深度感知的动态场景并支持区域级提示的新基准，通过混合自动化与人工验证的流程构建而成。我们的4D-RGPT在现有4D VQA基准和所提出的R4D-Bench基准上均取得了显著的性能提升。

</details>


### [10] [FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring](https://arxiv.org/abs/2512.17021)
*Martin Schwartz,Fajwel Fogel,Nikola Besic,Damien Robert,Louis Geist,Jean-Pierre Renaud,Jean-Matthieu Monnet,Clemens Mosig,Cédric Vega,Alexandre d'Aspremont,Loic Landrieu,Philippe Ciais*

Main category: cs.CV

TL;DR: 本文提出FORMSpoT方法，利用SPOT-6/7卫星十年时序数据和高分辨率机载激光扫描（ALS）训练的层次化Transformer模型，在1.5米分辨率下绘制法国全国森林冠层高度，并通过专用后处理流程生成年度扰动图（FORMSpoT-Δ），显著优于现有扰动产品，尤其在山区小尺度扰动检测中F1分数达0.44，为森林管理、衰退早期预警及碳损失量化提供新工具。


<details>
  <summary>Details</summary>
Motivation: 欧洲森林碳汇能力近期下降，亟需空间显式且高频更新的森林监测手段；而现有基于卫星的扰动产品分辨率不足，难以检测单株树木尺度（通常小于100平方米）的变化。

Method: 利用2014–2024年SPOT-6/7年度合成影像，结合基于高分辨率机载激光扫描（ALS）数据训练的层次化Transformer模型（PVTv2）估算1.5米分辨率的森林冠层高度；开发包含配准与时空总变分去噪的后处理流程，以实现跨异构影像的稳健变化检测，生成年度扰动图FORMSpoT-Δ。

Result: 在19个ALS重访站点和5,087个法国国家森林清查样地中验证表明，FORMSpoT-Δ显著优于现有扰动产品；在扰动小且空间破碎的山地森林中，其F1分数达0.44，比现有基准高出一个数量级。

Conclusion: FORMSpoT-Δ实现了国家级尺度的树木级别森林动态监测，为分析森林管理实践、早期发现森林衰退信号以及更准确量化如间伐或择伐等细微扰动造成的碳损失提供了独特工具；研究强调了维持SPOT等超高分辨率卫星任务及DINAMIS等开放数据计划对气候变化下森林监测的重要性。

Abstract: The recent decline of the European forest carbon sink highlights the need for spatially explicit and frequently updated forest monitoring tools. Yet, existing satellite-based disturbance products remain too coarse to detect changes at the scale of individual trees, typically below 100 m$^{2}$. Here, we introduce FORMSpoT (Forest Mapping with SPOT Time series), a decade-long (2014-2024) nationwide mapping of forest canopy height at 1.5 m resolution, together with annual disturbance polygons (FORMSpoT-$Δ$) covering mainland France. Canopy heights were derived from annual SPOT-6/7 composites using a hierarchical transformer model (PVTv2) trained on high-resolution airborne laser scanning (ALS) data. To enable robust change detection across heterogeneous acquisitions, we developed a dedicated post-processing pipeline combining co-registration and spatio-temporal total variation denoising. Validation against ALS revisits across 19 sites and 5,087 National Forest Inventory plots shows that FORMSpoT-$Δ$ substantially outperforms existing disturbance products. In mountainous forests, where disturbances are small and spatially fragmented, FORMSpoT-$Δ$ achieves an F1-score of 0.44, representing an order of magnitude higher than existing benchmarks. By enabling tree-level monitoring of forest dynamics at national scale, FORMSpoT-$Δ$ provides a unique tool to analyze management practices, detect early signals of forest decline, and better quantify carbon losses from subtle disturbances such as thinning or selective logging. These results underscore the critical importance of sustaining very high-resolution satellite missions like SPOT and open-data initiatives such as DINAMIS for monitoring forests under climate change.

Abstract (中文翻译): 近期欧洲森林碳汇的下降凸显了对空间显式且频繁更新的森林监测工具的需求。然而，现有的基于卫星的扰动产品分辨率仍然过于粗糙，无法检测单株树木尺度（通常小于100平方米）的变化。本文提出了FORMSpoT（Forest Mapping with SPOT Time series）方法，该方法基于十年（2014–2024年）SPOT卫星时序数据，以1.5米分辨率绘制了法国本土的森林冠层高度图，并同时生成了年度扰动图（FORMSpoT-Δ）。冠层高度由年度SPOT-6/7合成影像通过一个层次化Transformer模型（PVTv2）推导得出，该模型使用高分辨率机载激光扫描（ALS）数据进行训练。为实现跨异构影像采集条件下的稳健变化检测，我们开发了一套专用后处理流程，结合影像配准与时空总变分去噪技术。在19个ALS重访站点和5,087个法国国家森林清查样地上的验证结果表明，FORMSpoT-Δ显著优于现有扰动产品。在扰动规模小且空间破碎的山地森林中，FORMSpoT-Δ的F1分数达到0.44，比现有基准高出一个数量级。通过实现国家级尺度的树木级别森林动态监测，FORMSpoT-Δ为分析森林管理实践、早期识别森林衰退信号，以及更精确量化由间伐或择伐等细微扰动引起的碳损失提供了独特工具。这些结果强调了持续支持SPOT等超高分辨率卫星任务以及DINAMIS等开放数据倡议对于在气候变化背景下监测森林的关键意义。

</details>
