<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文通过大规模定量研究揭示了视频扩散模型中运动与外观在去噪时间步上的解耦规律，验证并形式化了“早期时间步主导运动、后期主导外观”的经验法则，并基于此提出一种简化且高效的运动定制方法。


<details>
  <summary>Details</summary>
Motivation: 当前对文本到视频扩散模型中运动如何在不同时间步上编码的理解尚不充分，尽管实践中常使用“早期时间步控制运动、后期细化外观”的启发式方法，但缺乏系统性分析和理论支撑。

Method: 作者通过在指定时间步范围内注入新条件，以由此引发的外观编辑与运动保持之间的权衡作为运动编码的代理指标，开展大规模定量实验，从而刻画运动与外观在去噪轨迹上的竞争关系。

Result: 在多种架构下均发现存在早期的“运动主导阶段”和后期的“外观主导阶段”，据此定义了操作性的运动-外观时间步边界；基于该发现，仅在运动主导阶段进行训练和推理即可实现强效运动迁移，无需额外去偏模块或特殊目标。

Conclusion: 该工作将广泛使用的经验法则转化为时空解耦原则，并提供了一种可直接集成到现有运动迁移与编辑方法中的时间步约束方案。

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

Abstract (中文翻译): 文本到视频扩散模型通过迭代去噪合成时间运动和空间外观，然而运动在各个时间步上如何被编码仍缺乏深入理解。实践中，人们常依赖一种经验法则：早期时间步主要塑造运动和布局，而后期时间步则细化外观，但这一行为尚未被系统性地刻画。本文通过在指定时间步范围内注入新条件所引发的外观编辑与运动保持之间的权衡，作为视频扩散模型中运动编码的代理指标，并通过大规模定量研究对该代理指标进行刻画。该方法使我们能够沿着去噪轨迹定量地分离运动与外观。在多种不同架构中，我们一致地识别出一个早期的“运动主导阶段”和一个后期的“外观主导阶段”，从而在时间步空间中确立了一个可操作的运动-外观边界。基于这一发现，我们将当前的一次性运动定制范式简化为仅在运动主导阶段进行训练和推理，即可在无需辅助去偏模块或专门优化目标的情况下实现强大的运动迁移效果。我们的分析将一种广泛使用的启发式方法转化为一种时空解耦原则，而所提出的时间步约束方案可直接集成到现有的运动迁移与编辑方法中。

</details>


### [2] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D CNN与LSTM的混合深度学习架构，用于实时识别美式手语（ASL）单词级别手势。系统基于网络摄像头视频流，在WLASL、ASL-LEX及专家标注数据集上训练，F1分数达0.71–0.99，并可在OAK-D相机上实现边缘部署。


<details>
  <summary>Details</summary>
Motivation: 全球有超过7000万聋人和听障人士面临沟通障碍，亟需高效、实时的手语识别系统以提升无障碍交流能力。

Method: 采用3D卷积神经网络（3D CNN）提取视频帧中的时空特征，再通过长短期记忆网络（LSTM）建模手语手势中的时序依赖关系；模型在WLASL数据集（2000个常用词）、ASL-LEX词汇库（约2700个手势）以及100个专家标注的手势数据上进行训练，并部署于AWS平台，支持OAK-D相机的边缘推理。

Result: 系统在各类手语类别上的F1分数介于0.71至0.99之间，展现出高准确率和实用性。

Conclusion: 所提出的混合深度学习架构能有效实现实时美式手语识别，具备良好的部署能力和实际应用前景，有助于提升听障人群的沟通可及性。

Abstract: This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

Abstract (中文翻译): 本文提出了一种实时美式手语（ASL）识别系统，该系统采用结合三维卷积神经网络（3D CNN）与长短期记忆网络（LSTM）的混合深度学习架构。该系统处理网络摄像头视频流，以识别单词级别的ASL手势，旨在缓解全球超过7000万聋人及听障人士所面临的沟通障碍。该架构利用3D卷积从视频帧中提取时空特征，随后通过LSTM层建模手语手势中固有的时序依赖关系。系统在WLASL数据集（包含2000个常用词）、ASL-LEX词汇数据库（约2700个手势）以及一组经专家标注的100个ASL手势数据上进行训练，在各类手势上的F1分数达到0.71至0.99。该模型部署于AWS基础设施，并支持在OAK-D相机上进行边缘部署以实现实时推理。文中还讨论了该架构的设计、训练方法、评估指标以及面向实际无障碍应用场景的部署考量。

</details>


### [3] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 本文提出一种结合人工智能与局部线性嵌入（LLE）的新方法，用于提升医疗计费和转录系统的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在医疗领域的快速发展，亟需更高效、准确的方法来处理高维医疗数据，特别是在医疗计费和转录等环节，以减少人为错误并提升整体运营效率。

Method: 提出一种AI增强的局部线性嵌入（LLE）模型，通过该模型对高维医疗数据进行降维与处理，并将其应用于医疗计费和转录流程中。

Result: 实验结果表明，所提方法在数据处理准确性和操作效率方面均有显著提升。

Conclusion: AI增强的LLE方法在医疗数据处理中具有巨大潜力，不仅可优化当前的计费与转录系统，也为未来更广泛的医疗应用研究奠定了基础。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

Abstract (中文翻译): 人工智能在医疗领域的迅速发展为提升各类流程（包括医疗计费和转录）开辟了新路径。本文提出了一种创新方法，将人工智能与局部线性嵌入（LLE）相结合，以革新高维医疗数据的处理方式。该AI增强的LLE模型专门针对提升医疗计费系统和转录服务的准确性与效率而设计。通过自动化这些流程，该模型旨在减少人为错误并简化操作，从而加快患者护理文档记录和财务交易的速度与准确性。本文提供了AI增强LLE的完整数学模型，并通过一系列实验展示了其在真实医疗场景中的应用。结果表明，该方法在数据处理准确性和运营效率方面均有显著提升。本研究不仅突显了AI增强LLE在医疗数据分析中的潜力，也为未来更广泛的医疗应用研究奠定了基础。

</details>


### [4] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: VISTA is a modular framework that separates visual perception from reasoning to reduce reliance on spurious correlations in vision-language models, improving robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: End-to-end vision-language models often rely on spurious correlations rather than true visual evidence, especially after fine-tuning, leading to shortcut learning and poor generalization.

Method: VISTA decouples perception and reasoning via an explicit information bottleneck: a frozen VLM handles short, objective perception queries, while a text-only LLM decomposes questions, plans queries, and aggregates visual facts. The system is trained with reinforcement learning (GRPO) using only 641 curated multi-step questions.

Result: VISTA significantly improves robustness on SpuriVerse (+16.29% with Qwen-2.5-VL-7B, +6.77% with Llama-3.2-Vision-11B), maintains competitive performance on MMVP and SeedBench, transfers well to unseen VLMs, and recovers from perception failures. Human evaluation shows more neutral, evidence-grounded reasoning.

Conclusion: By enforcing a modular, bottlenecked interface between perception and reasoning, VISTA enables more reliable, interpretable, and robust visual question answering that resists spurious correlations.

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

Abstract (中文翻译): 端到端的视觉-语言模型（VLM）通常通过利用虚假相关性而非因果视觉证据来回答视觉问题，并且在微调后更容易依赖捷径。我们提出了VISTA（用于文本分析的视觉信息分离），这是一种模块化框架，通过显式的信息瓶颈将感知与推理解耦。一个冻结的VLM传感器仅处理简短、客观的感知查询，而纯文本的大语言模型（LLM）推理器则负责分解问题、规划查询，并以自然语言聚合视觉事实。这种受控接口为使用强化学习训练无偏视觉推理提供了一个与奖励对齐的环境。在仅使用641个精心策划的多步问题并通过GRPO进行训练的情况下，基于Qwen2.5-VL和Llama3.2-Vision传感器实现的VISTA在SpuriVerse上对现实世界虚假相关性的鲁棒性显著提升（Qwen-2.5-VL-7B提升16.29%，Llama-3.2-Vision-11B提升6.77%），同时在MMVP和平衡版SeedBench子集上仍保持竞争力。VISTA能稳健地迁移到未见过的VLM传感器，并能识别和从VLM感知失败中恢复。人工分析进一步表明，与端到端VLM基线相比，VISTA的推理轨迹更加中立、更少依赖虚假属性，并更明确地基于视觉证据。

</details>


### [5] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: SAMM2D是一种双编码器框架，在颅内动脉瘤检测任务中显著优于临床基线（AUC提升32%），并发现强预训练模型下数据增强反而损害性能；通过阈值校准，模型达到95%敏感度，超越普通放射科医生，并在可视化上展现出临床相关性。


<details>
  <summary>Details</summary>
Motivation: 动脉瘤检测对预防致命性出血至关重要，但因动脉瘤形态细微、类别极度不平衡及标注数据稀缺而极具挑战。

Method: 提出SAMM2D双编码器框架，结合强ImageNet预训练主干，在无数据增强条件下训练，并通过决策阈值校准优化敏感度；同时进行六种增强策略的消融实验以评估增强效果。

Result: SAMM2D在RSNA数据集上AUC达0.686，比临床基线高32%；未使用数据增强的模型显著优于所有增强变体（提升1.75–2.23个百分点，p<0.01）；校准后敏感度达95%，超过平均放射科医生水平；Grad-CAM显示85%真阳性聚焦于相关血管区域（与专家标注IoU为62%）；预计每千名筛查患者可节省1390万美元。

Conclusion: 在低数据医学影像任务中，强预训练可能比复杂的数据增强更有效；未来工作应优先利用高质量预训练特征，而非过度依赖增强策略。

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

Abstract (中文翻译): 有效的动脉瘤检测对于预防危及生命的出血至关重要，但由于动脉瘤形态细微、类别严重不平衡以及标注数据稀缺，这一任务仍然具有挑战性。我们提出了SAMM2D，一种双编码器框架，在RSNA颅内动脉瘤数据集上实现了0.686的AUC，比临床基线提高了32%。在对六种数据增强策略进行全面消融实验时，我们发现了一个惊人现象：当与强大的预训练主干网络结合时，任何形式的数据增强都会降低模型性能。未使用数据增强的基线模型在所有增强变体中表现最佳，性能高出1.75至2.23个百分点（p < 0.01），推翻了“在低数据医学场景中，越多增强越好”的传统假设。我们推测，ImageNet预训练特征已具备足够的鲁棒不变性，额外的数据增强不仅冗余，还会破坏所学特征流形。通过校准决策阈值，SAMM2D达到了95%的敏感度，超越了普通放射科医生的表现，并在筛查应用中预计每1000名患者可节省1390万美元。Grad-CAM可视化证实，85%的真阳性样本聚焦于相关的血管区域（与专家标注的IoU为62%），表明模型具有临床意义的关注点。我们的结果表明，未来的医学影像工作流程或许应更重视强预训练，而非日益复杂的数据增强流程。

</details>


### [6] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: 本文提出HookMIL，一种基于可学习“钩子”（hook）token的高效多示例学习框架，用于全切片图像分析。该方法通过多模态初始化、线性复杂度注意力机制、多样性损失和token间通信，在保持计算效率的同时提升性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在病理全切片图像分析中丢失上下文信息，而基于Transformer的方法虽表达能力强但计算复杂度高且存在冗余。

Method: 提出HookMIL框架：引入紧凑、可学习的hook token进行结构化上下文聚合；支持从关键图像块特征、视觉-语言模型文本嵌入和空间转录组-视觉模型特征三种方式初始化；采用线性复杂度的双向注意力机制；设计Hook Diversity Loss促进token专业化；加入hook-to-hook通信机制减少冗余。

Result: 在四个公开病理数据集上达到SOTA性能，同时具备更高的计算效率和可解释性。

Conclusion: HookMIL有效解决了现有MIL方法在上下文建模和计算效率方面的不足，为弱监督病理图像分析提供了高效且可解释的新方案。

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

Abstract (中文翻译): 多示例学习（MIL）已推动了计算病理学中全切片图像（WSI）的弱监督分析。然而，传统的MIL方法常常丢失关键的上下文信息，而基于Transformer的变体虽然表达能力更强，却面临二次方复杂度和冗余计算的问题。为解决这些局限，我们提出了HookMIL——一种兼顾上下文感知与计算效率的MIL框架，该框架利用紧凑且可学习的“钩子”（hook）token进行结构化的上下文聚合。这些token可通过以下三种方式初始化：(i) 关键图像块的视觉特征，(ii) 视觉-语言病理模型的文本嵌入，以及(iii) 空间转录组-视觉模型提供的空间定位特征。这种多模态初始化使Hook Token能够融入丰富的文本与空间先验知识，从而加速收敛并提升表征质量。训练过程中，Hook token通过线性复杂度的双向注意力机制与实例进行交互。为进一步促进专业化，我们引入了Hook多样性损失（Hook Diversity Loss），鼓励每个token聚焦于不同的组织病理学模式。此外，一种token间的通信机制可优化上下文交互并最小化冗余。在四个公开病理数据集上的大量实验表明，HookMIL实现了最先进的性能，同时提升了计算效率与可解释性。代码已开源：https://github.com/lingxitong/HookMIL。

</details>


### [7] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: Tiny-YOLOSAM combines YOLOv12 detection with TinySAM to achieve fast, high-coverage segmentation by using box prompts for salient objects and sparse point prompts only where needed, greatly improving speed and coverage over standard "segment-everything" approaches.


<details>
  <summary>Details</summary>
Motivation: The Segment Anything Model (SAM) is too slow for latency-sensitive applications. Although TinySAM offers a lightweight alternative, its "segment-everything" mode remains inefficient due to requiring hundreds of prompts. A faster yet effective full-scene segmentation method is needed.

Method: The authors first validate TinySAM on COCO val2017. Then they propose Tiny-YOLOSAM—a hybrid pipeline that uses YOLOv12 to generate box prompts for foreground objects and adds sparse point prompts only in regions not covered by YOLO-guided masks.

Result: On COCO val2017, Tiny-YOLOSAM boosts class-agnostic AR from 16.4% to 77.1% and mIoU from 19.2% to 67.8%, while reducing runtime from 49.20s/image to 10.39s/image (4.7× faster) on an Apple M1 Pro CPU.

Conclusion: Detector-guided prompting with targeted sparse sampling is a practical and efficient alternative to dense “segment-everything” strategies for full-scene segmentation.

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

Abstract (中文翻译): Segment Anything Model（SAM）支持可提示的高质量分割，但在对延迟敏感的场景中计算开销过大。TinySAM 是一种轻量级的蒸馏版 SAM，在保持强大零样本掩码质量的同时，其“全图分割”模式仍需数百个提示，实际运行速度较慢。作者首先使用官方检查点在 COCO val2017 上复现了 TinySAM，其 AP 指标与原文报告结果相差不超过 0.03%，建立了可靠的实验基线。在此基础上，提出了一种快速混合流水线 Tiny-YOLOSAM：利用最新的 YOLO 检测器（YOLOv12）为显著前景目标生成框提示，并仅在 YOLO 引导的掩码未覆盖区域补充稀疏点提示。在 COCO val2017 上，该混合系统显著提升了类别无关的覆盖率（AR 从 16.4% 提升至 77.1%，mIoU 从 19.2% 提升至 67.8%），同时在 Apple M1 Pro CPU 上将端到端推理时间从每图 49.20 秒降至 10.39 秒（提速 4.7 倍）。结果表明，检测器引导提示结合针对性稀疏采样是实现高效全场景分割的有效替代方案。

</details>


### [8] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: 本文提出一种新型多模态可解释性模型，利用视觉语言模型（VLM）结合少样本学习，通过分析眼底图像中视网膜象限的病灶分布来模拟眼科医生的推理过程，并生成配对的Grad-CAM热图，同时整合OCT和眼底图像以提升糖尿病视网膜病变（DR）分类的可解释性与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前糖尿病视网膜病变（DR）诊断面临两大挑战：一是依赖单一成像模态的AI模型可解释性有限，仅能高亮病灶位置而无法提供推理依据；二是人工标注病灶对临床医生而言不切实际。因此，亟需一种能以自然语言识别并解释DR病灶、融合多模态数据的定量检测系统，以支持筛查、治疗和研究。

Method: 本研究提出一种基于视觉语言模型（VLM）与少样本学习的多模态可解释性方法。该模型通过分析眼底图像中视网膜四个象限内的病灶分布，模拟眼科医生的诊断逻辑，并生成配对的Grad-CAM热图，可视化OCT和眼底图像中对DR严重程度分类起关键作用的区域。

Result: 在包含3,000张眼底图像和1,000张OCT图像的数据集上验证，该方法有效克服了现有DR诊断模型在可解释性和多模态融合方面的局限性，提供了更全面、实用的辅助诊断工具。

Conclusion: 所提出的多模态可解释性模型不仅提升了DR分类的透明度和临床可信度，还为未来在筛查、治疗规划和医学研究中的应用奠定了基础，有望显著改善患者预后。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

Abstract (中文翻译): 糖尿病视网膜病变（DR）是全球致盲的主要原因之一，早期检测对保护视力至关重要。然而，由于医疗资源有限，DR常被漏诊。为解决此问题，现有AI模型常采用病灶分割以提高可解释性，但人工标注病灶对临床医生而言不切实际。医生更需要的是能够解释分类依据的模型，而非仅标出病灶位置。此外，当前模型多为单模态，在可解释性方面效果有限。相比之下，一种能以自然语言识别个体DR病灶的定量检测系统将突破上述限制，广泛应用于筛查、治疗和研究。为此，本文提出一种新颖的多模态可解释性模型，利用视觉语言模型（VLM）结合少样本学习，通过分析眼底图像中视网膜象限内的病灶分布来模拟眼科医生的推理过程。该模型生成配对的Grad-CAM热图，展示OCT和眼底图像中各神经元权重，直观突出影响DR严重程度分类的关键区域。基于3,000张眼底图像和1,000张OCT图像的数据集，该创新方法有效解决了当前DR诊断中的关键局限，为改善患者预后提供了实用且全面的工具。

</details>


### [9] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: 本文提出TCFormer，一个仅含500万参数的超轻量级弱监督Transformer模型，通过可学习密度加权平均模块和密度等级分类损失，在仅使用图像级计数标签的情况下实现高效准确的人群计数。


<details>
  <summary>Details</summary>
Motivation: 人群计数通常依赖耗时费力的点级标注和计算开销大的骨干网络，限制了其在资源受限环境中的可扩展性和部署能力。

Method: 采用高效视觉Transformer作为特征提取器；设计可学习密度加权平均（Learnable Density-Weighted Averaging）模块以动态重加权局部token；引入密度等级分类损失，将人群密度离散为多个等级以正则化训练过程。

Result: 在ShanghaiTech A/B、UCF-QNRF和NWPU四个基准数据集上的实验表明，TCFormer在参数效率与计数精度之间取得了优越的平衡。

Conclusion: TCFormer是一种适用于边缘设备的高效人群计数解决方案，在仅使用图像级弱监督信号的情况下仍能实现高精度估计。

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

Abstract (中文翻译): 人群计数通常依赖于劳动密集型的点级标注和计算密集型的骨干网络，这限制了其在资源受限环境中的可扩展性和部署能力。为应对这些挑战，本文提出了TCFormer——一种微型、超轻量级的弱监督Transformer人群计数框架，仅包含500万参数却能实现具有竞争力的性能。首先，采用一个强大而高效的视觉Transformer作为特征提取器，其全局上下文感知能力可在极小内存占用下提供语义丰富的人群特征。其次，为弥补空间监督信息的缺失，我们设计了一种称为“可学习密度加权平均”（Learnable Density-Weighted Averaging）的特征聚合机制，该模块根据预测的密度分数动态地对局部token进行重加权，使网络能够根据各区域的具体密度特性自适应地调节特征，而无需额外标注。此外，本文引入了一种密度等级分类损失，将人群密度离散为若干等级，从而正则化训练过程并增强模型在不同密度级别下的分类能力。因此，尽管TCFormer仅利用图像级全局计数进行弱监督训练，但通过对计数损失和密度等级损失的联合优化，该框架仍能实现高精度的估计。在包括ShanghaiTech A/B、UCF-QNRF和NWPU在内的四个基准数据集上的大量实验表明，我们的方法在参数效率与计数精度之间取得了更优的平衡，可成为边缘设备上人群计数任务的良好解决方案。

</details>


### [10] [A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability](https://arxiv.org/abs/2512.22205)
*Md. Ismiel Hossen Abir,Awolad Hossain*

Main category: cs.CV

TL;DR: 本文提出一种基于自定义CNN的深度学习方法，用于自动识别疟原虫感染的血细胞图像，在准确率、精确率和召回率方面表现优异，并结合可解释AI技术增强模型透明度，适用于资源有限地区。


<details>
  <summary>Details</summary>
Motivation: 传统疟疾诊断方法（如显微镜血涂片分析）灵敏度低、依赖专家判断且在偏远地区缺乏必要资源，亟需一种更高效、准确且可部署的自动化诊断方案。

Method: 采用自定义卷积神经网络（CNN）对血细胞图像进行自动分类（感染/未感染），并与ResNet50、VGG16、MobileNetV2和DenseNet121等主流架构进行比较；同时应用SHAP、LIME和显著图等可解释AI技术提升模型可解释性。

Result: 所提模型准确率达96%，两类别的精确率和召回率均超过0.95，优于或媲美主流深度学习架构，并通过可解释性方法验证了其决策可靠性。

Conclusion: 该研究证明深度学习可在资源受限地区实现快速、准确且可解释的疟疾诊断，具有重要的临床与公共卫生应用潜力。

Abstract: Malaria remains a prevalent health concern in regions with tropical and subtropical climates. The cause of malaria is the Plasmodium parasite, which is transmitted through the bites of infected female Anopheles mosquitoes. Traditional diagnostic methods, such as microscopic blood smear analysis, are low in sensitivity, depend on expert judgment, and require resources that may not be available in remote settings. To overcome these limitations, this study proposes a deep learning-based approach utilizing a custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. This study also compares the custom CNN with established deep learning architectures, including ResNet50, VGG16, MobileNetV2, and DenseNet121. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied. The proposed system shows how deep learning can provide quick, accurate and understandable malaria diagnosis, especially in areas with limited resources.

Abstract (中文翻译): 疟疾在热带和亚热带地区仍是一个普遍的健康问题，由受感染的雌性按蚊叮咬传播疟原虫引起。传统的诊断方法（如显微镜下血涂片分析）灵敏度较低，依赖专家判断，且在偏远地区往往缺乏所需资源。为克服这些局限，本研究提出一种基于深度学习的方法，利用自定义的卷积神经网络（CNN）自动将血细胞图像分类为感染或未感染。该模型准确率达到96%，两类别的精确率和召回率均超过0.95。研究还将其与ResNet50、VGG16、MobileNetV2和DenseNet121等成熟深度学习架构进行了比较。为增强模型可解释性，采用了SHAP、LIME和显著图等可解释人工智能技术。所提出的系统展示了深度学习如何在资源有限的地区提供快速、准确且可理解的疟疾诊断。

</details>
