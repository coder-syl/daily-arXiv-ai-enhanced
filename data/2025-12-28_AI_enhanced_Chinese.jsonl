{"id": "2512.20735", "pdf": "https://arxiv.org/pdf/2512.20735", "abs": "https://arxiv.org/abs/2512.20735", "authors": ["Shijing Wang", "Chaoqun Cui", "Yaping Huang", "Hyung Jin Chang", "Yihua Cheng"], "title": "VL4Gaze: Unleashing Vision-Language Models for Gaze Following", "categories": ["cs.CV"], "comment": null, "summary": "Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa VL4Gaze\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7406\u89e3\u4eba\u7c7b\u51dd\u89c6\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b489K\u81ea\u52a8\u751f\u6210\u7684\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u56db\u4e2a\u4e92\u8865\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709VLM\u5728\u7f3a\u4e4f\u9488\u5bf9\u6027\u76d1\u7763\u65f6\u96be\u4ee5\u6709\u6548\u7406\u89e3\u51dd\u89c6\u8bed\u4e49\u4e0e\u5b9a\u4f4d\uff0c\u800c\u57fa\u4e8eVL4Gaze\u7684\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u867d\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u672a\u7cfb\u7edf\u7814\u7a76\u5176\u5bf9\u4eba\u7c7b\u51dd\u89c6\uff08gaze\uff09\u7684\u7406\u89e3\u80fd\u529b\u3002\u7531\u4e8e\u7f3a\u4e4f\u4e13\u95e8\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c1a\u4e0d\u6e05\u695a\u51dd\u89c6\u7406\u89e3\u80fd\u5426\u901a\u8fc7\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u81ea\u7136\u6d8c\u73b0\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86 VL4Gaze \u57fa\u51c6\uff0c\u5305\u542b124K\u56fe\u50cf\u548c489K\u81ea\u52a8\u751f\u6210\u7684\u95ee\u7b54\u5bf9\uff0c\u5c06\u51dd\u89c6\u7406\u89e3\u5efa\u6a21\u4e3a\u7edf\u4e00\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u56db\u4e2a\u4e92\u8865\u4efb\u52a1\uff1a\u51dd\u89c6\u5bf9\u8c61\u63cf\u8ff0\u3001\u51dd\u89c6\u65b9\u5411\u63cf\u8ff0\u3001\u51dd\u89c6\u70b9\u5b9a\u4f4d\u548c\u6a21\u7cca\u95ee\u9898\u8bc6\u522b\u3002\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u591a\u4e2a\u5f00\u6e90\u4e0e\u5546\u7528VLM\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u5927\u89c4\u6a21VLM\u5728\u65e0\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u4e5f\u96be\u4ee5\u53ef\u9760\u5730\u63a8\u65ad\u51dd\u89c6\u8bed\u4e49\u548c\u7a7a\u95f4\u4f4d\u7f6e\uff1b\u800c\u5728 VL4Gaze \u4e0a\u8bad\u7ec3\u540e\uff0c\u6240\u6709\u4efb\u52a1\u6027\u80fd\u5747\u663e\u8457\u4e14\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "\u9488\u5bf9\u51dd\u89c6\u7406\u89e3\u7684\u591a\u4efb\u52a1\u76d1\u7763\u5bf9\u63d0\u5347VLM\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u901a\u7528\u9884\u8bad\u7ec3\u4e0d\u8db3\u4ee5\u81ea\u52a8\u83b7\u5f97\u8be5\u80fd\u529b\u3002\u4f5c\u8005\u5c06\u516c\u5f00\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u4ee5\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002", "summary_cn": "\u4eba\u7c7b\u51dd\u89c6\u4e3a\u7406\u89e3\u89c6\u89c9\u573a\u666f\u4e2d\u7684\u6ce8\u610f\u529b\u3001\u610f\u56fe\u548c\u793e\u4f1a\u4e92\u52a8\u63d0\u4f9b\u4e86\u5173\u952e\u7ebf\u7d22\uff0c\u7136\u800c\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u51dd\u89c6\u7406\u89e3\u65b9\u9762\u4ecd\u9c9c\u6709\u63a2\u7d22\u3002\u5c3d\u7ba1\u8fd1\u671f\u7684VLM\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u573a\u666f\u7ea7\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5c1a\u65e0\u7cfb\u7edf\u6027\u8bc4\u4f30\u6216\u8bad\u7ec3\u5176\u51dd\u89c6\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u56e0\u6b64\u5c1a\u4e0d\u6e05\u695a\u51dd\u89c6\u7406\u89e3\u662f\u5426\u80fd\u4ece\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 VL4Gaze\u2014\u2014\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u65e8\u5728\u63a2\u7a76\u3001\u8bc4\u4f30\u5e76\u91ca\u653eVLM\u5728\u51dd\u89c6\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002VL4Gaze \u5305\u542b124K\u5f20\u56fe\u50cf\u4e0a\u768448.9\u4e07\u6761\u81ea\u52a8\u751f\u6210\u7684\u95ee\u7b54\u5bf9\uff0c\u5e76\u901a\u8fc7\u56db\u9879\u4e92\u8865\u4efb\u52a1\u5c06\u51dd\u89c6\u7406\u89e3\u5f62\u5f0f\u5316\u4e3a\u7edf\u4e00\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u95ee\u9898\uff1a\uff081\uff09\u51dd\u89c6\u5bf9\u8c61\u63cf\u8ff0\uff0c\uff082\uff09\u51dd\u89c6\u65b9\u5411\u63cf\u8ff0\uff0c\uff083\uff09\u51dd\u89c6\u70b9\u5b9a\u4f4d\uff0c\u4ee5\u53ca\uff084\uff09\u6a21\u7cca\u95ee\u9898\u8bc6\u522b\u3002\u6211\u4eec\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u5168\u9762\u8bc4\u4f30\u4e86\u591a\u4e2a\u5546\u7528\u548c\u5f00\u6e90VLM\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5927\u89c4\u6a21VLM\u5728\u7f3a\u4e4f\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u4e5f\u96be\u4ee5\u53ef\u9760\u5730\u63a8\u65ad\u51dd\u89c6\u8bed\u4e49\u548c\u7a7a\u95f4\u5b9a\u4f4d\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728 VL4Gaze \u4e0a\u8fdb\u884c\u8bad\u7ec3\u53ef\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5e26\u6765\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u51f8\u663e\u4e86\u9488\u5bf9\u6027\u591a\u4efb\u52a1\u76d1\u7763\u5bf9\u4e8e\u53d1\u5c55VLM\u51dd\u89c6\u7406\u89e3\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u8be5\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4ee5\u652f\u6301\u8be5\u65b9\u5411\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0e\u5f00\u53d1\u3002"}}
{"id": "2512.20746", "pdf": "https://arxiv.org/pdf/2512.20746", "abs": "https://arxiv.org/abs/2512.20746", "authors": ["Tony Tran", "Bin Hu"], "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages. The paper has been accepted by the WACV 2026 workshop", "summary": "This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$\u03bc$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u5728TACO\u6570\u636e\u96c6\u4e0a\u4e3aTinyML\u8bbe\u5907\u9ad8\u6548\u641c\u7d22\u51fa\u4e00\u7cfb\u5217\u540d\u4e3aTrashDets\u7684\u8f7b\u91cf\u7ea7\u5783\u573e\u68c0\u6d4b\u5668\uff0c\u5728\u7cbe\u5ea6\u3001\u53c2\u6570\u91cf\u548c\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u8d44\u6e90\u6781\u5ea6\u53d7\u9650\u7684TinyML\u8fb9\u7f18\u548c\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5783\u573e\u68c0\u6d4b\uff0c\u9700\u8981\u5728\u4e25\u683c\u7ea6\u675f\u4e0b\u8bbe\u8ba1\u517c\u987e\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u6a21\u578b\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8fed\u4ee3\u5f0f\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff1a\u6784\u5efaOnce-for-All\u98ce\u683c\u7684ResDets\u8d85\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u9aa8\u5e72\u7f51\u7edc\u4e0e\u68c0\u6d4b\u5934/\u9888\u90e8\u7684\u8fdb\u5316\u641c\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u79cd\u7fa4\u4f20\u9012\u673a\u5236\u548c\u7cbe\u5ea6\u9884\u6d4b\u5668\u4ee5\u964d\u4f4e\u641c\u7d22\u6210\u672c\u5e76\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "result": "\u5728TACO\u4e94\u7c7b\u5b50\u96c6\u4e0a\uff0c\u6700\u5f3a\u6a21\u578bTrashDet-l\u8fbe\u523019.5 mAP50\uff0c\u53c2\u6570\u4ec530.5M\uff0c\u6bd4\u5148\u524d\u5de5\u4f5c\u6700\u9ad8\u63d0\u53473.6 mAP50\u4e14\u53c2\u6570\u66f4\u5c11\uff1b\u6574\u4e2aTrashDet\u7cfb\u5217\u8986\u76d61.2M\u201330.5M\u53c2\u6570\u8303\u56f4\u3002\u5728MAX78002\u5fae\u63a7\u5236\u5668\u4e0a\uff0cTrashDet-ResNet\u548cTrashDet-MBNet\u5206\u522b\u5728\u80fd\u6548\u548c\u7cbe\u5ea6\u4e0a\u5927\u5e45\u8d85\u8d8a\u57fa\u7ebf\uff0c\u6700\u9ad8\u964d\u4f4e88%\u80fd\u8017\u300178%\u5ef6\u8fdf\u548c53%\u5e73\u5747\u529f\u8017\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u6210\u529f\u751f\u6210\u4e86\u4e00\u65cf\u9002\u7528\u4e8e\u4e0d\u540cTinyML\u90e8\u7f72\u9884\u7b97\u7684\u9ad8\u6027\u80fd\u3001\u4f4e\u529f\u8017\u5783\u573e\u68c0\u6d4b\u5668\uff0c\u5728\u7cbe\u5ea6\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709TinyML\u68c0\u6d4b\u5668\uff0c\u9a8c\u8bc1\u4e86\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "summary_cn": "\u672c\u6587\u9488\u5bf9TACO\u6570\u636e\u96c6\u4e0a\u7684\u5783\u573e\u68c0\u6d4b\u4efb\u52a1\uff0c\u5728\u4e25\u683c\u7684TinyML\u7ea6\u675f\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7684\u8fed\u4ee3\u5f0f\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2aOnce-for-All\u98ce\u683c\u7684ResDets\u8d85\u7f51\u7edc\uff0c\u5e76\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u9aa8\u5e72\u7f51\u7edc\u4e0e\u68c0\u6d4b\u5934/\u9888\u90e8\u7684\u8fed\u4ee3\u8fdb\u5316\u641c\u7d22\u7b56\u7565\uff0c\u8f85\u4ee5\u79cd\u7fa4\u4f20\u9012\u673a\u5236\u548c\u7cbe\u5ea6\u9884\u6d4b\u5668\uff0c\u4ee5\u964d\u4f4e\u641c\u7d22\u6210\u672c\u5e76\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002\u8be5\u6846\u67b6\u751f\u6210\u4e86\u4e00\u7cfb\u5217\u53ef\u76f4\u63a5\u90e8\u7f72\u7684\u68c0\u6d4b\u5668\uff0c\u79f0\u4e3aTrashDets\u3002\u5728\u5305\u542b\u7eb8\u5f20\u3001\u5851\u6599\u3001\u74f6\u5b50\u3001\u6613\u62c9\u7f50\u548c\u70df\u5934\u7684\u4e94\u7c7bTACO\u5b50\u96c6\u4e0a\uff0c\u6027\u80fd\u6700\u5f3a\u7684\u53d8\u4f53TrashDet-l\u4ee530.5M\u53c2\u6570\u5b9e\u73b0\u4e8619.5 mAP50\u7684\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u5148\u524d\u7684\u68c0\u6d4b\u5668\u6700\u9ad8\u63d0\u53473.6 mAP50\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\u3002TrashDet\u7cfb\u5217\u6a21\u578b\u53c2\u6570\u91cf\u8986\u76d61.2M\u81f330.5M\uff0cmAP50\u572811.4\u81f319.5\u4e4b\u95f4\uff0c\u4e3a\u4e0d\u540cTinyML\u90e8\u7f72\u9884\u7b97\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9009\u62e9\u3002\u5728MAX78002\u5fae\u63a7\u5236\u5668\u4e0a\u4f7f\u7528TrashNet\u6570\u636e\u96c6\u65f6\uff0c\u4e24\u4e2a\u4e13\u7528\u53d8\u4f53TrashDet-ResNet\u548cTrashDet-MBNet\u5171\u540c\u4f18\u4e8eai87-fpndetector\u57fa\u7ebf\uff1aTrashDet-ResNet\u5355\u6b21\u63a8\u7406\u80fd\u8017\u4e3a7525\u5fae\u7126\uff0c\u5ef6\u8fdf26.7\u6beb\u79d2\uff0c\u5e27\u7387\u8fbe37.45 FPS\uff1bTrashDet-MBNet\u5219\u5c06mAP50\u63d0\u5347\u4e8610.2%\u3002\u4e24\u8005\u76f8\u8f83\u73b0\u6709TinyML\u68c0\u6d4b\u5668\uff0c\u6700\u9ad8\u53ef\u51cf\u5c1188%\u7684\u80fd\u8017\u300178%\u7684\u5ef6\u8fdf\u548c53%\u7684\u5e73\u5747\u529f\u8017\u3002"}}
{"id": "2512.20770", "pdf": "https://arxiv.org/pdf/2512.20770", "abs": "https://arxiv.org/abs/2512.20770", "authors": ["Markus Gross", "Sai B. Matha", "Aya Fahmy", "Rui Song", "Daniel Cremers", "Henri Meess"], "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86OccuFly\uff0c\u9996\u4e2a\u57fa\u4e8e\u76f8\u673a\u7684\u771f\u5b9e\u4e16\u754c\u7a7a\u4e2d\u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u9700LiDAR\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u4f20\u7edf3D\u91cd\u5efa\u6280\u672f\u81ea\u52a8\u5c062D\u6807\u6ce8\u63d0\u5347\u81f33D\u70b9\u4e91\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u591a\u79cd\u5b63\u8282\u3001\u9ad8\u5ea6\u548c\u573a\u666f\u7c7b\u578b\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u9ad8\u7a7a\u89c6\u89d2\u4e0b\u76843D\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5730\u9762\u573a\u666f\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\uff0c\u800c\u7a7a\u4e2d\u573a\u666f\uff08\u5982\u81ea\u4e3b\u98de\u884c\uff09\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff1b\u540c\u65f6\uff0c\u4e3b\u6d41\u4f9d\u8d56LiDAR\u4f20\u611f\u5668\u7684SSC\u65b9\u6cd5\u96be\u4ee5\u9002\u7528\u4e8e\u53d7\u6cd5\u89c4\u3001\u91cd\u91cf\u4e0e\u80fd\u8017\u9650\u5236\u7684\u65e0\u4eba\u673a\uff0c\u4e14\u9ad8\u7a7a\u89c6\u89d2\u4e0bLiDAR\u70b9\u4e91\u7a00\u758f\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u57fa\u4e8e\u76f8\u673a\u6a21\u6001\u7684\u7a7a\u4e2dSSC\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86OccuFly\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0d\u540c\u5b63\u8282\u3001\u9ad8\u5ea6\uff0850m/40m/30m\uff09\u53ca\u57ce\u4e61\u5de5\u4e1a\u7b49\u591a\u6837\u5316\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0LiDAR\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff1a\u901a\u8fc7\u4f20\u7edf3D\u91cd\u5efa\u65b9\u6cd5\u5c06\u90e8\u5206\u5e26\u6807\u6ce8\u76842D\u56fe\u50cf\u63a9\u7801\u81ea\u52a8\u63d0\u5347\u5230\u91cd\u5efa\u76843D\u70b9\u4e91\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u65483D\u8bed\u4e49\u6807\u6ce8\u3002", "result": "OccuFly\u63d0\u4f9b22\u4e2a\u8bed\u4e49\u7c7b\u522b\uff0c\u683c\u5f0f\u517c\u5bb9\u73b0\u6709\u7814\u7a76\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5bf9\u5f53\u524d\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u9ad8\u7a7a\u89c6\u89d2\u4e0bSSC\u4efb\u52a1\u7684\u72ec\u7279\u6311\u6218\u3002", "conclusion": "OccuFly\u662f\u9996\u4e2a\u57fa\u4e8e\u76f8\u673a\u7684\u771f\u5b9e\u7a7a\u4e2dSSC\u57fa\u51c6\uff0c\u5176\u63d0\u51fa\u7684\u65e0LiDAR\u6807\u6ce8\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e863D\u6807\u6ce8\u6210\u672c\uff0c\u4e3a\u63a8\u52a8\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u7684\u6574\u4f533D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002", "summary_cn": "\u8bed\u4e49\u573a\u666f\u8865\u5168\uff08SSC\uff09\u5bf9\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u4e2d\u7684\u4e09\u7ef4\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u5bc6\u96c6\u4f53\u7d20\u5360\u7528\u7387\u548c\u6bcf\u4e2a\u4f53\u7d20\u7684\u8bed\u4e49\u4fe1\u606f\u6765\u5b9e\u73b0\u5bf9\u573a\u666f\u7684\u6574\u4f53\u7406\u89e3\u3002\u5c3d\u7ba1SSC\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5730\u9762\u9886\u57df\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u81ea\u4e3b\u98de\u884c\u7b49\u7a7a\u4e2d\u573a\u666f\u4e2d\u4ecd\u9c9c\u6709\u63a2\u7d22\uff0c\u4ece\u800c\u9650\u5236\u4e86\u4e0b\u6e38\u5e94\u7528\u7684\u53d1\u5c55\u3002\u6b64\u5916\uff0c\u6fc0\u5149\u96f7\u8fbe\uff08LiDAR\uff09\u4f20\u611f\u5668\u662f\u76ee\u524dSSC\u6570\u636e\u751f\u6210\u7684\u4e3b\u8981\u6a21\u6001\uff0c\u4f46\u7531\u4e8e\u98de\u884c\u6cd5\u89c4\u3001\u8d28\u91cf\u548c\u80fd\u8017\u9650\u5236\uff0c\u4ee5\u53ca\u4ece\u9ad8\u89c6\u89d2\u83b7\u53d6\u7684LiDAR\u70b9\u4e91\u7a00\u758f\u6027\uff0c\u8fd9\u5bf9\u5927\u591a\u6570\u65e0\u4eba\u98de\u884c\u5668\uff08UAV\uff09\u6784\u6210\u4e86\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OccuFly\u2014\u2014\u9996\u4e2a\u57fa\u4e8e\u76f8\u673a\u7684\u771f\u5b9e\u4e16\u754c\u7a7a\u4e2dSSC\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u6625\u5b63\u3001\u590f\u5b63\u3001\u79cb\u5b63\u548c\u51ac\u5b63\u5206\u522b\u4e8e50\u7c73\u300140\u7c73\u548c30\u7c73\u7684\u9ad8\u5ea6\u91c7\u96c6\uff0c\u6db5\u76d6\u57ce\u5e02\u3001\u5de5\u4e1a\u548c\u4e61\u6751\u573a\u666f\uff0c\u63d0\u4f9b22\u4e2a\u8bed\u4e49\u7c7b\u522b\uff0c\u4e14\u6570\u636e\u683c\u5f0f\u9075\u5faa\u65e2\u5b9a\u89c4\u8303\u4ee5\u65b9\u4fbf\u4e0e\u73b0\u6709\u7814\u7a76\u65e0\u7f1d\u96c6\u6210\u3002\u5c24\u4e3a\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u673a\u6a21\u6001\u7684\u65e0LiDAR\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6a21\u6001\u5728\u73b0\u4ee3UAV\u4e0a\u666e\u904d\u5b58\u5728\u3002\u901a\u8fc7\u5229\u7528\u4f20\u7edf\u7684\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\uff0c\u6211\u4eec\u7684\u6846\u67b6\u53ef\u5c06\u90e8\u5206\u5e26\u6ce8\u91ca\u7684\u4e8c\u7ef4\u63a9\u7801\u81ea\u52a8\u63d0\u5347\u81f3\u91cd\u5efa\u7684\u70b9\u4e91\u4e2d\uff0c\u4ece\u800c\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u4e09\u7ef4\u6807\u6ce8\u7684\u5de5\u4f5c\u91cf\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728OccuFly\u4e0a\u5bf9\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u7a81\u51fa\u4e86\u9ad8\u89c6\u89d2\u4e0b\u7279\u6709\u7684\u6311\u6218\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u89c6\u89c9\u57fa\u51c6\uff0c\u7528\u4e8e\u6574\u4f53\u7a7a\u4e2d\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2512.20783", "pdf": "https://arxiv.org/pdf/2512.20783", "abs": "https://arxiv.org/abs/2512.20783", "authors": ["Raja Mallina", "Bryar Shareef"], "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "5 pages, 2 figures, and 4 tables", "summary": "Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.", "AI": {"tldr": "NullBUS \u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u6df7\u5408\u76d1\u7763\u6846\u67b6\uff0c\u53ef\u5728\u4e73\u817a\u8d85\u58f0\u56fe\u50cf\u4e2d\u540c\u65f6\u5229\u7528\u6709\u63d0\u793a\u548c\u65e0\u63d0\u793a\u7684\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u53ef\u7a7a\u63d0\u793a\u673a\u5236\u5728\u7f3a\u5c11\u6587\u672c\u5143\u6570\u636e\u65f6\u56de\u9000\u5230\u4ec5\u56fe\u50cf\u6a21\u5f0f\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230 SOTA \u6027\u80fd\u3002", "motivation": "\u8bb8\u591a\u516c\u5f00\u7684\u4e73\u817a\u8d85\u58f0\uff08BUS\uff09\u6570\u636e\u96c6\u7f3a\u4e4f\u53ef\u9760\u7684\u5143\u6570\u636e\u6216\u62a5\u544a\uff0c\u5bfc\u81f4\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u65b9\u6cd5\u53ea\u80fd\u5728\u5c0f\u89c4\u6a21\u591a\u6a21\u6001\u5b50\u96c6\u4e0a\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa NullBUS \u6846\u67b6\uff0c\u5f15\u5165\u201c\u53ef\u7a7a\u63d0\u793a\u201d\u673a\u5236\u2014\u2014\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u7a7a\u5d4c\u5165\u548c\u5b58\u5728\u63a9\u7801\uff0c\u5728\u5143\u6570\u636e\u7f3a\u5931\u65f6\u56de\u9000\u81f3\u4ec5\u56fe\u50cf\u8bc1\u636e\uff0c\u800c\u5728\u5143\u6570\u636e\u5b58\u5728\u65f6\u5229\u7528\u6587\u672c\u63d0\u793a\uff0c\u5b9e\u73b0\u5355\u6a21\u578b\u5bf9\u6709/\u65e0\u63d0\u793a\u56fe\u50cf\u7684\u7edf\u4e00\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00 BUS \u6570\u636e\u96c6\u7684\u7edf\u4e00\u6d4b\u8bd5\u6c60\u4e2d\uff0cNullBUS \u8fbe\u5230\u5e73\u5747 IoU 0.8568 \u548c\u5e73\u5747 Dice 0.9103\uff0c\u5c55\u73b0\u51fa\u5728\u6df7\u5408\u63d0\u793a\u53ef\u7528\u6761\u4ef6\u4e0b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "NullBUS \u6709\u6548\u89e3\u51b3\u4e86\u4e73\u817a\u8d85\u58f0\u56fe\u50cf\u4e2d\u63d0\u793a\u4fe1\u606f\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "summary_cn": "\u4e73\u817a\u8d85\u58f0\uff08BUS\uff09\u5206\u5272\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u75c5\u7076\u8fb9\u754c\u4fe1\u606f\u3002\u5c3d\u7ba1\u53ef\u63d0\u793a\u65b9\u6cd5\u5728\u63d0\u4f9b\u6587\u672c\u6216\u7a7a\u95f4\u63d0\u793a\u65f6\u80fd\u591f\u63d0\u5347\u5206\u5272\u6027\u80fd\u548c\u80bf\u7624\u8f6e\u5ed3\u63cf\u7ed8\u6548\u679c\uff0c\u4f46\u8bb8\u591a\u516c\u5f00\u7684 BUS \u6570\u636e\u96c6\u7f3a\u4e4f\u53ef\u9760\u7684\u5143\u6570\u636e\u6216\u62a5\u544a\uff0c\u5bfc\u81f4\u8bad\u7ec3\u53ea\u80fd\u5c40\u9650\u4e8e\u5c0f\u89c4\u6a21\u7684\u591a\u6a21\u6001\u5b50\u96c6\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86 NullBUS\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6df7\u5408\u76d1\u7763\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u540c\u65f6\u4ece\u6709\u63d0\u793a\u548c\u65e0\u63d0\u793a\u7684\u56fe\u50cf\u4e2d\u5b66\u4e60\u3002\u4e3a\u5904\u7406\u7f3a\u5931\u7684\u6587\u672c\u4fe1\u606f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u201c\u53ef\u7a7a\u63d0\u793a\u201d\u673a\u5236\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7a7a\u5d4c\u5165\u548c\u5b58\u5728\u63a9\u7801\u5b9e\u73b0\uff1a\u5f53\u5143\u6570\u636e\u7f3a\u5931\u65f6\uff0c\u6a21\u578b\u53ef\u56de\u9000\u81f3\u4ec5\u4f9d\u8d56\u56fe\u50cf\u8bc1\u636e\uff1b\u5f53\u5143\u6570\u636e\u5b58\u5728\u65f6\uff0c\u5219\u53ef\u5229\u7528\u6587\u672c\u63d0\u793a\u3002\u5728\u4e09\u4e2a\u516c\u5f00 BUS \u6570\u636e\u96c6\u6784\u6210\u7684\u7edf\u4e00\u6d4b\u8bd5\u6c60\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cNullBUS \u5b9e\u73b0\u4e86\u5e73\u5747 IoU \u4e3a 0.8568\u3001\u5e73\u5747 Dice \u4e3a 0.9103 \u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u6df7\u5408\u63d0\u793a\u53ef\u7528\u6761\u4ef6\u4e0b\u6700\u5148\u8fdb\u7684\u5206\u5272\u6548\u679c\u3002"}}
{"id": "2512.20815", "pdf": "https://arxiv.org/pdf/2512.20815", "abs": "https://arxiv.org/abs/2512.20815", "authors": ["Reeshad Khan amd John Gauch"], "title": "Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4efb\u52a1\u9a71\u52a8\u7684\u7aef\u5230\u7aefRAW-to-task\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c06\u5149\u5b66\u3001\u4f20\u611f\u5668\u5efa\u6a21\u4e0e\u8f7b\u91cf\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u7edf\u4e00\u4f18\u5316\uff0c\u5728KITTI-360\u4e0a\u663e\u8457\u63d0\u5347mIoU\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5c0f\u5de7\uff08\u7ea61M\u53c2\u6570\uff09\u548c\u5b9e\u65f6\u6027\uff08~28 FPS\uff09\uff0c\u8bc1\u660e\u4e86\u5168\u6808\u534f\u540c\u4f18\u5316\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5c06\u76f8\u673a\u8bbe\u8ba1\u4e0e\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u89e3\u8026\uff0c\u91c7\u7528\u56fa\u5b9a\u5149\u5b66\u7cfb\u7edf\u548c\u4eba\u5de5\u8bbe\u8ba1\u7684\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\uff08ISP\uff09\u6d41\u7a0b\uff0c\u4f18\u5148\u8003\u8651\u4eba\u773c\u53ef\u89c6\u56fe\u50cf\u800c\u975e\u673a\u5668\u8bed\u4e49\uff0c\u5bfc\u81f4\u5728\u53bb\u9a6c\u8d5b\u514b\u3001\u53bb\u566a\u6216\u91cf\u5316\u8fc7\u7a0b\u4e2d\u4e22\u5931\u5bf9\u611f\u77e5\u4efb\u52a1\u6709\u7528\u7684\u4fe1\u606f\uff0c\u5e76\u8feb\u4f7f\u6a21\u578b\u9002\u5e94\u4f20\u611f\u5668\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684RAW-to-task\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u6574\u5408\u771f\u5b9e\u624b\u673a\u7ea7\u955c\u5934\u6a21\u578b\u3001\u53ef\u5b66\u4e60\u5f69\u8272\u6ee4\u5149\u9635\u5217\uff08CFA\uff09\u3001\u6cca\u677e-\u9ad8\u65af\u566a\u58f0\u6a21\u578b\u548c\u91cf\u5316\u8fc7\u7a0b\uff0c\u5e76\u76f4\u63a5\u9488\u5bf9\u8bed\u4e49\u5206\u5272\u76ee\u6807\u8054\u5408\u4f18\u5316\u5149\u5b66\u3001\u4f20\u611f\u5668\u4e0e\u8f7b\u91cf\u5206\u5272\u7f51\u7edc\u3002", "result": "\u5728KITTI-360\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u56fa\u5b9a\u6d41\u6c34\u7ebf\u6301\u7eed\u63d0\u5347mIoU\uff0c\u5176\u4e2d\u5149\u5b66\u5efa\u6a21\u548cCFA\u5b66\u4e60\u5e26\u6765\u6700\u5927\u589e\u76ca\uff0c\u5c24\u5176\u5bf9\u7ec6\u957f\u6216\u4f4e\u5149\u7167\u654f\u611f\u7c7b\u522b\uff1b\u6a21\u578b\u4ec5\u542b\u7ea6100\u4e07\u53c2\u6570\uff0c\u8fd0\u884c\u901f\u5ea6\u8fbe28 FPS\uff0c\u5177\u5907\u8fb9\u7f18\u90e8\u7f72\u80fd\u529b\uff1b\u89c6\u89c9\u4e0e\u5b9a\u91cf\u5206\u6790\u8868\u660e\u534f\u540c\u8bbe\u8ba1\u7684\u4f20\u611f\u5668\u80fd\u6839\u636e\u8bed\u4e49\u7ed3\u6784\u81ea\u9002\u5e94\u91c7\u96c6\uff0c\u589e\u5f3a\u8fb9\u754c\u5e76\u7ef4\u6301\u5728\u6a21\u7cca\u3001\u566a\u58f0\u548c\u4f4e\u4f4d\u6df1\u6761\u4ef6\u4e0b\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u5168\u6808\u534f\u540c\u4f18\u5316\uff08\u5149\u5b66+\u4f20\u611f\u5668+\u7f51\u7edc\uff09\u662f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9760\u4e14\u53ef\u90e8\u7f72\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u6709\u6548\u8def\u5f84\u3002", "summary_cn": "\u4f20\u7edf\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5c06\u76f8\u673a\u8bbe\u8ba1\u4e0e\u4e0b\u6e38\u611f\u77e5\u4efb\u52a1\u89e3\u8026\uff0c\u4f9d\u8d56\u56fa\u5b9a\u7684\u5149\u5b66\u7cfb\u7edf\u548c\u4eba\u5de5\u8bbe\u8ba1\u7684\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\uff08ISP\uff09\u6d41\u7a0b\uff0c\u8fd9\u4e9b\u6d41\u7a0b\u4f18\u5148\u8003\u8651\u4eba\u773c\u53ef\u89c6\u56fe\u50cf\u800c\u975e\u673a\u5668\u8bed\u4e49\u3002\u8fd9\u79cd\u5206\u79bb\u65b9\u5f0f\u5728\u53bb\u9a6c\u8d5b\u514b\u3001\u53bb\u566a\u6216\u91cf\u5316\u8fc7\u7a0b\u4e2d\u4e22\u5f03\u4e86\u6709\u7528\u4fe1\u606f\uff0c\u540c\u65f6\u8feb\u4f7f\u6a21\u578b\u53bb\u9002\u5e94\u4f20\u611f\u5668\u5f15\u5165\u7684\u4f2a\u5f71\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u9a71\u52a8\u7684\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c06\u5149\u5b66\u3001\u4f20\u611f\u5668\u5efa\u6a21\u4e0e\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u7edf\u4e00\u4e3a\u5355\u4e00\u7684\u7aef\u5230\u7aefRAW\u5230\u4efb\u52a1\uff08RAW-to-task\uff09\u6d41\u6c34\u7ebf\u3002\u8be5\u7cfb\u7edf\u57fa\u4e8eDeepLens[19]\uff0c\u6574\u5408\u4e86\u903c\u771f\u7684\u624b\u673a\u7ea7\u955c\u5934\u6a21\u578b\u3001\u53ef\u5b66\u4e60\u7684\u5f69\u8272\u6ee4\u5149\u9635\u5217\uff08CFA\uff09\u3001\u6cca\u677e-\u9ad8\u65af\u566a\u58f0\u8fc7\u7a0b\u4ee5\u53ca\u91cf\u5316\u64cd\u4f5c\uff0c\u5e76\u76f4\u63a5\u9488\u5bf9\u5206\u5272\u76ee\u6807\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u5728KITTI-360\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u8f83\u4e8e\u56fa\u5b9a\u6d41\u6c34\u7ebf\u59cb\u7ec8\u53d6\u5f97\u66f4\u9ad8\u7684mIoU\uff0c\u5176\u4e2d\u5149\u5b66\u5efa\u6a21\u548cCFA\u5b66\u4e60\u5e26\u6765\u4e86\u6700\u5927\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u5bf9\u7ec6\u957f\u7269\u4f53\u6216\u5bf9\u4f4e\u5149\u7167\u654f\u611f\u7684\u7c7b\u522b\u6548\u679c\u663e\u8457\u3002\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4e9b\u9c81\u68d2\u6027\u63d0\u5347\u662f\u5728\u4e00\u4e2a\u7d27\u51d1\u7684\u7ea6100\u4e07\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b0\u7684\uff0c\u8fd0\u884c\u901f\u5ea6\u7ea6\u4e3a28 FPS\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u8fb9\u7f18\u90e8\u7f72\u80fd\u529b\u3002\u89c6\u89c9\u548c\u5b9a\u91cf\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\uff0c\u534f\u540c\u8bbe\u8ba1\u7684\u4f20\u611f\u5668\u80fd\u591f\u6839\u636e\u8bed\u4e49\u7ed3\u6784\u81ea\u9002\u5e94\u5730\u8c03\u6574\u56fe\u50cf\u91c7\u96c6\u8fc7\u7a0b\uff0c\u5728\u6a21\u7cca\u3001\u566a\u58f0\u548c\u4f4e\u4f4d\u6df1\u6761\u4ef6\u4e0b\u4ecd\u80fd\u9510\u5316\u8fb9\u754c\u5e76\u4fdd\u6301\u51c6\u786e\u7387\u3002\u8fd9\u4e9b\u53d1\u73b0\u5171\u540c\u8868\u660e\uff0c\u5bf9\u5149\u5b66\u3001\u4f20\u611f\u5668\u548c\u7f51\u7edc\u8fdb\u884c\u5168\u6808\u534f\u540c\u4f18\u5316\uff0c\u662f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9760\u4e14\u53ef\u90e8\u7f72\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2512.20833", "pdf": "https://arxiv.org/pdf/2512.20833", "abs": "https://arxiv.org/abs/2512.20833", "authors": ["Vidit Agrawal", "John Peters", "Tyler N. Thompson", "Mohammad Vali Sanian", "Chau Pham", "Nikita Moshkov", "Arshad Kazi", "Aditya Pillai", "Jack Freeman", "Byunguk Kang", "Samouil L. Farhi", "Ernest Fraenkel", "Ron Stewart", "Lassi Paavolainen", "Bryan A. Plummer", "Juan C. Caicedo"], "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images", "categories": ["cs.CV", "cs.LG"], "comment": "47 Pages, 23 Figures, 26 Tables", "summary": "Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CHAMMI-75\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea75\u9879\u7814\u7a76\u7684\u591a\u901a\u9053\u663e\u5fae\u56fe\u50cf\uff0c\u7528\u4e8e\u8bad\u7ec3\u80fd\u9002\u5e94\u4e0d\u540c\u6210\u50cf\u6761\u4ef6\u7684\u7ec6\u80de\u5f62\u6001\u5b66\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7ec6\u80de\u5f62\u6001\u5b66\u91cf\u5316\u6a21\u578b\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u663e\u5fae\u6210\u50cf\u7c7b\u578b\u8bad\u7ec3\uff0c\u96be\u4ee5\u8de8\u7814\u7a76\u590d\u7528\uff0c\u56e0\u6280\u672f\u89c4\u683c\uff08\u5982\u901a\u9053\u6570\uff09\u6216\u5b9e\u9a8c\u6761\u4ef6\u5dee\u5f02\u5bfc\u81f4\u5206\u5e03\u5916\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5e76\u516c\u5f00\u53d1\u5e03CHAMMI-75\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6574\u5408\u4e8675\u9879\u4e0d\u540c\u751f\u7269\u7814\u7a76\u7684\u5f02\u6784\u591a\u901a\u9053\u663e\u5fae\u56fe\u50cf\uff0c\u7528\u4e8e\u8bad\u7ec3\u901a\u9053\u81ea\u9002\u5e94\u7684\u7ec6\u80de\u5f62\u6001\u6a21\u578b\u3002", "result": "\u5728CHAMMI-75\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u901a\u9053\u751f\u7269\u6210\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e3b\u8981\u5f97\u76ca\u4e8e\u5176\u663e\u5fae\u6210\u50cf\u6a21\u6001\u7684\u9ad8\u5ea6\u591a\u6837\u6027\u3002", "conclusion": "CHAMMI-75\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u591a\u79cd\u751f\u7269\u7814\u7a76\u7684\u65b0\u4e00\u4ee3\u7ec6\u80de\u5f62\u6001\u5b66\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "summary_cn": "\u5229\u7528\u56fe\u50cf\u548c\u673a\u5668\u5b66\u4e60\u5bf9\u7ec6\u80de\u5f62\u6001\u8fdb\u884c\u91cf\u5316\u5df2\u88ab\u8bc1\u660e\u662f\u7814\u7a76\u7ec6\u80de\u5bf9\u5904\u7406\u54cd\u5e94\u7684\u5f3a\u5927\u5de5\u5177\u3002\u7136\u800c\uff0c\u7528\u4e8e\u91cf\u5316\u7ec6\u80de\u5f62\u6001\u7684\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u7c7b\u578b\u7684\u663e\u5fae\u6210\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e13\u7528\u6a21\u578b\u65e0\u6cd5\u5728\u4e0d\u540c\u751f\u7269\u5b66\u7814\u7a76\u95f4\u590d\u7528\uff0c\u539f\u56e0\u5728\u4e8e\u6280\u672f\u89c4\u683c\u4e0d\u5339\u914d\uff08\u4f8b\u5982\u901a\u9053\u6570\u91cf\u4e0d\u540c\uff09\u6216\u76ee\u6807\u5b9e\u9a8c\u6761\u4ef6\u8d85\u51fa\u5206\u5e03\u8303\u56f4\u3002\u5728\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CHAMMI-75\u2014\u2014\u4e00\u4e2a\u5f00\u653e\u83b7\u53d6\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea75\u9879\u591a\u6837\u5316\u751f\u7269\u5b66\u7814\u7a76\u7684\u5f02\u6784\u591a\u901a\u9053\u663e\u5fae\u56fe\u50cf\u3002\u6211\u4eec\u4ece\u516c\u5f00\u8d44\u6e90\u4e2d\u6574\u7406\u4e86\u8fd9\u4e00\u6570\u636e\u96c6\uff0c\u65e8\u5728\u7814\u7a76\u80fd\u591f\u81ea\u9002\u5e94\u901a\u9053\u5e76\u5904\u7406\u4efb\u610f\u663e\u5fae\u56fe\u50cf\u7c7b\u578b\u7684\u7ec6\u80de\u5f62\u6001\u6a21\u578b\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528CHAMMI-75\u8fdb\u884c\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u591a\u901a\u9053\u751f\u7269\u6210\u50cf\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fd9\u4e3b\u8981\u5f52\u529f\u4e8e\u5176\u5728\u663e\u5fae\u6210\u50cf\u6a21\u6001\u65b9\u9762\u7684\u9ad8\u5ea6\u591a\u6837\u6027\u3002\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u9002\u7528\u4e8e\u5404\u7c7b\u751f\u7269\u5b66\u7814\u7a76\u7684\u65b0\u4e00\u4ee3\u7ec6\u80de\u5f62\u6001\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.20839", "pdf": "https://arxiv.org/pdf/2512.20839", "abs": "https://arxiv.org/abs/2512.20839", "authors": ["Putu Indah Githa Cahyani", "Komang David Dananjaya Suartana", "Novanto Yudistira"], "title": "Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u89c6\u89c9\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u53ef\u6839\u636e\u56fe\u50cf\u5185\u5bb9\u52a8\u6001\u8c03\u6574\u5206\u8fa8\u7387\u548c\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\uff0c\u5728\u4e0d\u4fee\u6539 FastVLM \u67b6\u6784\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\u548c\u89c6\u89c9 token \u6570\u91cf\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u90e8\u7f72\u65f6\u9762\u4e34\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u89c6\u89c9\u9884\u5904\u7406\uff0c\u5bf9\u7b80\u5355\u56fe\u50cf\u4ecd\u8fdb\u884c\u5197\u4f59\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u89c6\u89c9\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408\u5185\u5bb9\u611f\u77e5\u56fe\u50cf\u5206\u6790\u3001\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u9009\u62e9\u548c\u5185\u5bb9\u611f\u77e5\u88c1\u526a\uff0c\u5728\u9001\u5165\u89c6\u89c9\u7f16\u7801\u5668\u524d\u51cf\u5c11\u89c6\u89c9\u5197\u4f59\uff0c\u5e76\u4e0e FastVLM \u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u7ed3\u6784\u6216\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728 DocVQA \u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u6bcf\u5f20\u56fe\u50cf\u7684\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc7 50%\uff0c\u5e73\u5747\u751f\u6210\u65f6\u95f4\u4e0b\u964d\uff0c\u89c6\u89c9 token \u6570\u91cf\u51cf\u5c11\u8d85\u8fc7 55%\u3002", "conclusion": "\u5185\u5bb9\u611f\u77e5\u7684\u81ea\u9002\u5e94\u9884\u5904\u7406\u662f\u4e00\u79cd\u8f7b\u91cf\u4e14\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u6548\u7387\u3002", "summary_cn": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\uff0c\u4f46\u5176\u90e8\u7f72\u4ecd\u9762\u4e34\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u8f93\u5165\u65f6\u3002\u5c3d\u7ba1\u8fd1\u671f\u5982 FastVLM \u7b49\u67b6\u6784\u901a\u8fc7\u4f18\u5316\u89c6\u89c9\u7f16\u7801\u5668\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4f46\u73b0\u6709\u6d41\u7a0b\u4ecd\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u9884\u5904\u7406\uff0c\u5bfc\u81f4\u5bf9\u89c6\u89c9\u4e0a\u7b80\u5355\u7684\u8f93\u5165\u4ea7\u751f\u5197\u4f59\u8ba1\u7b97\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u89c6\u89c9\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u53ef\u6839\u636e\u56fe\u50cf\u5185\u5bb9\u7279\u5f81\u52a8\u6001\u8c03\u6574\u8f93\u5165\u5206\u8fa8\u7387\u548c\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u5185\u5bb9\u611f\u77e5\u7684\u56fe\u50cf\u5206\u6790\u3001\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u9009\u62e9\u4ee5\u53ca\u5185\u5bb9\u611f\u77e5\u88c1\u526a\uff0c\u5728\u89c6\u89c9\u7f16\u7801\u524d\u51cf\u5c11\u89c6\u89c9\u5197\u4f59\u3002\u91cd\u8981\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539 FastVLM \u7684\u67b6\u6784\u6216\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u96c6\u6210\u3002\u6211\u4eec\u5728 DocVQA \u6570\u636e\u96c6\u7684\u4e00\u4e2a\u5b50\u96c6\u4e0a\u4ee5\u4ec5\u63a8\u7406\u65b9\u5f0f\u8bc4\u4f30\u4e86\u6240\u63d0\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u9762\u5411\u6548\u7387\u7684\u6307\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u9884\u5904\u7406\u76f8\u6bd4\u57fa\u7ebf\u6d41\u7a0b\uff0c\u5c06\u6bcf\u5f20\u56fe\u50cf\u7684\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u8d85\u8fc7 50%\uff0c\u964d\u4f4e\u4e86\u5e73\u5747\u5b8c\u6574\u751f\u6210\u65f6\u95f4\uff0c\u5e76\u4f7f\u89c6\u89c9 token \u6570\u91cf\u6301\u7eed\u51cf\u5c11\u8d85\u8fc7 55%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u8f93\u5165\u5185\u5bb9\u7684\u9884\u5904\u7406\u662f\u4e00\u79cd\u6709\u6548\u4e14\u8f7b\u91cf\u7684\u7b56\u7565\uff0c\u53ef\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u6548\u7387\u3002\u4e3a\u4fbf\u4e8e\u590d\u73b0\uff0c\u6211\u4eec\u7684\u5b9e\u73b0\u4f5c\u4e3a FastVLM \u4ed3\u5e93\u7684\u4e00\u4e2a\u5206\u652f\u63d0\u4f9b\uff0c\u5305\u542b\u6240\u63d0\u65b9\u6cd5\u7684\u76f8\u5173\u6587\u4ef6\uff0c\u5730\u5740\u4e3a https://github.com/kmdavidds/mlfastlm\u3002"}}
{"id": "2512.20858", "pdf": "https://arxiv.org/pdf/2512.20858", "abs": "https://arxiv.org/abs/2512.20858", "authors": ["Md Zabirul Islam", "Md Motaleb Hossen Manik", "Ge Wang"], "title": "ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.\n  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.\n  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.\n  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.", "AI": {"tldr": "ALIVE \u662f\u4e00\u4e2a\u672c\u5730\u8fd0\u884c\u7684\u4ea4\u4e92\u5f0f\u8bb2\u5ea7\u89c6\u9891\u5f15\u64ce\uff0c\u7ed3\u5408\u795e\u7ecf\u5316\u8eab\u3001\u5185\u5bb9\u611f\u77e5\u68c0\u7d22\u4e0e\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u5c06\u4f20\u7edf\u88ab\u52a8\u89c2\u770b\u8f6c\u53d8\u4e3a\u5b9e\u65f6\u4e92\u52a8\u5b66\u4e60\u4f53\u9a8c\u3002", "motivation": "\u4f20\u7edf\u8bb2\u5ea7\u89c6\u9891\u7f3a\u4e4f\u5b9e\u65f6\u7b54\u7591\u673a\u5236\uff0c\u5b66\u4e60\u8005\u9047\u56f0\u60d1\u9700\u5916\u90e8\u641c\u7d22\uff1b\u73b0\u6709\u4ea4\u4e92\u7cfb\u7edf\u5e38\u7f3a\u4e4f\u5bf9\u8bb2\u5ea7\u5185\u5bb9\u7684\u7406\u89e3\u3001\u4f9d\u8d56\u4e91\u7aef\u670d\u52a1\uff0c\u6216\u672a\u80fd\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u7edf\u4e00\u6574\u5408\u68c0\u7d22\u4e0e\u5316\u8eab\u8bb2\u89e3\u3002", "method": "ALIVE \u5728\u672c\u5730\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u6574\u5408\u4e09\u9879\u6838\u5fc3\u6280\u672f\uff1a(1) \u57fa\u4e8e ASR \u8f6c\u5f55\u3001\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u548c\u795e\u7ecf\u8bf4\u8bdd\u5934\u5408\u6210\u7684\u5316\u8eab\u8bb2\u89e3\uff1b(2) \u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0e\u65f6\u95f4\u6233\u5bf9\u9f50\u7684\u5185\u5bb9\u611f\u77e5\u68c0\u7d22\u673a\u5236\uff1b(3) \u652f\u6301\u6587\u672c/\u8bed\u97f3\u63d0\u95ee\u5e76\u4ee5\u6587\u672c\u6216\u5316\u8eab\u5f62\u5f0f\u8fd4\u56de\u7b54\u6848\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u3002\u7cfb\u7edf\u91c7\u7528\u8f7b\u91cf\u5d4c\u5165\u6a21\u578b\u3001FAISS \u68c0\u7d22\u53ca\u5206\u6bb5\u9884\u52a0\u8f7d\u7b56\u7565\u4fdd\u969c\u54cd\u5e94\u901f\u5ea6\u3002", "result": "\u5728\u5b8c\u6574\u533b\u5b66\u5f71\u50cf\u8bfe\u7a0b\u4e0a\u9a8c\u8bc1\u4e86 ALIVE \u7684\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u8868\u73b0\u4e0e\u7528\u6237\u4f53\u9a8c\uff0c\u8bc1\u660e\u5176\u80fd\u63d0\u4f9b\u51c6\u786e\u3001\u5185\u5bb9\u76f8\u5173\u4e14\u5177\u5438\u5f15\u529b\u7684\u5b9e\u65f6\u5b66\u4e60\u652f\u6301\u3002", "conclusion": "ALIVE \u5c55\u793a\u4e86\u591a\u6a21\u6001 AI \u4e0e\u5185\u5bb9\u611f\u77e5\u68c0\u7d22\u3001\u672c\u5730\u90e8\u7f72\u76f8\u7ed3\u5408\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5f55\u64ad\u8bb2\u5ea7\u7684\u6559\u5b66\u4ef7\u503c\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u4ea4\u4e92\u5f0f\u5b66\u4e60\u73af\u5883\u63d0\u4f9b\u53ef\u6269\u5c55\u8def\u5f84\u3002", "summary_cn": "\u4f20\u7edf\u7684\u8bb2\u5ea7\u89c6\u9891\u867d\u5177\u7075\u6d3b\u6027\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u65f6\u6f84\u6e05\u673a\u5236\uff0c\u8feb\u4f7f\u5b66\u4e60\u8005\u5728\u56f0\u60d1\u65f6\u9700\u501f\u52a9\u5916\u90e8\u641c\u7d22\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u795e\u7ecf\u5316\u8eab\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u4ea4\u4e92\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u901a\u5e38\u7f3a\u4e4f\u5bf9\u8bb2\u5ea7\u5185\u5bb9\u7684\u7406\u89e3\u3001\u4f9d\u8d56\u4e91\u7aef\u670d\u52a1\uff0c\u6216\u672a\u80fd\u5728\u7edf\u4e00\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6d41\u7a0b\u4e2d\u6574\u5408\u68c0\u7d22\u4e0e\u5316\u8eab\u8bb2\u89e3\u529f\u80fd\u3002  \n\u6211\u4eec\u63d0\u51fa\u4e86 ALIVE\uff08Avatar-Lecture Interactive Video Engine\uff09\uff0c\u5b83\u5c06\u88ab\u52a8\u7684\u8bb2\u5ea7\u89c2\u770b\u8f6c\u53d8\u4e3a\u52a8\u6001\u7684\u5b9e\u65f6\u5b66\u4e60\u4f53\u9a8c\u3002ALIVE \u5b8c\u5168\u5728\u672c\u5730\u786c\u4ef6\u4e0a\u8fd0\u884c\uff0c\u96c6\u6210\u4e86\u4ee5\u4e0b\u4e09\u9879\u6280\u672f\uff1a(1) \u901a\u8fc7\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u8f6c\u5f55\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f18\u5316\u548c\u795e\u7ecf\u8bf4\u8bdd\u5934\u5408\u6210\u751f\u6210\u7684\u5316\u8eab\u8bb2\u89e3\uff1b(2) \u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u4e0e\u65f6\u95f4\u6233\u5bf9\u9f50\u7684\u5185\u5bb9\u611f\u77e5\u68c0\u7d22\u673a\u5236\uff0c\u7528\u4e8e\u5b9a\u4f4d\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8bb2\u5ea7\u7247\u6bb5\uff1b(3) \u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u529f\u80fd\uff0c\u5141\u8bb8\u5b66\u751f\u6682\u505c\u8bb2\u5ea7\u5e76\u901a\u8fc7\u6587\u672c\u6216\u8bed\u97f3\u63d0\u95ee\uff0c\u5e76\u4ee5\u6587\u672c\u6216\u5316\u8eab\u5f62\u5f0f\u63a5\u6536\u57fa\u4e8e\u5185\u5bb9\u7684\u89e3\u91ca\u3002  \n\u4e3a\u4fdd\u8bc1\u54cd\u5e94\u901f\u5ea6\uff0cALIVE \u91c7\u7528\u8f7b\u91cf\u7ea7\u5d4c\u5165\u6a21\u578b\u3001\u57fa\u4e8e FAISS \u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5206\u6bb5\u5316\u8eab\u5408\u6210\u4e0e\u6e10\u8fdb\u5f0f\u9884\u52a0\u8f7d\u7b56\u7565\u3002\u6211\u4eec\u5728\u4e00\u95e8\u5b8c\u6574\u7684\u533b\u5b66\u5f71\u50cf\u8bfe\u7a0b\u4e0a\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u7279\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u7ed3\u679c\u8868\u660e ALIVE \u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u3001\u5185\u5bb9\u76f8\u5173\u4e14\u5f15\u4eba\u5165\u80dc\u7684\u5b9e\u65f6\u5b66\u4e60\u652f\u6301\u3002  \nALIVE \u8868\u660e\uff0c\u5f53\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u4e0e\u5185\u5bb9\u611f\u77e5\u68c0\u7d22\u548c\u672c\u5730\u90e8\u7f72\u76f8\u7ed3\u5408\u65f6\uff0c\u53ef\u663e\u8457\u63d0\u5347\u5f55\u64ad\u8bb2\u5ea7\u7684\u6559\u5b66\u4ef7\u503c\uff0c\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u4ea4\u4e92\u5f0f\u5b66\u4e60\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2512.20866", "pdf": "https://arxiv.org/pdf/2512.20866", "abs": "https://arxiv.org/abs/2512.20866", "authors": ["Haotian Lv", "Chao Li", "Jiangbo Dai", "Yuhui Zhang", "Zepeng Fan", "Yiqiu Tan", "Dawei Wang", "Binglei Xie"], "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5730\u4e0b\u7ba1\u7ebf3D\u63a2\u5730\u96f7\u8fbe\uff08GPR\uff09\u667a\u80fd\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u89c6\u56fe\u8054\u5408\u5206\u6790\u3001\u6539\u8fdbYOLO\u6a21\u578b\uff08DCO-YOLO\uff09\u548c3D-DIoU\u7a7a\u95f4\u5339\u914d\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u8bc6\u522b\u7cbe\u5ea6\u4e0e\u591a\u89c6\u56fe\u878d\u5408\u9c81\u68d2\u6027\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b33D\u63a2\u5730\u96f7\u8fbe\u5730\u4e0b\u7ba1\u7ebf\u68c0\u6d4b\u4e2d\u591a\u89c6\u56fe\u7279\u5f81\u5173\u8054\u5f31\u3001\u5c0f\u5c3a\u5ea6\u76ee\u6807\u8bc6\u522b\u7cbe\u5ea6\u4f4e\u4ee5\u53ca\u590d\u6742\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "1\uff09\u5efa\u7acb\u57fa\u4e8eB/C/D-Scan\u4e09\u89c6\u56fe\u8054\u5408\u5206\u6790\u7684\u4e09\u7ef4\u7ba1\u7ebf\u7279\u5f81\u8bc4\u4f30\u65b9\u6cd5\uff1b2\uff09\u63d0\u51faDCO-YOLO\u6846\u67b6\uff0c\u878d\u5408DySample\u3001CGLU\u548cOutlookAttention\u673a\u5236\u4ee5\u589e\u5f3a\u5c0f\u76ee\u6807\u8fb9\u7f18\u7279\u5f81\u63d0\u53d6\uff1b3\uff09\u8bbe\u8ba13D-DIoU\u7a7a\u95f4\u7279\u5f81\u5339\u914d\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e09\u7ef4\u51e0\u4f55\u7ea6\u675f\u4e0e\u4e2d\u5fc3\u8ddd\u79bb\u60e9\u7f5a\u9879\u5b9e\u73b0\u591a\u89c6\u56fe\u6807\u6ce8\u81ea\u52a8\u5173\u8054\u3002", "result": "\u5728\u771f\u5b9e\u57ce\u5e02\u5730\u4e0b\u7ba1\u7ebf\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u590d\u6742\u591a\u7ba1\u7ebf\u573a\u666f\u4e0b\u8fbe\u523096.2%\u51c6\u786e\u7387\u300193.3%\u53ec\u56de\u7387\u548c96.7%mAP\uff0c\u5206\u522b\u6bd4\u57fa\u7ebf\u9ad82.0%\u30012.1%\u548c0.9%\uff1b\u6d88\u878d\u5b9e\u9a8c\u548cGrad-CAM++\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u6a21\u578b\u5bf9\u7ba1\u7ebf\u51e0\u4f55\u7279\u5f81\u7684\u5173\u6ce8\u80fd\u529b\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u4e0e3D GPR\u7269\u7406\u7279\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u5730\u4e0b\u7ba1\u7ebf\u7684\u667a\u80fd\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u65b0\u6280\u672f\u6846\u67b6\u3002", "summary_cn": "\u4e3a\u89e3\u51b3\u5229\u7528\u4e09\u7ef4\u63a2\u5730\u96f7\u8fbe\uff083D GPR\uff09\u8fdb\u884c\u5730\u4e0b\u7ba1\u7ebf\u68c0\u6d4b\u65f6\u5b58\u5728\u7684\u591a\u89c6\u56fe\u7279\u5f81\u76f8\u5173\u6027\u5f31\u3001\u5c0f\u5c3a\u5ea6\u76ee\u6807\u8bc6\u522b\u7cbe\u5ea6\u4f4e\u4ee5\u53ca\u590d\u6742\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u7ef4\u7ba1\u7ebf\u667a\u80fd\u68c0\u6d4b\u6846\u67b6\u3002\u9996\u5148\uff0c\u57fa\u4e8eB/C/D-Scan\u4e09\u89c6\u56fe\u8054\u5408\u5206\u6790\u7b56\u7565\uff0c\u901a\u8fc7FDTD\u65b9\u6cd5\u83b7\u5f97\u7684\u6b63\u6f14\u6a21\u62df\u7ed3\u679c\u4e0e\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5efa\u7acb\u4e86\u4e09\u7ef4\u7ba1\u7ebf\u4e09\u89c6\u56fe\u7279\u5f81\u8bc4\u4f30\u65b9\u6cd5\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e86DCO-YOLO\u6846\u67b6\uff0c\u5c06DySample\u3001CGLU\u548cOutlookAttention\u7b49\u8de8\u7ef4\u5ea6\u5173\u8054\u673a\u5236\u96c6\u6210\u5230\u539f\u59cbYOLOv11\u7b97\u6cd5\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u5c0f\u5c3a\u5ea6\u7ba1\u7ebf\u8fb9\u7f18\u7279\u5f81\u7684\u63d0\u53d6\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd3D-DIoU\u7a7a\u95f4\u7279\u5f81\u5339\u914d\u7b97\u6cd5\uff0c\u878d\u5408\u4e09\u7ef4\u51e0\u4f55\u7ea6\u675f\u4e0e\u4e2d\u5fc3\u8ddd\u79bb\u60e9\u7f5a\u9879\uff0c\u5b9e\u73b0\u4e86\u591a\u89c6\u56fe\u6807\u6ce8\u7684\u81ea\u52a8\u5173\u8054\uff0c\u4e09\u89c6\u56fe\u878d\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5355\u89c6\u56fe\u68c0\u6d4b\u4e2d\u7684\u56fa\u6709\u6a21\u7cca\u6027\u3002\u57fa\u4e8e\u771f\u5b9e\u57ce\u5e02\u5730\u4e0b\u7ba1\u7ebf\u6570\u636e\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u590d\u6742\u591a\u7ba1\u7ebf\u573a\u666f\u4e0b\u7684\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\u5206\u522b\u8fbe\u523096.2%\u300193.3%\u548c96.7%\uff0c\u8f83\u57fa\u7ebf\u6a21\u578b\u5206\u522b\u63d0\u9ad8\u4e862.0%\u30012.1%\u548c0.9%\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u52a8\u6001\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u7684\u534f\u540c\u4f18\u5316\u6548\u679c\uff0cGrad-CAM++\u70ed\u529b\u56fe\u53ef\u89c6\u5316\u4e5f\u8868\u660e\u6539\u8fdb\u540e\u7684\u6a21\u578b\u663e\u8457\u589e\u5f3a\u4e86\u5bf9\u7ba1\u7ebf\u51e0\u4f55\u7279\u5f81\u7684\u5173\u6ce8\u80fd\u529b\u3002\u672c\u7814\u7a76\u5c06\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u4e0e3D GPR\u7684\u7269\u7406\u7279\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u5730\u4e0b\u7ba1\u7ebf\u7684\u667a\u80fd\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b0\u6280\u672f\u6846\u67b6\u3002"}}
{"id": "2512.20871", "pdf": "https://arxiv.org/pdf/2512.20871", "abs": "https://arxiv.org/abs/2512.20871", "authors": ["Daichi Arai", "Kyohei Unno", "Yasuko Sugito", "Yuichi Kusakabe"], "title": "NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "2026 IIEEJ International Conference on Image Electronics and Visual Computing (IEVC)", "summary": "Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.", "AI": {"tldr": "NeRV360 \u662f\u4e00\u79cd\u9762\u5411 360 \u5ea6\u89c6\u9891\u7684\u9ad8\u6548\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u89e3\u7801\u7528\u6237\u89c6\u53e3\u800c\u975e\u6574\u5e27\u5168\u666f\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u89e3\u7801\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u4f18\u753b\u8d28\u3002", "motivation": "\u5c06 NeRV \u5e94\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387 360 \u5ea6\u89c6\u9891\u65f6\u5b58\u5728\u5185\u5b58\u5360\u7528\u9ad8\u548c\u89e3\u7801\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa NeRV360 \u6846\u67b6\uff0c\u5c06\u89c6\u53e3\u63d0\u53d6\u96c6\u6210\u5230\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u5f15\u5165\u65f6\u7a7a\u4eff\u5c04\u53d8\u6362\u6a21\u5757\uff0c\u6839\u636e\u89c6\u89d2\u548c\u65f6\u95f4\u8fdb\u884c\u6761\u4ef6\u89e3\u7801\uff0c\u4ec5\u91cd\u5efa\u7528\u6237\u9009\u62e9\u7684\u89c6\u53e3\u533a\u57df\u3002", "result": "\u5728 6K \u5206\u8fa8\u7387\u89c6\u9891\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4ee3\u8868\u6027\u65b9\u6cd5 HNeRV \u76f8\u6bd4\uff0cNeRV360 \u5185\u5b58\u6d88\u8017\u51cf\u5c11 7 \u500d\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u5347 2.5 \u500d\uff0c\u4e14\u5ba2\u89c2\u753b\u8d28\u6307\u6807\u66f4\u4f18\u3002", "conclusion": "NeRV360 \u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387 360 \u5ea6\u89c6\u9891\u5728\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u4e0b\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u4fdd\u8bc1\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u7528\u5316\u7684\u5185\u5b58\u4e0e\u901f\u5ea6\u6027\u80fd\u3002", "summary_cn": "\u9762\u5411\u89c6\u9891\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08NeRV\uff09\u5728\u89c6\u9891\u538b\u7f29\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5c06 NeRV \u5e94\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387 360 \u5ea6\u89c6\u9891\u4f1a\u5bfc\u81f4\u5185\u5b58\u5360\u7528\u8fc7\u9ad8\u548c\u89e3\u7801\u901f\u5ea6\u7f13\u6162\uff0c\u4f7f\u5f97\u5b9e\u65f6\u5e94\u7528\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u6211\u4eec\u63d0\u51fa\u4e86 NeRV360\uff0c\u8fd9\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u4ec5\u89e3\u7801\u7528\u6237\u9009\u5b9a\u7684\u89c6\u53e3\uff0c\u800c\u4e0d\u662f\u91cd\u5efa\u6574\u4e2a\u5168\u666f\u5e27\u3002\u4e0e\u4f20\u7edf\u6d41\u7a0b\u4e0d\u540c\uff0cNeRV360 \u5c06\u89c6\u53e3\u63d0\u53d6\u96c6\u6210\u5230\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65f6\u7a7a\u4eff\u5c04\u53d8\u6362\u6a21\u5757\uff0c\u4ee5\u6839\u636e\u89c6\u89d2\u548c\u65f6\u95f4\u8fdb\u884c\u6761\u4ef6\u89e3\u7801\u3002\u5728 6K \u5206\u8fa8\u7387\u89c6\u9891\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4ee3\u8868\u6027\u5148\u524d\u5de5\u4f5c HNeRV \u76f8\u6bd4\uff0cNeRV360 \u5b9e\u73b0\u4e86 7 \u500d\u7684\u5185\u5b58\u6d88\u8017\u964d\u4f4e\u548c 2.5 \u500d\u7684\u89e3\u7801\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u5728\u5ba2\u89c2\u6307\u6807\u4e0a\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u56fe\u50cf\u8d28\u91cf\u3002"}}
