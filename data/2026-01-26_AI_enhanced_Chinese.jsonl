{"id": "2601.16272", "pdf": "https://arxiv.org/pdf/2601.16272", "abs": "https://arxiv.org/abs/2601.16272", "authors": ["Xiaoyan Xing", "Philipp Henzler", "Junhwa Hur", "Runze Li", "Jonathan T. Barron", "Pratul P. Srinivasan", "Dor Verbin"], "title": "GR3EN: Generative Relighting for 3D Environments", "categories": ["cs.CV"], "comment": "project page: https://gr3en-relight.github.io/", "summary": "We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u5927\u578b\u623f\u95f4\u5c3a\u5ea63D\u573a\u666f\u8fdb\u884c\u53ef\u63a7\u91cd\u5149\u7167\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u5230\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\u84b8\u998f\u52303D\u91cd\u5efa\u4e2d\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u9006\u6e32\u67d3\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u611f\u91cd\u5149\u7167\u6548\u679c\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u91cd\u5149\u7167\u65b9\u6cd5\u901a\u5e38\u9700\u89e3\u51b3\u6b20\u5b9a\u6216\u75c5\u6001\u7684\u9006\u6e32\u67d3\u95ee\u9898\uff0c\u96be\u4ee5\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\uff1b\u800c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u591a\u9650\u4e8e2D\u56fe\u50cf/\u89c6\u9891\u6216\u5355\u4e2a\u7269\u4f53\u76843D\u91cd\u5149\u7167\uff0c\u7f3a\u4e4f\u5bf9\u623f\u95f4\u5c3a\u5ea6\u573a\u666f\u7684\u652f\u6301\u3002", "method": "\u5c06\u89c6\u9891\u5230\u89c6\u9891\u7684\u91cd\u5149\u7167\u6269\u6563\u6a21\u578b\u7684\u8f93\u51fa\u84b8\u998f\u52303D\u91cd\u5efa\u4e2d\uff0c\u4ece\u800c\u7ed5\u8fc7\u590d\u6742\u7684\u9006\u6e32\u67d3\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u5bf9\u623f\u95f4\u5c3a\u5ea63D\u573a\u666f\u7684\u53ef\u63a7\u91cd\u5149\u7167\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5fe0\u5b9e\u6e32\u67d3\u65b0\u5149\u7167\u6761\u4ef6\u4e0b\u573a\u666f\u7684\u65b0\u89c6\u89d2\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a213D\u573a\u666f\u91cd\u5149\u7167\u7684\u6311\u6218\uff0c\u517c\u5177\u7075\u6d3b\u6027\u4e0e\u9ad8\u8d28\u91cf\uff0c\u4e3a\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u5149\u7167\u7f16\u8f91\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u5927\u578b\u623f\u95f4\u5c3a\u5ea6\u73af\u5883\u76843D\u91cd\u5efa\u8fdb\u884c\u91cd\u5149\u7167\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u76843D\u573a\u666f\u91cd\u5149\u7167\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9700\u8981\u6c42\u89e3\u6b20\u5b9a\u6216\u75c5\u6001\u7684\u9006\u6e32\u67d3\u95ee\u9898\uff0c\u56e0\u6b64\u96be\u4ee5\u5728\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u3002\u5c3d\u7ba1\u6700\u8fd1\u5229\u7528\u751f\u6210\u5f0f\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u91cd\u5149\u7167\u7684\u7814\u7a76\u53d6\u5f97\u4e86\u53ef\u559c\u8fdb\u5c55\uff0c\u4f46\u8fd9\u4e9b\u6280\u672f\u8981\u4e48\u5c40\u9650\u4e8e2D\u56fe\u50cf\u548c\u89c6\u9891\u7684\u91cd\u5149\u7167\uff0c\u8981\u4e48\u4ec5\u9002\u7528\u4e8e\u5355\u4e2a\u7269\u4f53\u76843D\u91cd\u5149\u7167\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u89c6\u9891\u5230\u89c6\u9891\u91cd\u5149\u7167\u6269\u6563\u6a21\u578b\u7684\u8f93\u51fa\u84b8\u998f\u52303D\u91cd\u5efa\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5bf9\u623f\u95f4\u5c3a\u5ea6\u573a\u666f\u7684\u53ef\u63a73D\u91cd\u5149\u7167\u3002\u8be5\u65b9\u6cd5\u89c4\u907f\u4e86\u6c42\u89e3\u56f0\u96be\u7684\u9006\u6e32\u67d3\u95ee\u9898\uff0c\u6784\u5efa\u51fa\u4e00\u4e2a\u7075\u6d3b\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u5bf9\u590d\u6742\u771f\u5b9e\u573a\u666f\u76843D\u91cd\u5efa\u8fdb\u884c\u91cd\u5149\u7167\u3002\u6211\u4eec\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u80fd\u591f\u5728\u65b0\u5149\u7167\u6761\u4ef6\u4e0b\u903c\u771f\u5730\u6e32\u67d3\u573a\u666f\u7684\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.16296", "pdf": "https://arxiv.org/pdf/2601.16296", "abs": "https://arxiv.org/abs/2601.16296", "authors": ["Dohun Lee", "Chun-Hao Paul Huang", "Xuelin Chen", "Jong Chul Ye", "Duygu Ceylan", "Hyeonho Jeong"], "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://dohunlee1.github.io/MemoryV2V", "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMemory-V2V\uff0c\u9996\u6b21\u89e3\u51b3\u591a\u8f6e\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u8de8\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u8bb0\u5fc6\u673a\u5236\u548c\u53ef\u5b66\u4e60\u7684token\u538b\u7f29\u5668\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e00\u81f4\u6027\u5e76\u964d\u4f4e30%\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u5355\u6b21\u7f16\u8f91\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8f6e\u4ea4\u4e92\u5f0f\u7f16\u8f91\u4e2d\u96be\u4ee5\u7ef4\u6301\u8de8\u7f16\u8f91\u7684\u4e00\u81f4\u6027\u3002\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u7f16\u8f91\u901a\u5e38\u662f\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u7528\u6237\u9700\u591a\u6b21\u8c03\u6574\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u4fdd\u6301\u5386\u53f2\u7f16\u8f91\u4fe1\u606f\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "Memory-V2V\u6846\u67b6\u4e3a\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u6dfb\u52a0\u663e\u5f0f\u8bb0\u5fc6\u6a21\u5757\uff1a\u5229\u7528\u5916\u90e8\u7f13\u5b58\u5b58\u50a8\u5148\u524d\u7f16\u8f91\u7ed3\u679c\uff0c\u901a\u8fc7\u7cbe\u51c6\u68c0\u7d22\u4e0e\u52a8\u6001token\u5316\u7b56\u7565\u5c06\u5386\u53f2\u4fe1\u606f\u4f5c\u4e3a\u5f53\u524d\u7f16\u8f91\u7684\u6761\u4ef6\uff1b\u540c\u65f6\u5728DiT\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u7684token\u538b\u7f29\u5668\uff0c\u51cf\u5c11\u5197\u4f59\u6761\u4ef6token\u4ee5\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u89c6\u9891\u65b0\u89c6\u89d2\u5408\u6210\u548c\u6587\u672c\u5f15\u5bfc\u957f\u89c6\u9891\u7f16\u8f91\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u8868\u660e\uff0cMemory-V2V\u663e\u8457\u63d0\u5347\u4e86\u8de8\u7f16\u8f91\u4e00\u81f4\u6027\uff0c\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u6781\u5c0f\uff0c\u5e76\u5728\u4efb\u52a1\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6574\u4f53\u63d0\u901f30%\u3002", "conclusion": "Memory-V2V\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u8de8\u4e00\u81f4\u6027\u96be\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u8bb0\u5fc6\u4e0e\u9ad8\u6548token\u538b\u7f29\uff0c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0f\u89c6\u9891\u7f16\u8f91\u3002", "summary_cn": "\u8fd1\u671f\u7684\u57fa\u7840\u89c6\u9891\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u901a\u8fc7\u4fee\u6539\u5916\u89c2\u3001\u8fd0\u52a8\u6216\u6444\u50cf\u673a\u8fd0\u52a8\u6765\u7f16\u8f91\u7528\u6237\u63d0\u4f9b\u7684\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u7f16\u8f91\u901a\u5e38\u662f\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u7528\u6237\u9700\u8981\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u4e0d\u65ad\u4f18\u5316\u7ed3\u679c\u3002\u5728\u8fd9\u79cd\u591a\u8f6e\u8bbe\u5b9a\u4e0b\uff0c\u5f53\u524d\u7684\u89c6\u9891\u7f16\u8f91\u5668\u96be\u4ee5\u5728\u8fde\u7eed\u7f16\u8f91\u4e4b\u95f4\u4fdd\u6301\u8de8\u4e00\u81f4\u6027\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u9996\u6b21\u63a2\u8ba8\u4e86\u591a\u8f6e\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u8de8\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86Memory-V2V\u2014\u2014\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8bb0\u5fc6\u673a\u5236\u589e\u5f3a\u73b0\u6709\u89c6\u9891\u5230\u89c6\u9891\u6a21\u578b\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e00\u4e2a\u5916\u90e8\u7f13\u5b58\u5b58\u50a8\u5148\u524d\u7f16\u8f91\u8fc7\u7684\u89c6\u9891\uff0c\u5e76\u91c7\u7528\u7cbe\u786e\u68c0\u7d22\u548c\u52a8\u6001\u5206\u8bcd\u7b56\u7565\uff0c\u5c06\u5f53\u524d\u7f16\u8f91\u6b65\u9aa4\u4e0e\u5148\u524d\u7ed3\u679c\u76f8\u5173\u8054\u3002\u4e3a\u8fdb\u4e00\u6b65\u51cf\u5c11\u5197\u4f59\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u6211\u4eec\u5728DiT\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684token\u538b\u7f29\u5668\uff0c\u5728\u4fdd\u7559\u5173\u952e\u89c6\u89c9\u7ebf\u7d22\u7684\u540c\u65f6\u538b\u7f29\u5197\u4f59\u7684\u6761\u4ef6token\uff0c\u4ece\u800c\u5b9e\u73b0\u6574\u4f5330%\u7684\u901f\u5ea6\u63d0\u5347\u3002\u6211\u4eec\u5728\u89c6\u9891\u65b0\u89c6\u89d2\u5408\u6210\u548c\u6587\u672c\u6761\u4ef6\u4e0b\u7684\u957f\u89c6\u9891\u7f16\u8f91\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86Memory-V2V\u7684\u6709\u6548\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMemory-V2V\u751f\u6210\u7684\u89c6\u9891\u5728\u8de8\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\uff0c\u5e76\u5728\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2601.16302", "pdf": "https://arxiv.org/pdf/2601.16302", "abs": "https://arxiv.org/abs/2601.16302", "authors": ["Abhijeet Parida", "Antonia Alomar", "Zhifan Jiang", "Pooneh Roshanitabrizi", "Austin Tapp", "Ziyue Xu", "Syed Muhammad Anwar", "Maria J. Ledesma-Carbayo", "Holger R. Roth", "Marius George Linguraru"], "title": "FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.", "AI": {"tldr": "FeTTL is a novel federated learning framework that jointly learns a global template and task model to align data distributions across institutions, significantly improving performance on medical imaging tasks under domain shift.", "motivation": "Federated learning in medical imaging suffers from performance degradation due to domain shifts caused by differences in acquisition protocols, scanner types, and patient populations across institutions.", "method": "The proposed FeTTL framework harmonizes multi-institutional data by jointly learning a global template and a task-specific model in a federated setting to align client data distributions.", "result": "FeTTL significantly outperforms state-of-the-art federated baselines (p < 0.002) on two tasks: retinal fundus optical disc segmentation and histopathological metastasis classification across multiple institutions.", "conclusion": "FeTTL provides a principled and extensible approach to mitigate distribution shifts in federated medical imaging, enabling more robust and generalizable model deployment in real-world multi-institutional settings.", "summary_cn": "\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u5730\u7406\u5206\u5e03\u7684\u533b\u7597\u4e2d\u5fc3\u4e4b\u95f4\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u3002\u7136\u800c\uff0c\u6570\u636e\u4e2d\u7684\u57df\u504f\u79fb\u548c\u5f02\u8d28\u6027\u5e38\u5e38\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u5c24\u5176\u53d7\u5230\u91c7\u96c6\u534f\u8bae\u3001\u626b\u63cf\u4eea\u7c7b\u578b\u548c\u60a3\u8005\u7fa4\u4f53\u5dee\u5f02\u7684\u5f71\u54cd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8054\u90a6\u6a21\u677f\u4e0e\u4efb\u52a1\u5b66\u4e60\uff08FeTTL\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u65e8\u5728\u8054\u90a6\u73af\u5883\u4e2d\u534f\u8c03\u591a\u673a\u6784\u533b\u5b66\u5f71\u50cf\u6570\u636e\u3002FeTTL\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u4e00\u4e2a\u5168\u5c40\u6a21\u677f\u548c\u4e00\u4e2a\u4efb\u52a1\u6a21\u578b\uff0c\u4ee5\u5bf9\u9f50\u5404\u53c2\u4e0e\u65b9\u7684\u6570\u636e\u5206\u5e03\u3002\u6211\u4eec\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u4e14\u591a\u6837\u5316\u7684\u591a\u673a\u6784\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86FeTTL\uff1a\u89c6\u7f51\u819c\u773c\u5e95\u89c6\u76d8\u5206\u5272\u548c\u7ec4\u7ec7\u75c5\u7406\u5b66\u8f6c\u79fb\u7076\u5206\u7c7b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFeTTL\u5728\u89c6\u76d8\u5206\u5272\u548c\u8f6c\u79fb\u7076\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff08p\u503c<0.002\uff09\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u5f3a\u8c03\u4e86\u8054\u5408\u5b66\u4e60\u6a21\u677f\u4e0e\u4efb\u52a1\u7684\u91cd\u8981\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cFeTTL\u4e3a\u7f13\u89e3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u539f\u5219\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5728\u771f\u5b9e\u4e16\u754c\u7684\u591a\u673a\u6784\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u6a21\u578b\u90e8\u7f72\u3002"}}
{"id": "2601.16333", "pdf": "https://arxiv.org/pdf/2601.16333", "abs": "https://arxiv.org/abs/2601.16333", "authors": ["Aditya K Surikuchi", "Raquel Fern\u00e1ndez", "Sandro Pezzelle"], "title": "Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Foundation models are used for many real-world applications involving language generation from temporally-ordered multimodal events. In this work, we study the ability of models to identify the most important sub-events in a video, which is a fundamental prerequisite for narrating or summarizing multimodal events. Specifically, we focus on football games and evaluate models on their ability to distinguish between important and non-important sub-events in a game. To this end, we construct a new dataset by leveraging human preferences for importance implicit in football game highlight reels, without any additional annotation costs. Using our dataset, which we will publicly release to the community, we compare several state-of-the-art multimodal models and show that they are not far from chance level performance. Analyses of models beyond standard evaluation metrics reveal their tendency to rely on a single dominant modality and their ineffectiveness in synthesizing necessary information from multiple sources. Our findings underline the importance of modular architectures that can handle sample-level heterogeneity in multimodal data and the need for complementary training procedures that can maximize cross-modal synergy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8bc6\u522b\u8db3\u7403\u89c6\u9891\u4e2d\u91cd\u8981\u5b50\u4e8b\u4ef6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff0c\u4e3b\u8981\u56e0\u5176\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u4e14\u96be\u4ee5\u6709\u6548\u878d\u5408\u591a\u6e90\u4fe1\u606f\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e38\u9700\u4ece\u65f6\u5e8f\u591a\u6a21\u6001\u4e8b\u4ef6\u751f\u6210\u8bed\u8a00\u63cf\u8ff0\uff0c\u800c\u8bc6\u522b\u5173\u952e\u5b50\u4e8b\u4ef6\u662f\u5b9e\u73b0\u6709\u6548\u53d9\u8ff0\u6216\u6458\u8981\u7684\u524d\u63d0\u3002\u7136\u800c\u73b0\u6709\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u5982\u8db3\u7403\u6bd4\u8d5b\u4e2d\u3002", "method": "\u4f5c\u8005\u5229\u7528\u8db3\u7403\u6bd4\u8d5b\u96c6\u9526\u4e2d\u9690\u542b\u7684\u4eba\u7c7b\u5bf9\u201c\u91cd\u8981\u6027\u201d\u7684\u504f\u597d\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bc4\u4f30\u591a\u4e2a\u524d\u6cbf\u591a\u6a21\u6001\u6a21\u578b\u533a\u5206\u91cd\u8981\u4e0e\u975e\u91cd\u8981\u5b50\u4e8b\u4ef6\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ec5\u7565\u4f18\u4e8e\u968f\u673a\u731c\u6d4b\uff1b\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u5355\u4e00\u4e3b\u5bfc\u6a21\u6001\uff0c\u96be\u4ee5\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u80fd\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u6837\u672c\u7ea7\u5f02\u8d28\u6027\u7684\u6a21\u5757\u5316\u67b6\u6784\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5f00\u53d1\u80fd\u6700\u5927\u5316\u8de8\u6a21\u6001\u534f\u540c\u6548\u5e94\u7684\u8bad\u7ec3\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "summary_cn": "\u57fa\u7840\u6a21\u578b\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8bb8\u591a\u6d89\u53ca\u4ece\u65f6\u5e8f\u591a\u6a21\u6001\u4e8b\u4ef6\u751f\u6210\u8bed\u8a00\u7684\u73b0\u5b9e\u573a\u666f\u3002\u672c\u6587\u7814\u7a76\u4e86\u6a21\u578b\u8bc6\u522b\u89c6\u9891\u4e2d\u6700\u91cd\u8981\u5b50\u4e8b\u4ef6\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u5bf9\u591a\u6a21\u6001\u4e8b\u4ef6\u8fdb\u884c\u53d9\u8ff0\u6216\u6458\u8981\u7684\u57fa\u672c\u524d\u63d0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u805a\u7126\u4e8e\u8db3\u7403\u6bd4\u8d5b\uff0c\u8bc4\u4f30\u6a21\u578b\u533a\u5206\u6bd4\u8d5b\u4e2d\u91cd\u8981\u4e0e\u975e\u91cd\u8981\u5b50\u4e8b\u4ef6\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u901a\u8fc7\u5229\u7528\u8db3\u7403\u6bd4\u8d5b\u96c6\u9526\u4e2d\u9690\u542b\u7684\u4eba\u7c7b\u5bf9\u201c\u91cd\u8981\u6027\u201d\u7684\u504f\u597d\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6210\u672c\u7684\u65b0\u6570\u636e\u96c6\u3002\u6211\u4eec\u5c06\u516c\u5f00\u53d1\u5e03\u8be5\u6570\u636e\u96c6\uff0c\u5e76\u7528\u5176\u5bf9\u591a\u4e2a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u4eec\u7684\u8868\u73b0\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\u3002\u8d85\u8d8a\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u7684\u6df1\u5165\u5206\u6790\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u5355\u4e00\u4e3b\u5bfc\u6a21\u6001\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u878d\u5408\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u5fc5\u8981\u4fe1\u606f\u3002\u6211\u4eec\u7684\u53d1\u73b0\u5f3a\u8c03\u4e86\u91c7\u7528\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u6837\u672c\u7ea7\u5f02\u8d28\u6027\u7684\u6a21\u5757\u5316\u67b6\u6784\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5f00\u53d1\u80fd\u591f\u6700\u5927\u5316\u8de8\u6a21\u6001\u534f\u540c\u6548\u5e94\u7684\u914d\u5957\u8bad\u7ec3\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.16348", "pdf": "https://arxiv.org/pdf/2601.16348", "abs": "https://arxiv.org/abs/2601.16348", "authors": ["Aline Sindel", "Andreas Maier", "Vincent Christlein"], "title": "Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures", "categories": ["cs.CV"], "comment": "Preprint, submitted for review", "summary": "Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography. For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually. Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision. Due to varying image resolutions, huge image sizes, non-rigid distortions, and modality-dependent image content, registration is challenging. Therefore, we propose a coarse-to-fine non-rigid multi-modal registration method efficiently relying on sparse keypoints and thin-plate-splines. Historical paintings exhibit a fine crack pattern, called craquelure, on the paint layer, which is captured by all image systems and is well-suited as a feature for registration. In our one-stage non-rigid registration approach, we employ a convolutional neural network for joint keypoint detection and description based on the craquelure and a graph neural network for descriptor matching in a patch-based manner, and filter matches based on homography reprojection errors in local areas. For coarse-to-fine registration, we introduce a novel multi-level keypoint refinement approach to register mixed-resolution images up to the highest resolution. We created a multi-modal dataset of panel paintings with a high number of keypoint annotations, and a large test set comprising five multi-modal domains and varying image resolutions. The ablation study demonstrates the effectiveness of all modules of our refinement method. Our proposed approaches achieve the best registration results compared to competing keypoint and dense matching methods and refinement methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7c97\u5230\u7cbe\u975e\u521a\u6027\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u753b\u4f5c\u8868\u9762\u7684\u9f9f\u88c2\u7eb9\u7406\uff08craquelure\uff09\u4f5c\u4e3a\u5173\u952e\u7279\u5f81\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u7684\u591a\u6a21\u6001\u56fe\u50cf\u5bf9\u9f50\u3002", "motivation": "\u5386\u53f2\u6728\u677f\u7ed8\u753b\u7684\u591a\u6a21\u6001\u6210\u50cf\u5206\u6790\uff08\u5982\u53ef\u89c1\u5149\u3001\u7ea2\u5916\u3001\u7d2b\u5916\u3001X\u5c04\u7ebf\u7b49\uff09\u9700\u8981\u50cf\u7d20\u7ea7\u5bf9\u9f50\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u5bf9\u9f50\u8d39\u65f6\u8d39\u529b\u4e14\u7cbe\u5ea6\u6709\u9650\u3002\u73b0\u6709\u81ea\u52a8\u914d\u51c6\u65b9\u6cd5\u5728\u9762\u5bf9\u56fe\u50cf\u5206\u8fa8\u7387\u5dee\u5f02\u5927\u3001\u5c3a\u5bf8\u5de8\u5927\u3001\u975e\u521a\u6027\u5f62\u53d8\u53ca\u6a21\u6001\u5185\u5bb9\u5dee\u5f02\u7b49\u95ee\u9898\u65f6\u4ecd\u5177\u6311\u6218\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u81ea\u52a8\u914d\u51c6\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u5355\u9636\u6bb5\u3001\u7c97\u5230\u7cbe\u7684\u975e\u521a\u6027\u591a\u6a21\u6001\u914d\u51c6\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8054\u5408\u68c0\u6d4b\u548c\u63cf\u8ff0\u57fa\u4e8e\u9f9f\u88c2\u7eb9\u7406\u7684\u5173\u952e\u70b9\uff1b2\uff09\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u63cf\u8ff0\u7b26\u5339\u914d\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u5355\u5e94\u6027\u91cd\u6295\u5f71\u8bef\u5dee\u7b5b\u9009\u5339\u914d\u70b9\uff1b3\uff09\u8bbe\u8ba1\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c42\u7ea7\u5173\u952e\u70b9\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u6df7\u5408\u5206\u8fa8\u7387\u56fe\u50cf\u5411\u6700\u9ad8\u5206\u8fa8\u7387\u5bf9\u9f50\u3002", "result": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5927\u91cf\u5173\u952e\u70b9\u6807\u6ce8\u7684\u591a\u6a21\u6001\u6728\u677f\u7ed8\u753b\u6570\u636e\u96c6\u548c\u6db5\u76d6\u4e94\u79cd\u6a21\u6001\u3001\u591a\u79cd\u5206\u8fa8\u7387\u7684\u5927\u89c4\u6a21\u6d4b\u8bd5\u96c6\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5173\u952e\u70b9\u5339\u914d\u548c\u5bc6\u96c6\u5339\u914d\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u4f73\u914d\u51c6\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5145\u5206\u5229\u7528\u5386\u53f2\u753b\u4f5c\u5171\u6709\u7684\u9f9f\u88c2\u7eb9\u7406\u7279\u5f81\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u51e0\u4f55\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u3001\u591a\u5206\u8fa8\u7387\u3001\u975e\u521a\u6027\u5386\u53f2\u7ed8\u753b\u56fe\u50cf\u7684\u9ad8\u7cbe\u5ea6\u914d\u51c6\u96be\u9898\uff0c\u4e3a\u827a\u672f\u79d1\u6280\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u81ea\u52a8\u5316\u5de5\u5177\u3002", "summary_cn": "\u5bf9\u5386\u53f2\u6728\u677f\u7ed8\u753b\u8fdb\u884c\u827a\u672f\u79d1\u6280\u7814\u7a76\u4f9d\u8d56\u4e8e\u83b7\u53d6\u591a\u6a21\u6001\u56fe\u50cf\u6570\u636e\uff0c\u5305\u62ec\u53ef\u89c1\u5149\u6444\u5f71\u3001\u7ea2\u5916\u53cd\u5c04\u6210\u50cf\u3001\u7d2b\u5916\u8367\u5149\u6444\u5f71\u3001X\u5c04\u7ebf\u6210\u50cf\u548c\u5b8f\u89c2\u6444\u5f71\u3002\u4e3a\u4e86\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u8fd9\u4e9b\u591a\u6a21\u6001\u56fe\u50cf\u9700\u8981\u8fdb\u884c\u50cf\u7d20\u7ea7\u5bf9\u9f50\uff0c\u800c\u76ee\u524d\u8fd9\u4e00\u8fc7\u7a0b\u901a\u5e38\u4ecd\u9700\u624b\u52a8\u5b8c\u6210\u3002\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\u80fd\u591f\u51cf\u5c11\u8fd9\u79cd\u7e41\u91cd\u7684\u624b\u52a8\u5de5\u4f5c\uff0c\u663e\u8457\u52a0\u5feb\u5904\u7406\u901f\u5ea6\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u56fe\u50cf\u5206\u8fa8\u7387\u5404\u5f02\u3001\u5c3a\u5bf8\u5de8\u5927\u3001\u5b58\u5728\u975e\u521a\u6027\u5f62\u53d8\u4ee5\u53ca\u6a21\u6001\u95f4\u56fe\u50cf\u5185\u5bb9\u5dee\u5f02\uff0c\u914d\u51c6\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7c97\u5230\u7cbe\u7684\u975e\u521a\u6027\u591a\u6a21\u6001\u914d\u51c6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9ad8\u6548\u5730\u4f9d\u8d56\u4e8e\u7a00\u758f\u5173\u952e\u70b9\u548c\u8584\u677f\u6837\u6761\uff08thin-plate-splines\uff09\u3002\u5386\u53f2\u753b\u4f5c\u7684\u989c\u6599\u5c42\u4e0a\u666e\u904d\u5b58\u5728\u4e00\u79cd\u79f0\u4e3a\u201c\u9f9f\u88c2\u201d\uff08craquelure\uff09\u7684\u7cbe\u7ec6\u88c2\u7eb9\u56fe\u6848\uff0c\u8be5\u56fe\u6848\u53ef\u88ab\u6240\u6709\u6210\u50cf\u7cfb\u7edf\u6355\u6349\u5230\uff0c\u975e\u5e38\u9002\u5408\u4f5c\u4e3a\u914d\u51c6\u7279\u5f81\u3002\u5728\u6211\u4eec\u63d0\u51fa\u7684\u5355\u9636\u6bb5\u975e\u521a\u6027\u914d\u51c6\u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u57fa\u4e8e\u9f9f\u88c2\u7eb9\u7406\u8054\u5408\u8fdb\u884c\u5173\u952e\u70b9\u68c0\u6d4b\u4e0e\u63cf\u8ff0\uff0c\u5e76\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u4ee5\u56fe\u50cf\u5757\u65b9\u5f0f\u5339\u914d\u63cf\u8ff0\u7b26\uff0c\u540c\u65f6\u57fa\u4e8e\u5c40\u90e8\u533a\u57df\u7684\u5355\u5e94\u6027\u91cd\u6295\u5f71\u8bef\u5dee\u5bf9\u5339\u914d\u70b9\u8fdb\u884c\u7b5b\u9009\u3002\u4e3a\u4e86\u5b9e\u73b0\u7c97\u5230\u7cbe\u7684\u914d\u51c6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5c42\u7ea7\u5173\u952e\u70b9\u4f18\u5316\u65b9\u6cd5\uff0c\u53ef\u5c06\u6df7\u5408\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u914d\u51c6\u81f3\u6700\u9ad8\u5206\u8fa8\u7387\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5927\u91cf\u5173\u952e\u70b9\u6807\u6ce8\u7684\u6728\u677f\u7ed8\u753b\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6db5\u76d6\u4e94\u79cd\u591a\u6a21\u6001\u57df\u548c\u4e0d\u540c\u56fe\u50cf\u5206\u8fa8\u7387\u7684\u5927\u89c4\u6a21\u6d4b\u8bd5\u96c6\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u4e86\u6211\u4eec\u4f18\u5316\u65b9\u6cd5\u4e2d\u6240\u6709\u6a21\u5757\u7684\u6709\u6548\u6027\u3002\u4e0e\u73b0\u6709\u7684\u5173\u952e\u70b9\u5339\u914d\u3001\u5bc6\u96c6\u5339\u914d\u53ca\u4f18\u5316\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u914d\u51c6\u7ed3\u679c\u3002"}}
{"id": "2601.16378", "pdf": "https://arxiv.org/pdf/2601.16378", "abs": "https://arxiv.org/abs/2601.16378", "authors": ["Bridget Leonard", "Scott O. Murray"], "title": "Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent's visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u201c\u89c6\u89d2\u6807\u8bb0\u201d\uff08perspective tokens\uff09\u589e\u5f3a\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u8fdb\u884c\u4ed6\u8005\u89c6\u89d2\u7684\u89c6\u89c9\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u5728\u591a\u79cd\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u91c7\u7528\u4ed6\u4eba\u89c6\u89c9\u89c6\u89d2\u7684\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u81ea\u6211\u4e2d\u5fc3\u504f\u89c1\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u5f02\u6211\u4e2d\u5fc3\uff08allocentric\uff09\u63a8\u7406\u80fd\u529b\u3002", "method": "\u53d7\u4eba\u7c7b\u7a7a\u95f4\u8ba4\u77e5\u542f\u53d1\uff0c\u63d0\u51fa\u4e24\u79cd\u89c6\u89d2\u6807\u8bb0\uff1a(1) \u57fa\u4e8e\u8eab\u4f53\u5173\u952e\u70b9\u7684\u5177\u8eab\u7ebf\u7d22\uff1b(2) \u652f\u6301\u5fc3\u7406\u65cb\u8f6c\u7684\u62bd\u8c61\u8868\u793a\u3002\u5c06\u8fd9\u4e9b\u6807\u8bb0\u96c6\u6210\u5230LLaVA-1.5-13B\u6a21\u578b\u4e2d\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728Isle Bricks V2\u3001COCO\u548c3DSRBench\u7b49\u5408\u6210\u4e0e\u81ea\u7136\u57fa\u51c6\u4e0a\uff0c\u89c6\u89d2\u6807\u8bb0\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u7387\uff0c\u5176\u4e2d\u57fa\u4e8e\u65cb\u8f6c\u7684\u6807\u8bb0\u8fd8\u80fd\u6cdb\u5316\u81f3\u975e\u4eba\u7c7b\u53c2\u8003\u4e3b\u4f53\u3002", "conclusion": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5df2\u5177\u5907\u5f02\u6211\u4e2d\u5fc3\u63a8\u7406\u7684\u6f5c\u5728\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5408\u9002\u7684\u5185\u90e8\u7ed3\u6784\uff1b\u5c06\u8ba4\u77e5\u5408\u7406\u7684\u7a7a\u95f4\u7ed3\u6784\u76f4\u63a5\u5d4c\u5165token\u7a7a\u95f4\uff0c\u662f\u4e00\u79cd\u8f7b\u91cf\u4e14\u6a21\u578b\u65e0\u5173\u7684\u89c6\u89d2\u91c7\u62e9\u673a\u5236\u3002", "summary_cn": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLMs\uff09\u5728\u8bed\u4e49\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u91c7\u7528\u5176\u4ed6\u667a\u80fd\u4f53\u89c6\u89c9\u89c6\u89d2\u7684\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u7c7b\u9519\u8bef\u53cd\u6620\u51fa\u6a21\u578b\u5b58\u5728\u6301\u7eed\u7684\u81ea\u6211\u4e2d\u5fc3\u504f\u89c1\uff0c\u5e76\u5f15\u53d1\u5bf9\u5f53\u524d\u6a21\u578b\u662f\u5426\u652f\u6301\u5f02\u6211\u4e2d\u5fc3\uff08allocentric\uff09\u63a8\u7406\u7684\u8d28\u7591\u3002\u53d7\u4eba\u7c7b\u7a7a\u95f4\u8ba4\u77e5\u673a\u5236\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u201c\u89c6\u89d2\u6807\u8bb0\u201d\uff08perspective tokens\uff09\u2014\u2014\u4e00\u79cd\u4e13\u95e8\u7684\u5d4c\u5165\u8868\u793a\uff0c\u53ef\u901a\u8fc7\uff081\uff09\u5177\u8eab\u5316\u7684\u8eab\u4f53\u5173\u952e\u70b9\u7ebf\u7d22\u6216\uff082\uff09\u652f\u6301\u5fc3\u7406\u65cb\u8f6c\u7684\u62bd\u8c61\u8868\u793a\u6765\u7f16\u7801\u65b9\u5411\u4fe1\u606f\u3002\u5c06\u8fd9\u4e9b\u6807\u8bb0\u96c6\u6210\u5230LLaVA-1.5-13B\u6a21\u578b\u4e2d\u540e\uff0c\u5176\u5728\u4e8c\u7ea7\u89c6\u89c9\u89c6\u89d2\u91c7\u62e9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002\u5728\u5408\u6210\u4e0e\u81ea\u7136\u573a\u666f\u7684\u591a\u4e2a\u57fa\u51c6\uff08Isle Bricks V2\u3001COCO\u30013DSRBench\uff09\u4e0a\uff0c\u89c6\u89d2\u6807\u8bb0\u5747\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u7387\uff0c\u5176\u4e2d\u57fa\u4e8e\u65cb\u8f6c\u7684\u6807\u8bb0\u8fd8\u80fd\u6cdb\u5316\u81f3\u975e\u4eba\u7c7b\u53c2\u8003\u4e3b\u4f53\u3002\u8868\u5f81\u5206\u6790\u8868\u660e\uff0c\u5fae\u8c03\u589e\u5f3a\u4e86\u57fa\u7840\u6a21\u578b\u4e2d\u5df2\u5b58\u5728\u7684\u6f5c\u5728\u65b9\u5411\u654f\u611f\u6027\uff0c\u8bf4\u660e\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5df2\u5305\u542b\u5f02\u6211\u4e2d\u5fc3\u63a8\u7406\u7684\u96cf\u5f62\uff0c\u4f46\u7f3a\u4e4f\u9002\u5f53\u7684\u5185\u90e8\u7ed3\u6784\u3002\u603b\u4f53\u800c\u8a00\uff0c\u5c06\u8ba4\u77e5\u4e0a\u5408\u7406\uff08cognitively grounded\uff09\u7684\u7a7a\u95f4\u7ed3\u6784\u76f4\u63a5\u5d4c\u5165\u5230token\u7a7a\u95f4\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u89c6\u89d2\u91c7\u62e9\u673a\u5236\uff0c\u53ef\u5b9e\u73b0\u66f4\u7c7b\u4eba\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.16381", "pdf": "https://arxiv.org/pdf/2601.16381", "abs": "https://arxiv.org/abs/2601.16381", "authors": ["Yuxin Jiang", "Yunkang Cao", "Yuqi Cheng", "Yiheng Zhang", "Weiming Shen"], "title": "VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVTFusion\uff0c\u4e00\u79cd\u9762\u5411\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff08FSAD\uff09\u7684\u89c6\u89c9-\u6587\u672c\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4e13\u7528\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u5de5\u4e1a\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709FSAD\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5728\u81ea\u7136\u573a\u666f\u4e0a\u9884\u8bad\u7ec3\u7684\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5de5\u4e1a\u68c0\u6d4b\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u3001\u9886\u57df\u7279\u5b9a\u8bed\u4e49\uff1b\u540c\u65f6\uff0c\u4e3b\u6d41\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u89e3\u51b3\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u95f4\u7684\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "VTFusion\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\uff1a1\uff09\u4e3a\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u5f15\u5165\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u8868\u5f81\u4ee5\u5f25\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u5de5\u4e1a\u6570\u636e\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210\u5f02\u5e38\u589e\u5f3a\u7279\u5f81\u5224\u522b\u529b\uff1b2\uff09\u8bbe\u8ba1\u4e13\u7528\u7684\u591a\u6a21\u6001\u9884\u6d4b\u878d\u5408\u6a21\u5757\uff0c\u5305\u62ec\u4fc3\u8fdb\u8de8\u6a21\u6001\u4fe1\u606f\u4ea4\u4e92\u7684\u878d\u5408\u5757\u548c\u5728\u591a\u6a21\u6001\u6307\u5bfc\u4e0b\u751f\u6210\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u5f02\u5e38\u56fe\u7684\u5206\u5272\u7f51\u7edc\u3002", "result": "\u57282-shot\u8bbe\u7f6e\u4e0b\uff0cVTFusion\u5728MVTec AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523096.8%\u548c86.2%\u7684\u56fe\u50cf\u7ea7AUROC\uff1b\u5728\u8bba\u6587\u5f15\u5165\u7684\u771f\u5b9e\u5de5\u4e1a\u6c7d\u8f66\u5851\u6599\u90e8\u4ef6\u6570\u636e\u96c6\u4e0a\uff0c\u53d6\u5f97\u4e8693.5%\u7684AUPRO\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "VTFusion\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u81ea\u9002\u5e94\u7279\u5f81\u5b66\u4e60\u548c\u6df1\u5ea6\u591a\u6a21\u6001\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86FSAD\u4e2d\u9886\u57df\u5dee\u5f02\u548c\u6a21\u6001\u9519\u4f4d\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "summary_cn": "\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff08FSAD\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u5173\u952e\u8303\u5f0f\uff0c\u65e8\u5728\u5229\u7528\u7a00\u7f3a\u7684\u6b63\u5e38\u6837\u672c\u53c2\u8003\u6765\u8bc6\u522b\u5f02\u5e38\u3002\u5c3d\u7ba1\u8fd1\u671f\u65b9\u6cd5\u5df2\u5c06\u6587\u672c\u8bed\u4e49\u878d\u5165\u4ee5\u8865\u5145\u89c6\u89c9\u6570\u636e\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5728\u81ea\u7136\u573a\u666f\u4e0a\u9884\u8bad\u7ec3\u7684\u7279\u5f81\uff0c\u4ece\u800c\u5ffd\u7565\u4e86\u5de5\u4e1a\u68c0\u6d4b\u6240\u5fc5\u9700\u7684\u7ec6\u7c92\u5ea6\u3001\u9886\u57df\u7279\u5b9a\u8bed\u4e49\u3002\u6b64\u5916\uff0c\u5f53\u524d\u4e3b\u6d41\u7684\u878d\u5408\u7b56\u7565\u5f80\u5f80\u91c7\u7528\u7b80\u5355\u7684\u62fc\u63a5\u65b9\u5f0f\uff0c\u672a\u80fd\u89e3\u51b3\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u56fa\u6709\u7684\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u8fd9\u4f1a\u524a\u5f31\u6a21\u578b\u5bf9\u8de8\u6a21\u6001\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u5f25\u5408\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86VTFusion\uff0c\u4e00\u4e2a\u4e13\u4e3aFSAD\u8bbe\u8ba1\u7684\u89c6\u89c9-\u6587\u672c\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u4e24\u9879\u6838\u5fc3\u8bbe\u8ba1\uff1a\u9996\u5148\uff0c\u5f15\u5165\u4e86\u9488\u5bf9\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u7684\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ee5\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u8868\u5f81\uff0c\u5f25\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u5de5\u4e1a\u6570\u636e\u4e4b\u95f4\u7684\u9886\u57df\u9e3f\u6c9f\uff1b\u5e76\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210\u5f02\u5e38\u6765\u589e\u5f3a\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u3002\u5176\u6b21\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u7528\u7684\u591a\u6a21\u6001\u9884\u6d4b\u878d\u5408\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5305\u542b\u4e00\u4e2a\u4fc3\u8fdb\u4e30\u5bcc\u8de8\u6a21\u6001\u4fe1\u606f\u4ea4\u6362\u7684\u878d\u5408\u5757\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5728\u591a\u6a21\u6001\u5f15\u5bfc\u4e0b\u751f\u6210\u7cbe\u7ec6\u5316\u50cf\u7d20\u7ea7\u5f02\u5e38\u56fe\u7684\u5206\u5272\u7f51\u7edc\u3002VTFusion\u663e\u8457\u63a8\u8fdb\u4e86FSAD\u7684\u6027\u80fd\uff0c\u5728MVTec AD\u548cVisA\u6570\u636e\u96c6\u76842-shot\u573a\u666f\u4e0b\uff0c\u56fe\u50cf\u7ea7AUROC\u5206\u522b\u8fbe\u5230\u4e8696.8%\u548c86.2%\u3002\u6b64\u5916\uff0cVTFusion\u5728\u672c\u6587\u5f15\u5165\u7684\u4e00\u4e2a\u771f\u5b9e\u5de5\u4e1a\u6c7d\u8f66\u5851\u6599\u90e8\u4ef6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8693.5%\u7684AUPRO\uff0c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u5728\u4e25\u82db\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.16394", "pdf": "https://arxiv.org/pdf/2601.16394", "abs": "https://arxiv.org/abs/2601.16394", "authors": ["Yihao Wang", "Jusheng Zhang", "Ziyi Tang", "Keze Wang", "Meng Yang"], "title": "ResAgent: Entropy-based Prior Point Discovery and Visual Reasoning for Referring Expression Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages, 7gigures", "summary": "Referring Expression Segmentation (RES) is a core vision-language segmentation task that enables pixel-level understanding of targets via free-form linguistic expressions, supporting critical applications such as human-robot interaction and augmented reality. Despite the progress of Multimodal Large Language Model (MLLM)-based approaches, existing RES methods still suffer from two key limitations: first, the coarse bounding boxes from MLLMs lead to redundant or non-discriminative point prompts; second, the prevalent reliance on textual coordinate reasoning is unreliable, as it fails to distinguish targets from visually similar distractors. To address these issues, we propose \\textbf{\\model}, a novel RES framework integrating \\textbf{E}ntropy-\\textbf{B}ased Point \\textbf{D}iscovery (\\textbf{EBD}) and \\textbf{V}ision-\\textbf{B}ased \\textbf{R}easoning (\\textbf{VBR}). Specifically, EBD identifies high-information candidate points by modeling spatial uncertainty within coarse bounding boxes, treating point selection as an information maximization process. VBR verifies point correctness through joint visual-semantic alignment, abandoning text-only coordinate inference for more robust validation. Built on these components, \\model implements a coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding. Extensive evaluations on four benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg) demonstrate that \\model achieves new state-of-the-art performance across all four benchmarks, highlighting its effectiveness in generating accurate and semantically grounded segmentation masks with minimal prompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\\model\uff0c\u4e00\u79cd\u7ed3\u5408\u57fa\u4e8e\u71b5\u7684\u70b9\u53d1\u73b0\uff08EBD\uff09\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u63a8\u7406\uff08VBR\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u4ee3\u8868\u8fbe\u5206\u5272\uff08RES\uff09\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684RES\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4e00\u662fMLLM\u751f\u6210\u7684\u7c97\u7565\u8fb9\u754c\u6846\u5bfc\u81f4\u5197\u4f59\u6216\u975e\u5224\u522b\u6027\u70b9\u63d0\u793a\uff1b\u4e8c\u662f\u4f9d\u8d56\u6587\u672c\u5750\u6807\u63a8\u7406\u4e0d\u53ef\u9760\uff0c\u96be\u4ee5\u533a\u5206\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u5e72\u6270\u7269\u3002", "method": "\\model\u91c7\u7528\u7c97\u5230\u7cbe\u7684\u6d41\u7a0b\uff1a\u9996\u5148\u521d\u59cb\u5316\u8fb9\u754c\u6846\uff0c\u7136\u540e\u901a\u8fc7EBD\u6a21\u5757\u5728\u8fb9\u754c\u6846\u5185\u5efa\u6a21\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u4ee5\u53d1\u73b0\u9ad8\u4fe1\u606f\u91cf\u5019\u9009\u70b9\uff0c\u518d\u5229\u7528VBR\u6a21\u5757\u901a\u8fc7\u89c6\u89c9-\u8bed\u4e49\u8054\u5408\u5bf9\u9f50\u9a8c\u8bc1\u70b9\u7684\u6b63\u786e\u6027\uff0c\u6700\u540e\u8fdb\u884c\u63a9\u7801\u89e3\u7801\u3002EBD\u5c06\u70b9\u9009\u62e9\u89c6\u4e3a\u4fe1\u606f\u6700\u5927\u5316\u8fc7\u7a0b\uff0cVBR\u5219\u6452\u5f03\u7eaf\u6587\u672c\u5750\u6807\u63a8\u7406\uff0c\u91c7\u7528\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5728RefCOCO\u3001RefCOCO+\u3001RefCOCOg\u548cReasonSeg\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\\model\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\\model\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u70b9\u53d1\u73b0\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6307\u4ee3\u8868\u8fbe\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4ec5\u9700\u5c11\u91cf\u63d0\u793a\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u5206\u5272\u63a9\u7801\u3002", "summary_cn": "\u6307\u4ee3\u8868\u8fbe\u5206\u5272\uff08RES\uff09\u662f\u4e00\u9879\u6838\u5fc3\u7684\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u4efb\u52a1\uff0c\u5b83\u901a\u8fc7\u81ea\u7531\u5f62\u5f0f\u7684\u8bed\u8a00\u8868\u8fbe\u5b9e\u73b0\u5bf9\u76ee\u6807\u7684\u50cf\u7d20\u7ea7\u7406\u89e3\uff0c\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5173\u952e\u5e94\u7528\u3002\u5c3d\u7ba1\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u65b9\u6cd5\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709RES\u65b9\u6cd5\u4ecd\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u9996\u5148\uff0cMLLM\u751f\u6210\u7684\u7c97\u7565\u8fb9\u754c\u6846\u5bfc\u81f4\u5197\u4f59\u6216\u975e\u5224\u522b\u6027\u7684\u70b9\u63d0\u793a\uff1b\u5176\u6b21\uff0c\u666e\u904d\u4f9d\u8d56\u7684\u6587\u672c\u5750\u6807\u63a8\u7406\u4e0d\u53ef\u9760\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u5e72\u6270\u7269\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\\model\uff0c\u4e00\u79cd\u65b0\u9896\u7684RES\u6846\u67b6\uff0c\u6574\u5408\u4e86\u57fa\u4e8e\u71b5\u7684\u70b9\u53d1\u73b0\uff08EBD\uff09\u548c\u57fa\u4e8e\u89c6\u89c9\u7684\u63a8\u7406\uff08VBR\uff09\u3002\u5177\u4f53\u800c\u8a00\uff0cEBD\u901a\u8fc7\u5728\u7c97\u7565\u8fb9\u754c\u6846\u5185\u5efa\u6a21\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u6765\u8bc6\u522b\u9ad8\u4fe1\u606f\u91cf\u7684\u5019\u9009\u70b9\uff0c\u5c06\u70b9\u9009\u62e9\u89c6\u4e3a\u4e00\u4e2a\u4fe1\u606f\u6700\u5927\u5316\u8fc7\u7a0b\uff1bVBR\u5219\u901a\u8fc7\u89c6\u89c9-\u8bed\u4e49\u8054\u5408\u5bf9\u9f50\u9a8c\u8bc1\u70b9\u7684\u6b63\u786e\u6027\uff0c\u6452\u5f03\u4ec5\u4f9d\u8d56\u6587\u672c\u7684\u5750\u6807\u63a8\u7406\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u9a8c\u8bc1\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7ec4\u4ef6\uff0c\\model\u5b9e\u73b0\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a\u8fb9\u754c\u6846\u521d\u59cb\u5316\u3001\u71b5\u5f15\u5bfc\u7684\u70b9\u53d1\u73b0\u3001\u57fa\u4e8e\u89c6\u89c9\u7684\u9a8c\u8bc1\u548c\u63a9\u7801\u89e3\u7801\u3002\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08RefCOCO\u3001RefCOCO+\u3001RefCOCOg\u548cReasonSeg\uff09\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\\model\u5728\u6240\u6709\u56db\u4e2a\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u51f8\u663e\u4e86\u5176\u5728\u751f\u6210\u51c6\u786e\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u5206\u5272\u63a9\u7801\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e14\u4ec5\u9700\u6781\u5c11\u7684\u63d0\u793a\u3002"}}
{"id": "2601.16413", "pdf": "https://arxiv.org/pdf/2601.16413", "abs": "https://arxiv.org/abs/2601.16413", "authors": ["Chunwei Tian", "Chengyuan Zhang", "Bob Zhang", "Zhiwu Li", "C. L. Philip Chen", "David Zhang"], "title": "A Cosine Network for Image Super-Resolution", "categories": ["cs.CV"], "comment": "in IEEE Transactions on Image Processing (2025)", "summary": "Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u4f59\u5f26\u7f51\u7edc\uff08CSRNet\uff09\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5947\u5076\u5f02\u6784\u6a21\u5757\u63d0\u53d6\u4e92\u8865\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u4f59\u5f26\u9000\u706b\u673a\u5236\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u5f53\u524d\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0c\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u867d\u80fd\u9010\u5c42\u63d0\u53d6\u7ed3\u6784\u4fe1\u606f\uff0c\u4f46\u5982\u4f55\u6709\u6548\u4fdd\u7559\u5e76\u5229\u7528\u8fd9\u4e9b\u7ed3\u6784\u4fe1\u606f\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u4fe1\u606f\u7684\u591a\u6837\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCSRNet\u7f51\u7edc\uff1a1\uff09\u8bbe\u8ba1\u5947\u5076\u5f02\u6784\u5757\u4ee5\u589e\u5f3a\u67b6\u6784\u5dee\u5f02\uff0c\u63d0\u53d6\u4e92\u8865\u7684\u540c\u6e90\u7ed3\u6784\u4fe1\u606f\uff1b2\uff09\u878d\u5408\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u7ed3\u6784\u4fe1\u606f\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\uff1b3\uff09\u91c7\u7528\u4f59\u5f26\u9000\u706b\u673a\u5236\u8fdb\u884c\u5e26\u70ed\u91cd\u542f\u7684\u5b66\u4e60\u7387\u8c03\u6574\uff0c\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684CSRNet\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u6216\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0cCSRNet\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u4fe1\u606f\u591a\u6837\u6027\u4e0e\u4f18\u5316\u8bad\u7ec3\u673a\u5236\u7684\u91cd\u8981\u6027\u3002", "summary_cn": "\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5229\u7528\u5c42\u6b21\u5316\u4fe1\u606f\u9010\u6b65\u63d0\u53d6\u7ed3\u6784\u7279\u5f81\u4ee5\u6062\u590d\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u7136\u800c\uff0c\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\uff0c\u4fdd\u6301\u6240\u83b7\u5f97\u7ed3\u6784\u4fe1\u606f\u7684\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u4f59\u5f26\u7f51\u7edc\uff08CSRNet\uff09\uff0c\u901a\u8fc7\u6539\u8fdb\u7f51\u7edc\u67b6\u6784\u548c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u4e3a\u63d0\u53d6\u4e92\u8865\u7684\u540c\u6e90\u7ed3\u6784\u4fe1\u606f\uff0c\u8bbe\u8ba1\u4e86\u5947\u5076\u5f02\u6784\u6a21\u5757\uff0c\u4ee5\u6269\u5927\u67b6\u6784\u5dee\u5f02\u5e76\u63d0\u5347\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u3002\u7ed3\u5408\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u7ed3\u6784\u4fe1\u606f\u53ef\u514b\u670d\u540c\u6e90\u4fe1\u606f\u7684\u5c40\u9650\u6027\uff0c\u589e\u5f3a\u6240\u5f97\u7ed3\u6784\u4fe1\u606f\u7684\u9c81\u68d2\u6027\u3002\u8003\u8651\u5230\u68af\u5ea6\u4e0b\u964d\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u95ee\u9898\uff0c\u91c7\u7528\u4f59\u5f26\u9000\u706b\u673a\u5236\uff0c\u901a\u8fc7\u70ed\u91cd\u542f\u548c\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u6765\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684CSRNet\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u5177\u6709\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002"}}
{"id": "2601.16428", "pdf": "https://arxiv.org/pdf/2601.16428", "abs": "https://arxiv.org/abs/2601.16428", "authors": ["Shuying Li", "Qiang Ma", "San Zhang", "Chuang Yang"], "title": "DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \\href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}", "AI": {"tldr": "\u63d0\u51faDCCS-Det\u65b9\u6cd5\uff0c\u901a\u8fc7DSE\u6a21\u5757\u548cLaSEA\u6a21\u5757\u8054\u5408\u5efa\u6a21\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u5e76\u6291\u5236\u7279\u5f81\u5197\u4f59\uff0c\u5728\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\u8054\u5408\u5efa\u6a21\u4e0d\u8db3\u6216\u5b58\u5728\u7279\u5f81\u5197\u4f59\u4e0e\u8bed\u4e49\u7a00\u91ca\u95ee\u9898\uff0c\u5bfc\u81f4\u76ee\u6807-\u80cc\u666f\u533a\u5206\u80fd\u529b\u5f31\u3001\u76ee\u6807\u8868\u5f81\u8d28\u91cf\u5dee\u3002", "method": "\u63d0\u51faDCCS-Det\u68c0\u6d4b\u5668\uff0c\u5305\u542bDual-stream Saliency Enhancement (DSE) \u5757\u548cLatent-aware Semantic Extraction and Aggregation (LaSEA) \u6a21\u5757\uff1aDSE\u878d\u5408\u5c40\u90e8\u611f\u77e5\u4e0e\u65b9\u5411\u611f\u77e5\u4e0a\u4e0b\u6587\u4ee5\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u548c\u5c40\u90e8\u7ec6\u8282\uff1bLaSEA\u901a\u8fc7\u8de8\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u968f\u673a\u6c60\u5316\u91c7\u6837\u7b56\u7565\u7f13\u89e3\u7279\u5f81\u9000\u5316\uff0c\u589e\u5f3a\u5224\u522b\u6027\u7279\u5f81\u5e76\u6291\u5236\u566a\u58f0\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9886\u5148\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6709\u7ade\u4e89\u529b\u7684\u6548\u7387\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DSE\u548cLaSEA\u5bf9\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u76ee\u6807\u611f\u77e5\u4e0e\u7279\u5f81\u8868\u793a\u7684\u6709\u6548\u6027\u3002", "conclusion": "DCCS-Det\u6709\u6548\u89e3\u51b3\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5c40\u90e8-\u5168\u5c40\u5efa\u6a21\u4e0d\u8db3\u4e0e\u7279\u5f81\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "summary_cn": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff08IRSTD\uff09\u5728\u9065\u611f\u548c\u76d1\u89c6\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5176\u76ee\u6807\u662f\u5728\u590d\u6742\u80cc\u666f\u4e0b\u8bc6\u522b\u51fa\u5c0f\u800c\u4f4e\u5bf9\u6bd4\u5ea6\u7684\u76ee\u6807\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5e38\u96be\u4ee5\u5145\u5206\u8054\u5408\u5efa\u6a21\u5c40\u90e8-\u5168\u5c40\u7279\u5f81\uff08\u635f\u5bb3\u76ee\u6807-\u80cc\u666f\u533a\u5206\u80fd\u529b\uff09\uff0c\u6216\u5b58\u5728\u7279\u5f81\u5197\u4f59\u548c\u8bed\u4e49\u7a00\u91ca\u95ee\u9898\uff08\u964d\u4f4e\u76ee\u6807\u8868\u5f81\u8d28\u91cf\uff09\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DCCS-Det\uff08\u9762\u5411\u7ea2\u5916\u5c0f\u76ee\u6807\u7684\u65b9\u5411\u4e0a\u4e0b\u6587\u4e0e\u8de8\u5c3a\u5ea6\u611f\u77e5\u68c0\u6d4b\u5668\uff09\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u53cc\u6d41\u663e\u8457\u6027\u589e\u5f3a\uff08DSE\uff09\u6a21\u5757\u548c\u6f5c\u5728\u611f\u77e5\u8bed\u4e49\u63d0\u53d6\u4e0e\u805a\u5408\uff08LaSEA\uff09\u6a21\u5757\u3002DSE\u6a21\u5757\u7ed3\u5408\u5c40\u90e8\u611f\u77e5\u4e0e\u65b9\u5411\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u4ee5\u6355\u6349\u957f\u7a0b\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u5c40\u90e8\u7ec6\u8282\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cLaSEA\u6a21\u5757\u901a\u8fc7\u8de8\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u968f\u673a\u6c60\u5316\u91c7\u6837\u7b56\u7565\u7f13\u89e3\u7279\u5f81\u9000\u5316\uff0c\u589e\u5f3a\u5224\u522b\u6027\u7279\u5f81\u5e76\u6291\u5236\u566a\u58f0\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDCCS-Det\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6548\u7387\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86DSE\u548cLaSEA\u5728\u590d\u6742\u573a\u666f\u4e0b\u63d0\u5347\u76ee\u6807\u611f\u77e5\u4e0e\u7279\u5f81\u8868\u793a\u80fd\u529b\u7684\u8d21\u732e\u3002\\href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det\u5b98\u65b9\u4ee3\u7801\u5df2\u516c\u5f00\uff01}"}}
