<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

TL;DR: 本文提出在下游任务模型训练中引入新型鲁棒性损失，有效降低计算病理学基础模型对技术变异的敏感性，提升模型在真实临床数据中的泛化能力与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前组织病理学基础模型不仅学习生物学相关特征，还捕获了前处理和扫描仪相关的非生物变异，导致下游任务模型预测存在偏差。因此，亟需一种方法来缓解这种技术变异带来的影响，提高模型的鲁棒性和临床适用性。

Method: 在下游任务特定模型的训练过程中引入新的鲁棒性损失函数；利用包含6155名患者、27,042张全切片图像（WSI）的大规模实验设置，基于8个主流病理基础模型的特征训练数千个下游模型，系统评估其性能。

Result: 所提方法显著提升了模型对技术变异的鲁棒性，同时通过聚焦于生物学相关特征，提高了预测准确性。

Conclusion: 该方法无需重新训练基础模型，即可有效缓解计算病理学基础模型的鲁棒性问题，有助于开发适用于常规临床实践的真实世界数据的可靠计算病理模型。

Abstract: Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

Abstract (中文翻译): 组织病理学中的基础模型有望促进高性能、可泛化的深度学习系统的发展。然而，当前的模型不仅捕捉了生物学相关的特征，还包含了前处理和扫描仪特异性等技术变异，从而导致基于这些基础模型特征训练的任务特定模型产生偏差。本文表明，在下游任务特定模型的训练过程中引入新型鲁棒性损失，可以降低模型对技术变异的敏感性。我们设计了一个包含6155名患者、共计27,042张全切片图像（WSI）的综合性实验平台，基于八个流行的计算病理学基础模型的特征训练了数千个下游模型。除了显著提升模型鲁棒性外，我们还发现，聚焦于生物学相关特征可进一步提高预测准确性。本方法无需重新训练基础模型本身，即可有效缓解计算病理学基础模型的鲁棒性问题，从而推动适用于常规临床实践中真实世界数据的鲁棒计算病理模型的发展。

</details>


### [2] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

TL;DR: 本文提出MNAS-Unet，一种结合蒙特卡洛树搜索（MCTS）与神经架构搜索（NAS）的新型医学图像分割框架，在提升分割精度的同时显著降低搜索成本和模型参数量。


<details>
  <summary>Details</summary>
Motivation: 现有基于NAS的医学图像分割方法在搜索效率、模型轻量化和资源消耗方面仍存在不足，亟需一种更高效且实用的架构搜索策略。

Method: 将MCTS引入NAS流程，动态探索有潜力的网络结构；同时优化DownSC和UpSC单元，实现快速精准的模型调整。

Result: 在PROMISE12、Ultrasound Nerve和CHAOS等多个医学图像数据集上，MNAS-Unet优于NAS-Unet及其他SOTA模型；搜索预算减少54%，仅需0.6M参数，GPU内存占用更低。

Conclusion: MNAS-Unet在有限资源下兼顾搜索效率与分割精度，具有更强的实际应用价值。

Abstract: This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

Abstract (中文翻译): 本文提出了一种新颖的医学图像分割框架MNAS-Unet，该框架结合了蒙特卡洛树搜索（MCTS）与神经架构搜索（NAS）。MNAS-Unet通过MCTS动态探索有前景的网络架构，显著提升了架构搜索的效率与准确性。同时，该方法还优化了DownSC和UpSC单元结构，实现了快速而精确的模型调整。实验结果表明，MNAS-Unet在包括PROMISE12、超声神经（Ultrasound Nerve）和CHAOS在内的多个医学图像数据集上的分割精度优于NAS-Unet及其他当前最先进的模型。此外，与NAS-Unet相比，在相同搜索设置下，MNAS-Unet将架构搜索预算减少了54%（提前在139个epoch停止，而NAS-Unet需300个epoch），并获得了一个仅含0.6M参数的轻量级模型，同时降低了GPU内存消耗，进一步提升了其实用性。这些结果表明，在实际资源约束下，MNAS-Unet能够在保持竞争力的分割精度的同时提高搜索效率。

</details>


### [3] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

TL;DR: AeroDGS is a physics-guided 4D Gaussian splatting method for monocular UAV videos that addresses depth ambiguity and unstable motion estimation in dynamic aerial scenes by integrating geometric lifting and physical priors.


<details>
  <summary>Details</summary>
Motivation: Existing 4D reconstruction methods struggle with monocular aerial videos due to challenges like single-view capture, wide spatial coverage, small object footprint, and large motion disparity, leading to depth ambiguity and unreliable motion estimation.

Method: AeroDGS employs a Monocular Geometry Lifting module to reconstruct static and dynamic geometry from a single aerial sequence and a Physics-Guided Optimization module that enforces ground-support, upright-stability, and trajectory-smoothness priors to resolve monocular ambiguity and ensure physically plausible motion.

Result: Experiments on synthetic and real-world UAV datasets show that AeroDGS surpasses current state-of-the-art methods in reconstruction fidelity for dynamic aerial environments.

Conclusion: By combining geometry-aware initialization with physics-based constraints, AeroDGS effectively tackles the ill-posed nature of monocular dynamic aerial reconstruction, enabling robust and coherent 4D scene modeling.

Abstract: Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

Abstract (中文翻译): 近年来，4D场景重建的进展显著提升了多个领域中的动态建模能力。然而，在航拍条件下，现有方法仍面临单视角拍摄、大空间范围、动态物体空间占比小且运动差异大的限制。这些挑战导致严重的深度模糊和不稳定的运动估计，使得单目航拍重建本质上是病态问题。为此，我们提出了AeroDGS——一种面向单目无人机视频的物理引导4D高斯泼溅框架。AeroDGS引入了单目几何提升模块，可从单段航拍序列中重建可靠的静态与动态几何结构，为动态估计提供稳健基础。为进一步解决单目模糊性，我们提出了物理引导优化模块，融合可微分的地面支撑、直立稳定性与轨迹平滑性先验，将模糊的图像线索转化为物理一致的运动。该框架联合优化静态背景与动态实体，实现稳定几何结构与连贯时序演化。我们还构建了一个覆盖多种飞行高度与运动条件的真实世界无人机数据集，用于评估动态航拍重建性能。在合成与真实无人机场景上的实验表明，AeroDGS优于当前最先进的方法，在动态航拍环境中实现了更优的重建保真度。

</details>


### [4] [Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention](https://arxiv.org/abs/2602.22381)
*Zhengkang Fan,Chengkun Sun,Russell Terry,Jie Xu,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出了一种无需手动分割的深度学习框架，通过引入器官聚焦注意力（OFA）损失函数，在不依赖肿瘤区域分割的情况下实现肾肿瘤恶性程度预测，性能优于传统基于分割的方法。


<details>
  <summary>Details</summary>
Motivation: 现有影像学方法在术前预测肾肿瘤恶性程度时准确性不足，而依赖手动分割的传统深度学习方法虽能提升性能，但存在耗时、昂贵且依赖专家知识的问题，因此亟需一种无需分割即可准确预测的方法。

Method: 提出一种基于器官聚焦注意力（OFA）损失函数的深度学习框架，通过调整图像块的注意力机制，使器官区域仅关注其他器官区域，从而在部署阶段无需对3D肾CT图像进行分割即可进行恶性预测。

Result: 在UF IDR私有数据集上达到AUC 0.685、F1-score 0.872；在KiTS21公开数据集上达到AUC 0.760、F1-score 0.852，均优于依赖分割裁剪的传统模型。

Conclusion: 该方法在无需显式分割的前提下提升了恶性预测的准确性，为肾癌临床诊断提供了一种更高效、可靠的决策支持工具。

Abstract: Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.

Abstract (中文翻译): 准确预测肾肿瘤的恶性程度对于指导临床决策和优化治疗策略至关重要。然而，现有的影像学方法在手术干预前尚无法可靠地预测恶性程度。尽管深度学习在利用3D CT图像进行恶性预测方面展现出潜力，但传统方法通常依赖人工分割来分离肿瘤区域并减少噪声，从而提升预测性能。然而，人工分割过程费时费力、成本高昂，且依赖专家知识。本研究开发了一种深度学习框架，采用器官聚焦注意力（Organ Focused Attention, OFA）损失函数，调整图像块的注意力机制，使器官区域仅关注其他器官区域，从而在部署阶段无需对3D肾CT图像进行分割即可进行恶性预测。所提框架在来自UF综合数据仓库（IDR）的私有数据集上取得了AUC 0.685和F1分数0.872，在公开可用的KiTS21数据集上取得了AUC 0.760和F1分数0.852。这些结果优于依赖基于分割裁剪进行降噪的传统模型，表明该框架在无需显式分割输入的情况下仍能提升预测准确性。研究结果表明，该方法为肾癌恶性程度预测提供了一种更高效、可靠的新途径，有助于改善肾癌诊断中的临床决策。

</details>


### [5] [Vision Transformers Need More Than Registers](https://arxiv.org/abs/2602.22394)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

TL;DR: 本文发现Vision Transformer（ViT）中的伪影问题源于其“懒惰聚合”行为，即利用与语义无关的背景块作为捷径来表示全局语义，并提出通过选择性地将图像块特征整合到CLS token中以缓解该问题，在多种监督范式下提升性能。


<details>
  <summary>Details</summary>
Motivation: ViT在大规模数据预训练后虽能提供通用表征，但在不同监督范式和下游任务中普遍存在伪影（artifacts）问题，其根本机制尚不清晰，亟需深入分析与解决。

Method: 通过系统分析ViT中的伪影现象，揭示其源于全局注意力机制和粗粒度语义监督驱动下的“懒惰聚合”行为；提出一种选择性将图像块特征整合进CLS token的方法，以削弱背景主导的捷径效应。

Result: 所提方法在标签监督、文本监督和自监督三种范式下的12个基准任务中均取得一致性能提升。

Conclusion: 本研究揭示了ViT伪影问题的根源，并提供了一种有效缓解策略，为理解ViT的行为提供了新视角。

Abstract: Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.

Abstract (中文翻译): 视觉Transformer（ViT）在大规模数据上预训练后，能够为各种下游任务提供通用表征。然而，在不同监督范式和下游任务中，ViT中广泛存在伪影现象。通过对ViT中伪影的系统分析，我们发现其根本机制尚未得到充分阐明。本文通过系统分析得出结论：这些伪影源于一种“懒惰聚合”行为——ViT在全局注意力机制和粗粒度语义监督的驱动下，利用语义无关的背景图像块作为捷径来表示全局语义。我们的解决方案选择性地将图像块特征整合到CLS token中，减少背景主导捷径的影响，并在标签监督、文本监督和自监督三种范式下的12个基准任务中持续提升性能。我们希望这项工作能为理解ViT的行为提供新的视角。

</details>


### [6] [CLIP Is Shortsighted: Paying Attention Beyond the First Sentence](https://arxiv.org/abs/2602.22419)
*Marc-Antoine Lavoie,Anas Mahmoud,Aldo Zaimi,Arsene Fansi Tchango,Steven L. Waslander*

Main category: cs.CV

TL;DR: CLIP模型因训练数据中长文本描述常以简短摘要开头，导致注意力集中在开头句子而忽略后续细节。本文提出DeBias-CLIP，在训练时移除摘要句，并通过句子子采样和文本填充均衡各位置的监督信号，从而提升长文本检索性能且无需额外参数。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在预训练时主要使用图像与短文本配对，使其偏向于学习显著物体的简单描述，难以对复杂场景和密集描述进行细粒度对齐。尽管已有工作通过微调长文本数据集缓解此问题，但发现这些长文本（无论是人工还是LLM生成）通常以一句总结开头，造成模型依赖该“捷径”，削弱对后续内容的对齐能力。

Method: 提出DeBias-CLIP方法：在训练过程中移除长文本中的首句摘要，同时采用句子子采样（sentence sub-sampling）和文本token填充（text token padding），使监督信号均匀分布到所有文本位置，避免模型过度关注开头部分。

Result: DeBias-CLIP在长文本检索任务上达到SOTA性能，同时提升了短文本检索效果，并对句子顺序变化更鲁棒。该方法可直接替代Long-CLIP，无需引入额外可训练参数。

Conclusion: 通过消除长文本中首句摘要带来的训练偏差，DeBias-CLIP有效提升了CLIP模型对复杂描述的细粒度对齐能力，是一种高效且即插即用的改进方案。

Abstract: CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.

Abstract (中文翻译): CLIP模型通过在互联网规模数据上进行图像-文本对比学习，学习可迁移的多模态特征，广泛应用于零样本分类、多模态检索、文本到图像扩散模型以及作为大型视觉-语言模型中的图像编码器。然而，CLIP的预训练主要依赖于与简短标题配对的图像，这使得模型倾向于编码显著物体的简单描述，导致在复杂场景和密集描述上的对齐较为粗糙。尽管近期工作通过在小规模长标题数据集上微调来缓解这一问题，但我们发现一个重要的共性偏差：无论是人工撰写还是大语言模型（LLM）生成的长标题，通常都以一句总结性句子开头，随后才是详细描述。我们证明这种结构在训练中会形成“捷径”，使模型注意力集中在开头句子和早期token上，削弱了对标题其余部分的对齐能力。为解决此问题，我们提出了DeBias-CLIP方法：在训练过程中移除摘要句，并采用句子子采样和文本token填充策略，将监督信号均匀分布到所有token位置。DeBias-CLIP在长文本检索任务上达到当前最优性能，同时提升了短文本检索效果，并对句子顺序的排列变化具有更强的鲁棒性。该方法可作为Long-CLIP的即插即用替代方案，且无需引入额外的可训练参数。

</details>


### [7] [SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read](https://arxiv.org/abs/2602.22426)
*Yibo Peng,Peng Xia,Ding Zhong,Kaide Zeng,Siwei Han,Yiyang Zhou,Jiaqi Liu,Ruiyi Zhang,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文揭示了多模态大语言模型（MLLMs）在处理图像中嵌入文本时存在“模态惰性”问题，即模型倾向于依赖提示词中的参数化捷径而非真正读取图像中的文字。为此，作者提出Visualized-Question（VQ）设置进行诊断，并设计了一种名为SimpleOCR的即插即用训练策略，通过将训练样本转换为VQ格式并随机化样式，强制模型利用视觉文本提取路径。该方法在多个OOD基准上显著提升性能，且数据效率极高。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）是否真正“阅读”图像中的嵌入文本，还是仅依赖文本提示中的参数化捷径，这一关键问题尚未得到解答。作者旨在诊断并解决MLLMs在视觉接地机制中存在的“模态惰性”问题。

Method: 作者引入Visualized-Question（VQ）设置，将文本查询直接渲染到图像上，以结构性地强制模型进行视觉交互。基于此，提出SimpleOCR训练策略：将训练样本转换为VQ格式，并采用随机化样式，从而消除基于纯文本的捷径，迫使模型激活并优化其视觉文本提取通路。该方法无需修改模型架构，可即插即用，并能与高级强化学习策略（如NoisyRollout）结合。

Result: 在Qwen2.5-VL上的诊断实验显示，尽管模型具备强大的OCR能力，但在VQ设置下性能最多下降12.7%，证实了“模态惰性”的存在。应用SimpleOCR后，在四个OOD基准上平均提升5.4%，比基于原始图像的GRPO方法高2.7%。此外，SimpleOCR数据效率极高，仅用8.5K样本（为近期RL方法的1/30）即可实现更优性能，并能与NoisyRollout等RL策略协同增效。

Conclusion: MLLMs普遍存在“模态惰性”，倾向于回避对图像中文本的视觉解析。通过VQ设置可有效诊断该问题，而SimpleOCR作为一种简单、高效、即插即用的训练策略，能显著提升模型对视觉文本的利用能力，且具有良好的兼容性和数据效率。

Abstract: Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.

Abstract (中文翻译): 尽管多模态大语言模型（MLLMs）发展迅速，但其视觉接地机制中一个关键问题仍未解决：这些模型究竟是真正“阅读”图像中嵌入的文本，还是仅仅依赖文本提示中的参数化捷径？本文通过引入可视化问题（Visualized-Question, VQ）设置来诊断这一问题，该设置将文本查询直接渲染到图像上，从结构上强制模型进行视觉交互。在Qwen2.5-VL上的诊断实验揭示了一个惊人的能力-利用差距：尽管模型具备强大的OCR能力，但在VQ设置下性能最多下降12.7%，暴露出深层次的“模态惰性”。为弥合这一差距，我们提出了SimpleOCR——一种即插即用的训练策略，通过对学习过程施加结构性约束来解决问题。该方法将训练样本转换为VQ格式并采用随机化样式，有效消除了基于文本的捷径，迫使模型激活并优化其视觉文本提取通路。实验证明，SimpleOCR在不修改模型架构的情况下带来稳健的性能提升：在四个代表性OOD基准上，其性能超过基线模型5.4%，比基于原始图像的GRPO方法高2.7%；同时展现出极高的数据效率，仅用8.5K样本（为近期基于强化学习方法所需样本量的1/30）即可实现更优性能。此外，其即插即用特性使其能与NoisyRollout等先进强化学习策略无缝集成，产生互补性改进。代码已开源：https://github.com/aiming-lab/SimpleOCR。

</details>


### [8] [Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge](https://arxiv.org/abs/2602.22455)
*Giuseppe Lando,Rosario Forte,Antonino Furnari*

Main category: cs.CV

TL;DR: 该论文研究在边缘设备上部署多模态大语言模型（MLLMs）用于实时在线情景记忆问答的可行性，提出双线程流水线架构，在保证低延迟和隐私的前提下，取得了接近云端方案的准确率。


<details>
  <summary>Details</summary>
Motivation: 云端卸载虽常见，但存在隐私泄露和延迟问题，尤其对于可穿戴助手而言。因此，作者探索在边缘设备上实现基于MLLM的情景记忆问答系统。

Method: 提出一个包含两个异步线程的问答流水线：Descriptor Thread持续将视频流转换为轻量级文本记忆；QA Thread基于该文本记忆进行推理以回答问题。整个系统考虑了流式处理约束，并在资源受限的边缘设备上部署MLLM。

Result: 在QAEgo4D-Closed基准测试中，消费级8GB GPU上端到端配置达到51.76%准确率，首字生成时间（TTFT）为0.41秒；本地企业级服务器上达到54.40%准确率，TTFT为0.88秒；相比之下，云端方案准确率为56.00%。

Conclusion: 边缘部署MLLM用于情景记忆问答具有显著潜力，能在保障隐私的同时提供接近云端的性能，适用于可穿戴等对隐私和延迟敏感的应用场景。

Abstract: We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.

Abstract (中文翻译): 我们研究了使用多模态大语言模型（MLLMs）进行实时在线情景记忆问答的可行性。尽管云端卸载很常见，但它会引发可穿戴助手中的隐私和延迟问题，因此我们探索在边缘设备上实现该系统。我们将流式处理约束整合到问答流水线中，该流水线由两个异步线程组成：一个描述符线程（Descriptor Thread）持续将视频转换为轻量级文本记忆，另一个问答线程（QA Thread）则基于该文本记忆进行推理以回答查询。在QAEgo4D-Closed基准上的实验分析了MLLM在严格资源限制下的性能，结果表明其表现甚至可与云端解决方案相媲美。具体而言，在消费级8GB GPU上运行的端到端配置实现了51.76%的准确率，首字生成时间（TTFT）为0.41秒；扩展到本地企业级服务器时，准确率达到54.40%，TTFT为0.88秒。相比之下，云端解决方案的准确率为56.00%。这些具有竞争力的结果凸显了基于边缘的解决方案在隐私保护型情景记忆检索中的潜力。

</details>


### [9] [MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation](https://arxiv.org/abs/2602.22462)
*Raiyan Jahangir,Nafiz Imtiaz Khan,Amritanand Sudheerkumar,Vladimir Filkov*

Main category: cs.CV

TL;DR: MammoWise is a local, open-source multi-model pipeline that leverages Vision Language Models (VLMs) for mammography report generation and multi-task classification, supporting flexible prompting strategies and optional RAG, with demonstrated improvements via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current VLM-based mammography reporting systems often rely on closed or tightly coupled architectures, limiting privacy, reproducibility, and adaptability. There is a need for an open, local, and flexible framework to support clinical deployment.

Method: The authors propose MammoWise, a local pipeline that integrates any Ollama-hosted VLM with mammography datasets. It supports zero-shot, few-shot, and Chain-of-Thought prompting, optionally enhanced by multimodal Retrieval Augmented Generation (RAG). They evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets using metrics like BERTScore, ROUGE-L, and task-specific accuracies. Parameter-efficient fine-tuning (QLoRA) is applied to MedGemma.

Result: Report generation performance is strong and improves with few-shot prompting and RAG. Classification tasks (BI-RADS, density, findings) are feasible but vary by model and dataset. QLoRA-finetuned MedGemma achieves BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341, while maintaining high-quality reports.

Conclusion: MammoWise offers a practical, extensible, and reproducible local framework for deploying open-source VLMs in mammography reporting and classification, balancing performance, privacy, and adaptability.

Abstract: Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.

Abstract (中文翻译): 乳腺X线摄影筛查具有检查量大、时效性强和文档记录繁重的特点。放射科医生必须将细微的视觉发现转化为一致的BI-RADS评估、乳腺密度分类和结构化叙述报告。尽管近期的视觉语言模型（VLM）已实现图像到文本的报告生成，但许多系统依赖封闭的云平台或高度耦合的架构，限制了隐私性、可复现性和适应性。我们提出了MammoWise——一个本地多模型流水线，可将开源VLM转化为乳腺X线摄影报告生成器和多任务分类器。MammoWise支持任意Ollama托管的VLM和乳腺X线摄影数据集，并支持零样本、少样本和思维链提示，还可选地结合向量数据库进行多模态检索增强生成（RAG）以提供病例特定上下文。我们在VinDr-Mammo和DMID数据集上评估了MedGemma、LLaVA-Med和Qwen2.5-VL，评估指标包括报告质量（BERTScore、ROUGE-L）、BI-RADS分类、乳腺密度和关键发现。报告生成效果始终良好，并在少样本提示和RAG下进一步提升。分类任务可行但对模型和数据集选择敏感。通过对MedGemma进行参数高效微调（QLoRA），在保持报告质量的同时，实现了BI-RADS准确率0.7545、密度准确率0.8840和钙化准确率0.9341。MammoWise为在统一且可复现的工作流中本地部署VLM进行乳腺X线摄影报告提供了一个实用且可扩展的框架。

</details>


### [10] [Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models](https://arxiv.org/abs/2602.22469)
*Niamul Hassan Samin,Md Arifur Rahman,Abdullah Ibne Hanif,Juena Ahmed Noshin,Md Ashikur Rahman*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的推理时干预方法——空间信用重分配（SCR），通过将早期Transformer层中集中在稀疏视觉块上的激活重新分配到其上下文，有效缓解视觉语言模型中的幻觉问题，在多个基准上显著降低幻觉率，同时几乎不影响生成质量，且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）常在输入图像中不存在的物体上产生幻觉。作者发现这一问题源于“空间信用坍缩”：早期Transformer层中激活集中在稀疏的视觉块上，抑制了上下文证据，导致模型过度依赖语言先验。

Method: 提出Spatial Credit Redistribution (SCR) 方法，在推理阶段无需训练地将高注意力源块的隐藏状态激活重新分配给其上下文，该过程由低熵输入引导，并基于注意力机制选择源块。

Result: 在POPE和CHAIR基准上对六个模型家族（Chameleon、LLaVA、Qwen等）进行评估，SCR在POPE-Adversarial上降低幻觉4.7–6.0个百分点，在CHAIR-s和CHAIR-i上分别降低3.7–5.2和2.7–4.4个百分点（相对降幅达42–58%），CIDEr得分下降不超过0.8个百分点。SCR仅引入43–56毫秒延迟，优于现有方法（如OPERA、VCD、OVCD），并在幻觉率与CIDEr上实现帕累托优势。消融实验证明注意力引导的源块选择至关重要。

Conclusion: 空间信用坍缩是VLM幻觉的关键原因，而SCR作为一种高效、实用的推理时干预手段，能显著缓解该问题，适用于实时应用场景。

Abstract: Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.

Abstract (中文翻译): 视觉语言模型（VLMs）经常对输入图像中不存在的物体产生幻觉。我们将这一失败归因于“空间信用坍缩”：在早期Transformer层中，激活信用集中在稀疏的视觉块上，从而抑制了上下文证据并增强了对语言先验的依赖。我们提出了“空间信用重分配”（SCR），这是一种无需训练的推理阶段干预方法，它根据低熵输入的引导，将高注意力源块的隐藏状态激活重新分配给其上下文。我们在POPE和CHAIR基准上评估了六个模型系列（包括Chameleon、LLaVA和Qwen，涵盖Qwen-VL与Qwen2-VL），模型规模涵盖7B、13B和30B。实验表明，SCR在POPE-Adversarial上将幻觉率降低了约4.7–6.0个百分点，在CHAIR-s上降低了3.7–5.2个百分点（相对降低42–51%），在CHAIR-i上降低了2.7–4.4个百分点（相对降低44–58%），同时CIDEr得分下降不超过0.8个百分点。性能提升在低熵输入上最为显著，与理论框架一致。SCR仅带来43–56毫秒的额外延迟（小模型：+43–46毫秒；大模型：+54–56毫秒），约为OPERA和VCD延迟的1/3–1/6，也比OVCD（+72毫秒）低1.3–1.7倍，同时在幻觉率和CIDEr两个指标上均帕累托优于这三种方法，使其适用于实时场景。受控消融实验进一步证实，基于注意力的源块选择至关重要：若替换为均匀随机选择，幻觉率的改善将从约4.7–6.0个百分点降至仅2.6–3.4个百分点，表明信用坍缩确实是关键驱动因素。

</details>
