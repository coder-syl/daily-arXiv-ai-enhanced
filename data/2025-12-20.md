<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 提出一种结合规则掩膜变形与无配对图像翻译的两阶段生成式数据增强框架，用于提升遮挡人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 遮挡人脸识别面临数据稀缺和分布偏移两大挑战，现有方法难以生成足够真实多样的带口罩人脸图像。

Method: 采用两阶段生成式数据增强框架：首先进行基于规则的口罩形变，再利用GAN进行无配对图像到图像的转换；引入非口罩区域保留损失和随机噪声注入以稳定训练并提升样本多样性。

Result: 相比仅使用规则形变的方法，该方法在生成质量上取得一致提升，并能有效补充如IAMGAN等现有GAN方法。

Conclusion: 所提组件有效，为面向人脸识别的数据增强提供了新方向。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

Abstract (中文翻译): 数据稀缺和分布偏移给遮挡人脸的检测与识别带来了重大挑战。我们提出了一种两阶段生成式数据增强框架，该框架结合了基于规则的口罩形变与基于GAN的无配对图像到图像转换，能够生成超越纯合成变换的真实遮挡人脸样本。与仅使用规则形变的方法相比，所提出的方法在定性效果上持续提升，并可有效补充IAMGAN等现有的基于GAN的遮挡人脸生成方法。我们引入了非口罩区域保留损失和随机噪声注入机制，以稳定训练过程并增强样本多样性。实验结果验证了所提各组件的有效性，并为未来面向人脸识别任务的数据中心化增强方法指明了改进方向。

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出JARVIS框架，通过引入I-JEPA自监督学习范式增强多模态大语言模型（MLLMs）的视觉理解能力，显著提升其在以视觉为中心任务上的表现，同时不损害多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在基础视觉推理任务上表现有限，主要因其视觉理解依赖于主观且不完整的文本描述作为监督信号，且多模态指令微调数据规模远小于纯文本预训练，导致模型过度依赖语言先验而忽略视觉细节。

Method: 提出JARVIS框架，将I-JEPA自监督学习范式融入MLLMs的标准视觉-语言对齐流程中，利用冻结的视觉基础模型作为上下文和目标编码器，并训练以LLM早期层实现的预测器，从图像中学习结构与语义规律，减少对语言监督的依赖。

Result: 在标准MLLM基准测试中，JARVIS在不同LLM系列上均一致提升了以视觉为中心任务的性能，同时未降低多模态推理能力。

Conclusion: JARVIS有效增强了MLLMs的视觉理解能力，为解决其在基础视觉推理任务中的局限性提供了可行方案。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

Abstract (中文翻译): 多模态大语言模型（MLLMs）最近在连接视觉与语言方面展现出令人印象深刻的能力，但其在基础视觉推理任务上的表现仍显不足。这一局限性源于MLLMs主要从文本描述中学习视觉理解，而文本描述是一种主观且本质上不完整的监督信号。此外，与大规模纯文本预训练相比，多模态指令微调的数据规模较小，导致MLLMs过度拟合语言先验而忽视视觉细节。为解决这些问题，我们提出了JARVIS——一种受JEPA启发的MLLMs自监督视觉增强框架。具体而言，我们将I-JEPA学习范式整合到MLLMs训练的标准视觉-语言对齐流程中。该方法利用冻结的视觉基础模型作为上下文编码器和目标编码器，同时训练以LLM早期层实现的预测器，使其无需完全依赖语言监督即可从图像中学习结构和语义规律。在标准MLLM基准上的大量实验表明，JARVIS在不同LLM系列中均能持续提升以视觉为中心的基准任务性能，同时不会削弱多模态推理能力。我们的源代码已公开：https://github.com/aimagelab/JARVIS。

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: 本文提出了一种新的任务“稀疏接地视觉导航”（Sparsely Grounded Visual Navigation）和相应基准CityNav，用于评估多模态大语言模型（MLLMs）在真实城市环境中仅依靠视觉输入进行知识密集型导航的能力，并提出了新方法VoP以显著提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 当前对具身智能体的评估主要集中在语言任务或模拟环境中，缺乏对真实世界中复杂、知识密集型推理能力的有效评测。为填补这一空白，作者希望构建一个能真正测试MLLM在现实城市导航中序列决策能力的基准。

Method: 作者构建了CityNav基准，在四个全球城市中设置50多个决策点，要求MLLM驱动的智能体仅使用视觉输入和内部多模态推理完成导航，不依赖环境标注或架构修改。同时提出“路径言语化”（Verbalization of Path, VoP）方法，通过从MLLM中显式提取认知地图（关键地标和方向）来增强其导航能力。

Result: 实验表明，现有最先进的MLLM及其标准推理技术（如思维链、反思）在该任务上表现不佳；而所提出的VoP方法能显著提升导航成功率。

Conclusion: 稀疏接地视觉导航任务和CityNav基准有效揭示了当前MLLM在真实世界知识密集型导航中的局限性，而VoP方法通过显式构建认知地图，为提升MLLM的具身推理能力提供了有效途径。

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

Abstract (中文翻译): 利用多模态大语言模型（MLLMs）开发具身智能体在解决复杂的现实世界任务方面具有巨大潜力。然而，当前的评估基准仍主要以语言为中心，或严重依赖模拟环境，很少考察实际应用场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们提出了“稀疏接地视觉导航”（Sparsely Grounded Visual Navigation）任务，专门用于评估MLLM在具有挑战性的、知识密集型的真实世界环境中的序列决策能力。我们通过CityNav这一综合性基准来实现该任务，该基准涵盖全球四个不同的城市，专门用于评估由原始MLLM驱动的智能体在城市导航中的表现。智能体必须仅依靠视觉输入和内部多模态推理，在没有额外环境标注或专用架构修改的情况下，依次通过50多个决策点。至关重要的是，智能体必须能够自主通过解读城市特定线索和识别地标实现定位，进行空间推理，并战略性地规划和执行前往目的地的路线。通过广泛的评估，我们证明了当前最先进的MLLM及标准推理技术（例如思维链、反思）在此类具有挑战性的场景中表现显著不足。为解决这一问题，我们提出了“路径言语化”（Verbalization of Path, VoP）方法，该方法通过从MLLM中显式探查认知地图（关键地标及朝向目的地的方向），从而显著提升导航成功率。

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4 是一个无需训练的检索增强框架，通过构建四维（时空）结构化记忆，使视觉语言模型具备持久、可共享的世界模型，从而显著提升在动态环境中对时空信息的检索与推理能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过构建包含语义、空间和时间信息的四维内部表征来感知和推理环境，现有方法缺乏类似的能力。作者希望赋予视觉语言模型类似的结构化、持久记忆机制，以支持更强大的时空推理。

Method: 提出 R4 框架：在推理阶段将自然语言查询分解为语义、空间和时间键，在一个持续构建的四维知识库中检索相关观测；该知识库存储对象级语义描述及其在度量空间和时间中的锚点，无需训练即可实现基于检索的推理。

Result: 在具身问答和导航基准测试中，R4 相比基线方法显著提升了对时空信息的检索和推理性能。

Conclusion: R4 为动态环境中的具身智能体提供了一种新的四维推理范式，通过结构化、可共享的终身记忆机制，实现了无需训练的高效时空推理。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

Abstract (中文翻译): 人类通过构建持久、结构化的内部表征来在四维（三维空间加时间）中感知和推理周围环境，这些表征编码了语义含义、空间布局和时间动态。这种多模态记忆使人类能够回忆过去事件、推断未观察到的状态，并将新信息整合到依赖上下文的推理中。受此启发，我们提出了 R4——一种无需训练的检索增强推理框架，用于在四维时空空间中为视觉语言模型（VLMs）配备结构化的终身记忆。R4 通过将对象级语义描述锚定在度量空间和时间中，持续构建一个四维知识数据库，从而形成一个可在多个智能体之间共享的持久世界模型。在推理阶段，自然语言查询被分解为语义、空间和时间关键词，用于检索相关观测，并将其整合到 VLM 的推理过程中。与传统的检索增强生成方法不同，R4 的检索直接在四维空间中进行，无需训练即可实现情景式和协作式推理。在具身问答和导航基准上的实验表明，R4 在时空信息的检索与推理方面显著优于基线方法，推动了动态环境中具身四维推理的新范式。

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出“感知观测站”（The Perceptual Observatory）框架，用于系统评估多模态大语言模型（MLLMs）在受控扰动下的感知能力，超越传统端到端准确率指标，关注其视觉基础、鲁棒性和推理保真度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）虽在规模上不断增长，但其感知能力缺乏系统评估；多数模型仅扩展语言部分而复用相同视觉编码器，难以判断其性能提升源于真实视觉理解还是依赖文本先验知识。现有评测方法侧重任务准确率，忽视了鲁棒性、归因忠实度及扰动下的推理能力。

Method: 作者构建了“感知观测站”评估框架，涵盖两类垂直任务：(i) 简单视觉任务（如人脸匹配、图像中文本理解）；(ii) 从局部到全局的理解任务（如图像匹配、网格指向游戏、属性定位）。所有任务均基于含真值的人脸与文字数据集，并通过像素级增强和扩散模型生成的风格化错觉进行系统扰动。

Result: 该框架揭示了MLLM在扰动下如何保持感知基础和关系结构，提供了对其感知能力优势与缺陷的深入洞察，超越了传统排行榜准确率的局限。

Conclusion: “感知观测站”为评估当前及未来MLLM的感知能力提供了原则性基础，强调需关注模型在扰动环境下的视觉接地能力和结构保持能力，而非仅依赖端任务准确率。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

Abstract (中文翻译): 多模态大语言模型（MLLMs）的最新进展催生了日益强大的模型，但其感知能力仍缺乏充分刻画。在实践中，大多数模型系列仅扩展语言组件，而重复使用几乎相同的视觉编码器（例如 Qwen2.5-VL 的 3B/7B/72B 版本），这引发了一个关键问题：模型的进步究竟源于真实的视觉接地能力，还是依赖于互联网规模的文本世界知识？现有的评估方法侧重于端到端任务的准确率，忽略了模型的鲁棒性、归因保真度以及在受控扰动下的推理能力。为此，我们提出了“感知观测站”（The Perceptual Observatory）框架，从多个维度对 MLLMs 进行刻画：(i) 简单视觉任务，如人脸匹配和图像中文本理解能力；(ii) 从局部到全局的理解能力，包括图像匹配、网格指向游戏和属性定位，以测试通用的视觉接地能力。每个维度均基于包含人脸和文字真值的数据集，并通过基于像素的增强和基于扩散模型的风格化错觉进行系统性扰动。“感知观测站”超越了排行榜准确率，深入揭示了 MLLMs 在扰动下如何保持感知接地和关系结构，为分析当前及未来模型的优势与不足提供了原则性基础。

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 本文提出CAMP-VLM，一种基于视觉语言模型的框架，用于从第三人称视角预测多个人类行为，通过结合视觉上下文和场景图的空间信息，在合成与真实数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注以自我中心视角预测单人行为，但许多机器人应用场景需要从第三人称视角理解多人行为，因此亟需新的方法和数据支持。

Method: 提出CAMP-VLM框架，融合视觉输入的上下文特征和场景图的空间感知；利用逼真模拟器生成的合成数据进行监督微调（SFT）和直接偏好优化（DPO）。

Result: CAMP-VLM在预测准确率上比最佳基线方法最高提升66.9%，并在合成与真实世界序列上验证了其泛化能力。

Conclusion: CAMP-VLM有效提升了从观察者视角对多个人类行为的预测性能，为移动机器人在人群环境中的应用提供了可行方案。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

Abstract (中文翻译): 准确预测人类行为对于在有人环境中运行的移动机器人至关重要。尽管以往的研究主要集中在从自我中心视角预测单人场景中的行为，但许多机器人应用需要从第三人称视角理解多个人类的行为。为此，我们提出了CAMP-VLM（上下文感知的多人行为预测）：一种基于视觉语言模型（VLM）的框架，该框架结合了来自视觉输入的上下文特征和来自场景图的空间感知，以增强对人-场景交互的预测能力。由于缺乏适用于从观察者视角进行多人行为预测的数据集，我们使用由逼真模拟器生成的合成人类行为数据对CAMP-VLM进行微调，并在合成数据和真实世界序列上评估所得模型的泛化能力。通过监督微调（SFT）和直接偏好优化（DPO），CAMP-VLM在预测准确率上比表现最好的基线方法最高提升了66.9%。

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文探索利用视觉-语言模型（VLM）进行少样本多光谱目标检测，通过适配Grounding DINO和YOLO-World并融合文本、可见光与热成像模态，在FLIR和M3FD数据集上显著优于现有方法，证明了VLM的语义先验可有效迁移到多光谱感知任务中。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标检测在自动驾驶和监控等安全敏感应用中至关重要，但标注数据稀缺限制了深度检测器的训练；作者受视觉-语言模型（VLM）近期成功的启发，希望利用其语义先验提升少样本下的多光谱检测性能。

Method: 将Grounding DINO和YOLO-World两种代表性VLM检测器适配至多光谱输入，并设计机制融合文本、可见光与热成像三种模态信息。

Result: 在FLIR和M3FD两个多光谱基准上，所提方法在少样本设置下显著优于专用多光谱模型，在全监督设置下也达到具有竞争力甚至更优的性能。

Conclusion: 大规模VLM学习到的语义先验能有效迁移到未见的光谱模态，为数据高效的多光谱感知提供了有力途径。

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

Abstract (中文翻译): 多光谱目标检测对于自动驾驶和监控等对安全性要求较高的应用至关重要，因为在各种光照条件下实现鲁棒感知是必不可少的。然而，标注的多光谱数据有限，严重制约了深度检测器的训练。在这种数据稀缺的场景下，文本类别信息可作为有价值的语义监督来源。受视觉-语言模型（VLM）在计算机视觉领域近期取得成功的启发，我们探索其在少样本多光谱目标检测中的潜力。具体而言，我们将两种代表性的基于VLM的检测器——Grounding DINO和YOLO-World——适配用于处理多光谱输入，并提出一种有效机制来融合文本、可见光与热成像模态。通过对两个流行的多光谱图像基准（FLIR和M3FD）进行大量实验，我们证明基于VLM的检测器不仅在少样本场景下表现优异，显著优于使用同等数据训练的专用多光谱模型，而且在全监督设置下也取得了具有竞争力甚至更优的结果。我们的研究揭示，大规模VLM所学习到的语义先验能够有效迁移到未见过的光谱模态，为实现数据高效的多光谱感知提供了一条强有力的路径。

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 当前的视觉语言模型（VLM）在农业分类任务中表现远逊于专用监督模型（如YOLO11），尤其在开放式提示下准确率极低；尽管部分闭源模型（如Gemini-3 Pro）在多项选择提示下可达62%平均准确率，且开源模型Qwen-VL-72B表现最佳，但整体仍不足以作为独立的农业诊断系统，需结合约束界面与领域评估策略才能发挥辅助作用。


<details>
  <summary>Details</summary>
Motivation: 评估通用视觉语言模型（VLM）在农业决策支持任务中的可靠性，因为尽管VLM被广泛提出作为通用视觉识别方案，其在农业场景下的实际效能尚不明确。

Method: 在AgML集合的27个农业分类数据集（涵盖162个类别，包括植物病害、虫害与损伤、植物与杂草物种识别）上，对一系列开源和闭源VLM进行零样本基准测试，并与专用监督模型YOLO11对比；同时采用多项选择与开放式两种提示方式，并引入基于大语言模型的语义判断来评估不同评估方法对结果的影响。

Result: 零样本VLM整体显著落后于YOLO11；多项选择提示下最佳VLM（Gemini-3 Pro）平均准确率约62%，而开放式提示原始准确率通常低于25%；语义判断可提升开放式准确率（如从21%升至30%）并改变模型排名；开源模型中Qwen-VL-72B表现最优；植物与杂草识别比虫害与损伤识别更容易。

Conclusion: 当前现成的VLM尚不能作为独立的农业诊断系统，但在结合约束性交互界面、显式标签本体和领域感知评估策略时，可作为有效的辅助组件。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

Abstract (中文翻译): 视觉语言模型（VLM）越来越多地被提出作为视觉识别任务的通用解决方案，但其在农业决策支持中的可靠性仍缺乏充分理解。我们在AgML数据集集合中的27个农业分类数据集上对多种开源和闭源VLM进行了基准测试，这些数据集涵盖162个类别，包括植物病害、虫害与损伤，以及植物与杂草物种的识别。在所有任务中，零样本VLM的表现明显不如一个有监督的专用基线模型（YOLO11），后者始终显著优于任何基础模型。在多项选择提示下，表现最佳的VLM（Gemini-3 Pro）平均准确率约为62%，而开放式提示则导致性能大幅下降，原始准确率通常低于25%。采用基于大语言模型的语义判断可提升开放式提示的准确率（例如，顶级模型从21%提升至30%），并改变模型排名，表明评估方法会实质性影响结论。在开源模型中，Qwen-VL-72B表现最佳，在约束提示下接近闭源模型水平，但仍落后于顶尖专有系统。任务层面分析显示，植物与杂草物种分类始终比虫害与损伤识别更容易，后者是各类模型面临的最具挑战性的类别。总体而言，这些结果表明当前现成的VLM尚不适合作为独立的农业诊断系统，但在结合约束性界面、显式标签本体和领域感知评估策略时，可作为辅助组件发挥作用。

</details>


### [9] [Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings](https://arxiv.org/abs/2512.15993)
*Lars Beckers,Arno Waes,Aaron Van Campenhout,Toon Goedemé*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉感知与自适应决策的机器人割草框架，通过在图像中识别并保留视觉多样性高的植被斑块（选择性关闭刀片），主动提升花园生物多样性。


<details>
  <summary>Details</summary>
Motivation: 传统割草方式和被动再野化方法难以有效提升城市草坪的生物多样性；作者旨在开发一种能主动识别并保护高多样性植被区域的智能割草系统。

Method: 利用在PlantNet300K上预训练的ResNet50网络提取生态意义的视觉嵌入特征，通过计算特征空间中的全局偏离度作为无需物种标注的生物多样性代理指标，并据此驱动选择性割草算法，在割草与保护行为间动态切换。

Result: 在模拟草坪和真实花园数据集上的实验表明，嵌入空间的离散度与专家评估的生物多样性高度相关，验证了该方法的有效性和可行性。

Conclusion: 所提方法证明了深度视觉多样性可作为生态丰富度的有效代理，大规模部署此类系统可将单一草坪转变为促进城市生物多样性的宝贵生境。

Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.

Abstract (中文翻译): 本文提出了一种机器人割草框架，通过视觉感知与自适应决策主动提升花园生物多样性。与被动再野化方法不同，该系统利用深度特征空间分析，在摄像头图像中识别并保留视觉多样性高的植被斑块，并通过选择性关闭割草刀片加以保护。系统采用在PlantNet300K数据集上预训练的ResNet50网络生成具有生态意义的嵌入表示，并基于此构建一种无需物种级别监督的全局偏离度指标来估算生物多样性。该指标驱动一种选择性割草算法，使其能在割草与保护行为之间动态切换。该系统在改装的商用机器人割草机上实现，并在受控的模拟草坪和真实花园数据集上进行了验证。结果表明，嵌入空间的分散程度与专家对生物多样性的评估高度相关，证实了深度视觉多样性可作为生态丰富度的有效代理，也验证了所提割草决策方法的有效性。此类系统的广泛应用有望将生态价值低下的单一草坪转变为生机勃勃、富有价值的生物群落，从而提升城市生物多样性。

</details>


### [10] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出一种从初始图像和机器人关节状态出发、根据文本指令生成视频-动作对的方法，通过引入并行动作扩散模型、Bridge Attention机制和动作精炼模块，有效利用预训练视频扩散模型进行机器人策略学习，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用视频扩散模型进行机器人策略学习时面临动作标注缺失的问题，且要么采用限制跨模态信息交互的两阶段流程，要么无法充分利用预训练视频知识，因此需要一种能同时生成高质量视频与精确动作、并有效融合多模态信息的新方法。

Method: (1) 在预训练视频扩散模型基础上增加一个并行的专用动作扩散模型以保留预训练知识；(2) 引入Bridge Attention机制促进视频与动作之间的跨模态交互；(3) 设计动作精炼模块，将粗略动作转化为适用于低分辨率数据集的精确控制信号。

Result: 在多个公开基准和真实世界数据集上的大量实验表明，该方法生成的视频质量更高、动作更准确，显著优于现有基线方法。

Conclusion: 该方法提供了一个可扩展的框架，能够有效利用大规模视频数据进行机器人学习，克服了动作标注缺失和跨模态融合不足的问题。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

Abstract (中文翻译): 我们提出了一种根据文本指令生成视频-动作对的方法，该方法从初始图像观测和机器人关节状态出发。我们的方法自动为视频扩散模型提供动作标签，克服了动作标注普遍缺失的问题，从而使其能够充分用于机器人策略学习。现有方法要么采用两阶段流程，限制了紧密耦合的跨模态信息共享，要么依赖于将单模态扩散模型适配为联合分布，无法充分利用预训练的视频知识。为克服这些局限性，我们（1）在预训练视频扩散模型基础上扩展了一个并行的专用动作扩散模型，以保留预训练知识；（2）引入Bridge Attention机制，实现有效的跨模态交互；（3）设计了一个动作精炼模块，将粗略动作转化为适用于低分辨率数据集的精确控制。在多个公开基准和真实世界数据集上的广泛评估表明，我们的方法生成的视频质量更高、动作更准确，并显著优于现有基线方法，为利用大规模视频数据进行机器人学习提供了一个可扩展的框架。

</details>
