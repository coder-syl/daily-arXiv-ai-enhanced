{"id": "2601.20881", "pdf": "https://arxiv.org/pdf/2601.20881", "abs": "https://arxiv.org/abs/2601.20881", "authors": ["Matteo Rossi"], "title": "MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading", "categories": ["cs.CV"], "comment": null, "summary": "Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \\textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\\textit{Joint Spatial-Temporal Attention (JSTA)} and \\textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMA-LipNet\u7684\u591a\u6ce8\u610f\u529b\u5507\u8bfb\u7f51\u7edc\uff0c\u901a\u8fc7\u5728\u901a\u9053\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u4f9d\u6b21\u5e94\u7528\u4e09\u79cd\u6ce8\u610f\u529b\u6a21\u5757\uff08CA\u3001JSTA\u3001SSTA\uff09\u6765\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\u7684\u5224\u522b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728CMLR\u548cGRID\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u5b57\u7b26\u9519\u8bef\u7387\u548c\u8bcd\u9519\u8bef\u7387\u3002", "motivation": "\u73b0\u6709\u5507\u8bfb\u65b9\u6cd5\u56e0\u53d1\u97f3\u52a8\u4f5c\u7ec6\u5fae\uff0c\u5bfc\u81f4\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\u5224\u522b\u6027\u4e0d\u8db3\u3001\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u53d6\u5f97\u7406\u60f3\u6548\u679c\u3002", "method": "\u63d0\u51faMA-LipNet\uff0c\u4f9d\u6b21\u4f7f\u7528\u901a\u9053\u6ce8\u610f\u529b\uff08CA\uff09\u6a21\u5757\u6291\u5236\u4fe1\u606f\u91cf\u4f4e\u7684\u901a\u9053\uff0c\u518d\u7ed3\u5408\u7c97\u7c92\u5ea6\u7684\u8054\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\uff08JSTA\uff09\u4e0e\u7ec6\u7c92\u5ea6\u7684\u5206\u79bb\u65f6\u7a7a\u6ce8\u610f\u529b\uff08SSTA\uff09\u6a21\u5757\uff0c\u5206\u522b\u5bf9\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u7684\u65e0\u5173\u50cf\u7d20\u548c\u5e27\u8fdb\u884c\u6291\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u7ef4\u5ea6\u89c6\u89c9\u7279\u5f81\u63d0\u7eaf\u3002", "result": "\u5728CMLR\u548cGRID\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMA-LipNet\u663e\u8457\u964d\u4f4e\u4e86\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u548c\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u591a\u79cd\u5f53\u524d\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u7684\u89c6\u89c9\u7279\u5f81\u7cbe\u7ec6\u5316\u5904\u7406\u5bf9\u63d0\u5347\u5507\u8bfb\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u6240\u63d0MA-LipNet\u6709\u6548\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u601d\u8def\u3002", "summary_cn": "\u5507\u8bfb\u6280\u672f\u65e8\u5728\u4ece\u65e0\u58f0\u7684\u5507\u90e8\u8fd0\u52a8\u89c6\u9891\u4e2d\u89e3\u7801\u51fa\u8bf4\u8bdd\u5185\u5bb9\uff0c\u5728\u516c\u5171\u5b89\u5168\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u53d1\u97f3\u52a8\u4f5c\u672c\u8eab\u8f83\u4e3a\u7ec6\u5fae\uff0c\u73b0\u6709\u5507\u8bfb\u65b9\u6cd5\u5e38\u5e38\u9762\u4e34\u7279\u5f81\u5224\u522b\u80fd\u529b\u6709\u9650\u548c\u6cdb\u5316\u6027\u80fd\u8f83\u5dee\u7684\u95ee\u9898\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86\u4ece\u65f6\u95f4\u3001\u7a7a\u95f4\u548c\u901a\u9053\u4e09\u4e2a\u7ef4\u5ea6\u5bf9\u89c6\u89c9\u7279\u5f81\u8fdb\u884c\u63d0\u7eaf\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6ce8\u610f\u529b\u5507\u8bfb\u7f51\u7edc\uff08MA-LipNet\uff09\u3002\u8be5\u7f51\u7edc\u7684\u6838\u5fc3\u5728\u4e8e\u4f9d\u6b21\u5e94\u7528\u4e09\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff1a\u9996\u5148\u91c7\u7528\u901a\u9053\u6ce8\u610f\u529b\uff08CA\uff09\u6a21\u5757\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u6821\u51c6\u901a\u9053\u7ea7\u7279\u5f81\uff0c\u4ee5\u51cf\u8f7b\u4fe1\u606f\u91cf\u8f83\u5c11\u901a\u9053\u7684\u5e72\u6270\uff1b\u968f\u540e\u5f15\u5165\u4e24\u79cd\u4e0d\u540c\u7c92\u5ea6\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\u6a21\u5757\u2014\u2014\u8054\u5408\u65f6\u7a7a\u6ce8\u610f\u529b\uff08JSTA\uff09\u548c\u5206\u79bb\u65f6\u7a7a\u6ce8\u610f\u529b\uff08SSTA\uff09\uff0c\u4ee5\u6291\u5236\u65e0\u5173\u50cf\u7d20\u548c\u89c6\u9891\u5e27\u7684\u5f71\u54cd\u3002\u5176\u4e2d\uff0cJSTA\u6a21\u5757\u901a\u8fc7\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u8ba1\u7b97\u7edf\u4e00\u7684\u6743\u91cd\u56fe\u5b9e\u73b0\u7c97\u7c92\u5ea6\u8fc7\u6ee4\uff0c\u800cSSTA\u6a21\u5757\u5219\u901a\u8fc7\u5206\u522b\u5efa\u6a21\u65f6\u95f4\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u8fdb\u884c\u66f4\u7cbe\u7ec6\u7684\u4f18\u5316\u3002\u5728CMLR\u548cGRID\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMA-LipNet\u663e\u8457\u964d\u4f4e\u4e86\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u548c\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0c\u9a8c\u8bc1\u4e86\u5176\u76f8\u8f83\u4e8e\u591a\u79cd\u5f53\u524d\u5148\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002\u672c\u7814\u7a76\u51f8\u663e\u4e86\u591a\u7ef4\u7279\u5f81\u7cbe\u7ec6\u5316\u5bf9\u5b9e\u73b0\u9c81\u68d2\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.20911", "pdf": "https://arxiv.org/pdf/2601.20911", "abs": "https://arxiv.org/abs/2601.20911", "authors": ["Haochen Zhang", "Animesh Sinha", "Felix Juefei-Xu", "Haoyu Ma", "Kunpeng Li", "Zhipeng Fan", "Meng Dong", "Xiaoliang Dai", "Tingbo Hou", "Peizhao Zhang", "Zecheng He"], "title": "Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 19 figures, plan for TIP", "summary": "Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u9a6c\u5c14\u53ef\u592b\u591a\u8f6e\u5bf9\u8bdd\u56fe\u50cf\u751f\u6210\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5f3a\u8c03\u957f\u671f\u5386\u53f2\u4f9d\u8d56\u7684\u6570\u636e\u96c6\u3001\u5f15\u5165\u57fa\u4e8etoken\u7f13\u5b58\u7684\u5386\u53f2\u6761\u4ef6\u673a\u5236\uff0c\u4ee5\u53ca\u6539\u8fdb\u56fe\u50cf\u91cd\u5efa\u4e0e\u4e2a\u6027\u5316\u7f16\u8f91\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u591a\u6570\u591a\u8f6e\u5bf9\u8bdd\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u548c\u8bad\u7ec3\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u9a6c\u5c14\u53ef\u592b\u5f0f\u7684\uff0c\u5373\u4ec5\u4f9d\u8d56\u6700\u8fd1\u4e00\u8f6e\u7684\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u66f4\u65e9\u7684\u5bf9\u8bdd\u5386\u53f2\u3002\u7136\u800c\u771f\u5b9e\u573a\u666f\u4e2d\u7528\u6237\u53ef\u80fd\u56de\u6eaf\u65e9\u671f\u72b6\u6001\u3001\u64a4\u9500\u4fee\u6539\u6216\u5f15\u7528\u6570\u8f6e\u524d\u5f15\u5165\u7684\u5b9e\u4f53\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u6a21\u975e\u9a6c\u5c14\u53ef\u592b\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e09\u65b9\u9762\u65b9\u6cd5\uff1a(i) \u6784\u5efa\u975e\u9a6c\u5c14\u53ef\u592b\u591a\u8f6e\u6570\u636e\uff0c\u5305\u62ec\u5f3a\u5236\u56de\u6eda\u5230\u65e9\u671f\u89c6\u89c9\u72b6\u6001\u7684\u7f16\u8f91\u4efb\u52a1\u548c\u8de8\u8f6e\u6b21\u7ed1\u5b9a\u540d\u79f0\u4e0e\u5916\u89c2\u7684\u4e2a\u6027\u5316\u4efb\u52a1\uff1b(ii) \u8bbe\u8ba1\u5e26token\u7ea7\u7f13\u5b58\u7684\u5386\u53f2\u6761\u4ef6\u8bad\u7ec3\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u9632\u6b62\u591a\u8f6e\u8eab\u4efd\u6f02\u79fb\uff1b(iii) \u6539\u8fdb\u9ad8\u4fdd\u771f\u56fe\u50cf\u91cd\u5efa\u4e0e\u53ef\u7f16\u8f91\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u5982\u57fa\u4e8e\u91cd\u5efa\u7684DiT\u89e3\u7801\u5668\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u663e\u5f0f\u8bad\u7ec3\u975e\u9a6c\u5c14\u53ef\u592b\u4ea4\u4e92\u80fd\u663e\u8457\u63d0\u5347\u591a\u8f6e\u4e00\u81f4\u6027\u4e0e\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5355\u8f6e\u7f16\u8f91\u4e0e\u4e2a\u6027\u5316\u6027\u80fd\u3002", "conclusion": "\u5efa\u6a21\u975e\u9a6c\u5c14\u53ef\u592b\u957f\u671f\u5386\u53f2\u5bf9\u5b9e\u73b0\u9ad8\u8d28\u91cf\u591a\u8f6e\u5bf9\u8bdd\u56fe\u50cf\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0c\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u8eab\u4efd\u6f02\u79fb\u4e0e\u5386\u53f2\u5ffd\u7565\u95ee\u9898\u3002", "summary_cn": "\u5bf9\u8bdd\u5f0f\u56fe\u50cf\u751f\u6210\u8981\u6c42\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u9075\u5faa\u7528\u6237\u6307\u4ee4\uff0c\u5e76\u4ee5\u4ea4\u9519\u7684\u6587\u672c\u548c\u56fe\u50cf\u4f5c\u4e3a\u4e0d\u65ad\u7d2f\u79ef\u7684\u804a\u5929\u5386\u53f2\u4e3a\u57fa\u7840\u3002\u5c3d\u7ba1\u8fd1\u671f\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5df2\u80fd\u751f\u6210\u548c\u7f16\u8f91\u56fe\u50cf\uff0c\u4f46\u5927\u591a\u6570\u73b0\u6709\u7684\u591a\u8f6e\u57fa\u51c6\u548c\u8bad\u7ec3\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u9a6c\u5c14\u53ef\u592b\u5f0f\u7684\uff1a\u4e0b\u4e00\u8f93\u51fa\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6700\u8fd1\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u5141\u8bb8\u6a21\u578b\u91c7\u7528\u5ffd\u7565\u957f\u671f\u5386\u53f2\u7684\u6377\u5f84\u7b56\u7565\u3002\u672c\u6587\u5f62\u5f0f\u5316\u5e76\u805a\u7126\u4e8e\u66f4\u5177\u6311\u6218\u6027\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u8bbe\u5b9a\uff0c\u5176\u4e2d\u7528\u6237\u53ef\u80fd\u56de\u6eaf\u65e9\u671f\u72b6\u6001\u3001\u64a4\u9500\u66f4\u6539\uff0c\u6216\u5f15\u7528\u6570\u8f6e\u524d\u5f15\u5165\u7684\u5b9e\u4f53\u3002\u6211\u4eec\u63d0\u51fa\u4e86\uff1a(i) \u975e\u9a6c\u5c14\u53ef\u592b\u591a\u8f6e\u6570\u636e\u6784\u5efa\u7b56\u7565\uff0c\u5305\u62ec\u5f3a\u5236\u68c0\u7d22\u65e9\u671f\u89c6\u89c9\u72b6\u6001\u7684\u56de\u6eda\u5f0f\u7f16\u8f91\uff0c\u4ee5\u53ca\u8de8\u8f6e\u6b21\u5c06\u540d\u79f0\u4e0e\u5916\u89c2\u7ed1\u5b9a\u7684\u57fa\u4e8e\u540d\u79f0\u7684\u591a\u8f6e\u4e2a\u6027\u5316\uff1b(ii) \u4e00\u79cd\u5e26token\u7ea7\u7f13\u5b58\u7684\u5386\u53f2\u6761\u4ef6\u8bad\u7ec3\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u4ee5\u9632\u6b62\u591a\u8f6e\u8eab\u4efd\u6f02\u79fb\uff1b(iii) \u63d0\u5347\u9ad8\u4fdd\u771f\u56fe\u50cf\u91cd\u5efa\u4e0e\u53ef\u7f16\u8f91\u4e2a\u6027\u5316\u80fd\u529b\u7684\u6280\u672f\uff0c\u5305\u62ec\u57fa\u4e8e\u91cd\u5efa\u7684DiT\u89e3\u7801\u5668\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u8bfe\u7a0b\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u663e\u5f0f\u9488\u5bf9\u975e\u9a6c\u5c14\u53ef\u592b\u4ea4\u4e92\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u5728\u4fdd\u6301\u5f3a\u5927\u5355\u8f6e\u7f16\u8f91\u4e0e\u4e2a\u6027\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u591a\u8f6e\u4e00\u81f4\u6027\u4e0e\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002"}}
{"id": "2601.20990", "pdf": "https://arxiv.org/pdf/2601.20990", "abs": "https://arxiv.org/abs/2601.20990", "authors": ["Xuehua Ye", "Hongxu Yang", "Adam J. Schwarz"], "title": "Text controllable PET denoising", "categories": ["cs.CV"], "comment": "SPIE Medical Imaging 2026", "summary": "Positron Emission Tomography (PET) imaging is a vital tool in medical diagnostics, offering detailed insights into molecular processes within the human body. However, PET images often suffer from complicated noise, which can obscure critical diagnostic information. The quality of the PET image is impacted by various factors including scanner hardware, image reconstruction, tracer properties, dose/count level, and acquisition time. In this study, we propose a novel text-guided denoising method capable of enhancing PET images across a wide range of count levels within a single model. The model utilized the features from a pretrained CLIP model with a U-Net based denoising model. Experimental results demonstrate that the proposed model leads significant improvements in both qualitative and quantitative assessments. The flexibility of the model shows the potential for helping more complicated denoising demands or reducing the acquisition time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u7684PET\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u4e0eU-Net\u7ed3\u6784\uff0c\u5728\u5355\u4e2a\u6a21\u578b\u4e2d\u6709\u6548\u63d0\u5347\u4e0d\u540c\u8ba1\u6570\u6c34\u5e73\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "PET\u56fe\u50cf\u5e38\u53d7\u590d\u6742\u566a\u58f0\u5f71\u54cd\uff0c\u964d\u4f4e\u8bca\u65ad\u51c6\u786e\u6027\uff1b\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u540c\u6210\u50cf\u6761\u4ef6\u4e0b\uff08\u5982\u5242\u91cf\u3001\u91c7\u96c6\u65f6\u95f4\u7b49\uff09\u7edf\u4e00\u5904\u7406\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u901a\u7528\u7684\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u7684\u6587\u672c/\u56fe\u50cf\u7279\u5f81\u4e0eU-Net\u67b6\u6784\uff0c\u6784\u5efa\u4e00\u4e2a\u6587\u672c\u5f15\u5bfc\u7684\u53bb\u566a\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u591a\u79cd\u8ba1\u6570\u6c34\u5e73\u4e0b\u589e\u5f3aPET\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u671b\u6ee1\u8db3\u66f4\u590d\u6742\u7684\u53bb\u566a\u9700\u6c42\u6216\u7f29\u77edPET\u626b\u63cf\u65f6\u95f4\u3002", "summary_cn": "\u6b63\u7535\u5b50\u53d1\u5c04\u65ad\u5c42\u626b\u63cf\uff08PET\uff09\u6210\u50cf\u662f\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u91cd\u8981\u5de5\u5177\uff0c\u80fd\u591f\u63d0\u4f9b\u4eba\u4f53\u5185\u5206\u5b50\u8fc7\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\u7136\u800c\uff0cPET\u56fe\u50cf\u5e38\u5e38\u53d7\u5230\u590d\u6742\u566a\u58f0\u7684\u5e72\u6270\uff0c\u4ece\u800c\u63a9\u76d6\u5173\u952e\u7684\u8bca\u65ad\u4fe1\u606f\u3002PET\u56fe\u50cf\u8d28\u91cf\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u5305\u62ec\u626b\u63cf\u4eea\u786c\u4ef6\u3001\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\u3001\u793a\u8e2a\u5242\u7279\u6027\u3001\u5242\u91cf/\u8ba1\u6570\u6c34\u5e73\u4ee5\u53ca\u91c7\u96c6\u65f6\u95f4\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u5f15\u5bfc\u53bb\u566a\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u5bf9\u5e7f\u6cdb\u8ba1\u6570\u6c34\u5e73\u4e0b\u7684PET\u56fe\u50cf\u8fdb\u884c\u589e\u5f3a\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u7684\u7279\u5f81\u4e0e\u57fa\u4e8eU-Net\u7684\u53bb\u566a\u7f51\u7edc\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002\u8be5\u6a21\u578b\u7684\u7075\u6d3b\u6027\u663e\u793a\u51fa\u5176\u5728\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u53bb\u566a\u9700\u6c42\u6216\u7f29\u77ed\u91c7\u96c6\u65f6\u95f4\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.20995", "pdf": "https://arxiv.org/pdf/2601.20995", "abs": "https://arxiv.org/abs/2601.20995", "authors": ["Hongxu Yang", "Levente Lippenszky", "Edina Timko", "Lehel Ferenczi", "Gopal Avinash"], "title": "Low performing pixel correction in computed tomography with unrolled network and synthetic data training", "categories": ["cs.CV"], "comment": "ISBI 2026 accepted", "summary": "Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u53cc\u57df\u5c55\u5f00\u65b9\u6cd5\uff0c\u65e0\u9700\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u5373\u53ef\u6709\u6548\u6821\u6b63CT\u63a2\u6d4b\u5668\u4f4e\u6027\u80fd\u50cf\u7d20\u5f15\u8d77\u7684\u4f2a\u5f71\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4e\u6027\u80fd\u50cf\u7d20\uff08LPP\uff09\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u4e14\u4ec5\u5728\u56fe\u50cf\u57df\u6216\u6b63\u5f26\u56fe\u57df\u5355\u72ec\u5904\u7406\uff0c\u5ffd\u7565\u4e86CT\u51e0\u4f55\u524d\u5411\u64cd\u4f5c\u4e2d\u4e24\u4e2a\u57df\u4e4b\u95f4\u7684\u5185\u5728\u5173\u8054\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u53cc\u57df\u5c55\u5f00\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u7136\u56fe\u50cf\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5efa\u6a21\u6b63\u5f26\u56fe\u57df\u4e0e\u56fe\u50cf\u57df\u4e4b\u95f4LPP\u7684\u5185\u5728\u5173\u8054\uff0c\u4ece\u800c\u5728\u65e0\u9700\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u4f2a\u5f71\u6821\u6b63\u3002", "result": "\u5728\u6a21\u62df1-2%\u63a2\u6d4b\u5668\u7f3a\u9677\u9760\u8fd1\u7b49\u4e2d\u5fc3\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6536\u96c6\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u5373\u53ef\u6709\u6548\u6821\u6b63LPP\u4f2a\u5f71\uff0c\u5e76\u53ef\u9002\u5e94\u4e0d\u540c\u626b\u63cf\u4eea\u8bbe\u7f6e\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u8f6f\u4ef6\u7684\u90e8\u7f72\u3002", "summary_cn": "\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u63a2\u6d4b\u5668\u4e2d\u7684\u4f4e\u6027\u80fd\u50cf\u7d20\uff08LPP\uff09\u4f1a\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u4e2d\u51fa\u73b0\u73af\u72b6\u548c\u6761\u7eb9\u4f2a\u5f71\uff0c\u4f7f\u5176\u65e0\u6cd5\u7528\u4e8e\u4e34\u5e8a\u8bca\u65ad\u3002\u8fd1\u5e74\u6765\uff0c\u5df2\u6709\u591a\u79cd\u65b9\u6cd5\u88ab\u63d0\u51fa\u7528\u4e8e\u6821\u6b63LPP\u4f2a\u5f71\uff0c\u5305\u62ec\u5728\u56fe\u50cf\u57df\u6216\u6b63\u5f26\u56fe\u57df\u4e2d\u4f7f\u7528\u6709\u76d1\u7763\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u6b64\u7c7b\u6570\u636e\u7684\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u3002\u6b64\u5916\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fe\u50cf\u7a7a\u95f4\u6216\u6b63\u5f26\u56fe\u7a7a\u95f4\u7684\u6821\u6b63\uff0c\u5ffd\u7565\u4e86CT\u51e0\u4f55\u524d\u5411\u64cd\u4f5c\u6240\u5e26\u6765\u7684\u4e24\u4e2a\u57df\u4e4b\u95f4\u7684\u5185\u5728\u5173\u8054\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u53cc\u57df\u5c55\u5f00\u65b9\u6cd5\u6765\u6821\u6b63LPP\u4f2a\u5f71\u3002\u5177\u4f53\u800c\u8a00\uff0c\u901a\u8fc7\u4ece\u81ea\u7136\u56fe\u50cf\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff0c\u5229\u7528\u6b63\u5f26\u56fe\u57df\u4e0e\u56fe\u50cf\u57df\u4e4b\u95f4LPP\u7684\u5185\u5728\u5173\u8054\uff0c\u4f7f\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u65e0\u9700\u4efb\u4f55\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u5373\u53ef\u6821\u6b63\u4f2a\u5f71\u3002\u5728\u6a21\u62df1\u20132%\u63a2\u6d4b\u5668\u7f3a\u9677\u9760\u8fd1\u7b49\u4e2d\u5fc3\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u65e0\u9700\u4e3a\u6a21\u578b\u8bad\u7ec3\u6536\u96c6\u6570\u636e\u5373\u53ef\u6709\u6548\u6821\u6b63LPP\u4f2a\u5f71\uff0c\u5e76\u4e14\u53ef\u9002\u5e94\u4e0d\u540c\u7684\u626b\u63cf\u4eea\u8bbe\u7f6e\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u8f6f\u4ef6\u7684\u5e94\u7528\u3002"}}
{"id": "2601.21022", "pdf": "https://arxiv.org/pdf/2601.21022", "abs": "https://arxiv.org/abs/2601.21022", "authors": ["Andrea Camilloni", "Chiara Micoli", "Nita Mulliqi", "Erik Everett Palm", "Thorgerdur Palsdottir", "Kelvin Szolnoky", "Xiaoyi Ji", "Sol Erika Boman", "Andrea Discacciati", "Henrik Gr\u00f6nberg", "Lars Egevad", "Tobias Nordstr\u00f6m", "Kimmo Kartasalo", "Martin Eklund"], "title": "AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples", "categories": ["cs.CV"], "comment": "39 pages, 6 tables, 11 figures", "summary": "Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6a21\u578b\uff0c\u5229\u7528\u8bca\u65ad\u6027\u524d\u5217\u817a\u6d3b\u68c0\u5207\u7247\u9884\u6d4b\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\u540e\u751f\u5316\u590d\u53d1\uff08BCR\uff09\u98ce\u9669\uff0c\u5e76\u5728\u591a\u4e2a\u5916\u90e8\u961f\u5217\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002\u7ed3\u5408\u4e34\u5e8a\u53d8\u91cf\u540e\uff0c\u6a21\u578b\u5728\u9884\u540e\u5206\u5c42\u4e0a\u4f18\u4e8e\u4f20\u7edfCAPRA-S\u8bc4\u5206\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u9884\u6d4b\u524d\u5217\u817a\u764c\u6839\u6cbb\u672f\u540e\u751f\u5316\u590d\u53d1\uff08BCR\uff09\u7684\u9884\u540e\u5de5\u5177\u4e0d\u591f\u7cbe\u786e\uff0c\u4e9f\u9700\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u5177\u6709\u4e0d\u826f\u9884\u540e\u7684\u9ad8\u5371\u60a3\u8005\u3002", "method": "\u7814\u7a76\u57fa\u4e8eSTHLM3\u961f\u5217\uff08n=676\uff09\u7684\u8bca\u65ad\u6027\u524d\u5217\u817a\u6d3b\u68c0\u5207\u7247\uff0c\u91c7\u7528\u57fa\u7840\u6a21\u578b\uff08foundation models\uff09\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08attention-based multiple instance learning\uff09\u8bad\u7ec3AI\u6a21\u578b\u4ee5\u9884\u6d4b\u4e2a\u4f53\u60a3\u8005\u7684BCR\u98ce\u9669\uff0c\u5e76\u5728\u4e09\u4e2a\u5916\u90e8\u6839\u6cbb\u672f\u961f\u5217\uff08LEOPARD\u3001CHIMERA\u3001TCGA-PRAD\uff09\u4e2d\u8bc4\u4f30\u5176\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u8be5\u56fe\u50cf\u9a71\u52a8\u6a21\u578b\u5728\u4e09\u4e2a\u5916\u90e8\u961f\u5217\u4e2d\u5206\u522b\u5b9e\u73b0\u4e860.64\u30010.70\u548c0.70\u76845\u5e74\u65f6\u95f4\u4f9d\u8d56AUC\uff1b\u7ed3\u5408\u4e34\u5e8a\u53d8\u91cf\u540e\u663e\u8457\u63d0\u5347\u4e86\u98ce\u9669\u5206\u5c42\u80fd\u529b\uff0c\u5e76\u5728\u672f\u540e\u9884\u540e\u5224\u65ad\u4e0a\u4f18\u4e8e\u6307\u5357\u63a8\u8350\u7684CAPRA-S\u8bc4\u5206\u3002", "conclusion": "\u57fa\u4e8e\u6d3b\u68c0\u8bad\u7ec3\u7684\u7ec4\u7ec7\u75c5\u7406AI\u6a21\u578b\u53ef\u5728\u4e0d\u540c\u6807\u672c\u7c7b\u578b\u95f4\u6cdb\u5316\uff0c\u8f85\u52a9\u672f\u524d\u4e0e\u672f\u540e\u51b3\u7b56\uff0c\u4f46\u5176\u76f8\u8f83\u4e8e\u7b80\u5355\u9884\u6d4b\u6a21\u578b\u7684\u9644\u52a0\u4ef7\u503c\u4ecd\u9700\u5728\u540e\u7eed\u7814\u7a76\u4e2d\u5ba1\u614e\u8bc4\u4f30\u3002", "summary_cn": "\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\uff08RP\uff09\u540e\u7684\u751f\u5316\u590d\u53d1\uff08BCR\uff09\u662f\u4fb5\u88ad\u6027\u524d\u5217\u817a\u764c\u548c\u4e0d\u826f\u9884\u540e\u7684\u66ff\u4ee3\u6807\u5fd7\u7269\uff0c\u4f46\u76ee\u524d\u7684\u9884\u540e\u5de5\u5177\u4ecd\u4e0d\u591f\u7cbe\u786e\u3002\u6211\u4eec\u5728STHLM3\u961f\u5217\uff08n = 676\uff09\u7684\u8bca\u65ad\u6027\u524d\u5217\u817a\u6d3b\u68c0\u5207\u7247\u4e0a\u8bad\u7ec3\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7684\u6a21\u578b\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u9884\u6d4b\u60a3\u8005\u4e2a\u4f53\u5316\u7684BCR\u98ce\u9669\u3002\u8be5\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5728\u4e09\u4e2a\u5916\u90e8RP\u961f\u5217\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1aLEOPARD\uff08n = 508\uff09\u3001CHIMENA\uff08n = 95\uff09\u548cTCGA-PRAD\uff08n = 379\uff09\u3002\u8be5\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u5728\u4e0a\u8ff0\u4e09\u4e2a\u961f\u5217\u4e2d\u5206\u522b\u8fbe\u5230\u4e860.64\u30010.70\u548c0.70\u76845\u5e74\u65f6\u95f4\u4f9d\u8d56AUC\u3002\u6574\u5408\u4e34\u5e8a\u53d8\u91cf\u540e\uff0c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u9884\u540e\u4ef7\u503c\uff0c\u5e76\u5b9e\u73b0\u4e86\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u7684\u98ce\u9669\u5206\u5c42\u3002\u4e0e\u6307\u5357\u63a8\u8350\u7684CAPRA-S\u8bc4\u5206\u76f8\u6bd4\uff0cAI\u6a21\u578b\u5728\u672f\u540e\u9884\u540e\u5224\u65ad\u65b9\u9762\u5e26\u6765\u4e86\u589e\u91cf\u6539\u8fdb\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6d3b\u68c0\u8bad\u7ec3\u7684\u7ec4\u7ec7\u75c5\u7406AI\u6a21\u578b\u80fd\u591f\u8de8\u6807\u672c\u7c7b\u578b\u6cdb\u5316\uff0c\u652f\u6301\u672f\u524d\u548c\u672f\u540e\u51b3\u7b56\uff0c\u4f46AI\u9a71\u52a8\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u76f8\u8f83\u4e8e\u66f4\u7b80\u5355\u7684\u9884\u6d4b\u6a21\u578b\u6240\u589e\u52a0\u7684\u4ef7\u503c\uff0c\u4ecd\u9700\u5728\u672a\u6765\u7814\u7a76\u4e2d\u8fdb\u884c\u4e25\u683c\u5ba1\u89c6\u3002"}}
{"id": "2601.21066", "pdf": "https://arxiv.org/pdf/2601.21066", "abs": "https://arxiv.org/abs/2601.21066", "authors": ["Kealan Dunnett", "Reza Arablouei", "Dimity Miller", "Volkan Dedeoglu", "Raja Jurdak"], "title": "BadDet+: Robust Backdoor Attacks for Object Detection", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBadDet+\uff0c\u4e00\u79cd\u57fa\u4e8e\u60e9\u7f5a\u673a\u5236\u7684\u540e\u95e8\u653b\u51fb\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u533a\u57df\u8bef\u5206\u7c7b\u653b\u51fb\uff08RMA\uff09\u548c\u76ee\u6807\u6d88\u5931\u653b\u51fb\uff08ODA\uff09\uff0c\u5728\u4fdd\u6301\u5e72\u51c0\u6837\u672c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u771f\u5b9e\u7269\u7406\u573a\u666f\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u5305\u62ec\u4f9d\u8d56\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\u4ee5\u53ca\u7f3a\u4e4f\u7269\u7406\u4e16\u754c\u9a8c\u8bc1\uff0c\u5bfc\u81f4\u5176\u5b9e\u9645\u5a01\u80c1\u88ab\u9ad8\u4f30\u6216\u4f4e\u4f30\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u8d34\u8fd1\u73b0\u5b9e\u3001\u5177\u5907\u7269\u7406\u9c81\u68d2\u6027\u7684\u653b\u51fb\u65b9\u6cd5\u4ee5\u63ed\u793a\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u771f\u5b9e\u8106\u5f31\u6027\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86BadDet+\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u5229\u7528\u5bf9\u6570\u969c\u788d\u60e9\u7f5a\uff08log-barrier penalty\uff09\u6765\u6291\u5236\u89e6\u53d1\u6837\u672c\u7684\u771f\u5b9e\u7c7b\u522b\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u7edf\u4e00\u4e86RMA\u548cODA\uff0c\u5e76\u80fd\u5b9e\u73b0\u4f4d\u7f6e\u4e0e\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u7269\u7406\u4e16\u754c\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBadDet+\u5728\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u6027\u80fd\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u73b0\u6709RMA\u548cODA\u57fa\u7ebf\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u4ece\u5408\u6210\u5230\u7269\u7406\u7684\u8fc1\u79fb\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u540e\u95e8\u653b\u51fb\u4e0b\u7684\u663e\u8457\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u4e13\u95e8\u9632\u5fa1\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002\u7406\u8bba\u5206\u6790\u4e5f\u8bc1\u5b9e\u4e86\u6240\u63d0\u60e9\u7f5a\u673a\u5236\u5728\u7279\u5b9a\u89e6\u53d1\u7279\u5f81\u5b50\u7a7a\u95f4\u5185\u6709\u6548\uff0c\u4e0d\u4f1a\u635f\u5bb3\u6b63\u5e38\u63a8\u7406\u3002", "summary_cn": "\u540e\u95e8\u653b\u51fb\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u4e0e\u56fe\u50cf\u5206\u7c7b\u76f8\u6bd4\uff0c\u5176\u5bf9\u76ee\u6807\u68c0\u6d4b\u7684\u5f71\u54cd\u4ecd\u77e5\u4e4b\u751a\u5c11\u3002\u5c3d\u7ba1\u5df2\u6709\u76f8\u5173\u653b\u51fb\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u57fa\u4e8e\u68c0\u6d4b\u7684\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u5f31\u70b9\uff0c\u7279\u522b\u662f\u5b83\u4eec\u4f9d\u8d56\u4e8e\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\u4e14\u7f3a\u4e4f\u7269\u7406\u9a8c\u8bc1\u3002\u4e3a\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86BadDet+\uff0c\u4e00\u4e2a\u57fa\u4e8e\u60e9\u7f5a\u673a\u5236\u7684\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u533a\u57df\u8bef\u5206\u7c7b\u653b\u51fb\uff08RMA\uff09\u548c\u76ee\u6807\u6d88\u5931\u653b\u51fb\uff08ODA\uff09\u3002\u5176\u6838\u5fc3\u673a\u5236\u5229\u7528\u5bf9\u6570\u969c\u788d\u60e9\u7f5a\u6765\u6291\u5236\u89e6\u53d1\u8f93\u5165\u7684\u771f\u5b9e\u7c7b\u522b\u9884\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\uff08i\uff09\u4f4d\u7f6e\u4e0e\u5c3a\u5ea6\u4e0d\u53d8\u6027\uff0c\u4ee5\u53ca\uff08ii\uff09\u589e\u5f3a\u7684\u7269\u7406\u9c81\u68d2\u6027\u3002\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBadDet+\u5728\u4fdd\u6301\u5e72\u51c0\u6027\u80fd\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u73b0\u6709\u7684RMA\u548cODA\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u4ece\u5408\u6210\u5230\u7269\u7406\u7684\u8fc1\u79fb\u6548\u679c\u3002\u7406\u8bba\u5206\u6790\u8bc1\u5b9e\uff0c\u6240\u63d0\u51fa\u7684\u60e9\u7f5a\u673a\u5236\u4f5c\u7528\u4e8e\u7279\u5b9a\u89e6\u53d1\u5668\u7684\u7279\u5f81\u5b50\u7a7a\u95f4\u5185\uff0c\u80fd\u591f\u53ef\u9760\u5730\u8bf1\u53d1\u653b\u51fb\u800c\u4e0d\u635f\u5bb3\u6807\u51c6\u63a8\u7406\u3002\u8fd9\u4e9b\u7ed3\u679c\u51f8\u663e\u4e86\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u91cd\u5927\u6f0f\u6d1e\uff0c\u4ee5\u53ca\u5f00\u53d1\u4e13\u95e8\u9632\u5fa1\u63aa\u65bd\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.21078", "pdf": "https://arxiv.org/pdf/2601.21078", "abs": "https://arxiv.org/abs/2601.21078", "authors": ["Jiaqi Li", "Guangming Wang", "Shuntian Zheng", "Minzhe Ni", "Xiaoman Lu", "Guanghui Ye", "Yu Guan"], "title": "Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.", "AI": {"tldr": "ActionVLM\u901a\u8fc7\u52a8\u6001\u91cd\u52a0\u6743\u548c\u6b8b\u5dee\u805a\u5408\u7b56\u7565\uff0c\u5728\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4e2d\u7f13\u89e3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u6001\u504f\u7f6e\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u5bfc\u81f4\u6a21\u6001\u504f\u7f6e\uff0c\u524a\u5f31\u4e86\u89c6\u89c9\u4fe1\u606f\u7684\u4f5c\u7528\uff0c\u5f71\u54cd\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u63d0\u51faActionVLM\u6846\u67b6\uff0c\u5305\u542b\uff1a(i) \u53bb\u504f\u91cd\u52a0\u6743\u6a21\u5757\uff0c\u52a8\u6001\u8bc4\u4f30\u8bed\u8a00\u76f8\u5bf9\u4e8e\u7eaf\u89c6\u89c9\u9884\u6d4b\u7684\u589e\u76ca\u5e76\u8c03\u6574\u5176\u6743\u91cd\uff1b(ii) \u6b8b\u5dee\u805a\u5408\u7b56\u7565\uff0c\u5c06\u8bed\u8a00\u4f5c\u4e3a\u89c6\u89c9\u4e3b\u5bfc\u4fe1\u53f7\u7684\u8865\u5145\u800c\u975e\u4e3b\u9a71\u52a8\u529b\u3002", "result": "\u5728THUMOS14\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u76f8\u8f83\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u5347\u6700\u9ad8\u8fbe3.2% mAP\u3002", "conclusion": "\u901a\u8fc7\u4fdd\u7559\u89c6\u89c9\u4e3b\u5bfc\u5730\u4f4d\u5e76\u81ea\u9002\u5e94\u5229\u7528\u8bed\u8a00\u4fe1\u606f\uff0cActionVLM\u6709\u6548\u7f13\u89e3\u6a21\u6001\u504f\u7f6e\uff0c\u63d0\u5347\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "summary_cn": "\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\uff08TAL\uff09\u9700\u8981\u5728\u672a\u526a\u8f91\u89c6\u9891\u4e2d\u8bc6\u522b\u52a8\u4f5c\u7684\u8fb9\u754c\u548c\u7c7b\u522b\u3002\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u80fd\u591f\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u4ee5\u8865\u5145\u89c6\u89c9\u8bc1\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u8fc7\u5ea6\u5f3a\u8c03\u8bed\u8a00\u5148\u9a8c\uff0c\u727a\u7272\u4e86\u89c6\u89c9\u6027\u80fd\uff0c\u4ece\u800c\u5bfc\u81f4\u663e\u8457\u7684\u6a21\u6001\u504f\u7f6e\u3002\u6211\u4eec\u63d0\u51fa\u4e86ActionVLM\uff0c\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00\u805a\u5408\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u7f13\u89e3TAL\u4e2d\u7684\u6a21\u6001\u504f\u7f6e\u3002\u6211\u4eec\u7684\u6838\u5fc3\u601d\u60f3\u662f\u5728\u4fdd\u6301\u89c6\u89c9\u4e3a\u4e3b\u5bfc\u4fe1\u53f7\u7684\u540c\u65f6\uff0c\u4ec5\u5728\u6709\u76ca\u65f6\u81ea\u9002\u5e94\u5730\u5229\u7528\u8bed\u8a00\u4fe1\u606f\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\uff1a(i) \u4e00\u4e2a\u53bb\u504f\u91cd\u52a0\u6743\u6a21\u5757\uff0c\u7528\u4e8e\u4f30\u8ba1\u8bed\u8a00\u4f18\u52bf\u2014\u2014\u5373\u8bed\u8a00\u76f8\u5bf9\u4e8e\u4ec5\u4f7f\u7528\u89c6\u89c9\u9884\u6d4b\u6240\u5e26\u6765\u7684\u589e\u91cf\u6536\u76ca\uff0c\u5e76\u636e\u6b64\u52a8\u6001\u8c03\u6574\u8bed\u8a00\u6a21\u6001\u7684\u6743\u91cd\uff1b(ii) \u4e00\u79cd\u6b8b\u5dee\u805a\u5408\u7b56\u7565\uff0c\u5c06\u8bed\u8a00\u89c6\u4e3a\u8865\u5145\u6027\u7684\u7ec6\u5316\u624b\u6bb5\uff0c\u800c\u975e\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002\u8fd9\u79cd\u7ec4\u5408\u6709\u6548\u7f13\u89e3\u4e86\u6a21\u6001\u504f\u7f6e\uff0c\u964d\u4f4e\u4e86\u8bed\u8a00\u5148\u9a8c\u5e26\u6765\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5e76\u589e\u5f3a\u4e86\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002\u5728THUMOS14\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0cmAP\u6700\u9ad8\u63d0\u5347\u4e863.2%\u3002"}}
{"id": "2601.21081", "pdf": "https://arxiv.org/pdf/2601.21081", "abs": "https://arxiv.org/abs/2601.21081", "authors": ["Yu Huo", "Siyu Zhang", "Kun Zeng", "Haoyue Liu", "Owen Lee", "Junlin Chen", "Yuquan Lu", "Yifu Guo", "Yaodong Liang", "Xiaoying Tang"], "title": "Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought", "categories": ["cs.CV"], "comment": "The code is available at https://anonymous.4open.science/r/16FE/", "summary": "Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faShape-of-Thought\uff08SoT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8\u5f15\u64ce\u7684\u8fde\u8d2f2D\u6295\u5f71\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u5f62\u72b6\u7ec4\u88c5\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7ec4\u5408\u7ed3\u6784\u7ea6\u675f\uff08\u5982\u6570\u91cf\u3001\u5c5e\u6027\u7ed1\u5b9a\u548c\u90e8\u4ef6\u5173\u7cfb\uff09\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u7684\u591a\u6a21\u6001\u6a21\u578b\u867d\u5177\u5907\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4f46\u5728\u5904\u7406\u7ec4\u5408\u7ed3\u6784\u6027\u7ea6\u675f\uff08\u5982\u751f\u6210\u6570\u5b57\u51c6\u786e\u6027\u3001\u5c5e\u6027\u7ed1\u5b9a\u548c\u90e8\u4ef6\u7ea7\u5173\u7cfb\uff09\u65f6\u4ecd\u663e\u8106\u5f31\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u601d\u7ef4\u94fe\uff08CoT\uff09\u65b9\u6cd5\u3002", "method": "SoT\u8bad\u7ec3\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4ea4\u66ff\u751f\u6210\u6587\u672c\u8ba1\u5212\u4e0e\u6e32\u67d3\u7684\u4e2d\u95f4\u72b6\u6001\uff0c\u4ece\u800c\u5728\u4e0d\u663e\u5f0f\u751f\u6210\u51e0\u4f55\u8868\u793a\u7684\u524d\u63d0\u4e0b\u5b66\u4e60\u5f62\u72b6\u7ec4\u88c5\u903b\u8f91\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u6784\u5efa\u4e86SoT-26K\u6570\u636e\u96c6\uff08\u57fa\u4e8eCAD\u90e8\u4ef6\u5c42\u6b21\u7ed3\u6784\u7684\u5927\u89c4\u6a21\u88c5\u914d\u8f68\u8ff9\u6570\u636e\uff09\u548cT2S-CompBench\u8bc4\u6d4b\u57fa\u51c6\u3002", "result": "\u5728SoT-26K\u4e0a\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u7ec4\u4ef6\u6570\u91cf\u4efb\u52a1\u4e0a\u8fbe\u523088.4%\uff0c\u5728\u7ed3\u6784\u62d3\u6251\u4efb\u52a1\u4e0a\u8fbe84.8%\uff0c\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u9ad8\u51fa\u7ea620%\u3002", "conclusion": "SoT\u4e3a\u900f\u660e\u3001\u8fc7\u7a0b\u76d1\u7763\u7684\u7ec4\u5408\u5f0f\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u7ed3\u6784\u6027\u51c6\u786e\u6027\u3002", "summary_cn": "\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u591a\u6a21\u6001\u6a21\u578b\u5df2\u5b9e\u73b0\u8f83\u5f3a\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4f46\u5728\u7ec4\u5408\u7ed3\u6784\u6027\u7ea6\u675f\u4e0b\u4ecd\u663e\u8106\u5f31\u2014\u2014\u5c24\u5176\u662f\u751f\u6210\u6570\u503c\u51c6\u786e\u6027\u3001\u5c5e\u6027\u7ed1\u5b9a\u4ee5\u53ca\u90e8\u4ef6\u7ea7\u5173\u7cfb\u7b49\u65b9\u9762\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u5f62\u72b6\u201d\uff08Shape-of-Thought, SoT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u89c6\u89c9\u601d\u7ef4\u94fe\uff08CoT\uff09\u6846\u67b6\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8\u5f15\u64ce\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8fde\u8d2f\u7684\u4e8c\u7ef4\u6295\u5f71\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u5f62\u72b6\u7ec4\u88c5\u3002SoT\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4ee5\u4ea4\u66ff\u751f\u6210\u6587\u672c\u8ba1\u5212\u548c\u6e32\u67d3\u7684\u4e2d\u95f4\u72b6\u6001\uff0c\u5e2e\u52a9\u6a21\u578b\u6355\u6349\u5f62\u72b6\u7ec4\u88c5\u903b\u8f91\uff0c\u800c\u65e0\u9700\u751f\u6210\u663e\u5f0f\u7684\u51e0\u4f55\u8868\u793a\u3002\u4e3a\u652f\u6301\u8be5\u8303\u5f0f\uff0c\u6211\u4eec\u5f15\u5165\u4e86SoT-26K\u6570\u636e\u96c6\u2014\u2014\u4e00\u4e2a\u4ece\u57fa\u4e8e\u90e8\u4ef6\u7684CAD\u5c42\u6b21\u7ed3\u6784\u4e2d\u63d0\u53d6\u7684\u5927\u89c4\u6a21\u5177\u8eab\u88c5\u914d\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u4ee5\u53caT2S-CompBench\u8bc4\u6d4b\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8f68\u8ff9\u5fe0\u5b9e\u5ea6\u3002\u5728SoT-26K\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u7ec4\u4ef6\u6570\u503c\u4efb\u52a1\u4e0a\u8fbe\u523088.4%\uff0c\u5728\u7ed3\u6784\u62d3\u6251\u4efb\u52a1\u4e0a\u8fbe\u523084.8%\uff0c\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u9ad8\u51fa\u7ea620%\u3002SoT\u4e3a\u900f\u660e\u3001\u8fc7\u7a0b\u76d1\u7763\u7684\u7ec4\u5408\u5f0f\u751f\u6210\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\u3002\u4ee3\u7801\u5df2\u5728https://anonymous.4open.science/r/16FE/\u516c\u5f00\uff0cSoT-26K\u6570\u636e\u96c6\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u6536\u540e\u53d1\u5e03\u3002"}}
{"id": "2601.21120", "pdf": "https://arxiv.org/pdf/2601.21120", "abs": "https://arxiv.org/abs/2601.21120", "authors": ["Yan Meng", "Eduardo J. Torres-Rodr\u00edguez", "Marcelle Altshuler", "Nishanth Gowda", "Arhum Naeem", "Recai Yilmaz", "Omar Arnaout", "Daniel A. Donoho"], "title": "An AI Framework for Microanastomosis Motion Assessment", "categories": ["cs.CV"], "comment": "Accepted by IEEE/EMBS NER 2025. \\c{opyright} 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses", "summary": "Proficiency in microanastomosis is a fundamental competency across multiple microsurgical disciplines. These procedures demand exceptional precision and refined technical skills, making effective, standardized assessment methods essential. Traditionally, the evaluation of microsurgical techniques has relied heavily on the subjective judgment of expert raters. They are inherently constrained by limitations such as inter-rater variability, lack of standardized evaluation criteria, susceptibility to cognitive bias, and the time-intensive nature of manual review. These shortcomings underscore the urgent need for an objective, reliable, and automated system capable of assessing microsurgical performance with consistency and scalability. To bridge this gap, we propose a novel AI framework for the automated assessment of microanastomosis instrument handling skills. The system integrates four core components: (1) an instrument detection module based on the You Only Look Once (YOLO) architecture; (2) an instrument tracking module developed from Deep Simple Online and Realtime Tracking (DeepSORT); (3) an instrument tip localization module employing shape descriptors; and (4) a supervised classification module trained on expert-labeled data to evaluate instrument handling proficiency. Experimental results demonstrate the effectiveness of the framework, achieving an instrument detection precision of 97%, with a mean Average Precision (mAP) of 96%, measured by Intersection over Union (IoU) thresholds ranging from 50% to 95% (mAP50-95).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u663e\u5fae\u8840\u7ba1\u543b\u5408\u672f\u4e2d\u7684\u5668\u68b0\u64cd\u4f5c\u6280\u80fd\uff0c\u6574\u5408\u4e86\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u3001\u5668\u68b0\u5c16\u7aef\u5b9a\u4f4d\u548c\u5206\u7c7b\u6a21\u5757\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e8697%\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c96%\u7684mAP\u3002", "motivation": "\u4f20\u7edf\u663e\u5fae\u5916\u79d1\u6280\u672f\u8bc4\u4f30\u4f9d\u8d56\u4e13\u5bb6\u4e3b\u89c2\u5224\u65ad\uff0c\u5b58\u5728\u8bc4\u5206\u8005\u95f4\u5dee\u5f02\u5927\u3001\u6807\u51c6\u4e0d\u7edf\u4e00\u3001\u6613\u53d7\u8ba4\u77e5\u504f\u5dee\u5f71\u54cd\u53ca\u8017\u65f6\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u5ba2\u89c2\u3001\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a(1) \u57fa\u4e8eYOLO\u7684\u5668\u68b0\u68c0\u6d4b\u6a21\u5757\uff1b(2) \u57fa\u4e8eDeepSORT\u7684\u5668\u68b0\u8ddf\u8e2a\u6a21\u5757\uff1b(3) \u5229\u7528\u5f62\u72b6\u63cf\u8ff0\u7b26\u7684\u5668\u68b0\u5c16\u7aef\u5b9a\u4f4d\u6a21\u5757\uff1b(4) \u57fa\u4e8e\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u7684\u76d1\u7763\u5206\u7c7b\u6a21\u5757\uff0c\u7528\u4e8e\u8bc4\u4f30\u5668\u68b0\u64cd\u4f5c\u719f\u7ec3\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u5668\u68b0\u68c0\u6d4b\u4e0a\u8fbe\u523097%\u7684\u7cbe\u786e\u7387\uff0cmAP50-95\u4e3a96%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u663e\u5fae\u543b\u5408\u64cd\u4f5c\u6280\u80fd\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684AI\u6846\u67b6\u80fd\u6709\u6548\u5b9e\u73b0\u5bf9\u663e\u5fae\u8840\u7ba1\u543b\u5408\u672f\u4e2d\u5668\u68b0\u64cd\u4f5c\u6280\u80fd\u7684\u81ea\u52a8\u5316\u3001\u5ba2\u89c2\u5316\u8bc4\u4f30\uff0c\u5177\u6709\u826f\u597d\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6709\u671b\u6539\u5584\u5f53\u524d\u4f9d\u8d56\u4e3b\u89c2\u8bc4\u4ef7\u7684\u5c40\u9650\u3002", "summary_cn": "\u663e\u5fae\u8840\u7ba1\u543b\u5408\u672f\u7684\u719f\u7ec3\u7a0b\u5ea6\u662f\u591a\u4e2a\u663e\u5fae\u5916\u79d1\u9886\u57df\u7684\u4e00\u9879\u57fa\u672c\u80fd\u529b\u3002\u6b64\u7c7b\u624b\u672f\u8981\u6c42\u6781\u9ad8\u7684\u7cbe\u786e\u5ea6\u548c\u7cbe\u6e5b\u7684\u6280\u672f\uff0c\u56e0\u6b64\u6709\u6548\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u4e0a\uff0c\u663e\u5fae\u5916\u79d1\u6280\u672f\u7684\u8bc4\u4f30\u4e25\u91cd\u4f9d\u8d56\u4e13\u5bb6\u8bc4\u5206\u8005\u7684\u4e3b\u89c2\u5224\u65ad\uff0c\u800c\u8fd9\u79cd\u65b9\u6cd5\u672c\u8eab\u5b58\u5728\u8bc4\u5206\u8005\u95f4\u5dee\u5f02\u3001\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\u3001\u6613\u53d7\u8ba4\u77e5\u504f\u5dee\u5f71\u54cd\u4ee5\u53ca\u4eba\u5de5\u590d\u6838\u8017\u65f6\u7b49\u56fa\u6709\u5c40\u9650\u3002\u8fd9\u4e9b\u7f3a\u9677\u51f8\u663e\u4e86\u5f00\u53d1\u4e00\u79cd\u5ba2\u89c2\u3001\u53ef\u9760\u4e14\u53ef\u81ea\u52a8\u8bc4\u4f30\u663e\u5fae\u5916\u79d1\u64cd\u4f5c\u8868\u73b0\u7684\u7cfb\u7edf\u7684\u8feb\u5207\u9700\u6c42\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u663e\u5fae\u8840\u7ba1\u543b\u5408\u672f\u4e2d\u7684\u5668\u68b0\u64cd\u4f5c\u6280\u80fd\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\uff081\uff09\u57fa\u4e8e\u201cYou Only Look Once\u201d\uff08YOLO\uff09\u67b6\u6784\u7684\u5668\u68b0\u68c0\u6d4b\u6a21\u5757\uff1b\uff082\uff09\u57fa\u4e8eDeep Simple Online and Realtime Tracking\uff08DeepSORT\uff09\u5f00\u53d1\u7684\u5668\u68b0\u8ddf\u8e2a\u6a21\u5757\uff1b\uff083\uff09\u91c7\u7528\u5f62\u72b6\u63cf\u8ff0\u7b26\u7684\u5668\u68b0\u5c16\u7aef\u5b9a\u4f4d\u6a21\u5757\uff1b\u4ee5\u53ca\uff084\uff09\u57fa\u4e8e\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u7684\u76d1\u7763\u5206\u7c7b\u6a21\u5757\uff0c\u7528\u4e8e\u8bc4\u4f30\u5668\u68b0\u64cd\u4f5c\u719f\u7ec3\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5177\u6709\u826f\u597d\u7684\u6548\u679c\uff0c\u5668\u68b0\u68c0\u6d4b\u7cbe\u786e\u7387\u8fbe\u523097%\uff0c\u5728IoU\u9608\u503c\u4ece50%\u523095%\u8303\u56f4\u5185\u7684\u5e73\u5747\u7cbe\u5ea6\uff08mAP50-95\uff09\u8fbe\u523096%\u3002"}}
{"id": "2601.21159", "pdf": "https://arxiv.org/pdf/2601.21159", "abs": "https://arxiv.org/abs/2601.21159", "authors": ["Jianzheng Wang", "Huan Ni"], "title": "Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using \"one-way injection\" and \"shallow post-processing\" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u6846\u67b6SDCI\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u534f\u540c\u63a8\u7406\u3001\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u548c\u8d85\u50cf\u7d20\u7ed3\u6784\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u5f71\u50cf\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u5f71\u50cf\u65f6\uff0c\u56e0\u91c7\u7528\u5355\u5411\u6ce8\u5165\u548c\u6d45\u5c42\u540e\u5904\u7406\u7b56\u7565\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u51e0\u4f55\u5b9a\u4f4d\u548c\u8bed\u4e49\u9884\u6d4b\u7684\u9ad8\u8981\u6c42\u3002", "method": "\u63d0\u51faSDCI\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u4ea4\u53c9\u6a21\u578b\u6ce8\u610f\u529b\u878d\u5408\uff08CAF\uff09\u6a21\u5757\uff0c\u5728\u7279\u5f81\u7f16\u7801\u9636\u6bb5\u5b9e\u73b0\u53cc\u6a21\u578b\u81ea\u6ce8\u610f\u529b\u56fe\u4e92\u6ce8\uff1b2\uff09\u8bbe\u8ba1\u53cc\u5411\u4ea4\u53c9\u56fe\u6269\u6563\u7cbe\u70bc\uff08BCDR\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u8fed\u4ee3\u968f\u673a\u6e38\u8d70\u6269\u6563\u63d0\u5347\u53cc\u5206\u652f\u5206\u5272\u5f97\u5206\u53ef\u9760\u6027\uff1b3\uff09\u7ed3\u5408\u4f4e\u5c42\u8d85\u50cf\u7d20\u7ed3\u6784\uff0c\u6784\u5efa\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u8d85\u50cf\u7d20\u534f\u540c\u9884\u6d4b\uff08CSCP\uff09\u673a\u5236\u4ee5\u4f18\u5316\u8fb9\u754c\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u4e0a\uff0cSDCI\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8d85\u50cf\u7d20\u7ed3\u6784\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u4ecd\u5177\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0SDCI\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u5f71\u50cf\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u51e0\u4f55\u4e0e\u8bed\u4e49\u6311\u6218\uff0c\u8bc1\u660e\u4f20\u7edf\u8d85\u50cf\u7d20\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u53ef\u6709\u6548\u878d\u5408\u3002", "summary_cn": "\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u5f71\u50cf\u5177\u6709\u5730\u7269\u5bc6\u96c6\u5206\u5e03\u548c\u8fb9\u754c\u590d\u6742\u7684\u7279\u70b9\uff0c\u5bf9\u51e0\u4f55\u5b9a\u4f4d\u548c\u8bed\u4e49\u9884\u6d4b\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002\u73b0\u6709\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff08OVSS\uff09\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u201c\u5355\u5411\u6ce8\u5165\u201d\u548c\u201c\u6d45\u5c42\u540e\u5904\u7406\u201d\u7b56\u7565\u878d\u5408CLIP\u4e0e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e0a\u8ff0\u9700\u6c42\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u7a7a\u95f4\u6b63\u5219\u5316\u7684\u53cc\u5206\u652f\u534f\u540c\u63a8\u7406\u6846\u67b6SDCI\uff0c\u7528\u4e8e\u65e0\u9700\u8bad\u7ec3\u7684OVSS\u3002\u9996\u5148\uff0c\u5728\u7279\u5f81\u7f16\u7801\u9636\u6bb5\uff0cSDCI\u5f15\u5165\u4ea4\u53c9\u6a21\u578b\u6ce8\u610f\u529b\u878d\u5408\uff08CAF\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u5404\u81ea\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u56fe\u76f8\u4e92\u6ce8\u5165\uff0c\u5f15\u5bfc\u534f\u540c\u63a8\u7406\u3002\u5176\u6b21\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u53cc\u5411\u4ea4\u53c9\u56fe\u6269\u6563\u7cbe\u70bc\uff08BCDR\uff09\u6a21\u5757\uff0c\u5229\u7528\u8fed\u4ee3\u5f0f\u968f\u673a\u6e38\u8d70\u6269\u6563\u589e\u5f3a\u53cc\u5206\u652f\u5206\u5272\u5f97\u5206\u7684\u53ef\u9760\u6027\u3002\u6700\u540e\uff0c\u7ed3\u5408\u4f4e\u5c42\u8d85\u50cf\u7d20\u7ed3\u6784\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u51f8\u4f18\u5316\u7684\u8d85\u50cf\u7d20\u534f\u540c\u9884\u6d4b\uff08CSCP\uff09\u673a\u5236\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u5730\u7269\u8fb9\u754c\u3002\u5728\u591a\u4e2a\u9065\u611f\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u8d85\u50cf\u7d20\u7ed3\u6784\u7684\u4f20\u7edf\u9762\u5411\u5bf9\u8c61\u9065\u611f\u5f71\u50cf\u5206\u6790\u65b9\u6cd5\u4f9d\u7136\u6709\u6548\u3002"}}
