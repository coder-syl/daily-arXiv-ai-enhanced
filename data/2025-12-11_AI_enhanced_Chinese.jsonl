{"id": "2512.08979", "pdf": "https://arxiv.org/pdf/2512.08979", "abs": "https://arxiv.org/abs/2512.08979", "authors": ["Daechul Ahn", "Yura Choi", "Hyeonbeom Choi", "Seongwon Cho", "San Kim", "Jonghyun Choi"], "title": "What Happens When: Learning Temporal Orders of Events in Videos", "categories": ["cs.CV", "cs.AI"], "comment": "WACV 2026", "summary": "Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5f53\u524d\u89c6\u9891\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08VLMMs\uff09\u5728\u4e8b\u4ef6\u65f6\u5e8f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5373\u4f7f\u8f93\u5165\u5e27\u88ab\u6270\u4e71\u4ecd\u80fd\u53d6\u5f97\u9ad8\u5206\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u573a\u666f\u5148\u9a8c\u800c\u975e\u771f\u5b9e\u65f6\u5e8f\u63a8\u7406\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u57fa\u51c6VECTOR\u4ee5\u8bc4\u4f30\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u51faMECOT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8b\u4ef6\u7ea7\u6307\u4ee4\u5fae\u8c03\u4e0e\u63a8\u7406\u65f6\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u63d0\u5347\u6a21\u578b\u5bf9\u4e8b\u4ef6\u987a\u5e8f\u7684\u611f\u77e5\uff0c\u5728VECTOR\u53ca\u73b0\u6709\u89c6\u9891\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u66f4\u597d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u6807\u51c6\u8bc4\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u7406\u89e3\u89c6\u9891\u4e2d\u591a\u4e2a\u4e8b\u4ef6\u7684\u65f6\u95f4\u987a\u5e8f\u5c1a\u4e0d\u6e05\u695a\u3002\u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u5728\u5e27\u88ab\u6253\u4e71\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u53d6\u5f97\u9ad8\u5206\uff0c\u8bf4\u660e\u5176\u53ef\u80fd\u4f9d\u8d56\u5e38\u8bc6\u5148\u9a8c\u800c\u975e\u771f\u5b9e\u65f6\u5e8f\u5efa\u6a21\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u65f6\u5e8f\u7406\u89e3\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u540d\u4e3aVECTOR\u7684\u65b0\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5bf9\u4e8b\u4ef6\u65f6\u5e8f\u7684\u7406\u89e3\uff1b\u5e76\u63d0\u51faMECOT\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a(1) \u4f7f\u7528\u9010\u4e8b\u4ef6\u7684\u8be6\u7ec6\u89c6\u9891\u63cf\u8ff0\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\uff1b(2) \u5728\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\u63d0\u793a\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u65f6\u5e8f\u610f\u8bc6\u3002", "result": "\u5728VECTOR\u57fa\u51c6\u4e0a\uff0c\u591a\u79cd\u73b0\u6709VLMM\u5747\u8868\u73b0\u4e0d\u4f73\uff0c\u800cMECOT\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff1b\u540c\u65f6\uff0cMECOT\u4e5f\u5728\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u5176\u5bf9\u65f6\u5e8f\u7406\u89e3\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f53\u524dVLMM\u5728\u4e8b\u4ef6\u65f6\u5e8f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u4ec5\u9760\u573a\u666f\u5148\u9a8c\u4e0d\u8db3\u4ee5\u5b8c\u6210\u771f\u6b63\u7684\u65f6\u95f4\u63a8\u7406\u3002\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u6570\u636e\u548c\u8bad\u7ec3\u7b56\u7565\uff08\u5982MECOT\uff09\uff0c\u53ef\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u65f6\u5e8f\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u89c6\u9891\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.08980", "pdf": "https://arxiv.org/pdf/2512.08980", "abs": "https://arxiv.org/abs/2512.08980", "authors": ["Chengqi Dong", "Chuhuai Yue", "Hang He", "Rongge Mao", "Fenghe Tang", "S Kevin Zhou", "Zekun Xu", "Xiaohan Wang", "Jiajun Chai", "Wei Lin", "Guojun Yin"], "title": "Training Multi-Image Vision Agents via End2End Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent VLM-based agents aim to replicate OpenAI O3's ``thinking with images\" via tool use, but most open-source methods limit input to a single image, falling short on real-world multi-image QA tasks. To address this, we propose IMAgent, an open-source vision agent trained via end-to-end reinforcement learning dedicated for complex multi-image tasks. By leveraging a multi-agent system, we generate challenging and visually-rich multi-image QA pairs to fully activate the tool-use potential of the base VLM. Through manual verification, we obtain MIFG-QA, comprising 10k samples for training and evaluation. With deeper reasoning steps, VLMs may increasingly ignore visual inputs. We therefore develop two specialized tools for visual reflection and confirmation, allowing the model to proactively reallocate its attention to image content during inference. Benefiting from our well-designed action-trajectory two-level mask strategy, IMAgent achieves stable tool use behavior via pure RL training without requiring costly supervised fine-tuning data. Extensive experiments demonstrate that IMAgent maintains strong performance on existing single-image benchmarks while achieving substantial improvements on our proposed multi-image dataset, with our analysis providing actionable insights for the research community. Codes and data will be released soon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIMAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7684\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u667a\u80fd\u4f53\uff0c\u4e13\u4e3a\u590d\u6742\u591a\u56fe\u50cf\u95ee\u7b54\u4efb\u52a1\u8bbe\u8ba1\u3002\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u56fe\u50cf\u95ee\u7b54\u6570\u636e\u96c6MIFG-QA\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u4e13\u7528\u5de5\u5177\u589e\u5f3a\u6a21\u578b\u5bf9\u56fe\u50cf\u5185\u5bb9\u7684\u5173\u6ce8\uff0c\u7ed3\u5408\u4e24\u7ea7\u63a9\u7801\u7b56\u7565\u5b9e\u73b0\u7a33\u5b9a\u5de5\u5177\u8c03\u7528\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff0c\u5728\u5355\u56fe\u548c\u591a\u56fe\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90VLM\u667a\u80fd\u4f53\u5927\u591a\u4ec5\u652f\u6301\u5355\u56fe\u50cf\u8f93\u5165\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u7684\u591a\u56fe\u50cf\u95ee\u7b54\u4efb\u52a1\uff1b\u540c\u65f6\uff0c\u968f\u7740\u63a8\u7406\u6b65\u9aa4\u52a0\u6df1\uff0c\u6a21\u578b\u5bb9\u6613\u5ffd\u7565\u89c6\u89c9\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5176\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "method": "\u63d0\u51faIMAgent\u6846\u67b6\uff1a1\uff09\u5229\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u5177\u6311\u6218\u6027\u7684\u591a\u56fe\u50cf\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u542b1\u4e07\u6837\u672c\u7684\u6570\u636e\u96c6MIFG-QA\uff1b2\uff09\u8bbe\u8ba1\u4e24\u79cd\u4e13\u7528\u5de5\u5177\u7528\u4e8e\u89c6\u89c9\u53cd\u601d\u4e0e\u786e\u8ba4\uff0c\u5f15\u5bfc\u6a21\u578b\u5728\u63a8\u7406\u4e2d\u4e3b\u52a8\u5173\u6ce8\u56fe\u50cf\uff1b3\uff09\u91c7\u7528\u52a8\u4f5c-\u8f68\u8ff9\u4e24\u7ea7\u63a9\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u7eaf\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7a33\u5b9a\u7684\u5de5\u5177\u8c03\u7528\u884c\u4e3a\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u3002", "result": "IMAgent\u5728\u73b0\u6709\u5355\u56fe\u50cf\u57fa\u51c6\u4e0a\u4fdd\u6301\u5f3a\u6027\u80fd\uff0c\u540c\u65f6\u5728\u65b0\u63d0\u51fa\u7684\u591a\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "IMAgent\u6210\u529f\u89e3\u51b3\u4e86\u591a\u56fe\u50cf\u7406\u89e3\u4e0e\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5c55\u793a\u4e86\u7eaf\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3VLM\u667a\u80fd\u4f53\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5f00\u6e90\u65b9\u6848\u4e0e\u6570\u636e\u96c6\u3002"}}
{"id": "2512.08981", "pdf": "https://arxiv.org/pdf/2512.08981", "abs": "https://arxiv.org/abs/2512.08981", "authors": ["Tahar Chettaoui", "Naser Damer", "Fadi Boutros"], "title": "Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at BMVC workshop (SRBS) 2025", "summary": "Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUTIE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5c06\u5176\u4ed6\u4eba\u53e3\u7fa4\u4f53\u7684\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u4eba\u8138\u5d4c\u5165\u4e2d\uff0c\u4ee5\u6a21\u7cca\u4eba\u53e3\u5c5e\u6027\u3001\u589e\u5f3a\u8eab\u4efd\u76f8\u5173\u7279\u5f81\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4eba\u8138\u8bc6\u522b\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u8de8\u7fa4\u4f53\u7684\u4eba\u8138\u8bc6\u522b\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5e38\u56e0\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\u4e0e\u8eab\u4efd\u7279\u5f81\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7ea0\u7f20\u800c\u5bfc\u81f4\u5bf9\u4e0d\u540c\u7fa4\u4f53\u7684\u8bc6\u522b\u6027\u80fd\u4e0d\u5747\uff0c\u5c24\u5176\u5728\u591a\u5143\u6587\u5316\u57ce\u5e02\u4e2d\u95ee\u9898\u7a81\u51fa\uff1b\u4e3a\u7f13\u89e3\u8fd9\u4e00\u504f\u89c1\uff0c\u9700\u89e3\u8026\u8eab\u4efd\u4e0e\u4eba\u53e3\u5c5e\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51faUnified Text-Image Embedding\uff08UTIE\uff09\u7b56\u7565\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\u3001OpenCLIP\u3001SigLIP\uff09\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u5c06\u6765\u81ea\u5176\u4ed6\u4eba\u53e3\u7fa4\u4f53\u7684\u6587\u672c\u63cf\u8ff0\u7279\u5f81\u6ce8\u5165\u5f53\u524d\u4eba\u8138\u5d4c\u5165\uff0c\u4f7f\u5176\u5728\u4fdd\u7559\u8eab\u4efd\u4fe1\u606f\u7684\u540c\u65f6\u5f31\u5316\u7279\u5b9a\u4eba\u53e3\u5c5e\u6027\uff0c\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u8868\u793a\u3002", "result": "\u5728RFW\u548cBFW\u4e24\u4e2a\u516c\u5e73\u6027\u8bc4\u6d4b\u57fa\u51c6\u4e0a\uff0cUTIE\u5728\u4e09\u79cdVLM\u4e0a\u5747\u6709\u6548\u964d\u4f4e\u4e86\u504f\u89c1\u6307\u6807\uff0c\u540c\u65f6\u7ef4\u6301\u751a\u81f3\u63d0\u5347\u4e86\u4eba\u8138\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "UTIE\u901a\u8fc7\u5f15\u5165\u8de8\u7fa4\u4f53\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u6210\u529f\u5b9e\u73b0\u4e86\u66f4\u516c\u5e73\u4e14\u9ad8\u7cbe\u5ea6\u7684\u4eba\u8138\u8bc6\u522b\uff0c\u4e3a\u7f13\u89e3FR\u7cfb\u7edf\u4e2d\u7684\u4eba\u53e3\u504f\u89c1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08982", "pdf": "https://arxiv.org/pdf/2512.08982", "abs": "https://arxiv.org/abs/2512.08982", "authors": ["Jian Xu", "Wei Chen", "Shigui Li", "Delu Zeng", "John Paisley", "Qibin Zhao"], "title": "Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \\textit{unconditional synthesis}, their application to \\textit{conditional enhancement} remains unexplored. We present \\textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \\textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \\textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \\textbf{state-of-the-art performance with single-step sampling} (\\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \\textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faConsist-Retinex\uff0c\u9996\u6b21\u5c06\u4e00\u81f4\u6027\u6a21\u578b\u5e94\u7528\u4e8eRetinex\u4f4e\u5149\u589e\u5f3a\u4efb\u52a1\uff0c\u901a\u8fc7\u53cc\u76ee\u6807\u4e00\u81f4\u6027\u635f\u5931\u548c\u81ea\u9002\u5e94\u566a\u58f0\u5f3a\u8c03\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b0\u5355\u6b65\u751f\u6210\u7684SOTA\u6027\u80fd\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a\u4f20\u7edf\u6269\u6563\u6a21\u578b\u76841/8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u867d\u6548\u679c\u826f\u597d\uff0c\u4f46\u9700\u6570\u767e\u6b21\u8fed\u4ee3\u91c7\u6837\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\uff1b\u800c\u5f53\u524d\u4e00\u81f4\u6027\u6a21\u578b\u4ec5\u7528\u4e8e\u65e0\u6761\u4ef6\u751f\u6210\uff0c\u5c1a\u672a\u63a2\u7d22\u5176\u5728\u6709\u6761\u4ef6\u589e\u5f3a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faConsist-Retinex\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u6838\u5fc3\u521b\u65b0\uff1a(1) \u53cc\u76ee\u6807\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5728\u968f\u673a\u65f6\u95f4\u91c7\u6837\u4e0b\u7ed3\u5408\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u771f\u503c\u5bf9\u9f50\uff0c\u63d0\u4f9b\u5168\u8c31\u76d1\u7763\uff1b(2) \u81ea\u9002\u5e94\u566a\u58f0\u5f3a\u8c03\u91c7\u6837\u7b56\u7565\uff0c\u4f18\u5148\u8bad\u7ec3\u5927\u566a\u58f0\u533a\u57df\u4ee5\u652f\u6301\u5355\u6b65\u6761\u4ef6\u751f\u6210\u3002", "result": "\u5728VE-LOL-L\u6570\u636e\u96c6\u4e0a\uff0cConsist-Retinex\u4ee5\u5355\u6b65\u91c7\u6837\u8fbe\u5230PSNR 25.51\u3001FID 44.73\uff0c\u4f18\u4e8eDiff-Retinex++\uff08PSNR 23.41\u3001FID 49.59\uff09\uff0c\u4e14\u8bad\u7ec3\u5f00\u9500\u4ec5\u4e3a1000\u6b65Diff-Retinex\u57fa\u7ebf\u76841/8\u3002", "conclusion": "Consist-Retinex\u6210\u529f\u5c06\u4e00\u81f4\u6027\u5efa\u6a21\u5f15\u5165Retinex\u4f4e\u5149\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u4e0e\u8bad\u7ec3\u7ecf\u6d4e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u6269\u6563\u6a21\u578b\u7684\u589e\u5f3a\u8d28\u91cf\u3002"}}
{"id": "2512.08983", "pdf": "https://arxiv.org/pdf/2512.08983", "abs": "https://arxiv.org/abs/2512.08983", "authors": ["Maoyu Wang", "Yao Lu", "Bo Zhou", "Zhuangzhi Chen", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\\%$ parameter reduction and $84.44\\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHSCP\u7684\u5206\u5c42\u8c31\u805a\u7c7b\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5c42\u526a\u679d\u4e0e\u901a\u9053\u526a\u679d\uff0c\u5728\u663e\u8457\u538b\u7f29\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4f4e\u4fe1\u566a\u6bd4\u73af\u5883\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u65e0\u4eba\u673a\u8bc6\u522b\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u63d0\u53d6\u53ef\u9760\u4fe1\u53f7\u7279\u5f81\u4e14\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff1b\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u867d\u63d0\u5347\u4e86\u51c6\u786e\u7387\uff0c\u4f46\u6a21\u578b\u5927\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u538b\u7f29\u7387\u3001\u786c\u4ef6\u52a0\u901f\u4e0e\u8bc6\u522b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faHSCP\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u57fa\u4e8eCKA\uff08Centered Kernel Alignment\uff09\u5f15\u5bfc\u7684\u8c31\u805a\u7c7b\u8bc6\u522b\u5e76\u79fb\u9664\u5197\u4f59\u5c42\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u76f8\u540c\u7b56\u7565\u5e94\u7528\u4e8e\u901a\u9053\u7ef4\u5ea6\u4ee5\u6d88\u9664\u66f4\u7ec6\u7c92\u5ea6\u7684\u5197\u4f59\uff1b\u6700\u540e\u91c7\u7528\u6297\u566a\u5fae\u8c03\u7b56\u7565\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728UAV-M100\u57fa\u51c6\u4e0a\uff0cHSCP\u5728ResNet18\u4e0a\u5b9e\u73b0\u4e8686.39%\u7684\u53c2\u6570\u51cf\u5c11\u548c84.44%\u7684FLOPs\u51cf\u5c11\uff0c\u540c\u65f6\u76f8\u6bd4\u672a\u526a\u679d\u57fa\u7ebf\u63d0\u5347\u4e861.49%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u4ecd\u4fdd\u6301\u4f18\u5f02\u9c81\u68d2\u6027\u3002", "conclusion": "HSCP\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5728\u538b\u7f29\u7387\u3001\u63a8\u7406\u6548\u7387\u4e0e\u8bc6\u522b\u7cbe\u5ea6\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u65e0\u4eba\u673a\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.08984", "pdf": "https://arxiv.org/pdf/2512.08984", "abs": "https://arxiv.org/abs/2512.08984", "authors": ["Nirhoshan Sivaroopan", "Hansi Karunarathna", "Chamara Madarasingha", "Anura Jayasumana", "Kanchana Thilakarathna"], "title": "RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAG-HAR\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728HAR\u4efb\u52a1\u4e2d\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u3001\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u9ad8\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002", "method": "RAG-HAR\u901a\u8fc7\u8ba1\u7b97\u8f7b\u91cf\u7ea7\u7edf\u8ba1\u63cf\u8ff0\u7b26\uff0c\u4ece\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\u5229\u7528LLM\u8fdb\u884c\u6d3b\u52a8\u8bc6\u522b\uff1b\u8fdb\u4e00\u6b65\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u548cLLM\u751f\u6210\u7684\u6d3b\u52a8\u63cf\u8ff0\u7b26\u6784\u5efa\u5bcc\u542b\u4e0a\u4e0b\u6587\u7684\u5411\u91cf\u6570\u636e\u5e93\u3002", "result": "\u5728\u516d\u4e2a\u591a\u6837\u5316\u7684HAR\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u80fd\u8bc6\u522b\u5e76\u5408\u7406\u6807\u6ce8\u591a\u79cd\u672a\u89c1\u8fc7\u7684\u4eba\u7c7b\u6d3b\u52a8\u3002", "conclusion": "RAG-HAR\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86HAR\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u90e8\u7f72\u7075\u6d3b\u6027\u3002"}}
{"id": "2512.08985", "pdf": "https://arxiv.org/pdf/2512.08985", "abs": "https://arxiv.org/abs/2512.08985", "authors": ["Vignesh Sundaresha", "Akash Haridas", "Vikram Appia", "Lav Varshney"], "title": "An Efficient Test-Time Scaling Approach for Image Generation", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVerifier-Threshold\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\u66f4\u6709\u6548\u5730\u91cd\u65b0\u5206\u914d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u4fdd\u6301GenEval\u57fa\u51c6\u6027\u80fd\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e862-4\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e3a\u6269\u6563\u548c\u6d41\u6a21\u578b\u5206\u914d\u975e\u5747\u5300\u63a8\u7406\u8ba1\u7b97\u9884\u7b97\u65f6\u4f9d\u8d56\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u6548\u7387\u4f4e\u4e0b\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faVerifier-Threshold\u65b9\u6cd5\uff0c\u81ea\u52a8\u91cd\u65b0\u5206\u914d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f18\u5316\u4e0d\u540c\u53bb\u566a\u6b65\u9aa4\u4e2d\u7684\u8ba1\u7b97\u9884\u7b97\u5206\u914d\u3002", "result": "\u5728GenEval\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u540c\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c112-4\u500d\u3002", "conclusion": "\u901a\u8fc7\u66f4\u667a\u80fd\u5730\u5206\u914d\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\uff0cVerifier-Threshold\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u4f18\u5316\u8ba1\u7b97\u5206\u914d\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.08986", "pdf": "https://arxiv.org/pdf/2512.08986", "abs": "https://arxiv.org/abs/2512.08986", "authors": ["Anca Mihai", "Adrian Groza"], "title": "Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u56fe\u50cf\u6570\u636e\u7684\u8d28\u91cf\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u5206\u7c7b\u5668\u7b5b\u9009\u4f4e\u8d28\u91cf\u56fe\u50cf\u3001\u7ed3\u5408\u56fe\u50cf\u589e\u5f3a\u4e0e\u6df1\u5ea6\u5b66\u4e60\u8f85\u52a9\u6807\u6ce8\uff0c\u5e76\u5229\u7528\u6807\u6ce8\u8005\u4e00\u81f4\u6027\u8bc4\u4f30\u786e\u4fdd\u7528\u4e8eAI\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u6570\u636e\u5177\u6709\u9ad8\u8d28\u91cf\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u82e5\u672a\u53ca\u65e9\u8bca\u65ad\u53ef\u80fd\u5bfc\u81f4\u89c6\u529b\u4e27\u5931\uff0c\u800cAI\u8f85\u52a9\u8bca\u65ad\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff1b\u7136\u800c\uff0c\u7531\u4e8e\u89c6\u7f51\u819c\u7ed3\u6784\u590d\u6742\uff0c\u4eba\u5de5\u91c7\u96c6\u548c\u6807\u6ce8\u8fc7\u7a0b\u4e2d\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u53ef\u9760\u7684\u8d28\u91cf\u63a7\u5236\u673a\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1\uff09\u4f7f\u7528\u57fa\u4e8e\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u5206\u7c7b\u5668\uff08\u7ed3\u5408\u56fe\u50cf\u5904\u7406\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u63d0\u53d6\u7279\u5f81\uff09\u8fc7\u6ee4\u4e0d\u5408\u683c\u56fe\u50cf\uff1b2\uff09\u5bf9\u4fdd\u7559\u56fe\u50cf\u8fdb\u884c\u589e\u5f3a\u5e76\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u8f85\u52a9\u4eba\u5de5\u6807\u6ce8\uff1b3\uff09\u901a\u8fc7\u6240\u63a8\u5bfc\u7684\u4e00\u81f4\u6027\u516c\u5f0f\u8bc4\u4f30\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\uff0c\u4ee5\u5224\u65ad\u6807\u6ce8\u662f\u5426\u53ef\u7528\u3002", "result": "\u8be5\u6846\u67b6\u6709\u6548\u7b5b\u9009\u51fa\u9ad8\u8d28\u91cf\u56fe\u50cf\u5e76\u63d0\u5347\u6807\u6ce8\u53ef\u9760\u6027\uff0c\u4ece\u800c\u4e3aAI\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u63d0\u4f9b\u66f4\u53ef\u4fe1\u7684\u6570\u636e\u57fa\u7840\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8d28\u91cf\u63a7\u5236\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u9ad8\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8AI\u8bca\u65ad\u7cfb\u7edf\u4e2d\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u7684\u8d28\u91cf\uff0c\u8fdb\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u4e0e\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.08987", "pdf": "https://arxiv.org/pdf/2512.08987", "abs": "https://arxiv.org/abs/2512.08987", "authors": ["Yuze Hao", "Linchao Zhu", "Yi Yang"], "title": "3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at NeurIPS 2025", "summary": "Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3DID\u7684\u4e09\u7ef4\u9006\u5411\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u4e0e\u7269\u7406\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff0c\u76f4\u63a5\u5728\u4e09\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9ad8\u6548\u641c\u7d22\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u4e14\u591a\u6837\u5316\u7684\u4e09\u7ef4\u51e0\u4f55\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8c\u7ef4\u6295\u5f71\u6216\u5bf9\u5df2\u6709\u4e09\u7ef4\u5f62\u72b6\u5fae\u8c03\uff0c\u727a\u7272\u4e86\u4f53\u79ef\u7ec6\u8282\u5e76\u9650\u5236\u4e86\u4ece\u96f6\u5f00\u59cb\u7684\u8bbe\u8ba1\u63a2\u7d22\u80fd\u529b\uff1b\u540c\u65f6\uff0c\u4e09\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\u7684\u6307\u6570\u7ea7\u589e\u957f\u4f7f\u5f97\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51fa3DID\u6846\u67b6\uff1a\u9996\u5148\u5b66\u4e60\u4e00\u4e2a\u7edf\u4e00\u7684\u7269\u7406-\u51e0\u4f55\u5d4c\u5165\uff0c\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7d27\u51d1\u5730\u8868\u793a\u5f62\u72b6\u548c\u7269\u7406\u573a\u6570\u636e\uff1b\u7136\u540e\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\u2014\u2014\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u68af\u5ea6\u5f15\u5bfc\u7684\u6269\u6563\u91c7\u6837\u5668\u63a2\u7d22\u5168\u5c40\u6f5c\u5728\u6d41\u5f62\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u76ee\u6807\u9a71\u52a8\u3001\u62d3\u6251\u4fdd\u6301\u7684\u7cbe\u7ec6\u5316\u8c03\u6574\u3002", "result": "3DID\u5728\u89e3\u7684\u8d28\u91cf\u548c\u8bbe\u8ba1\u591a\u6837\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e09\u7ef4\u51e0\u4f55\u7ed3\u6784\u3002", "conclusion": "\u6240\u63d0\u51fa\u76843DID\u6846\u67b6\u6709\u6548\u514b\u670d\u4e86\u5f53\u524d\u4e09\u7ef4\u9006\u5411\u8bbe\u8ba1\u65b9\u6cd5\u5728\u7ec6\u8282\u4fdd\u7559\u4e0e\u8bbe\u8ba1\u81ea\u7531\u5ea6\u65b9\u9762\u7684\u5c40\u9650\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u4ece\u96f6\u5f00\u59cb\u4e09\u7ef4\u9006\u5411\u8bbe\u8ba1\u3002"}}
{"id": "2512.08989", "pdf": "https://arxiv.org/pdf/2512.08989", "abs": "https://arxiv.org/abs/2512.08989", "authors": ["Lu Huo", "Wenjian Huang", "Jianguo Zhang", "Min Xu", "Haimin Zhang"], "title": "Enhancing Knowledge Transfer in Hyperspectral Image Classification via Cross-scene Knowledge Integration", "categories": ["cs.CV"], "comment": null, "summary": "Knowledge transfer has strong potential to improve hyperspectral image (HSI) classification, yet two inherent challenges fundamentally restrict effective cross-domain transfer: spectral variations caused by different sensors and semantic inconsistencies across heterogeneous scenes. Existing methods are limited by transfer settings that assume homogeneous domains or heterogeneous scenarios with only co-occurring categories. When label spaces do not overlap, they further rely on complete source-domain coverage and therefore overlook critical target-private information. To overcome these limitations and enable knowledge transfer in fully heterogeneous settings, we propose Cross-scene Knowledge Integration (CKI), a framework that explicitly incorporates target-private knowledge during transfer. CKI includes: (1) Alignment of Spectral Characteristics (ASC) to reduce spectral discrepancies through domain-agnostic projection; (2) Cross-scene Knowledge Sharing Preference (CKSP), which resolves semantic mismatch via a Source Similarity Mechanism (SSM); and (3) Complementary Information Integration (CII) to maximize the use of target-specific complementary cues. Extensive experiments verify that CKI achieves state-of-the-art performance with strong stability across diverse cross-scene HSI scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8de8\u573a\u666f\u77e5\u8bc6\u6574\u5408\uff08CKI\uff09\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6807\u7b7e\u7a7a\u95f4\u4e0d\u91cd\u53e0\u4e14\u76ee\u6807\u57df\u5305\u542b\u79c1\u6709\u7c7b\u522b\u7684\u5b8c\u5168\u5f02\u6784\u573a\u666f\u4e0b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5149\u8c31\u56fe\u50cf\u8de8\u57df\u8fc1\u79fb\u65f6\u53d7\u9650\u4e8e\u540c\u8d28\u57df\u5047\u8bbe\u6216\u4ec5\u5305\u542b\u5171\u73b0\u7c7b\u522b\u7684\u5f02\u6784\u573a\u666f\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u76ee\u6807\u57df\u79c1\u6709\u4fe1\u606f\uff0c\u5c24\u5176\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u6807\u7b7e\u7a7a\u95f4\u65e0\u91cd\u53e0\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCKI\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u5149\u8c31\u7279\u5f81\u5bf9\u9f50\uff08ASC\uff09\uff0c\u901a\u8fc7\u57df\u65e0\u5173\u6295\u5f71\u51cf\u5c11\u5149\u8c31\u5dee\u5f02\uff1b(2) \u8de8\u573a\u666f\u77e5\u8bc6\u5171\u4eab\u504f\u597d\uff08CKSP\uff09\uff0c\u5229\u7528\u6e90\u76f8\u4f3c\u6027\u673a\u5236\uff08SSM\uff09\u7f13\u89e3\u8bed\u4e49\u4e0d\u4e00\u81f4\uff1b(3) \u4e92\u8865\u4fe1\u606f\u6574\u5408\uff08CII\uff09\uff0c\u6700\u5927\u5316\u5229\u7528\u76ee\u6807\u57df\u7279\u6709\u4fe1\u606f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCKI\u5728\u591a\u79cd\u8de8\u573a\u666f\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "CKI\u6709\u6548\u89e3\u51b3\u4e86\u5b8c\u5168\u5f02\u6784\u573a\u666f\u4e0b\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u6574\u5408\u76ee\u6807\u57df\u79c1\u6709\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
