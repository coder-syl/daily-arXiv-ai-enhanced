<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文通过大规模定量研究揭示了视频扩散模型中运动与外观在去噪时间步上的解耦规律，验证并形式化了“早期时间步主导运动、后期主导外观”的经验法则，并基于此提出一种简化且高效的运动定制方法。


<details>
  <summary>Details</summary>
Motivation: 当前对文本到视频扩散模型中运动如何在不同时间步上被编码的理解仍不充分，尽管实践中常使用“早期时间步控制运动和布局、后期细化外观”的启发式方法，但缺乏系统性刻画。

Method: 作者通过在指定时间步范围内注入新条件，以由此引发的外观编辑与运动保持之间的权衡作为运动编码的代理指标，并通过大规模定量实验刻画这一代理关系，从而在去噪轨迹上量化运动与外观的竞争关系。

Result: 在多种架构中一致发现存在早期的“运动主导阶段”和后期的“外观主导阶段”，据此确定了一个操作性的运动-外观时间步边界；基于此，仅在运动主导阶段进行训练和推理即可实现强效运动迁移，无需额外去偏模块或特殊目标。

Conclusion: 该工作将广泛使用的启发式经验转化为时空解耦原则，所提出的基于时间步约束的方法可直接集成到现有运动迁移与编辑方法中。

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

Abstract (中文翻译): 文本到视频扩散模型通过迭代去噪合成时间运动和空间外观，然而运动在不同时间步上如何被编码仍缺乏深入理解。实践中，人们常依赖一种经验性启发：早期时间步主要塑造运动和布局，而后期时间步则细化外观，但这种行为尚未被系统性地刻画。本文通过在指定时间步范围内注入新条件所引发的外观编辑与运动保持之间的权衡，作为视频扩散模型中运动编码的代理指标，并通过大规模定量研究对该代理关系进行刻画。该方法使我们能够沿着去噪轨迹定量映射运动与外观之间的竞争关系，从而将二者解耦。在多种不同架构中，我们一致地识别出一个早期的“运动主导阶段”和一个后期的“外观主导阶段”，从而在时间步空间中确立了一个可操作的运动-外观边界。基于这一发现，我们简化了当前的一次性运动定制范式，仅在运动主导阶段进行训练和推理，即可在无需辅助去偏模块或专门优化目标的情况下实现强大的运动迁移效果。我们的分析将一种广泛使用的启发式方法转化为一种时空解耦原则，而所提出的基于时间步约束的方案可直接集成到现有的运动迁移与编辑方法中。

</details>


### [2] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

TL;DR: 本文提出了一种结合3D CNN与LSTM的混合深度学习架构，用于实时识别美式手语（ASL）单词，系统在WLASL、ASL-LEX等数据集上训练，F1分数达0.71–0.99，并支持在OAK-D摄像头边缘部署。


<details>
  <summary>Details</summary>
Motivation: 解决全球超过7000万聋人及听力障碍者面临的沟通障碍问题，通过构建实时美式手语识别系统提升无障碍交流能力。

Method: 采用3D卷积神经网络（3D CNN）提取视频帧中的时空特征，再通过长短期记忆网络（LSTM）建模手语手势中的时序依赖关系；模型在WLASL数据集（2000个常用词）、ASL-LEX词库（约2700个手势）以及100个专家标注的手势上进行训练，并部署于AWS平台，支持OAK-D摄像头边缘推理。

Result: 系统在各类手语类别上的F1分数范围为0.71至0.99，表现出优异的识别性能，并成功实现云端与边缘端的实时推理部署。

Conclusion: 所提出的3D CNN-LSTM混合架构能有效实现高精度、低延迟的美式手语单词识别，具备实际应用于无障碍通信场景的潜力。

Abstract: This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

Abstract (中文翻译): 本文提出了一种实时美式手语（ASL）识别系统，该系统采用结合三维卷积神经网络（3D CNN）与长短期记忆网络（LSTM）的混合深度学习架构。该系统处理网络摄像头视频流，以识别单词级别的ASL手势，旨在缓解全球超过7000万聋人及听力障碍人士所面临的沟通障碍。我们的架构利用3D卷积从视频帧中捕捉时空特征，随后通过LSTM层建模手语手势中固有的时序依赖关系。该系统在WLASL数据集（包含2000个常用词）、ASL-LEX词库（约2700个手势）以及一组经专家标注的100个ASL手势上进行训练，在各类手势上的F1分数介于0.71至0.99之间。该模型部署于AWS基础设施，并支持在OAK-D摄像头上的边缘部署以实现实时推理。文中还讨论了该架构设计、训练方法、评估指标以及面向实际无障碍应用的部署考量。

</details>


### [3] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 本文提出一种结合人工智能与局部线性嵌入（LLE）的新方法，用于提升医疗计费与转录系统的准确性与效率，实验表明该AI增强LLE模型显著改善了高维医疗数据的处理效果。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在医疗领域的快速发展，亟需更高效、准确的方法来处理高维医疗数据，特别是在医疗计费和语音转录等环节，以减少人为错误并提升运营效率。

Method: 作者将人工智能技术与局部线性嵌入（LLE）相结合，构建了一个AI增强的LLE数学模型，专门用于优化医疗计费和转录服务中的高维数据处理。

Result: 通过一系列实验验证，该模型在真实医疗场景中显著提高了数据处理的准确性和操作效率。

Conclusion: AI增强的LLE方法在医疗数据处理中展现出巨大潜力，不仅提升了现有系统的性能，也为未来在更广泛医疗应用中的研究奠定了基础。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

Abstract (中文翻译): 人工智能在医疗领域的快速发展为提升包括医疗计费和转录在内的多种流程开辟了新路径。本文提出了一种创新方法，将人工智能与局部线性嵌入（LLE）相结合，以革新高维医疗数据的处理方式。该AI增强的LLE模型专为提高医疗计费系统和转录服务的准确性与效率而设计。通过自动化这些流程，该模型旨在减少人为错误、简化操作，从而加快并提高患者护理文档记录和财务交易的准确性。本文提供了AI增强LLE的完整数学模型，并通过一系列实验展示了其在现实医疗场景中的应用。结果表明，该模型在数据处理准确性和运营效率方面均有显著提升。本研究不仅突显了AI增强LLE在医疗数据分析中的潜力，也为未来拓展至更广泛的医疗应用奠定了基础。

</details>


### [4] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: VISTA is a modular framework that separates visual perception from reasoning to reduce reliance on spurious correlations in vision-language models, improving robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: End-to-end vision-language models often rely on spurious correlations rather than true visual evidence, especially after fine-tuning, leading to shortcut learning and poor generalization.

Method: VISTA decouples perception and reasoning by using a frozen VLM for short, objective perception queries and a text-only LLM to decompose questions, plan queries, and aggregate visual facts. It uses reinforcement learning with a reward-aligned interface trained on only 641 curated multi-step questions.

Result: VISTA significantly improves robustness on SpuriVerse (+16.29% with Qwen-2.5-VL-7B, +6.77% with Llama-3.2-Vision-11B), maintains competitive performance on MMVP and SeedBench, transfers well to unseen VLMs, and recovers from perception failures. Human evaluation shows its reasoning is more grounded and less biased.

Conclusion: By explicitly separating perception from reasoning and using natural language as an information bottleneck, VISTA enables more reliable, interpretable, and robust visual reasoning that avoids spurious correlations.

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

Abstract (中文翻译): 端到端的视觉-语言模型（VLM）通常通过利用虚假相关性而非因果视觉证据来回答视觉问题，并且在微调后更容易依赖捷径。我们提出了VISTA（用于基于文本分析的视觉信息分离），这是一种模块化框架，通过显式的信息瓶颈将感知与推理解耦。该框架中，一个冻结的VLM传感器仅处理简短、客观的感知查询，而一个纯文本的大语言模型（LLM）推理器则负责分解问题、规划查询，并以自然语言形式整合视觉事实。这种受控接口为使用强化学习训练无偏视觉推理提供了一个与奖励对齐的环境。使用Qwen2.5-VL和Llama3.2-Vision作为传感器，并仅用641个精心策划的多步问题通过GRPO进行训练，VISTA在SpuriVerse上对真实世界虚假相关性的鲁棒性显著提升（Qwen-2.5-VL-7B提升16.29%，Llama-3.2-Vision-11B提升6.77%），同时在MMVP和平衡版SeedBench子集上保持竞争力。VISTA还能稳健地迁移到未见过的VLM传感器，并能识别和从VLM感知失败中恢复。人工分析进一步表明，与端到端VLM基线相比，VISTA的推理轨迹更加中立、更少依赖虚假属性，并更明确地基于视觉证据。

</details>


### [5] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: SAMM2D是一种双编码器框架，在颅内动脉瘤检测任务中显著优于临床基线，且发现强预训练模型下数据增强反而损害性能；通过阈值校准可达到95%敏感度，超越放射科医生平均水平，并具备显著的临床经济效益。


<details>
  <summary>Details</summary>
Motivation: 动脉瘤形态细微、类别极度不平衡以及标注数据稀缺，使得有效检测极具挑战性；同时医学图像领域普遍依赖数据增强，但在低数据场景下其有效性尚未充分验证。

Method: 提出SAMM2D双编码器框架，结合强ImageNet预训练主干，在RSNA颅内动脉瘤数据集上进行实验，并系统评估六种数据增强策略的影响；使用Grad-CAM进行可视化分析，并通过决策阈值校准优化敏感度。

Result: SAMM2D在RSNA数据集上AUC达0.686，比临床基线提升32%；未使用数据增强的模型显著优于所有增强变体（p < 0.01）；校准后敏感度达95%，超过平均放射科医生表现；每千名筛查患者预计节省1390万美元；Grad-CAM显示85%真阳性聚焦于相关血管区域（与专家标注IoU为62%）。

Conclusion: 在医学图像分析中，强预训练可能比复杂的数据增强策略更有效；未来工作应优先利用预训练特征的鲁棒不变性，而非盲目增加增强手段。

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

Abstract (中文翻译): 有效的动脉瘤检测对于预防危及生命的出血至关重要，但由于动脉瘤形态细微、类别严重不平衡以及标注数据稀缺，这一任务仍然具有挑战性。我们提出了SAMM2D，一种双编码器框架，在RSNA颅内动脉瘤数据集上实现了0.686的AUC，比临床基线提高了32%。在对六种数据增强策略进行全面消融实验时，我们发现了一个显著现象：当与强大的预训练主干网络结合时，任何形式的数据增强都会降低模型性能。未使用数据增强的基线模型在所有增强变体中表现最佳，性能高出1.75至2.23个百分点（p < 0.01），推翻了“在低数据医学场景中越多增强越好”的假设。我们推测，ImageNet预训练特征已捕获了足够的鲁棒不变性，额外的数据增强不仅冗余，还会破坏已学习的特征流形。通过校准决策阈值，SAMM2D达到了95%的敏感度，超越了放射科医生的平均水平，并在筛查应用中预计每1,000名患者可节省1390万美元。Grad-CAM可视化结果表明，85%的真阳性样本关注于相关的血管区域（与专家标注的IoU为62%），证明了模型具有临床意义的关注焦点。我们的结果表明，未来的医学影像工作流程可能从强预训练中获益更多，而非依赖日益复杂的数据增强流程。

</details>


### [6] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: HookMIL 是一种高效且上下文感知的多示例学习框架，通过可学习的“钩子”（hook）token实现线性复杂度的上下文聚合，在多个病理数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统MIL方法在全切片图像分析中丢失关键上下文信息，而基于Transformer的方法虽表达能力强但计算复杂度高且存在冗余。

Method: 提出HookMIL框架，引入可学习的hook token进行结构化上下文聚合；token可通过视觉特征、视觉-语言模型文本嵌入或空间转录组-视觉模型特征进行多模态初始化；采用双向注意力机制（线性复杂度）实现实例与token交互，并设计Hook Diversity Loss促进token专业化，以及hook-to-hook通信机制减少冗余。

Result: 在四个公开病理数据集上取得最先进的性能，同时提升计算效率和可解释性。

Conclusion: HookMIL有效结合多模态先验知识与高效注意力机制，在保持低计算开销的同时显著提升WSI弱监督分析的性能与可解释性。

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

Abstract (中文翻译): 多示例学习（MIL）已推动了计算病理学中全切片图像（WSI）的弱监督分析。然而，传统的MIL方法常常丢失关键的上下文信息，而基于Transformer的变体虽然表达能力更强，却面临二次方复杂度和冗余计算的问题。为解决这些局限，我们提出了HookMIL——一种上下文感知且计算高效的MIL框架，利用紧凑、可学习的“钩子”（hook）token进行结构化的上下文聚合。这些token可从以下三种方式初始化：(i) 关键图像块的视觉特征，(ii) 视觉-语言病理模型的文本嵌入，以及(iii) 空间转录组-视觉模型的空间定位特征。这种多模态初始化使Hook Token能够融合丰富的文本和空间先验知识，从而加速收敛并提升表征质量。训练过程中，Hook token通过线性复杂度的双向注意力机制与实例进行交互。为进一步促进专业化，我们引入了Hook多样性损失（Hook Diversity Loss），鼓励每个token聚焦于不同的组织病理学模式。此外，还设计了hook-to-hook通信机制，在减少冗余的同时优化上下文交互。在四个公开病理数据集上的大量实验表明，HookMIL实现了最先进的性能，并提升了计算效率与可解释性。代码已在https://github.com/lingxitong/HookMIL开源。

</details>


### [7] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: Tiny-YOLOSAM combines YOLOv12 with TinySAM to accelerate full-scene segmentation by using object detection boxes as prompts and sparse point sampling only where needed, achieving much faster speed and better coverage than dense prompting.


<details>
  <summary>Details</summary>
Motivation: The original Segment Anything Model (SAM) and its lightweight variant TinySAM are too slow for real-time applications due to the need for hundreds of prompts in "segment-everything" mode. A more efficient prompting strategy is needed for practical deployment.

Method: The authors propose Tiny-YOLOSAM, a hybrid pipeline that uses YOLOv12 to generate bounding box prompts for salient objects and adds sparse point prompts only in regions not covered by YOLO-guided masks.

Result: On COCO val2017, Tiny-YOLOSAM improves class-agnostic AR from 16.4% to 77.1% and mIoU from 19.2% to 67.8%, while reducing runtime from 49.20s/image to 10.39s/image (4.7× faster) on an Apple M1 Pro CPU.

Conclusion: Detector-guided prompting combined with targeted sparse sampling is an effective and efficient alternative to dense "segment-everything" prompting for full-scene segmentation in latency-sensitive scenarios.

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

Abstract (中文翻译): Segment Anything Model（SAM）支持可提示的高质量分割，但在对延迟敏感的场景中计算开销过大。TinySAM 是一种轻量级蒸馏版 SAM，在保持强大零样本掩码质量的同时，其“全图分割”模式仍需数百个提示，实际运行速度较慢。作者首先使用官方检查点在 COCO val2017 上复现了 TinySAM，其 AP 与原文报告结果相差不超过 0.03%，建立了可靠的实验基线。在此基础上，提出了一种快速混合流水线 Tiny-YOLOSAM：利用最新的 YOLO 检测器（YOLOv12）为显著前景物体生成框提示，并仅在 YOLO 引导的掩码未覆盖区域采样稀疏点提示以补充缺失部分。在 COCO val2017 上，该混合系统显著提升了类别无关的覆盖率（AR 从 16.4% 提升至 77.1%，mIoU 从 19.2% 提升至 67.8%），同时在 Apple M1 Pro CPU 上将端到端运行时间从每张图像 49.20 秒降至 10.39 秒（提速 4.7 倍）。结果表明，结合检测器引导提示与目标导向的稀疏采样，是替代密集“全图分割”提示策略的一种高效实用方案。

</details>


### [8] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: 本文提出一种新型多模态可解释性模型，利用视觉语言模型（VLM）结合少样本学习，通过分析眼底图像中视网膜象限的病灶分布来模拟眼科医生的推理过程，并生成配对的Grad-CAM热力图以可视化OCT和眼底图像中的关键区域，从而提升糖尿病视网膜病变（DR）诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前糖尿病视网膜病变（DR）诊断面临两大挑战：一是临床医生难以手动标注病灶用于AI模型训练；二是现有模型多依赖单一成像模态，仅能高亮病灶位置而缺乏对分类决策的合理解释。医生更需要能以自然语言解释个体病灶的定量检测系统，以支持筛查、治疗和研究。

Method: 本研究提出一种基于视觉语言模型（VLM）与少样本学习的多模态可解释性方法。该模型通过分析眼底图像中四个视网膜象限的病灶分布，模拟眼科医生的诊断逻辑，并结合OCT与眼底图像生成配对的Grad-CAM热力图，以展示影响DR严重程度分类的关键区域。

Result: 在包含3,000张眼底图像和1,000张OCT图像的数据集上验证，该方法有效克服了现有DR诊断模型在可解释性和多模态融合方面的局限性，提供了一种实用且全面的辅助诊断工具。

Conclusion: 所提出的多模态可解释AI模型不仅提升了DR诊断的准确性，还通过自然语言描述和可视化热力图增强了模型的临床可信度与实用性，有望改善患者预后并推动DR相关研究与应用。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

Abstract (中文翻译): 糖尿病视网膜病变（DR）是全球致盲的主要原因之一，需早期检测以保护视力。然而，由于医生资源有限，DR常被漏诊。为解决此问题，现有AI模型常采用病灶分割以提高可解释性，但人工标注病灶对临床医生而言不切实际。医生更需要的是能够解释分类依据的模型，而非仅仅标出病灶位置。此外，当前模型多为单模态，在可解释性方面效果有限。相比之下，一个能以自然语言识别并描述个体DR病灶的定量检测系统将突破上述限制，广泛应用于筛查、治疗及研究场景。为此，本文提出一种新颖的多模态可解释性模型，利用视觉语言模型（VLM）结合少样本学习，通过分析眼底图像中视网膜各象限的病灶分布来模拟眼科医生的推理过程。该模型生成配对的Grad-CAM热力图，同时展示OCT和眼底图像中各神经元的权重，直观突出对DR严重程度分类起关键作用的区域。基于3,000张眼底图像和1,000张OCT图像的数据集，该创新方法有效解决了当前DR诊断中的关键局限，为改善患者预后提供了实用且全面的工具。

</details>


### [9] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: 本文提出TCFormer，一个仅含500万参数的超轻量级弱监督Transformer模型，通过可学习密度加权平均模块和密度等级分类损失，在仅使用图像级计数标签的情况下实现高效准确的人群计数。


<details>
  <summary>Details</summary>
Motivation: 人群计数通常依赖耗时费力的点级标注和计算开销大的骨干网络，限制了其在资源受限环境中的可扩展性和部署能力。

Method: 采用高效视觉Transformer作为特征提取器；设计可学习密度加权平均（Learnable Density-Weighted Averaging）模块，根据预测密度动态重加权局部token；引入密度等级分类损失，将人群密度离散为多个等级以正则化训练过程。

Result: 在ShanghaiTech A/B、UCF-QNRF和NWPU四个基准数据集上的实验表明，TCFormer在参数效率与计数精度之间取得了优越的平衡。

Conclusion: 尽管仅使用图像级全局计数进行弱监督训练，TCFormer通过联合优化计数损失与密度等级损失，实现了高精度的人群估计，是边缘设备上人群计数任务的良好解决方案。

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

Abstract (中文翻译): 人群计数通常依赖于劳动密集型的点级标注和计算密集型的骨干网络，这限制了其在资源受限环境中的可扩展性和部署能力。为应对这些挑战，本文提出了TCFormer——一种微型、超轻量级、弱监督的基于Transformer的人群计数框架，仅包含500万参数却能实现具有竞争力的性能。首先，采用一个强大而高效的视觉Transformer作为特征提取器，其全局上下文感知能力能够以极小的内存占用提供语义丰富的人群特征。其次，为弥补空间监督信息的缺失，设计了一种称为“可学习密度加权平均”（Learnable Density-Weighted Averaging）的特征聚合机制，该机制根据预测的密度分数动态地对局部token进行重加权，使网络能够根据各区域特定的密度特征自适应地调节区域特征，而无需额外标注。此外，本文还引入了一种密度等级分类损失，将人群密度离散为若干等级，从而正则化训练过程并增强模型在不同密度级别下的分类能力。因此，尽管TCFormer仅利用图像级全局计数在弱监督范式下进行训练，但通过对计数损失和密度等级损失的联合优化，该框架仍能实现高精度的估计。在包括ShanghaiTech A/B、UCF-QNRF和NWPU在内的四个基准数据集上的大量实验表明，该方法在参数效率与计数精度之间取得了更优的平衡，可成为边缘设备上人群计数任务的良好解决方案。

</details>


### [10] [A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability](https://arxiv.org/abs/2512.22205)
*Md. Ismiel Hossen Abir,Awolad Hossain*

Main category: cs.CV

TL;DR: 本文提出一种基于自定义CNN的深度学习方法，用于自动检测疟疾感染的血细胞图像，在准确率和可解释性方面均表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统疟疾诊断方法（如显微镜血涂片分析）敏感性低、依赖专家判断，且在资源匮乏地区难以实施，亟需更高效、自动化的诊断手段。

Method: 采用自定义卷积神经网络（CNN）对血细胞图像进行寄生/未感染分类，并与ResNet50、VGG16、MobileNetV2和DenseNet121等主流模型对比；同时应用SHAP、LIME和显著图等可解释AI技术提升模型透明度。

Result: 所提模型准确率达96%，两类别的精确率和召回率均超过0.95，优于或媲美主流深度学习架构。

Conclusion: 该深度学习系统能为资源有限地区提供快速、准确且可解释的疟疾诊断方案，具有实际应用潜力。

Abstract: Malaria remains a prevalent health concern in regions with tropical and subtropical climates. The cause of malaria is the Plasmodium parasite, which is transmitted through the bites of infected female Anopheles mosquitoes. Traditional diagnostic methods, such as microscopic blood smear analysis, are low in sensitivity, depend on expert judgment, and require resources that may not be available in remote settings. To overcome these limitations, this study proposes a deep learning-based approach utilizing a custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. This study also compares the custom CNN with established deep learning architectures, including ResNet50, VGG16, MobileNetV2, and DenseNet121. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied. The proposed system shows how deep learning can provide quick, accurate and understandable malaria diagnosis, especially in areas with limited resources.

Abstract (中文翻译): 疟疾在热带和亚热带地区仍是一个普遍的健康问题，由通过受感染雌性按蚊叮咬传播的疟原虫引起。传统的诊断方法（如显微镜下血涂片分析）敏感性较低，依赖专家判断，且在偏远地区可能缺乏必要资源。为克服这些局限，本研究提出一种基于深度学习的方法，利用自定义卷积神经网络（CNN）自动将血细胞图像分类为寄生感染或未感染。该模型准确率达到96%，两类别的精确率和召回率均超过0.95。研究还将自定义CNN与ResNet50、VGG16、MobileNetV2和DenseNet121等成熟深度学习架构进行了比较。为增强模型可解释性，采用了SHAP、LIME和显著图等可解释人工智能技术。所提出的系统展示了深度学习如何在资源有限的地区实现快速、准确且可理解的疟疾诊断。

</details>
