<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading](https://arxiv.org/abs/2601.20881)
*Matteo Rossi*

Main category: cs.CV

TL;DR: 本文提出了一种名为MA-LipNet的多注意力唇读网络，通过通道、空间和时间三个维度的注意力机制净化视觉特征，显著提升了唇读性能。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法因发音动作细微，常面临特征判别力不足和泛化能力差的问题，亟需提升视觉特征的纯净度与表达能力。

Method: 提出MA-LipNet，依次应用三个注意力模块：通道注意力（CA）用于重校准通道特征；联合时空注意力（JSTA）进行粗粒度时空滤波；分离时空注意力（SSTA）进行细粒度时空建模。

Result: 在CMLR和GRID数据集上，MA-LipNet显著降低了字符错误率（CER）和词错误率（WER），优于多种当前先进方法。

Conclusion: 多维度特征精炼对提升视觉语音识别的鲁棒性至关重要，所提方法有效增强了唇读系统的性能。

Abstract: Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\textit{Joint Spatial-Temporal Attention (JSTA)} and \textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.

Abstract (中文翻译): 唇读技术通过无声视频中的嘴唇运动解码说话内容，在公共安全等领域具有重要应用价值。然而，由于发音动作细微，现有唇读方法常受限于特征判别能力不足和泛化性能较差。为应对这些挑战，本文深入研究了从时间、空间和通道三个维度对视觉特征进行净化，并提出了一种新颖的多注意力唇读网络（MA-LipNet）。MA-LipNet的核心在于依次应用三个专用注意力模块：首先，采用通道注意力（CA）模块自适应地重新校准通道级特征，以减轻信息量较少通道的干扰；随后，利用两种不同粒度的时空注意力模块——联合时空注意力（JSTA）和分离时空注意力（SSTA）——来抑制无关像素和视频帧的影响。其中，JSTA模块通过在时空维度上计算统一的权重图实现粗粒度过滤，而SSTA模块则通过分别建模时间和空间注意力进行更精细的优化。在CMLR和GRID数据集上的大量实验表明，MA-LipNet显著降低了字符错误率（CER）和词错误率（WER），验证了其相较于多种前沿方法的有效性和优越性。本研究突显了多维特征精炼对实现鲁棒视觉语音识别的重要性。

</details>


### [2] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

TL;DR: 本文提出了一种针对非马尔可夫式多轮对话图像生成的训练与推理框架，通过引入回滚编辑、基于名称的个性化绑定、历史条件缓存等策略，显著提升了模型在多轮交互中对长期历史的依赖性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多轮对话图像生成基准和训练方法本质上是马尔可夫式的，即仅依赖最近一轮的图像，忽略了更早的对话历史。然而真实场景中用户可能引用早期状态、撤销修改或提及多轮前引入的实体，因此需要建模非马尔可夫式的长期依赖。

Method: 作者提出了三项关键技术：(i) 构建非马尔可夫多轮数据，包括强制回溯早期视觉状态的“回滚式编辑”和跨轮次将名称与外观绑定的“基于名称的个性化”；(ii) 历史条件化的训练与推理框架，采用token级缓存防止身份漂移；(iii) 高保真图像重建与可编辑个性化技术，如基于重建的DiT解码器和多阶段微调课程。

Result: 实验表明，显式针对非马尔可夫交互进行训练，显著提升了多轮一致性与指令遵循能力，同时保持了强大的单轮编辑与个性化性能。

Conclusion: 建模非马尔可夫式的长期上下文对实现真正连贯、可控的多轮对话图像生成至关重要，所提方法为该方向提供了有效解决方案。

Abstract: Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

Abstract (中文翻译): 对话式图像生成要求模型在多轮交互中遵循用户指令，并以交错的文本和图像作为不断累积的聊天历史为基础。尽管近期的多模态大语言模型（MLLMs）已能生成和编辑图像，但大多数现有的多轮基准和训练方法本质上是马尔可夫式的：下一输出主要依赖于最近的图像，从而允许模型通过忽略长期历史的捷径策略完成任务。本文正式定义并聚焦于更具挑战性的非马尔可夫设定，即用户可能回溯早期状态、撤销更改，或引用数轮前引入的实体。我们提出了：(i) 非马尔可夫多轮数据构建策略，包括强制检索早期视觉状态的回滚式编辑，以及跨轮次将名称与外观绑定的基于名称的多轮个性化；(ii) 一种历史条件化的训练与推理框架，通过token级缓存防止多轮身份漂移；(iii) 支持高保真图像重建与可编辑个性化的改进技术，包括基于重建的DiT解码器和多阶段微调课程。我们证明，显式地针对非马尔可夫交互进行训练，能在保持强大单轮编辑与个性化能力的同时，显著提升多轮一致性和指令遵循能力。

</details>


### [3] [Text controllable PET denoising](https://arxiv.org/abs/2601.20990)
*Xuehua Ye,Hongxu Yang,Adam J. Schwarz*

Main category: cs.CV

TL;DR: 本文提出一种基于文本引导的PET图像去噪方法，利用预训练CLIP模型与U-Net结构，在单一模型中实现对不同计数水平PET图像的有效增强。


<details>
  <summary>Details</summary>
Motivation: PET图像常受复杂噪声影响，降低诊断质量；现有方法难以在不同成像条件（如计数水平）下通用，因此需要一种更灵活、高效的去噪方法。

Method: 结合预训练CLIP模型提取的文本/语义特征与U-Net架构，构建一个文本引导的去噪模型，用于处理多种计数水平下的PET图像。

Result: 实验表明，该方法在定性和定量评估上均显著优于现有方法，并具备适应复杂去噪需求或缩短采集时间的潜力。

Conclusion: 所提出的文本引导去噪框架具有良好的泛化能力和临床应用前景，为提升PET图像质量提供了新思路。

Abstract: Positron Emission Tomography (PET) imaging is a vital tool in medical diagnostics, offering detailed insights into molecular processes within the human body. However, PET images often suffer from complicated noise, which can obscure critical diagnostic information. The quality of the PET image is impacted by various factors including scanner hardware, image reconstruction, tracer properties, dose/count level, and acquisition time. In this study, we propose a novel text-guided denoising method capable of enhancing PET images across a wide range of count levels within a single model. The model utilized the features from a pretrained CLIP model with a U-Net based denoising model. Experimental results demonstrate that the proposed model leads significant improvements in both qualitative and quantitative assessments. The flexibility of the model shows the potential for helping more complicated denoising demands or reducing the acquisition time.

Abstract (中文翻译): 正电子发射断层扫描（PET）成像是医学诊断中的重要工具，能够提供人体内分子过程的详细信息。然而，PET图像常常受到复杂噪声的干扰，从而掩盖关键的诊断信息。图像质量受多种因素影响，包括扫描仪硬件、图像重建方法、示踪剂特性、剂量/计数水平以及采集时间。本研究提出了一种新颖的文本引导去噪方法，能够在单一模型中有效提升各种计数水平下的PET图像质量。该模型结合了预训练CLIP模型提取的特征与基于U-Net的去噪网络。实验结果表明，所提模型在定性和定量评估方面均取得了显著改进。该模型的灵活性显示出其在应对更复杂的去噪需求或缩短图像采集时间方面的潜力。

</details>


### [4] [Low performing pixel correction in computed tomography with unrolled network and synthetic data training](https://arxiv.org/abs/2601.20995)
*Hongxu Yang,Levente Lippenszky,Edina Timko,Lehel Ferenczi,Gopal Avinash*

Main category: cs.CV

TL;DR: 本文提出了一种基于合成数据的双域展开方法，用于校正CT探测器中低性能像素（LPP）引起的伪影，无需真实临床数据训练，且在模拟实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LPP伪影校正方法依赖昂贵的真实临床数据集进行监督训练，且仅在图像域或正弦图域单独处理，忽略了CT几何前向操作带来的双域内在关联。

Method: 提出一种基于合成数据的双域展开方法，利用自然图像生成的合成数据建模正弦图域与图像域之间LPP的内在关联，从而在无需真实数据的情况下训练模型进行伪影校正。

Result: 在模拟1-2%探测器缺陷靠近等中心点的实验中，所提方法大幅优于当前最先进的方法。

Conclusion: 该方法无需收集真实临床数据即可有效校正LPP伪影，且适用于不同扫描仪设置，具有良好的软件部署潜力。

Abstract: Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.

Abstract (中文翻译): 计算机断层扫描（CT）探测器中的低性能像素（LPP）会导致重建图像中出现环状和条纹伪影，使其无法用于临床诊断。近年来，已有若干方法被提出用于校正LPP伪影，包括在图像域或正弦图域中采用监督式深度学习方法。然而，这些方法需要专门的数据集进行训练，而此类数据的获取成本高昂。此外，现有方法仅关注图像空间或正弦图空间中的单一校正路径，忽略了CT几何前向操作所引入的双域内在关联。本文提出了一种基于合成数据的双域展开方法来校正LPP伪影。具体而言，通过从自然图像生成的合成数据，利用正弦图域与图像域之间LPP的内在关联，使训练模型无需任何真实临床数据即可完成伪影校正。在模拟等中心附近1–2%探测器缺陷的实验中，所提方法显著优于当前最先进的方法。结果表明，该方案无需为模型训练收集数据，即可有效校正LPP伪影，并可适配不同扫描仪设置，适用于基于软件的应用场景。

</details>


### [5] [AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples](https://arxiv.org/abs/2601.21022)
*Andrea Camilloni,Chiara Micoli,Nita Mulliqi,Erik Everett Palm,Thorgerdur Palsdottir,Kelvin Szolnoky,Xiaoyi Ji,Sol Erika Boman,Andrea Discacciati,Henrik Grönberg,Lars Egevad,Tobias Nordström,Kimmo Kartasalo,Martin Eklund*

Main category: cs.CV

TL;DR: 本文开发了一种基于AI的模型，利用诊断性前列腺活检切片预测根治性前列腺切除术后生化复发（BCR）风险，在多个外部队列中验证了其泛化能力，并表明结合临床变量可提升预后分层效果。


<details>
  <summary>Details</summary>
Motivation: 当前用于预测前列腺癌根治术后生化复发（BCR）的预后工具不够精确，亟需更可靠的方法来识别高风险患者以改善临床决策。

Method: 基于STHLM3队列（n=676）的诊断性前列腺活检切片，采用基础模型（foundation models）和基于注意力机制的多实例学习（attention-based multiple instance learning）训练AI模型；在LEOPARD、CHIMERA和TCGA-PRAD三个外部根治术队列中评估泛化性能，并结合临床变量进行多模态分析。

Result: 该图像模型在三个外部队列中分别达到了0.64、0.70和0.70的5年时间依赖AUC；整合临床变量后显著提升了风险分层能力，且优于指南推荐的CAPRA-S评分。

Conclusion: 基于活检训练的病理AI模型可在不同标本类型间泛化，辅助术前与术后决策，但其相比更简单预测模型的增量价值仍需在后续研究中审慎评估。

Abstract: Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.

Abstract (中文翻译): 根治性前列腺切除术（RP）后的生化复发（BCR）是侵袭性前列腺癌不良结局的替代标志物，然而目前的预后工具仍不够精确。我们基于STHLM3队列（n = 676）的诊断性前列腺活检切片，利用基础模型和基于注意力机制的多实例学习方法训练了一种人工智能（AI）模型，用于预测患者个体化的BCR风险。模型的泛化能力在三个外部RP队列中进行了评估：LEOPARD（n = 508）、CHIMERA（n = 95）和TCGA-PRAD（n = 379）。该基于图像的方法在上述队列中分别实现了0.64、0.70和0.70的5年时间依赖AUC。将临床变量整合进模型后提供了互补的预后价值，并实现了具有统计学意义的风险分层。与指南推荐的CAPRA-S评分相比，AI模型在术后预后判断方面带来了增量改进。这些结果表明，基于活检训练的组织病理AI模型能够在不同标本类型之间泛化，支持术前和术后临床决策，但AI多模态方法相较于更简单预测模型的附加价值仍需在未来研究中加以严格审视。

</details>


### [6] [BadDet+: Robust Backdoor Attacks for Object Detection](https://arxiv.org/abs/2601.21066)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: 本文提出BadDet+，一种基于惩罚的后门攻击框架，统一了区域误分类攻击（RMA）和目标消失攻击（ODA），通过log-barrier惩罚机制实现位置与尺度不变性，并在真实场景中展现出更强的物理鲁棒性和优越的合成到物理迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对目标检测的后门攻击方法存在严重缺陷，包括依赖不切实际的假设以及缺乏物理世界验证，导致对目标检测系统的真实威胁尚不明确。

Method: 提出BadDet+框架，利用log-barrier惩罚项抑制触发样本的真实类别预测，从而在触发特定特征子空间内可靠地诱导攻击，同时保持干净样本上的正常性能。

Result: 在真实世界基准上，BadDet+相比现有RMA和ODA方法实现了更优的合成到物理迁移效果，同时具备位置与尺度不变性及更强的物理鲁棒性，且不影响干净样本的检测性能。

Conclusion: 目标检测模型存在显著的后门漏洞，需开发专门的防御机制；BadDet+揭示了当前攻击方法的不足，并为未来研究提供了更贴近现实的评估基准。

Abstract: Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.

Abstract (中文翻译): 后门攻击对深度学习构成严重威胁，但相较于图像分类，其在目标检测中的影响仍缺乏深入理解。尽管已有相关攻击方法被提出，但我们发现现有基于检测的攻击方法存在关键缺陷，特别是依赖不切实际的假设且缺乏物理验证。为弥补这一差距，我们提出了BadDet+——一种基于惩罚的统一框架，整合了区域误分类攻击（RMA）和目标消失攻击（ODA）。其核心机制采用对数障碍惩罚（log-barrier penalty）来抑制触发输入的真实类别预测，从而实现（i）位置与尺度不变性，以及（ii）增强的物理鲁棒性。在真实世界基准测试中，BadDet+在保持干净样本性能的同时，在合成到物理的迁移能力上优于现有的RMA和ODA基线方法。理论分析表明，所提出的惩罚项作用于触发器特定的特征子空间，能够在不损害标准推理性能的前提下可靠地诱发攻击。这些结果凸显了目标检测系统中存在的重大安全漏洞，并强调了开发专用防御机制的必要性。

</details>


### [7] [Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization](https://arxiv.org/abs/2601.21078)
*Jiaqi Li,Guangming Wang,Shuntian Zheng,Minzhe Ni,Xiaoman Lu,Guanghui Ye,Yu Guan*

Main category: cs.CV

TL;DR: 本文提出ActionVLM，通过动态重加权和残差聚合策略，在时序动作定位任务中缓解视觉-语言模型中的模态偏置问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（VLM）的时序动作定位方法过度依赖语言先验，忽视了视觉信息的主导作用，导致明显的模态偏置，影响定位性能。

Method: 提出ActionVLM框架，包含：(i) 去偏重加权模块，动态评估语言相对于纯视觉预测的增量收益并调整其权重；(ii) 残差聚合策略，将语言作为对视觉主信号的补充性修正而非主导信号。

Result: 在THUMOS14数据集上，该方法比当前最优方法提升最高达3.2% mAP。

Conclusion: 通过保留视觉为主导信号并自适应地利用语言信息，ActionVLM有效缓解了模态偏置，提升了时序动作定位的准确性和鲁棒性。

Abstract: Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.

Abstract (中文翻译): 时序动作定位（TAL）需要在未剪辑视频中识别动作的边界和类别。尽管视觉-语言模型（VLM）能够提供丰富的语义信息以补充视觉证据，但现有方法往往过度强调语言先验，牺牲了视觉性能，从而导致显著的模态偏置。我们提出了ActionVLM，一种视觉-语言融合框架，系统性地缓解TAL中的模态偏置问题。我们的核心思想是在保持视觉为主导信号的同时，仅在有益时自适应地利用语言信息。为此，我们引入了：(i) 一个去偏重加权模块，用于估计语言优势——即语言相对于纯视觉预测所带来的增量收益，并据此动态调整语言模态的权重；(ii) 一种残差聚合策略，将语言视为互补性的精调手段，而非主要驱动因素。这种组合有效缓解了模态偏置，降低了语言先验带来的过度自信，并增强了时序推理能力。在THUMOS14上的实验表明，我们的模型相较于当前最先进的方法，mAP最高提升了3.2%。

</details>


### [8] [Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought](https://arxiv.org/abs/2601.21081)
*Yu Huo,Siyu Zhang,Kun Zeng,Haoyue Liu,Owen Lee,Junlin Chen,Yuquan Lu,Yifu Guo,Yaodong Liang,Xiaoying Tang*

Main category: cs.CV

TL;DR: 本文提出Shape-of-Thought（SoT）框架，通过在推理时无需外部引擎的连贯2D投影实现渐进式形状组装，提升多模态生成模型在结构组合性（如数量、属性绑定和部件关系）方面的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像的多模态生成模型虽具有高视觉保真度，但在面对组合结构性约束（如生成数量、属性绑定和部件层级关系）时表现脆弱，亟需一种能显式建模结构逻辑的生成方法。

Method: 提出SoT框架：训练一个统一的多模态自回归模型，交替生成文本计划与渲染的中间状态，以捕捉形状组装逻辑；同时构建SoT-26K数据集（基于CAD部件层次的组装轨迹）和T2S-CompBench评测基准。

Result: 在SoT-26K上微调后，组件数量准确率达88.4%，结构拓扑准确率达84.8%，比纯文本基线高出约20%。

Conclusion: SoT为透明、过程监督的组合式生成建立了新范式，显著提升了生成图像在结构层面的准确性与可控性。

Abstract: Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.

Abstract (中文翻译): 用于文本到图像生成的多模态模型已实现较强的视觉保真度，但在组合结构性约束下仍显脆弱，尤其是在生成数量、属性绑定和部件级关系方面。为应对这些挑战，我们提出了“思维形状”（Shape-of-Thought, SoT）——一种视觉思维链（CoT）框架，该框架能在推理时无需外部引擎的情况下，通过连贯的二维投影实现渐进式形状组装。SoT训练了一个统一的多模态自回归模型，以交替生成文本计划和渲染的中间状态，从而帮助模型捕捉形状组装逻辑，而无需生成显式的几何表示。为支持该范式，我们构建了SoT-26K数据集——一个从基于部件的CAD层次结构中提取的大规模接地组装轨迹数据集，以及T2S-CompBench评测基准，用于评估结构完整性和轨迹忠实度。在SoT-26K上进行微调后，模型在组件数量任务上达到88.4%的准确率，在结构拓扑任务上达到84.8%，比纯文本基线高出约20%。SoT为透明、过程监督的组合式生成建立了一种新范式。代码已公开于 https://anonymous.4open.science/r/16FE/，SoT-26K数据集将在论文被接收后发布。

</details>


### [9] [An AI Framework for Microanastomosis Motion Assessment](https://arxiv.org/abs/2601.21120)
*Yan Meng,Eduardo J. Torres-Rodríguez,Marcelle Altshuler,Nishanth Gowda,Arhum Naeem,Recai Yilmaz,Omar Arnaout,Daniel A. Donoho*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化框架，用于客观评估显微吻合术中的器械操作技能，整合了YOLO检测、DeepSORT跟踪、器械尖端定位和专家标注的分类模块，在实验中实现了97%的检测精度和96%的mAP。


<details>
  <summary>Details</summary>
Motivation: 传统显微外科技术评估依赖专家主观判断，存在评分者间差异大、标准不统一、易受认知偏差影响及耗时等问题，亟需一种客观、可靠且可扩展的自动化评估系统。

Method: 该框架包含四个核心模块：基于YOLO的器械检测模块、基于DeepSORT的器械跟踪模块、基于形状描述符的器械尖端定位模块，以及基于专家标注数据训练的监督分类模块，用于评估器械操作熟练度。

Result: 实验结果表明，该框架在器械检测上达到97%的精确率，mAP50-95为96%，验证了其有效性。

Conclusion: 所提出的AI框架能够有效、客观地自动评估显微吻合术中的器械操作技能，具备良好的应用前景。

Abstract: Proficiency in microanastomosis is a fundamental competency across multiple microsurgical disciplines. These procedures demand exceptional precision and refined technical skills, making effective, standardized assessment methods essential. Traditionally, the evaluation of microsurgical techniques has relied heavily on the subjective judgment of expert raters. They are inherently constrained by limitations such as inter-rater variability, lack of standardized evaluation criteria, susceptibility to cognitive bias, and the time-intensive nature of manual review. These shortcomings underscore the urgent need for an objective, reliable, and automated system capable of assessing microsurgical performance with consistency and scalability. To bridge this gap, we propose a novel AI framework for the automated assessment of microanastomosis instrument handling skills. The system integrates four core components: (1) an instrument detection module based on the You Only Look Once (YOLO) architecture; (2) an instrument tracking module developed from Deep Simple Online and Realtime Tracking (DeepSORT); (3) an instrument tip localization module employing shape descriptors; and (4) a supervised classification module trained on expert-labeled data to evaluate instrument handling proficiency. Experimental results demonstrate the effectiveness of the framework, achieving an instrument detection precision of 97%, with a mean Average Precision (mAP) of 96%, measured by Intersection over Union (IoU) thresholds ranging from 50% to 95% (mAP50-95).

Abstract (中文翻译): 显微吻合术的熟练掌握是多个显微外科领域的基本能力。此类手术要求极高的精准度和精细的技术技能，因此有效的标准化评估方法至关重要。传统上，显微外科技术的评估严重依赖专家评分者的主观判断，而这种方法本质上受限于评分者间差异、缺乏统一的评估标准、易受认知偏差影响以及人工评审耗时等缺陷。这些不足凸显了对一种客观、可靠且可自动化的系统之迫切需求，以实现一致性和可扩展性的显微外科表现评估。为填补这一空白，我们提出了一种新颖的AI框架，用于自动评估显微吻合术中的器械操作技能。该系统整合了四个核心组件：（1）基于“You Only Look Once”（YOLO）架构的器械检测模块；（2）基于Deep Simple Online and Realtime Tracking（DeepSORT）开发的器械跟踪模块；（3）采用形状描述符的器械尖端定位模块；以及（4）基于专家标注数据训练的监督分类模块，用于评估器械操作熟练程度。实验结果证明了该框架的有效性，在IoU阈值从50%到95%范围内测得的平均精度（mAP50-95）为96%，器械检测精确率达到97%。

</details>


### [10] [Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2601.21159)
*Jianzheng Wang,Huan Ni*

Main category: cs.CV

TL;DR: 提出了一种无需训练的开放词汇语义分割框架SDCI，通过双分支协同推理、交叉图扩散优化和超像素结构融合，在高分辨率遥感影像上显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的开放词汇语义分割方法在处理高分辨率遥感影像时，难以同时满足几何定位和语义预测的高要求，主要受限于单向特征注入和浅层后处理策略。

Method: 提出SDCI框架，包含三个核心模块：1）交叉模型注意力融合（CAF）模块，在特征编码阶段实现双模型自注意力图互注；2）双向交叉图扩散优化（BCDR）模块，通过迭代随机游走提升双分支分割得分可靠性；3）基于凸优化的超像素协同预测（CSCP）机制，融合低层超像素结构以精细化边界。

Result: 在多个遥感语义分割基准数据集上，SDCI优于现有方法；消融实验证明超像素结构在深度学习框架中仍具有效性。

Conclusion: 所提SDCI框架有效解决了高分辨率遥感影像开放词汇语义分割中的几何与语义挑战，验证了传统超像素方法与现代深度学习结合的潜力。

Abstract: High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.

Abstract (中文翻译): 高分辨率遥感影像具有地物密集分布和边界复杂的特点，这对几何定位和语义预测提出了更高要求。现有的无需训练的开放词汇语义分割（OVSS）方法通常采用“单向注入”和“浅层后处理”策略融合CLIP与视觉基础模型（VFMs），难以满足上述需求。为解决此问题，我们提出一种面向无需训练OVSS的空间正则化感知双分支协同推理框架（SDCI）。首先，在特征编码阶段，SDCI引入交叉模型注意力融合（CAF）模块，通过将各自模型的自注意力图相互注入，引导双分支协同推理。其次，我们提出双向交叉图扩散优化（BCDR）模块，利用迭代式随机游走扩散增强双分支分割得分的可靠性。最后，我们融合低层超像素结构，并设计了一种基于凸优化的超像素协同预测（CSCP）机制，进一步优化地物边界。在多个遥感语义分割基准上的实验表明，本方法性能优于现有方法。此外，消融研究进一步证实，在深度学习框架中，利用超像素结构的传统遥感影像面向对象分析方法依然有效。

</details>
