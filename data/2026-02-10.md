<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 本文提出一种基于分层贝叶斯模型的新方法，用于量化鞋印“偶然特征”（如磨损、划痕）的稀有性，从而提升法庭科学中鞋印证据的评估准确性。


<details>
  <summary>Details</summary>
Motivation: 在犯罪现场提取的鞋印虽可识别鞋款型号，但由于同款鞋大量生产，仅靠型号匹配不足以确认唯一性。因此需分析鞋底使用后产生的独特“偶然特征”，并量化其稀有程度，以准确评估证据强度。

Method: 开发了一种分层贝叶斯模型：1）采用潜在高斯模型框架，结合嵌套拉普拉斯积分近似（INLA），实现对大规模标注鞋印数据的高效推断；2）引入空间变化系数，建模鞋底花纹与偶然特征位置之间的关系。

Result: 该方法在保留测试集上表现优于现有方法，提高了鞋印分析的准确性和可靠性。

Conclusion: 所提出的模型通过更有效地建模偶然特征的空间分布及其与鞋底花纹的关系，为法庭科学提供了更可靠、可扩展的鞋印证据评估工具。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

Abstract (中文翻译): 从犯罪现场提取的鞋印证据在法庭调查中起着关键作用。通过检查鞋印，调查人员可以确定嫌疑人所穿鞋履的细节。然而，仅证明嫌疑人的鞋子与犯罪现场鞋印的品牌和型号一致可能并不充分，因为通常会有成千上万双相同尺码、品牌和型号的鞋子被制造出来，其中任何一双都可能是该鞋印的来源。因此，调查人员常用的一种方法是检查鞋印是否存在“偶然特征”（accidentals），即购买后因穿着而在鞋底积累的割痕、刮痕等特征。尽管某些类型的鞋子上会出现常见的偶然特征模式，但另一些则高度独特，有可能将嫌疑人的鞋子与其他所有同款鞋区分开来。因此，量化某种偶然特征模式的稀有性对于准确衡量法庭证据的强度至关重要。在本研究中，我们通过构建一个分层贝叶斯模型来解决这一任务。相较于现有方法，我们的改进主要体现在两个方面：首先，我们将方法构建为潜在高斯模型，从而借助嵌套拉普拉斯积分近似（INLA）高效地扩展到大规模标注鞋印数据集的推断；其次，我们引入空间变化系数，以建模鞋底花纹与偶然特征位置之间的关系。我们在保留测试数据上的实验表明，该方法性能更优，提升了法庭鞋印分析的准确性与可靠性。

</details>


### [2] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出一种基于归因的人类先验对齐方法，通过在训练中约束模型的决策依据区域，使其与人类标注的证据区域一致，从而提升模型准确性和决策合理性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习仅提供类别标签，导致模型依赖捷径相关性而非真正证据进行预测；尽管人类先验可约束此类行为，但模型学到的表示常与人类感知不一致，难以有效对齐。

Method: 将人类先验编码为输入中期望模型依赖的区域（如边界框），利用高保真的子集选择归因方法在训练中揭示模型的决策依据，并在归因区域偏离先验区域时施加惩罚，引导模型将注意力集中在预期区域。

Result: 在图像分类和基于MLLM的GUI智能体点击决策任务上验证了该方法的有效性，在常规分类和自回归生成设置下，人类先验对齐均提升了任务准确率和决策合理性。

Conclusion: 通过引入归因约束，使模型决策依据与人类先验对齐，不仅提高了性能，也增强了模型的可解释性与合理性。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

Abstract (中文翻译): 可靠的模型不仅应做出正确预测，还应以可接受的证据来解释其决策。然而，传统的监督学习通常仅提供类别级别的标签，使得模型可以通过捷径相关性而非预期证据获得高准确率。人类先验有助于约束此类行为，但由于模型学习到的表示常常与人类感知存在偏差，将模型与这些先验对齐仍具挑战性。为解决这一问题，我们提出了一种基于归因的人类先验对齐方法。我们将人类先验编码为模型应依赖的输入区域（例如边界框），并利用一种高度保真的基于子集选择的归因方法，在训练过程中揭示模型的决策依据。当归因区域显著偏离先验区域时，我们对模型依赖非先验证据的行为施加惩罚，从而鼓励其将归因转向预期区域。这通过一个由人类先验诱导的归因约束训练目标实现。我们在图像分类任务和基于多模态大语言模型（MLLM）的GUI智能体点击决策任务上验证了该方法。在常规分类和自回归生成设置下，人类先验对齐始终提升了任务准确率，同时增强了模型决策的合理性。

</details>


### [3] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: 本文提出了MAU-Set数据集和MAU-GPT模型，用于多类型工业异常理解，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工业产品图像分析方法受限于数据集覆盖不足和模型在多样复杂异常模式下的泛化能力差。

Method: 构建了涵盖多工业领域的MAU-Set数据集及配套评估协议，并提出MAU-GPT多模态大模型，采用新型AMoE-LoRA机制统一异常感知与通用专家适配。

Result: 实验表明，MAU-GPT在所有领域均一致优于先前的最先进方法。

Conclusion: MAU-GPT在可扩展、自动化的工业检测中展现出强大潜力。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

Abstract (中文翻译): 随着工业制造规模的扩大，自动化细粒度产品图像分析对质量控制变得至关重要。然而，现有方法受限于数据集覆盖范围有限以及模型在多样且复杂的异常模式下泛化能力不足。为应对这些挑战，我们提出了MAU-Set——一个面向多类型工业异常理解的综合性数据集。该数据集涵盖多个工业领域，并设计了从二分类到复杂推理的层次化任务结构。同时，我们建立了严格的评估协议，以支持公平而全面的模型评估。在此基础上，我们进一步提出了MAU-GPT，一种专为工业异常理解而领域适配的多模态大模型。该模型引入了一种新颖的AMoE-LoRA机制，统一了异常感知专家与通用专家的适配过程，从而增强了对各类缺陷的理解与推理能力。大量实验表明，MAU-GPT在所有领域均持续优于以往最先进的方法，展现出在可扩展、自动化工业检测中的巨大潜力。

</details>


### [4] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: RetSAM 是一个通用的视网膜分割与量化框架，可从眼底图像中稳健地分割多种结构和病变，并生成30多个标准化生物标志物，支持大规模眼科研究和系统性疾病关联分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏公开的多标签数据集和统一的从分割到量化的流程，限制了视网膜定量表型与眼部及全身疾病之间关系的大规模分析。

Method: 提出 RetSAM 框架，在超过20万张眼底图像上训练，支持三类任务，分割五种解剖结构、四种表型模式和20多种病变类型，并将其转化为30多个标准化生物标志物；采用多阶段训练策略，结合私有和公开数据。

Result: 在17个公开数据集上实现优越的分割性能，平均DSC指标比先前最优方法高3.9个百分点，在挑战性多任务基准上最高提升15个百分点，并在不同人群、设备和临床环境中具有良好泛化能力。

Conclusion: RetSAM 将眼底图像转化为标准化、可解释的定量表型，为大规模眼科研究和转化医学提供有力工具。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

Abstract (中文翻译): 视网膜成像快速、无创且广泛可用，为眼科和全身健康评估提供了可量化的结构和血管信号。这种可及性为研究定量视网膜表型与眼部及全身疾病之间的关系创造了机会。然而，由于公开的多标签数据集有限，且缺乏统一的从分割到量化的流程，此类分析在大规模应用中仍面临困难。我们提出了 RetSAM——一个用于眼底成像的通用视网膜分割与量化框架。该框架能够稳健地进行多目标分割并提取标准化生物标志物，支持下游眼科研究和眼组学相关性分析。RetSAM 在超过20万张眼底图像上进行训练，支持三类任务，可分割五种解剖结构、四种视网膜表型模式以及20多种不同的病变类型，并将这些分割结果转化为30多个涵盖结构形态、血管几何和退行性变化的标准化生物标志物。通过结合私有和公开眼底数据的多阶段训练策略，RetSAM 在17个公开数据集上实现了卓越的分割性能，平均Dice相似系数（DSC）比以往最佳方法高出3.9个百分点，在具有挑战性的多任务基准上最高提升达15个百分点，并在不同人群、成像设备和临床环境中表现出良好的泛化能力。所生成的生物标志物可用于系统性关联分析多种主要眼科疾病，包括糖尿病视网膜病变、年龄相关性黄斑变性、青光眼和病理性近视。总体而言，RetSAM 将眼底图像转化为标准化、可解释的定量表型，推动了大规模眼科研究及其临床转化。

</details>


### [5] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: 本文提出CR-VLM，一种基于激活操控的可配置拒绝机制，使视觉语言模型能根据用户需求和上下文灵活调整拒绝行为，避免过度或不足拒绝。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）的拒绝机制过于僵化，采用“一刀切”策略，无法适应不同用户需求和上下文约束，导致拒绝不足或过度拒绝，影响模型的安全性与实用性。

Method: 提出CR-VLM方法，包含三个组件：(1) 通过教师强制机制提取可配置拒绝向量以增强拒绝信号；(2) 引入门控机制，在保留对合法请求接受能力的同时缓解过度拒绝；(3) 设计反事实视觉增强模块，使视觉表征与拒绝要求对齐。

Result: 在多个数据集和多种VLM上的实验表明，CR-VLM能实现高效、有效且鲁棒的可配置拒绝。

Conclusion: CR-VLM为视觉语言模型提供了一条可扩展的路径，以实现面向用户自适应的安全对齐。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

Abstract (中文翻译): 随着视觉语言模型（VLM）的快速发展，拒绝机制已成为确保模型行为负责任和安全的关键组成部分。然而，现有的拒绝策略大多是“一刀切”的，无法适应多样化的用户需求和上下文约束，从而导致拒绝不足或过度拒绝。在本研究中，我们首先探讨了上述挑战，并提出了**CR-VLM**（Configurable Refusal in VLMs），这是一种基于激活操控的鲁棒且高效的可配置拒绝方法。CR-VLM包含三个集成组件：(1) 通过教师强制机制提取可配置拒绝向量，以放大拒绝信号；(2) 引入门控机制，在保留对范围内查询接受能力的同时缓解过度拒绝；(3) 设计反事实视觉增强模块，使视觉表征与拒绝要求对齐。在多个数据集和多种VLM上进行的全面实验表明，CR-VLM能够实现有效、高效且鲁棒的可配置拒绝，为VLM中实现用户自适应的安全对齐提供了一条可扩展的路径。

</details>


### [6] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: 本文提出Vectra，首个用于跨境电商图像内机器翻译（IIMT）的无参考、多模态大语言模型（MLLM）驱动的视觉质量评估框架，包含评分系统、数据集和模型三部分，在人类偏好对齐和评分性能上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有IIMT研究忽视了视觉渲染质量对用户参与的关键作用；当前评估方法要么缺乏可解释性（如SSIM、FID），要么缺乏领域细粒度奖励信号（如模型即评委方法），亟需一种兼顾可解释性、无参考且适用于电商场景的评估方案。

Method: 提出Vectra框架：(1) Vectra Score：将视觉质量分解为14个可解释维度，并引入空间感知的缺陷区域比（DAR）以减少标注歧义；(2) Vectra Dataset：基于110万真实商品图构建，含2K评测基准、30K推理标注和3.5K专家偏好标签；(3) Vectra Model：一个40亿参数的MLLM，能同时输出量化评分与诊断性推理。

Result: 实验表明Vectra在与人类排序的相关性上达到SOTA水平，其模型在评分任务上优于包括GPT-5和Gemini-3在内的主流MLLM。

Conclusion: Vectra为电商IIMT提供了一种高效、可解释且无需参考图像的视觉质量评估新范式，相关数据集与模型将在论文接收后开源。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

Abstract (中文翻译): 图像内机器翻译（IIMT）赋能跨境电子商务商品列表；现有研究聚焦于机器翻译评估，而视觉渲染质量对用户参与至关重要。面对信息密集的商品图像和多模态缺陷，当前基于参考的方法（如SSIM、FID）缺乏可解释性，而“模型即评委”方法则缺乏领域内细粒度的奖励信号。为弥合这一差距，我们提出了Vectra——据我们所知，这是首个用于电商IIMT的无参考、多模态大语言模型（MLLM）驱动的视觉质量评估框架。Vectra包含三个组成部分：(1) Vectra Score，一个多维质量度量系统，将视觉质量分解为14个可解释维度，并通过具有空间感知能力的缺陷区域比（DAR）量化以减少标注歧义；(2) Vectra Dataset，通过多样性感知采样从110万张真实商品图像中构建，包含用于系统评估的2K基准集、用于指令微调的30K条推理型标注，以及用于对齐与评估的3.5K条专家偏好标签；(3) Vectra Model，一个拥有40亿参数的MLLM，可同时生成量化评分与诊断性推理。实验表明，Vectra在与人类排序的相关性方面达到当前最优水平，且我们的模型在评分性能上超越了包括GPT-5和Gemini-3在内的领先MLLM。相关数据集与模型将在论文被接收后发布。

</details>


### [7] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 本文提出一种新型混合CNN架构（结合MobileNetV3-Large和EfficientNetB0）用于孟加拉国纸币识别，在多种数据集上取得高准确率，并通过可解释AI方法增强模型透明度。


<details>
  <summary>Details</summary>
Motivation: 视障人士在识别纸币时依赖他人，易受欺诈和剥削，因此亟需准确、可靠的自动纸币识别技术作为辅助工具。

Method: 构建包含受控与真实场景的孟加拉国纸币新数据集，并融合四个额外数据集以提升多样性；提出结合MobileNetV3-Large与EfficientNetB0的混合CNN架构，后接多层感知机（MLP）分类器；使用五折交叉验证及七种评估指标，并引入LIME和SHAP等可解释AI方法。

Result: 模型在受控数据集上达到97.95%准确率，在复杂背景中达92.84%，合并所有数据集时达94.98%；并通过多种指标和可解释性分析验证了其有效性。

Conclusion: 所提出的轻量级混合CNN模型在准确性和计算效率之间取得良好平衡，适用于资源受限设备，并为视障人士提供可靠、透明的纸币识别解决方案。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

Abstract (中文翻译): 准确的货币识别对于辅助技术至关重要，尤其是对于依赖他人识别纸币的视障人士而言。这种依赖使他们面临欺诈和剥削的风险。为应对这些挑战，我们首先构建了一个新的孟加拉国纸币数据集，涵盖受控环境和真实场景，以确保更全面、多样化的表示。其次，为增强数据集的鲁棒性，我们整合了四个额外的数据集（包括公开基准数据集），以涵盖各种复杂情况并提升模型的泛化能力。为克服现有识别模型的局限性，我们提出了一种新颖的混合CNN架构，结合MobileNetV3-Large和EfficientNetB0以实现高效的特征提取，随后采用高效的多层感知机（MLP）分类器，在保持较低计算成本的同时提升性能，使系统适用于资源受限的设备。实验结果表明，所提模型在受控数据集上准确率达97.95%，在复杂背景中达92.84%，合并所有数据集时达94.98%。模型性能通过五折交叉验证及七项指标（准确率、精确率、召回率、F1分数、Cohen's Kappa、MCC和AUC）进行了全面评估。此外，还引入了LIME和SHAP等可解释AI方法，以增强模型的透明度和可解释性。

</details>


### [8] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 本文在IMC2025挑战背景下，探索使用受LeJEPA启发的高斯约束图像嵌入表示，通过三个渐进式流程验证其对无监督3D场景重建中场景发现与相机姿态估计的实用价值，实验表明该方法在视觉模糊条件下优于启发式基线。


<details>
  <summary>Details</summary>
Motivation: 无结构图像集合中的无监督3D场景重建极具挑战，尤其当图像来自多个无关场景并包含显著视觉歧义时；IMC2025挑战要求在真实世界条件下同时完成场景发现和相机姿态估计，促使作者探索更鲁棒的表示学习方法。

Method: 提出三种渐进优化的流程，最终采用受LeJEPA启发的方法，在学习到的图像嵌入上施加各向同性高斯约束，以提升聚类一致性和姿态估计鲁棒性。

Result: 在IMC2025数据集上的实验表明，相比启发式基线，高斯约束嵌入能更好地分离场景并生成更合理的相机姿态，尤其在视觉模糊场景中表现更优。

Conclusion: 理论驱动的表示约束（如高斯约束）为连接自监督学习与实际运动恢复结构（SfM）流程提供了有前景的方向。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

Abstract (中文翻译): 从无结构图像集合中进行无监督三维场景重建仍是计算机视觉中的一个基本挑战，特别是当图像来源于多个无关场景并包含显著视觉歧义时。2025年图像匹配挑战赛（IMC2025）突显了这些困难，它要求在包含离群点和混合内容的真实世界条件下同时完成场景发现和相机姿态估计。本文研究了受LeJEPA（联合嵌入预测架构）启发的高斯约束表示方法在应对这些挑战中的应用。我们提出了三种逐步优化的流程，最终形成一种受LeJEPA启发的方法，该方法对所学图像嵌入施加各向同性高斯约束。我们的工作并非提出新的理论保证，而是通过实验评估这些约束在实践中如何影响聚类一致性与姿态估计的鲁棒性。在IMC2025上的实验结果表明，相比于基于启发式的基线方法，高斯约束嵌入能够在视觉模糊的情况下更好地实现场景分离并提升姿态估计的合理性。这些发现表明，基于理论动机的表示约束为连接自监督学习原则与实用的运动恢复结构（Structure-from-Motion）流程提供了一个有前景的方向。

</details>


### [9] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出XAI-CLIP，一种结合视觉-语言模型引导的区域定位与扰动方法，用于提升医学图像分割模型的可解释性，在减少计算开销的同时生成更清晰、解剖学上更合理的显著图。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在医学图像分割中表现优异但缺乏可解释性，阻碍了其临床应用；现有XAI方法计算成本高、解释结果噪声大或解剖学无关。

Method: 利用多模态视觉-语言模型（如CLIP）进行感兴趣区域（ROI）定位，引导对医学图像分割模型的区域感知扰动，生成边界清晰的显著图。

Result: 在FLARE22和CHAOS数据集上，相比传统扰动方法，XAI-CLIP运行时间减少60%，Dice分数提升44.6%，遮挡解释的IoU提升96.7%，且显著图更干净、解剖一致性更强。

Conclusion: 将视觉-语言表示融入基于扰动的XAI框架，能显著提升医学图像分割系统的可解释性与效率，有助于实现透明且可临床部署的AI系统。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

Abstract (中文翻译): 医学图像分割是临床工作流中的关键环节，有助于实现精准诊断、治疗规划和疾病监测。然而，尽管基于Transformer的模型在性能上优于卷积架构，但其有限的可解释性仍是阻碍临床信任与部署的主要障碍。现有的可解释人工智能（XAI）技术，包括基于梯度的显著性方法和基于扰动的方法，通常计算开销大、需要大量前向传播，并且常常产生噪声较大或解剖学上无关的解释。为解决这些问题，我们提出了XAI-CLIP——一种ROI引导的扰动框架，该框架利用多模态视觉-语言模型嵌入来定位具有临床意义的解剖区域，并引导解释过程。通过将语言信息引导的区域定位与医学图像分割相结合，并施加有针对性的区域感知扰动，所提出的方法在显著降低计算开销的同时，生成更清晰、边界感知更强的显著图。在FLARE22和CHAOS数据集上的实验表明，与传统扰动方法相比，XAI-CLIP的运行时间最多减少60%，Dice分数提升44.6%，基于遮挡解释的交并比（IoU）提升96.7%。定性结果进一步证实，其归因图更加干净、解剖学一致性更高，伪影更少。这表明将多模态视觉-语言表征融入基于扰动的XAI框架，可显著增强医学图像分割系统的可解释性与效率，从而推动透明且可临床部署的AI系统的发展。

</details>


### [10] [Deep Learning Based Multi-Level Classification for Aviation Safety](https://arxiv.org/abs/2602.07019)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Jonathan King*

Main category: cs.CV

TL;DR: 本文提出一种基于CNN的图像识别框架，用于在航空鸟击防范中自动识别鸟类种类、群体类型和群体规模，以提升飞行路径预测精度和风险评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于雷达的鸟击预防系统无法识别鸟类种类，而不同种类的鸟具有不同的飞行行为和高度偏好，限制了预测准确性与风险评估效果。

Method: 设计并应用卷积神经网络（CNN）模型，结合摄像头系统进行视觉检测，实现鸟类物种识别，并进一步开发专用CNN分类器估计鸟群类型和规模。

Result: 该框架能有效识别鸟类物种、群体类型和规模，为物种特异性飞行路径预测模型提供关键输入，并增强对撞击严重程度的评估能力。

Conclusion: 所提出的图像驱动CNN方法可弥补传统雷达系统的不足，显著提升鸟击风险预警与航空安全水平。

Abstract: Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.

Abstract (中文翻译): 鸟击对航空安全构成重大威胁，常导致人员伤亡、飞机严重损坏以及巨额经济损失。现有的鸟击预防策略主要依赖于实时探测和追踪鸟类的鸟类雷达系统。然而，这些系统的一大局限在于无法识别鸟类物种——而这是至关重要的因素，因为不同物种表现出不同的飞行行为和高度偏好。为应对这一挑战，我们提出了一种基于图像的鸟类分类框架，利用卷积神经网络（CNN），配合摄像系统实现自主视觉检测。该CNN旨在识别鸟类物种，并为物种特异性的预测模型提供关键输入，以实现更准确的飞行路径预测。除物种识别外，我们还实现了专门的CNN分类器，用于估计鸟群的组成类型和群体规模。这些特征为航空安全提供了宝贵的补充信息：鸟群类型和规模可揭示群体飞行行为和轨迹分散情况；而群体规模直接关联潜在撞击严重程度，因为多只鸟的总动能会显著增加整体损害风险。

</details>
