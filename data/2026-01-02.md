<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments](https://arxiv.org/abs/2512.23786)
*Ankan Aich,Yangming Lee*

Main category: cs.CV

TL;DR: 本文提出一种结合Depth Anything V2高保真合成先验与动态向量低秩适配（DV-LORA）的方法，用于提升单目深度估计在腹腔镜手术高反光环境下的鲁棒性，并在SCARED数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督单目深度估计方法在手术内窥镜的高反光、液体环境中表现脆弱，尤其在细小器械和透明表面上容易出现边界塌陷问题，亟需更鲁棒的解决方案。

Method: 利用Depth Anything V2架构提供的高保真合成先验，通过动态向量低秩适配（DV-LORA）高效地将其迁移到医疗领域，并设计了一种基于物理分层的评估协议以更准确衡量高反光条件下的性能。

Result: 在SCARED数据集上，该方法达到98.1%的准确率（δ < 1.25），相比基线方法将平方相对误差降低超过17%，显著优于现有方法。

Conclusion: 所提方法有效缓解了手术场景中高反光和透明结构带来的深度估计挑战，在参数开销较低的前提下实现了当前最优性能，展现出在复杂手术照明条件下的强鲁棒性。

Abstract: Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.

Abstract (中文翻译): 精确的单目深度估计（MDE）对机器人手术至关重要，但在具有高反光和充满液体的内窥镜环境中仍然表现脆弱。现有的自监督方法通常依赖于使用含噪声的真实世界伪标签训练的基础模型，在细小手术器械和透明表面上常常出现边界塌陷问题。本文通过利用Depth Anything V2架构所提供的高保真合成先验来解决这一问题，该先验能够天然捕捉细小结构的精确几何细节。我们采用动态向量低秩适配（DV-LORA）技术，以极低的参数开销高效地将这些先验知识迁移到医学领域，从而弥合合成与真实之间的差距。此外，我们在SCARED数据集上引入了一种基于物理分层的评估协议，以严格量化在高反光区域的性能表现——这些区域常被整体指标所掩盖。我们的方法建立了新的最先进水平，在准确率（< 1.25）上达到98.1%，并将平方相对误差较现有基线降低了超过17%，在恶劣的手术光照条件下展现出卓越的鲁棒性。

</details>


### [2] [Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments](https://arxiv.org/abs/2512.23819)
*Surya Rayala,Marcos Quinones-Grueiro,Naveeduddin Mohammed,Ashwin T S,Benjamin Goldberg,Randall Spain,Paige Lawton,Gautam Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的自动评估系统，用于城市作战训练中的“进入并清房”（ECR）演练，通过计算机视觉技术从训练视频中提取人体姿态、视线和轨迹等信息，构建任务特定指标以量化认知、心理运动和团队协作能力，并整合到扩展的认知任务分析（CTA）框架中生成综合评分，支持在GIFT等平台中进行可操作的复盘反馈。


<details>
  <summary>Details</summary>
Motivation: 当前军事合成训练环境中缺乏可扩展、客观且非侵入式的训练表现自动评估方法，传统依赖昂贵传感器或主观人工观察的方式限制了评估的准确性和可推广性。

Method: 利用计算机视觉模型从训练视频中提取2D骨架、视线向量和运动轨迹，构建衡量心理运动流畅性、态势感知和团队协作的任务特定指标，并将其整合进加权的扩展认知任务分析（CTA）层次结构中，生成团队与认知维度的整体表现评分。

Result: 在真实ECR演练案例中验证了该方法的有效性，生成了具有领域针对性的可操作指标，可用于个体与团队表现评估，并通过Gamemaster和GIFT平台的交互式仪表板支持行动后复盘。

Conclusion: 该视频分析方法为合成训练环境提供了无需额外硬件的可扩展评估手段，但仍面临追踪精度、真值验证和泛化能力等挑战；未来工作将拓展至3D视频分析并进一步提升在STE中的应用规模。

Abstract: Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.

Abstract (中文翻译): 有效的城市作战训练需要通过在逼真但受控的环境中反复练习来培养态势感知能力和肌肉记忆。其中一项关键训练科目是“进入并清房”（Enter and Clear the Room, ECR），要求参训人员进行威胁评估、协同配合并控制密闭空间。军队采用合成训练环境（Synthetic Training Environments, STEs）来提供可扩展且受控的重复训练场景。然而，实现对认知能力、心理运动技能和团队协作的客观自动评估仍然具有挑战性。传统方法通常依赖昂贵且侵入式的传感器或主观的人工观察，限制了评估的可扩展性和准确性。本文提出了一种基于视频的评估流程，仅利用训练视频即可生成表现分析结果，无需额外硬件。该系统利用计算机视觉模型提取2D人体骨架、视线向量和运动轨迹，并基于这些数据开发出针对任务的指标，用以衡量心理运动流畅性、态势感知和团队协调能力。这些指标被整合进一个扩展的认知任务分析（Cognitive Task Analysis, CTA）层级结构中，通过加权组合生成团队协作与认知能力的总体表现评分。我们通过真实世界ECR演练的案例研究展示了该方法的有效性，提供了可操作的、领域特定的指标以捕捉个体与团队表现。同时，我们探讨了如何将这些洞察集成到Gamemaster和通用智能辅导框架（GIFT）的交互式仪表板中，以提供直观易懂的行动后复盘反馈。最后，我们讨论了本方法的局限性，包括目标追踪困难、真值验证问题以及方法的广泛适用性。未来工作将扩展至3D视频数据分析，并利用视频分析技术实现在合成训练环境中的大规模评估。

</details>


### [3] [Pretraining Frame Preservation in Autoregressive Video Memory Compression](https://arxiv.org/abs/2512.23851)
*Lvmin Zhang,Shengqu Cai,Muyang Li,Chong Zeng,Beijia Lu,Anyi Rao,Song Han,Gordon Wetzstein,Maneesh Agrawala*

Main category: cs.CV

TL;DR: 提出PFP神经网络结构，用于将长视频压缩为短上下文，在预训练阶段显式保留任意时间位置单帧的高频细节；该模型可直接微调为自回归视频模型的记忆编码器，以较低上下文代价实现长期记忆。


<details>
  <summary>Details</summary>
Motivation: 现有视频压缩方法在保留高频率细节方面存在不足，难以在压缩后准确还原任意时间点的帧内容；同时，自回归视频模型需要高效记忆机制以支持长时间历史建模。

Method: 设计PFP神经网络结构，通过显式的预训练目标将长视频（如20秒）压缩为约5k长度的上下文表示，并确保任意时间位置的帧能被高质量重建；预训练后的模型可直接作为记忆编码器用于自回归视频生成模型。

Result: 实验表明，PFP能在压缩视频的同时较好地保留感知质量，支持随机帧的高质量检索，并在微调后有效降低上下文开销与保真度损失。

Conclusion: PFP提供了一种高效的视频压缩与记忆编码方案，在保持帧级细节的同时显著减少上下文长度，适用于长视频建模任务。

Abstract: We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.

Abstract (中文翻译): 我们提出了PFP，一种用于将长视频压缩为短上下文的神经网络结构，其预训练目标显式地保留任意时间位置单帧的高频细节。基础模型可将一段20秒的视频压缩为长度约为5k的上下文，并能从中以感知上保持一致的外观随机检索帧。此类预训练模型可直接微调为自回归视频模型的记忆编码器，从而以较低的上下文代价实现长期历史记忆，并带来相对较小的保真度损失。我们通过消融实验评估了该框架，并讨论了不同神经架构设计之间的权衡。

</details>


### [4] [Lifelong Domain Adaptive 3D Human Pose Estimation](https://arxiv.org/abs/2512.23860)
*Qucheng Peng,Hongfei Xue,Pu Wang,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种面向3D人体姿态估计（3D HPE）的终身域自适应新任务，并设计了一个结合3D姿态生成器、2D姿态判别器和3D姿态估计器的GAN框架，以应对目标域非平稳性和灾难性遗忘问题，在多个数据集上验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计方法依赖于受控环境中采集的标注3D数据，难以泛化到多样化的野外场景；同时，当前的域自适应方法未考虑目标姿态数据集的非平稳性，且在终身学习设定下无法访问源域和历史目标域，导致灾难性遗忘问题。

Method: 提出一种新的终身域自适应3D HPE任务，并构建一个创新的GAN框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器；其中3D姿态生成器融合姿态感知、时序感知和域感知知识，用于缓解域偏移、增强当前域适应能力并减轻对历史域的灾难性遗忘。

Result: 在多个域自适应3D HPE数据集上的大量实验表明，所提方法性能优于现有方法。

Conclusion: 本文首次将终身域自适应引入3D人体姿态估计任务，通过提出的GAN框架和新型3D姿态生成器有效解决了非平稳目标域适应与灾难性遗忘问题，显著提升了模型在复杂真实场景中的泛化能力。

Abstract: 3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.

Abstract (中文翻译): 3D人体姿态估计（3D HPE）在人员重识别、动作识别和虚拟现实等多种应用中至关重要。然而，现有方法依赖于在受控环境中采集的带标注3D数据，这使其难以泛化到多样化的野外场景。当前针对3D HPE的域自适应（DA）范式，如通用DA和无源DA，忽略了目标姿态数据集非平稳性的问题。为应对这些挑战，我们提出了一项名为“终身域自适应3D HPE”的新任务。据我们所知，这是首次将终身域自适应引入3D HPE任务。在此设定下，姿态估计器首先在源域上预训练，随后依次适应到不同的目标域；并且在适应当前目标域时，无法访问源域及所有先前的目标域。该任务的核心挑战在于既要适应当前域的姿态分布，又要保留来自先前域的知识，特别是要克服灾难性遗忘。为此，我们提出了一种创新的生成对抗网络（GAN）框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器，有效缓解域偏移并实现原始姿态与增强姿态的对齐。此外，我们构建了一种新颖的3D姿态生成器范式，融合了姿态感知、时序感知和域感知知识，以增强对当前域的适应能力，并缓解对先前域的灾难性遗忘。大量实验在多个域自适应3D HPE数据集上验证了所提方法的优越性能。

</details>


### [5] [MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework](https://arxiv.org/abs/2512.23894)
*Krithika Iyer,Austin Tapp,Athelia Paulli,Gabrielle Dickerson,Syed Muhammad Anwar,Natasha Lepore,Marius George Linguraru*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的方法，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），从而实现颅骨分割和颅缝可视化，克服了MRI无法清晰显示骨缝的局限性。


<details>
  <summary>Details</summary>
Motivation: 量化儿童颅骨发育和颅缝骨化对诊断和治疗头颅生长相关疾病至关重要。虽然CT常用于评估颅骨和颅缝畸形，但其电离辐射对无明显异常的儿童存在禁忌；而MRI虽无辐射且软组织对比度高，却难以显示颅缝、估计骨密度或评估颅穹隆生长。

Method: 提出一个深度学习驱动的流程，利用变分自编码器将0.2–2岁儿童的T1加权MRI转化为合成CT（sCT），并在此基础上进行颅骨分割、生成颅缝概率热图，并从中提取颅缝分割结果。

Result: 在内部儿科数据集上，合成CT与真实CT的结构相似度达99%，Frechet inception distance为1.01；七块颅骨的平均Dice系数为85%，颅缝Dice系数为80%；通过TOST检验（p < 0.05）确认了sCT与真实CT在颅骨和颅缝分割上的等效性。

Conclusion: 这是首个能在MRI衍生的合成CT上实现颅缝分割的儿科颅骨CT合成框架，通过结合领域特定的变分自编码器，从常规儿科MRI生成视觉上难以区分的颅骨合成CT，填补了无创颅骨评估的关键空白。

Abstract: Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.

Abstract (中文翻译): 量化儿童规范性颅骨发育和颅缝骨化对于诊断和治疗与生长相关的头颅疾病至关重要。计算机断层扫描（CT）广泛用于评估颅骨和颅缝畸形，但其电离辐射对无明显异常的儿童是禁忌的。磁共振成像（MRI）提供无辐射扫描并具有优异的软组织对比度，但与CT不同，MRI无法清晰显示颅缝、估算颅骨密度或评估颅穹隆生长。本研究提出了一种基于深度学习的流程，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），预测详细的颅骨分割结果，生成颅缝概率热图，并从中直接提取颅缝分割。利用我们内部的儿科数据，合成CT与真实CT相比达到了99%的结构相似度，Fréchet Inception Distance为1.01。七块颅骨的平均Dice系数达到85%，颅缝Dice系数为80%。通过双单侧检验（TOST，p < 0.05）确认了合成CT与真实CT在颅骨和颅缝分割上的等效性。据我们所知，这是首个能够在源自MRI的合成CT上实现颅缝分割的儿科颅骨CT合成框架，尽管MRI对骨骼和颅缝的描绘能力有限。通过结合稳健的、领域特定的变分自编码器，我们的方法能够从常规儿科MRI中生成视觉上难以区分的颅骨合成CT，从而弥合了无创颅骨评估中的关键缺口。

</details>


### [6] [Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale](https://arxiv.org/abs/2512.23903)
*Charith Wickrema,Eliza Mace,Hunter Brown,Heidys Cabrera,Nick Krall,Matthew O'Neill,Shivangi Sarkar,Lowell Weissman,Eric Hughes,Guido Zarrella*

Main category: cs.CV

TL;DR: 该论文研究了在超高分辨率遥感电光（EO）数据集上训练基础模型的扩展行为，发现即使在千万亿像素规模下，性能仍受数据限制而非模型参数限制，并为未来遥感基础模型的数据收集、算力预算和优化策略提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前多模态机器学习（如生成式AI）依赖于针对非文本模态的领域专用编码器。虽然自然图像领域已有成熟的缩放定律，但在遥感等高价值领域，模型容量、计算资源与数据规模之间的关系尚不明确，亟需探索。

Method: 利用超过一千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练更大规模的视觉Transformer（ViT）骨干网络，在拍字节级（petascale）训练中记录成功与失败模式，并分析跨遥感模态的领域差距问题。

Result: 实验表明，即便在如此大规模下，模型性能仍处于数据受限状态，而非模型参数受限；同时揭示了在遥感领域扩展基础模型时的关键挑战与可行路径。

Conclusion: 研究结果为遥感基础模型的未来发展提供了实践指导，强调应优先优化数据收集策略、合理分配算力预算并设计有效的训练调度方案。

Abstract: We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.

Abstract (中文翻译): 我们探索了人工智能的扩展行为，以建立在超高分辨率电光（EO）数据集上训练基础模型的实用技术，这些数据集的规模比当前最先进水平高出数个数量级。现代多模态机器学习（ML）应用，例如用于图像描述、搜索和推理的生成式人工智能（GenAI）系统，依赖于针对非文本模态的鲁棒且领域专用的编码器。在互联网规模数据丰富的自然图像领域，已有成熟的扩展定律可优化模型容量、训练算力和数据集规模的联合扩展。然而，在遥感（RS）等高价值领域，这些关系远未被充分理解。我们利用超过一千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练更大规模的视觉Transformer（ViT）骨干网络，报告在拍字节级训练中观察到的成功与失败模式，并分析其对弥合其他遥感模态间领域差距的启示。我们发现，即使在此规模下，模型性能仍符合数据受限状态，而非模型参数受限状态。这些实践洞见旨在为数据收集策略、算力预算和优化调度提供参考，以推动前沿遥感基础模型的未来发展。

</details>


### [7] [Learning to learn skill assessment for fetal ultrasound scanning](https://arxiv.org/abs/2512.23920)
*Yipei Wang,Qianye Yang,Lior Drukker,Aris T. Papageorghiou,Yipeng Hu,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出一种无需人工标注技能等级的双层优化框架，通过评估胎儿超声图像上临床任务的完成质量来自动量化操作者的超声技能水平。


<details>
  <summary>Details</summary>
Motivation: 传统超声技能评估依赖专家主观监督，耗时且缺乏客观性；现有自动化方法多采用监督学习，并局限于预设的技能影响因素，难以全面反映真实技能水平。

Method: 提出一种新颖的双层优化框架，包含临床任务预测器和技能预测器，二者联合优化，通过在获取的胎儿超声图像上任务执行效果来评估技能，无需人工预定义技能评分。

Result: 在真实胎儿头部扫描超声视频上验证了该方法，结果表明所提框架可通过优化后的任务表现有效预测超声操作技能。

Conclusion: 该框架为超声技能评估提供了一种客观、自动且无需人工标注的新途径，证明了以任务性能作为技能指标的可行性。

Abstract: Traditionally, ultrasound skill assessment has relied on expert supervision and feedback, a process known for its subjectivity and time-intensive nature. Previous works on quantitative and automated skill assessment have predominantly employed supervised learning methods, often limiting the analysis to predetermined or assumed factors considered influential in determining skill levels. In this work, we propose a novel bi-level optimisation framework that assesses fetal ultrasound skills by how well a task is performed on the acquired fetal ultrasound images, without using manually predefined skill ratings. The framework consists of a clinical task predictor and a skill predictor, which are optimised jointly by refining the two networks simultaneously. We validate the proposed method on real-world clinical ultrasound videos of scanning the fetal head. The results demonstrate the feasibility of predicting ultrasound skills by the proposed framework, which quantifies optimised task performance as a skill indicator.

Abstract (中文翻译): 传统上，超声技能评估依赖于专家的监督与反馈，这一过程具有主观性强且耗时的特点。以往关于定量和自动化技能评估的研究主要采用监督学习方法，通常将分析局限于预先设定或假定的影响技能水平的因素。在本研究中，我们提出了一种新颖的双层优化框架，通过评估在所获取的胎儿超声图像上任务执行的质量来评估胎儿超声技能，而无需使用人工预定义的技能评分。该框架包含一个临床任务预测器和一个技能预测器，通过同时优化两个网络进行联合训练。我们在真实临床胎儿头部扫描超声视频上验证了所提出的方法。结果表明，所提出的框架能够通过量化优化后的任务表现作为技能指标，有效预测超声技能水平。

</details>


### [8] [MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation](https://arxiv.org/abs/2512.23936)
*Yulong Zou,Bo Liu,Cun-Jing Zheng,Yuan-ming Geng,Siyue Li,Qiankun Zuo,Shuihua Wang,Yudong Zhang,Jin Hong*

Main category: cs.CV

TL;DR: 本文提出了一种名为MGML的新框架，用于在多模态MRI缺失情况下提升脑肿瘤分割性能，包含元参数自适应模态融合（Meta-AMF）和一致性正则化模块，在BraTS2020/2023数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床中多模态MRI数据常不完整，难以充分利用多模态信息进行病灶分割，因此如何高效利用不完整的多模态信息成为关键挑战。

Method: 提出元引导多模态学习（MGML）框架，包括元参数化自适应模态融合（Meta-AMF）和一致性正则化模块；Meta-AMF根据可用模态生成软标签监督信号以促进模态融合，一致性模块提升分割性能并增强模型鲁棒性与泛化能力，且无需修改原模型结构，可端到端训练。

Result: 在BraTS2020和BraTS2023数据集上进行大量实验，相比多个SOTA方法表现更优；在BraTS2020的15种缺失模态组合平均Dice分数上，WT、TC、ET分别达到87.55、79.36和62.67。

Conclusion: 所提出的MGML框架能有效应对多模态MRI缺失问题，在不改变原始模型结构的前提下显著提升脑肿瘤分割性能，并具备良好泛化能力。

Abstract: Leveraging multimodal information from Magnetic Resonance Imaging (MRI) plays a vital role in lesion segmentation, especially for brain tumors. However, in clinical practice, multimodal MRI data are often incomplete, making it challenging to fully utilize the available information. Therefore, maximizing the utilization of this incomplete multimodal information presents a crucial research challenge. We present a novel meta-guided multi-modal learning (MGML) framework that comprises two components: meta-parameterized adaptive modality fusion and consistency regularization module. The meta-parameterized adaptive modality fusion (Meta-AMF) enables the model to effectively integrate information from multiple modalities under varying input conditions. By generating adaptive soft-label supervision signals based on the available modalities, Meta-AMF explicitly promotes more coherent multimodal fusion. In addition, the consistency regularization module enhances segmentation performance and implicitly reinforces the robustness and generalization of the overall framework. Notably, our approach does not alter the original model architecture and can be conveniently integrated into the training pipeline for end-to-end model optimization. We conducted extensive experiments on the public BraTS2020 and BraTS2023 datasets. Compared to multiple state-of-the-art methods from previous years, our method achieved superior performance. On BraTS2020, for the average Dice scores across fifteen missing modality combinations, building upon the baseline, our method obtained scores of 87.55, 79.36, and 62.67 for the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET), respectively. We have made our source code publicly available at https://github.com/worldlikerr/MGML.

Abstract (中文翻译): 利用磁共振成像（MRI）的多模态信息在病灶分割（尤其是脑肿瘤）中起着至关重要的作用。然而在临床实践中，多模态MRI数据常常不完整，使得难以充分利用现有信息。因此，如何最大化利用这种不完整的多模态信息成为一个关键的研究挑战。我们提出了一种新颖的元引导多模态学习（MGML）框架，该框架包含两个组成部分：元参数化自适应模态融合模块和一致性正则化模块。元参数化自适应模态融合（Meta-AMF）使模型能够在不同输入条件下有效整合多模态信息。通过基于可用模态生成自适应的软标签监督信号，Meta-AMF显式地促进更一致的多模态融合。此外，一致性正则化模块提升了分割性能，并隐式增强了整个框架的鲁棒性和泛化能力。值得注意的是，我们的方法无需更改原始模型架构，可方便地集成到端到端模型优化的训练流程中。我们在公开的BraTS2020和BraTS2023数据集上进行了大量实验。与以往多年的多种先进方法相比，我们的方法取得了更优的性能。在BraTS2020数据集上，针对十五种缺失模态组合的平均Dice分数，相较于基线方法，我们的方法在全肿瘤（WT）、肿瘤核心（TC）和增强肿瘤（ET）上分别获得了87.55、79.36和62.67的分数。我们已将源代码公开于 https://github.com/worldlikerr/MGML。

</details>


### [9] [Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation](https://arxiv.org/abs/2512.23938)
*Hualin Ye,Bingxi Liu,Jixiang Du,Yu Qin,Ziyi Chen,Hong Zhang*

Main category: cs.CV

TL;DR: 本文提出一种新型跨视角地理定位（CVGL）系统，通过DINOv2骨干网络、多尺度通道重分配模块和基于MoE的聚合模块，在减少参数量的同时取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 跨视角地理定位面临视角差异大导致的特征聚合与对齐困难问题，现有方法难以有效处理这种异构输入。

Method: 1）采用DINOv2骨干网络结合卷积适配器微调；2）设计多尺度通道重分配模块以增强空间表示的多样性与稳定性；3）在特征聚合中引入混合专家（MoE）路由机制，动态选择专家子空间处理键值对。

Result: 在University-1652和SUES-200数据集上的大量实验表明，该方法以更少的训练参数实现了具有竞争力的性能。

Conclusion: 所提出的CVGL系统通过三项关键技术有效缓解了跨视角差异带来的挑战，在保持模型轻量化的同时提升了定位准确率。

Abstract: Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.

Abstract (中文翻译): 跨视角地理定位（CVGL）旨在通过将查询图像与大规模数据库中的图像进行匹配，来估计其地理位置。然而，显著的视角差异给有效的特征聚合与对齐带来了巨大挑战。为应对这些挑战，我们提出了一种新颖的CVGL系统，包含三项关键改进：首先，利用DINOv2骨干网络并结合卷积适配器进行微调，以增强模型对跨视角变化的适应能力；其次，提出一个多尺度通道重分配模块，以增强空间表示的多样性和稳定性；最后，提出一种改进的聚合模块，将混合专家（MoE）路由机制融入特征聚合过程。具体而言，该模块在交叉注意力框架中动态为键（keys）和值（values）选择专家子空间，从而实现对异构输入域的自适应处理。在University-1652和SUES-200数据集上的大量实验表明，我们的方法在使用更少训练参数的情况下取得了具有竞争力的性能。

</details>


### [10] [Kinematic-Based Assessment of Surgical Actions in Microanastomosis](https://arxiv.org/abs/2512.23942)
*Yan Meng,Daniel Donoho,Marcelle Altshuler,Omar Arnaout*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化框架，用于显微吻合术中的动作分割与技能评估，可在边缘计算设备上高效运行，并在58个专家评分视频上验证了其高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统显微外科手术评估依赖专家主观打分，存在评分者间差异大、耗时长、一致性差等问题，亟需客观、可扩展的自动化评估方法。

Method: 该系统包含三个模块：(1) 基于YOLO和DeepSORT的器械尖端追踪定位模块；(2) 利用自相似矩阵进行动作边界检测与无监督聚类的动作分割模块；(3) 用于评估手术动作熟练度的有监督分类模块。

Result: 在58个专家评分的显微吻合视频数据集上，该方法实现了92.4%的帧级动作分割准确率和85.5%的整体技能分类准确率。

Conclusion: 所提方法有望为显微外科教学提供客观、实时的反馈，推动标准化、数据驱动的培训体系，并提升高风险手术环境中的能力评估水平。

Abstract: Proficiency in microanastomosis is a critical surgical skill in neurosurgery, where the ability to precisely manipulate fine instruments is crucial to successful outcomes. These procedures require sustained attention, coordinated hand movements, and highly refined motor skills, underscoring the need for objective and systematic methods to evaluate and enhance microsurgical training. Conventional assessment approaches typically rely on expert raters supervising the procedures or reviewing surgical videos, which is an inherently subjective process prone to inter-rater variability, inconsistency, and significant time investment. These limitations highlight the necessity for automated and scalable solutions. To address this challenge, we introduce a novel AI-driven framework for automated action segmentation and performance assessment in microanastomosis procedures, designed to operate efficiently on edge computing platforms. The proposed system comprises three main components: (1) an object tip tracking and localization module based on YOLO and DeepSORT; (2) an action segmentation module leveraging self-similarity matrix for action boundary detection and unsupervised clustering; and (3) a supervised classification module designed to evaluate surgical gesture proficiency. Experimental validation on a dataset of 58 expert-rated microanastomosis videos demonstrates the effectiveness of our approach, achieving a frame-level action segmentation accuracy of 92.4% and an overall skill classification accuracy of 85.5% in replicating expert evaluations. These findings demonstrate the potential of the proposed method to provide objective, real-time feedback in microsurgical education, thereby enabling more standardized, data-driven training protocols and advancing competency assessment in high-stakes surgical environments.

Abstract (中文翻译): 显微吻合术的熟练掌握是神经外科中一项关键的手术技能，其中精准操控精细器械对成功结果至关重要。这些操作需要持续注意力、协调的手部动作以及高度精细的运动技能，凸显了对显微外科训练进行客观、系统化评估与提升的必要性。传统的评估方法通常依赖专家观察手术过程或回看手术视频，这一过程本质上具有主观性，易受评分者间差异、不一致性和大量时间投入的影响。这些局限性突显了开发自动化、可扩展解决方案的必要性。为应对这一挑战，我们提出了一种新颖的、基于人工智能的框架，用于显微吻合术中的自动化动作分割与表现评估，该框架专为在边缘计算平台上高效运行而设计。所提出的系统包含三个主要组成部分：（1）基于YOLO和DeepSORT的器械尖端追踪与定位模块；（2）利用自相似矩阵进行动作边界检测和无监督聚类的动作分割模块；（3）用于评估手术动作熟练程度的有监督分类模块。在包含58个经专家评分的显微吻合视频数据集上的实验验证表明，我们的方法具有良好的效果，实现了92.4%的帧级动作分割准确率和85.5%的整体技能分类准确率，能够有效复现专家评估结果。这些发现表明，所提出的方法有潜力在显微外科教育中提供客观、实时的反馈，从而实现更加标准化、数据驱动的培训方案，并推动高风险手术环境中能力评估的发展。

</details>
