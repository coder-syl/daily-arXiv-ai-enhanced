<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: GRAFNet是一种受人类视觉系统启发的新型网络架构，通过模拟生物视觉机制，在多个公开数据集上显著提升了息肉分割的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在结肠镜息肉分割中存在单向处理、多尺度融合弱和缺乏解剖约束等问题，导致假阳性和假阴性结果，影响临床可靠性。

Method: 提出GRAFNet架构，包含三个核心模块：引导非对称注意力模块（GAAM）、多尺度视网膜模块（MSRM）和引导皮层注意力反馈模块（GCAFM），并整合于息肉编解码模块（PEDM）中，实现空间-语义一致性。

Result: 在五个公开数据集上取得SOTA性能，Dice指标提升3-8%，泛化能力提高10-20%，且具备可解释性。

Conclusion: 该研究将神经计算原理引入医学图像分割，弥合了AI精度与临床可信推理之间的鸿沟，为可信AI在医疗中的应用提供了新范式。

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

Abstract (中文翻译): 结肠镜检查中准确的息肉分割对于癌症预防至关重要，但由于以下原因仍具挑战性：(1) 形态高度可变（从扁平到隆起病变），(2) 与皱襞和血管等正常结构在视觉上高度相似，(3) 需要鲁棒的多尺度检测能力。现有深度学习方法受限于单向处理、弱多尺度融合以及缺乏解剖约束，常导致假阳性（将正常结构过度分割）和假阴性（漏检细微扁平病变）。本文提出GRAFNet，一种受生物启发的架构，模拟人类视觉系统的层级组织。GRAFNet整合三个关键模块：(1) 引导非对称注意力模块（GAAM），模拟方向调谐皮层神经元以强化息肉边界；(2) 多尺度视网膜模块（MSRM），复现视网膜神经节细胞通路以进行并行多特征分析；(3) 引导皮层注意力反馈模块（GCAFM），采用预测编码进行迭代优化。这些模块统一于息肉编码器-解码器模块（PEDM）中，通过分辨率自适应反馈确保空间-语义一致性。在五个公开基准数据集（Kvasir-SEG、CVC-300、CVC-ColonDB、CVC-Clinic和PolypGen）上的大量实验表明，该方法始终达到最先进水平，Dice指标提升3-8%，泛化能力比主流方法高10-20%，同时提供可解释的决策路径。本工作建立了一种新范式，即利用神经计算原理弥合人工智能精度与临床可信推理之间的差距。代码已开源：https://github.com/afofanah/GRAFNet。

</details>


### [2] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 本文提出了一种解耦的零样本人-物交互（HOI）检测框架，将目标检测与交互识别分离，并利用多模态大语言模型（MLLMs）进行零样本交互识别，通过确定性生成方法和空间感知池化模块，在多个数据集上实现了优越的性能和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本人-物交互（HOI）检测方法通常将交互识别（IR）与特定检测器紧密耦合，并依赖粗粒度的视觉-语言模型（VLM）特征，限制了对未见交互的泛化能力。为解决这一问题，作者希望设计一个更灵活、通用且高效的框架。

Method: 提出一种解耦框架，将目标检测与交互识别分离；利用多模态大语言模型（MLLMs），将交互识别建模为视觉问答任务，并采用确定性生成方法实现无需训练的零样本推理；为进一步提升性能，设计了空间感知池化模块以融合外观与空间关系特征，并提出单次前向传播的确定性匹配策略。

Result: 在HICO-DET和V-COCO数据集上的实验表明，该方法在零样本设置下表现优异，具有强大的跨数据集泛化能力，并能灵活适配任意目标检测器而无需重新训练。

Conclusion: 所提出的解耦框架有效提升了零样本人-物交互检测的性能与灵活性，验证了多模态大语言模型在该任务中的潜力，并为未来研究提供了新的思路。

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

Abstract (中文翻译): 零样本人-物交互（HOI）检测旨在定位图像中的人和物体并识别它们之间的交互关系。尽管开放词汇目标检测的进展为目标定位提供了有前景的解决方案，但由于交互组合的多样性，交互识别（IR）仍然具有挑战性。现有方法（包括两阶段方法）通常将IR与特定检测器紧密耦合，并依赖粗粒度的视觉-语言模型（VLM）特征，这限制了其对未见过交互的泛化能力。在本研究中，我们提出了一种解耦框架，将目标检测与IR分离，并利用多模态大语言模型（MLLMs）进行零样本IR。我们引入了一种确定性生成方法，将IR建模为视觉问答任务，并强制输出确定性结果，从而实现无需训练的零样本IR。为进一步提升性能和效率，我们设计了一个空间感知池化模块，用于融合外观特征与成对空间线索，并提出了一种单次前向传播的确定性匹配方法，可在一次推理中预测所有候选交互。在HICO-DET和V-COCO上的大量实验表明，我们的方法在零样本性能、跨数据集泛化能力方面表现优异，并且能够灵活地与任意目标检测器集成而无需重新训练。代码已公开于 https://github.com/SY-Xuan/DA-HOI。

</details>


### [3] [MB-DSMIL-CL-PL: Scalable Weakly Supervised Ovarian Cancer Subtype Classification and Localisation Using Contrastive and Prototype Learning with Frozen Patch Features](https://arxiv.org/abs/2602.15138)
*Marcus Jenkins,Jasenka Mazibrada,Bogdan Leahu,Michal Mackiewicz*

Main category: cs.CV

TL;DR: 本文提出一种基于对比学习和原型学习的新方法，利用预计算的冻结特征对卵巢癌组织病理图像进行亚型分类与定位，在保持特征冻结的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 卵巢癌组织病理亚型研究对个性化治疗至关重要，但英国病理部门面临诊断工作量激增的问题，促使AI方法的发展。现有端到端方法虽提高准确率，却牺牲了训练可扩展性和实验效率。

Method: 采用对比学习和原型学习，结合特征空间增强技术，利用预计算且冻结的图像块特征进行卵巢癌亚型的分类与定位。

Result: 相比DSMIL方法，该方法在实例级和切片级分类的F1分数分别提升70.4%和15.3%，在实例定位和切片分类的AUC分别提升16.9%和2.3%，同时保持使用冻结的patch特征。

Conclusion: 所提方法在不重新训练底层特征提取器的前提下，显著提升了卵巢癌组织病理亚型分类与定位的性能，兼顾效率与准确性。

Abstract: The study of histopathological subtypes is valuable for the personalisation of effective treatment strategies for ovarian cancer. However, increasing diagnostic workloads present a challenge for UK pathology departments, leading to the rise in AI approaches. While traditional approaches in this field have relied on pre-computed, frozen image features, recent advances have shifted towards end-to-end feature extraction, providing an improvement in accuracy but at the expense of significantly reduced scalability during training and time-consuming experimentation. In this paper, we propose a new approach for subtype classification and localisation in ovarian cancer histopathology images using contrastive and prototype learning with pre-computed, frozen features via feature-space augmentations. Compared to DSMIL, our method achieves an improvement of 70.4\% and 15.3\% in F1 score for instance- and slide-level classification, respectively, along with AUC gains of 16.9\% for instance localisation and 2.3\% for slide classification, while maintaining the use of frozen patch features.

Abstract (中文翻译): 组织病理学亚型的研究对于卵巢癌个性化有效治疗策略的制定具有重要价值。然而，日益增加的诊断工作量给英国病理部门带来了挑战，推动了人工智能方法的应用。尽管该领域传统方法依赖于预计算且冻结的图像特征，但近期研究已转向端到端特征提取，虽然提高了准确性，却以训练阶段可扩展性大幅降低和实验耗时增加为代价。本文提出一种新方法，通过特征空间增强，利用预计算且冻结的特征，结合对比学习与原型学习，实现卵巢癌组织病理图像的亚型分类与定位。与DSMIL相比，该方法在实例级和切片级分类的F1分数分别提升了70.4%和15.3%，在实例定位和切片分类的AUC分别提升了16.9%和2.3%，同时仍保持使用冻结的图像块特征。

</details>


### [4] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出一种基于累积样本损失（CSL）的模型无关方法，用于检测视频数据集中帧级标注错误（如误标和时序错乱），无需真实错误标签，在EgoPER和Cholec80上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视频数据集常存在标注错误（如误标和时序错乱），尤其在对时间一致性要求高的阶段标注任务中危害显著，亟需有效方法进行自动检测以提升模型训练可靠性。

Method: 通过分析每帧在训练过程中各轮次模型检查点上的平均损失（即累积样本损失，CSL）来识别异常帧；误标或错序帧通常表现出持续高或不规则的损失模式，而正确标注帧则早期收敛至低损失；该方法不依赖真实错误标签，且适用于不同数据集。

Result: 在EgoPER和Cholec80数据集上的实验表明，该方法能有效识别包括误标和帧顺序错乱在内的细微标注不一致问题，展现出强健的错误检测性能。

Conclusion: 所提方法为视频数据集提供了一种通用、无监督的标注质量审计工具，有助于提升视频机器学习任务的训练可靠性和数据质量。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

Abstract (中文翻译): 高质量视频数据集是训练动作识别、阶段检测和事件分割等任务鲁棒模型的基础。然而，许多现实世界的视频数据集存在标注错误，例如“误标”（即片段被赋予错误的类别标签）和“时序错乱”（即时间序列未遵循正确的进展顺序）。这些错误在阶段标注任务中尤其有害，因为这类任务对时间一致性要求极高。我们提出了一种新颖的、与模型无关的方法，通过分析累积样本损失（Cumulative Sample Loss, CSL）来检测标注错误——CSL定义为一帧在训练过程中各轮次保存的模型检查点上所产生的平均损失。这种逐帧的损失轨迹可作为帧级可学习性的动态指纹：误标或时序错乱的帧通常在整个训练过程中表现出持续较高或不规则的损失模式，而正确标注的帧则往往在早期就收敛到较低损失。为计算CSL，我们训练一个视频分割模型，并在每个训练轮次保存其权重；随后利用这些检查点评估测试视频中每一帧的损失。持续高CSL的帧将被标记为可能存在标注错误（包括误标或时间错位）的候选对象。该方法无需标注错误的真实标签，且可在不同数据集间泛化。在EgoPER和Cholec80上的实验表明，该方法具有强大的检测性能，能有效识别诸如误标和帧顺序错乱等细微不一致问题。所提出的方法为视频机器学习中的数据集审计和提升训练可靠性提供了有力工具。

</details>


### [5] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 本文提出一种基于分布深度学习的超分辨率框架，用于提升4D Flow MRI图像质量，在存在域偏移的真实临床场景中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率方法依赖于通过简单下采样生成的配对数据进行训练，但在真实临床环境中，低分辨率数据往往源于与下采样机制不同的采集过程，导致模型因域偏移而泛化能力差。

Method: 作者构建了一个分布深度学习框架：首先在高分辨率计算流体动力学（CFD）模拟及其下采样版本上训练模型，然后在少量配对的4D Flow MRI与CFD数据上进行微调，并推导了该分布估计器的理论性质。

Result: 在真实数据上的实验表明，所提框架显著优于传统深度学习超分辨率方法。

Conclusion: 分布学习能有效应对域偏移问题，在临床实际场景中提升超分辨率性能，尤其适用于4D Flow MRI等新型医学成像模态。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

Abstract (中文翻译): 超分辨率技术广泛应用于医学成像，以增强低质量数据，从而缩短扫描时间并提高异常检测能力。传统的超分辨率方法通常依赖于由下采样图像和原始高分辨率图像组成的配对数据集，训练模型从人为退化的图像中重建高分辨率图像。然而，在真实临床环境中，低分辨率数据往往源于与简单下采样显著不同的采集机制，导致输入数据超出训练数据的分布范围，因域偏移而使模型泛化能力下降。为解决这一局限性，本文提出一种分布深度学习框架，以提升模型的鲁棒性和域泛化能力。我们将该方法应用于提升4D Flow MRI（4DF）的分辨率。4D Flow MRI是一种新型成像模态，可捕捉血流速度及血管壁应力等临床相关指标，这些指标对于评估动脉瘤破裂风险至关重要。我们的模型首先在高分辨率计算流体动力学（CFD）模拟及其下采样版本上进行训练，随后在一个小规模但经过协调的4D Flow MRI与CFD配对数据集上进行微调。我们推导了所提分布估计器的理论性质，并通过真实数据应用证明该框架显著优于传统深度学习方法。这凸显了分布学习在应对域偏移、提升临床现实场景中超分辨率性能方面的有效性。

</details>


### [6] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 本文提出一种基于神经体渲染的新方法，用于动态场景的相机虚拟化，支持高质量、时空一致的新视角合成与时间回溯功能，特别适用于体育赛事等快速运动场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅（3DGS）的方法在处理多主体、非刚性、快速运动（如翻转、跳跃、球员间快速切换）时存在局限，且依赖精确的运动结构（SfM）点云，难以实现高效的时间存档与回溯。因此，亟需一种能兼顾渲染质量、时空一致性与时间回溯能力的新方法。

Method: 作者重新采用神经体渲染框架，将动态场景建模为在给定时刻多个同步相机视角下的刚性变换，并通过神经表示学习提升测试时的渲染质量。该方法支持时间存档，允许用户回溯任意历史时刻并进行新视角合成。

Result: 所提方法在视觉渲染质量上优于现有动态高斯泼溅方法，并首次实现了对动态场景的高效时间存档与回溯渲染，适用于体育直播等需要回放与分析的应用。

Conclusion: 本文提出的神经体渲染方法有效解决了现有动态新视角合成技术在处理复杂运动和时间存档方面的不足，为体育广播等实时视觉应用提供了实用且高效的解决方案。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

Abstract (中文翻译): 相机虚拟化——一种新兴的新视角合成解决方案——通过使用有限数量的经过标定的静态物理相机图像生成逼真的新视角图像，在视觉娱乐、现场表演和体育转播领域具有变革性潜力。尽管近期有所进展，但现有方法在实现具有高效时间存档能力的动态场景的空间和时间一致且逼真的渲染方面仍然面临挑战，尤其是在快节奏的体育赛事和舞台表演中。最近基于3D高斯泼溅（3DGS）的动态场景方法虽能提供实时视图合成结果，但其受限于对运动结构（Structure-from-Motion）方法所生成的精确3D点云的依赖，并且无法处理不同主体的大尺度、非刚性、快速运动（例如翻转、跳跃、关节运动、球员间的突然转换）。此外，多个主体的独立运动会破坏4DGS、ST-GS及其他动态泼溅变体中常用的高斯跟踪假设。本文主张重新考虑用于相机虚拟化的神经体渲染公式，以实现高效的时间存档能力，使其在体育转播及相关应用中更为实用。通过将动态场景建模为在给定时间点跨多个同步相机视角的刚性变换，我们的方法执行神经表示学习，从而在测试时提供增强的视觉渲染质量。我们方法的一个关键贡献在于其支持时间存档，即用户可以回溯动态场景的任意过去时间点并执行新视角合成，从而实现对现场事件的回放、分析和存档的回溯渲染功能，而这一功能在现有的神经渲染方法和新视角合成技术中是缺失的。

</details>


### [7] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 本文首次对长达344K上下文的视觉语言模型进行了大规模训练研究，系统探索了24B和32B参数模型在长文档视觉问答任务中的持续预训练、监督微调和偏好优化策略，在MMLongBenchDoc上达到SOTA，并揭示了若干关键发现，包括上下文长度匹配、页码索引使用、合成数据自提升以及视觉-文本长上下文迁移等。


<details>
  <summary>Details</summary>
Motivation: 当前虽有如Qwen3 VL和GLM 4.5/6V等开源长上下文视觉语言模型，但其训练方法和数据流程不可复现。为填补这一空白，作者开展系统性研究，旨在建立可复现的长上下文视觉语言模型训练范式，并深入理解影响长文档视觉问答性能的关键因素。

Method: 作者对24B和32B参数规模的模型进行系统性实验，涵盖持续预训练（continued pretraining）、监督微调（supervised finetuning）和偏好优化（preference optimization）；构建合成数据管道；在多种长上下文评估设置下进行消融实验；并引入页码索引作为输入特征。

Result: 在MMLongBenchDoc基准上，两个参数规模的模型均取得当前最优性能；发现匹配训练与评估上下文长度更有效；使用页码索引显著提升长文档性能；合成数据支持模型通过持续预训练和微调实现自我改进；首次验证了视觉长上下文训练可反向迁移到纯文本长上下文任务。

Conclusion: 该研究提供了首个可复现的长上下文视觉语言模型训练框架，揭示了提升长文档视觉问答性能的关键技术路径，并证实了视觉与文本长上下文能力之间的双向迁移潜力，为未来多模态长上下文模型发展奠定基础。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

Abstract (中文翻译): 我们提出了首个全面、大规模的长上下文视觉语言模型训练研究，上下文长度最高达344K，聚焦于长文档视觉问答任务，并考察其向长上下文纯文本任务的迁移能力。尽管目前已有若干强大的开源模型（如Qwen3 VL和GLM 4.5/6V），但其训练方案和数据流程不可复现。我们系统地研究了24B和32B参数规模模型的持续预训练、监督微调和偏好优化策略，辅以广泛的长上下文评估和消融实验，以弥合这一差距，并在MMLongBenchDoc基准上对两种参数规模均取得了最先进的性能。此外，我们的关键发现包括：(i) 使用与评估上下文长度相匹配的长度进行训练，优于使用更长上下文进行训练；(ii) 在训练和评估中引入页码索引，能简单而显著地提升长文档性能；(iii) 我们的合成数据管道能够通过持续预训练和监督微调实现模型的自我改进；(iv) 我们将已知的文本到视觉的长上下文迁移扩展到了反向情形，证明了视觉长上下文训练也能迁移到长上下文文本任务的性能提升。我们还发布了MMLBD-C，这是MMLongBenchDoc的一个手动修正版本，旨在减少基准中错误和低质量的样本。

</details>


### [8] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: 提出了一种名为探索-利用蒸馏（E²D）的新方法，在大规模数据集蒸馏中同时实现高准确率和高效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于解耦的大规模数据集蒸馏方法在效率与准确率之间存在权衡：基于优化的方法准确率高但计算开销大，无优化方法效率高但牺牲准确率。本文旨在解决这一权衡问题。

Method: 提出E²D方法，采用高效流水线：首先以全图像初始化保留语义完整性和特征多样性；然后采用两阶段优化策略——探索阶段进行均匀更新并识别高损失区域，利用阶段聚焦于这些区域加速收敛。

Result: 在ImageNet-1K上超越SOTA且快18倍；在ImageNet-21K上显著提升准确率且快4.3倍。

Conclusion: 通过有针对性、减少冗余的更新策略，而非暴力优化，可在大规模数据集蒸馏中有效弥合准确率与效率之间的差距。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

Abstract (中文翻译): 数据集蒸馏将原始数据压缩为紧凑的合成数据集，在减少训练时间和存储需求的同时保持模型性能，从而支持在资源受限环境下的部署。尽管近期基于解耦的蒸馏方法已能实现大规模数据集蒸馏，但仍面临效率差距：基于优化的解耦方法虽然准确率更高，但计算开销巨大；而无需优化的解耦方法虽高效，却牺牲了准确率。为克服这一权衡，我们提出了探索-利用蒸馏（Exploration-Exploitation Distillation, E²D），这是一种简单实用的方法，通过高效的流水线来最小化冗余计算：首先采用全图像初始化以保留语义完整性和特征多样性，随后采用两阶段优化策略——探索阶段执行均匀更新并识别高损失区域，利用阶段则聚焦于这些区域以加速收敛。我们在大规模基准上评估了E²D，在ImageNet-1K上超越了当前最先进方法且速度提升18倍；在ImageNet-21K上，我们的方法在保持4.3倍加速的同时显著提升了准确率。这些结果表明，有针对性地减少冗余更新，而非依赖暴力优化，能够有效弥合大规模数据集蒸馏中准确率与效率之间的鸿沟。代码已开源：https://github.com/ncsu-dk-lab。

</details>


### [9] [Visual Persuasion: What Influences Decisions of Vision-Language Models?](https://arxiv.org/abs/2602.15278)
*Manuel Cherep,Pranav M R,Pattie Maes,Nikhil Singh*

Main category: cs.CV

TL;DR: 本文提出一个框架，通过可控图像选择任务和系统性扰动输入，研究视觉语言模型（VLM）的视觉偏好。利用图像生成模型对常见图像进行视觉上合理的修改，并评估哪些修改会提高被选中的概率。实验表明优化后的编辑能显著改变VLM的选择倾向，并通过自动可解释性流程揭示驱动偏好的一致视觉主题，为AI代理的视觉安全审计提供高效方法。


<details>
  <summary>Details</summary>
Motivation: 当前大量网络图像正被基于视觉语言模型（VLM）的智能体大规模解读并用于决策（如点击、推荐或购买），但我们对其视觉偏好的结构知之甚少。为了主动发现潜在的视觉漏洞与安全风险，有必要建立一种系统性方法来研究和理解这些模型的视觉决策机制。

Method: 作者构建了一个受控图像选择任务框架，将VLM的决策函数视为可通过“显示性偏好”推断的潜在视觉效用。从常见图像（如产品照片）出发，借鉴文本优化方法，提出视觉提示优化策略：利用图像生成模型迭代地提出并应用在构图、光照或背景等方面视觉上合理的修改，再评估这些编辑对选择概率的影响。同时开发自动可解释性流程以识别驱动偏好的视觉主题。

Result: 在前沿VLM上的大规模实验表明，经过优化的图像编辑能显著提升其在两两比较中的被选中概率。自动可解释性分析揭示了若干一致的视觉偏好主题，说明该方法能有效揭示模型的视觉决策逻辑。

Conclusion: 该研究提供了一种实用且高效的手段，用于揭示基于图像的AI智能体的视觉偏好及其潜在安全风险，有助于在实际部署前主动进行审计与治理，避免在真实场景中隐式暴露问题。

Abstract: The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.

Abstract (中文翻译): 网络上充斥着大量图像，这些图像最初是为人类消费而创建的，如今越来越多地被使用视觉语言模型（VLM）的智能体所解读。这些智能体在大规模上做出视觉决策，决定点击、推荐或购买什么。然而，我们对其视觉偏好的结构知之甚少。我们引入了一个研究框架，通过将VLM置于受控的图像选择任务中，并系统性地扰动其输入来探究这一问题。我们的核心思想是将智能体的决策函数视为一种潜在的视觉效用，可通过“显示性偏好”进行推断：即在系统性编辑后的图像之间进行选择。从常见图像（如产品照片）出发，我们提出了视觉提示优化方法，借鉴文本优化技术，利用图像生成模型迭代地提出并应用在构图、光照或背景等方面视觉上合理的修改。随后，我们评估哪些编辑能提高被选中的概率。通过对前沿VLM的大规模实验，我们证明了优化后的编辑在两两比较中能显著改变选择概率。我们还开发了一个自动可解释性流程，用于解释这些偏好，识别出驱动选择的一致性视觉主题。我们认为，该方法为揭示视觉漏洞和安全问题提供了一种实用且高效的方式，这些问题若非如此，可能只能在现实环境中被隐式发现，从而支持对基于图像的AI智能体进行更主动的审计与治理。

</details>


### [10] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出一种用于视频生成的联合采样框架，在提升批次多样性的同时保持时间一致性，避免昂贵的视频解码和反向传播。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成成本高，通常每个提示仅生成少量样本。在低样本情况下，需提高每批样本的多样性，但现有方法常牺牲视频内的时间一致性，且依赖昂贵的解码器反向传播。

Method: 提出一种联合采样框架，先施加多样性驱动的更新，再移除会降低时间一致性目标的成分；使用轻量级潜在空间模型计算多样性和一致性目标，避免图像空间梯度、视频解码及解码器反向传播。

Result: 在最先进的文本到视频流匹配模型上实验表明，该方法在多样性方面与强基线相当，同时显著提升了时间一致性和色彩自然度。

Conclusion: 所提方法有效平衡了视频生成中的多样性与时间一致性，且计算效率高，适用于实际应用。

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

Abstract (中文翻译): 文本到视频生成成本高昂，因此通常每个提示仅生成少量样本。在这种低样本场景下，最大化每批样本的价值需要较高的跨视频多样性。近期的方法虽提升了图像生成的多样性，但在视频生成中往往损害了视频内部的时间一致性，并且需要通过视频解码器进行代价高昂的反向传播。我们提出了一种用于流匹配视频生成器的联合采样框架，在提升批次多样性的同时保留时间一致性。该方法首先应用由多样性驱动的更新，然后仅移除那些会降低时间一致性目标的成分。为避免图像空间梯度，我们在轻量级潜在空间模型中计算多样性和一致性目标，从而避免视频解码和解码器反向传播。在当前最先进的文本到视频流匹配模型上的实验表明，该方法在多样性方面与强大的联合采样基线相当，同时显著改善了时间一致性和色彩自然度。代码将被公开。

</details>
