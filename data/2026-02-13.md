<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration](https://arxiv.org/abs/2602.11214)
*Manuel Hetzel,Kerim Turacan,Hannes Reichert,Konrad Doll,Bernhard Sick*

Main category: cs.CV

TL;DR: 本文提出DD-MDN模型，一种端到端的概率性人类轨迹预测方法，在保证高位置精度的同时，具备校准的不确定性估计和对短观测时长的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注轨迹预测的准确性、社会交互建模和多样性，但对不确定性建模、校准能力以及短观测窗口下的预测性能关注不足，而这些对于路径规划和避障等下游任务至关重要。

Method: DD-MDN结合少样本去噪扩散骨干网络与双混合密度网络，学习自校准的驻留区域和按概率排序的锚点路径，从而生成多样化的轨迹假设，无需预定义锚点或终点。

Result: 在ETH/UCY、SDD、inD和IMPTC数据集上的实验表明，该方法在预测精度、短观测鲁棒性和不确定性建模方面均达到SOTA水平。

Conclusion: DD-MDN有效解决了人类轨迹预测中不确定性校准和短观测鲁棒性的问题，为实际应用如自动驾驶提供了更可靠的支持。

Abstract: Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.

Abstract (中文翻译): 人类轨迹预测（HTF）通过历史轨迹和环境上下文预测未来的人类运动，广泛应用于自动驾驶、智能监控和人机交互等领域。尽管以往的研究聚焦于预测准确性、社会交互建模和多样性，但对不确定性建模、校准能力以及短观测窗口下的预测性能关注较少，而这些对于路径规划和碰撞规避等下游任务至关重要。本文提出DD-MDN，一种端到端的概率性HTF模型，兼具高位置精度、校准的不确定性估计以及对短观测的鲁棒性。该方法采用少样本去噪扩散骨干网络与双混合密度网络，学习自校准的驻留区域和按概率排序的锚点路径，从而生成多样化的轨迹假设，且无需预定义锚点或终点。在ETH/UCY、SDD、inD和IMPTC数据集上的实验表明，该方法在预测精度、短观测鲁棒性和不确定性建模方面均达到当前最优水平。代码已开源：https://github.com/kav-institute/ddmdn。

</details>


### [2] [ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning](https://arxiv.org/abs/2602.11236)
*Yandan Yang,Shuang Zeng,Tong Lin,Xinyuan Chang,Dekang Qi,Junjin Xiao,Haoyun Liu,Ronghan Chen,Yuzhi Chen,Dongjie Huo,Feng Xiong,Xing Wei,Zhiheng Ma,Mu Xu*

Main category: cs.CV

TL;DR: 本文提出ABot-M0框架，通过系统化数据处理、统一预训练和动作流形学习，实现跨多种机器人形态的通用具身智能。


<details>
  <summary>Details</summary>
Motivation: 构建适用于多种硬件平台的通用具身智能体（“一脑多形”）面临数据碎片化、表征不一致和训练目标错位等挑战。

Method: 作者构建了UniACT数据集（含600万轨迹、9500小时数据），并提出动作流形假设与动作流形学习（AML）方法，结合DiT骨干网络直接预测连续动作序列；同时采用双流机制融合VLM语义与几何先验，并集成即插即用的3D模块以增强空间理解。

Result: 实验表明各组件可独立运行且效果叠加，显著提升跨平台知识迁移、泛化能力、动作预测效率与策略稳定性。

Conclusion: ABot-M0为通用具身智能提供了高效、可扩展的框架，代码与数据管道将开源以促进后续研究。

Abstract: Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.

Abstract (中文翻译): 在机器人领域，构建适用于各种硬件平台的通用具身智能体始终是一项核心挑战，通常被称为“一脑多形”范式。当前进展受限于数据碎片化、表征不一致以及训练目标错位等问题。我们提出了ABot-M0框架，该框架建立了一套系统化的数据整理流程，并联合优化模型架构与训练策略，从而实现从异构原始数据到统一高效表征的端到端转换。基于六个公开数据集，我们清洗、标准化并平衡样本，构建了大规模UniACT数据集，包含超过600万条轨迹和9500小时的数据，涵盖多样化的机器人形态与任务场景。统一预训练增强了跨平台和跨任务的知识迁移与泛化能力，支持通用具身智能的发展。为提升动作预测的效率与稳定性，我们提出“动作流形假设”：有效的机器人动作并非分布在整个高维空间中，而是位于一个由物理规律和任务约束所决定的低维光滑流形上。基于此假设，我们引入动作流形学习（Action Manifold Learning, AML），利用DiT骨干网络直接预测干净、连续的动作序列，将学习过程从去噪转变为向可行流形上的投影，从而提升解码速度与策略稳定性。ABot-M0还通过双流机制支持模块化感知，将视觉语言模型（VLM）的语义信息与几何先验相结合，并整合来自即插即用3D模块（如VGGT和Qwen-Image-Edit）的多视角输入，在不修改主干网络的前提下增强空间理解能力，并缓解标准VLM在三维推理方面的局限性。实验表明，各组件可独立运作且具有叠加增益。我们将开源全部代码与数据处理流程，以确保可复现性并推动未来研究。

</details>


### [3] [Toward Reliable Tea Leaf Disease Diagnosis Using Deep Learning Model: Enhancing Robustness With Explainable AI and Adversarial Training](https://arxiv.org/abs/2602.11239)
*Samanta Ghosh,Jannatul Adan Mahi,Shayan Abrar,Md Parvez Mia,Asaduzzaman Rayhan,Abdul Awal Yasir,Asaduzzaman Hridoy*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动化茶树叶病害分类方法，利用茶LeafBD数据集（包含5278张高分辨率图像，分为7类）训练DenseNet201和EfficientNetB3模型，并结合对抗训练与Grad-CAM可视化技术，最终EfficientNetB3达到93%的准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国经济高度依赖茶叶产业，但茶树易受多种叶部病害影响，导致产量下降和品质降低。传统人工检测效率低、易出错，因此亟需一种高效、准确的自动病害识别方法以支持农业管理。

Method: 研究构建了一个完整的深度学习流程，包括数据预处理、划分、对抗训练、数据增强、模型训练与评估，并引入可解释AI（XAI）策略。采用DenseNet201和EfficientNetB3进行分类，通过对抗训练提升模型鲁棒性，并使用Grad-CAM可视化模型决策依据。

Result: 实验结果表明，EfficientNetB3在茶叶病害分类任务中取得了93%的最高准确率，DenseNet201达到91%，验证了所提方法的有效性和实用性。

Conclusion: 该研究成功开发了一种高效、准确且具备可解释性的茶树叶病害自动识别系统，为现代农业管理提供了可行的技术支持，有助于提升茶叶生产的质量与效率。

Abstract: Tea is a valuable asset for the economy of Bangladesh. So, tea cultivation plays an important role to boost the economy. These valuable plants are vulnerable to various kinds of leaf infections which may cause less production and low quality. It is not so easy to detect these diseases manually. It may take time and there could be some errors in the detection.Therefore, the purpose of the study is to develop an automated deep learning model for tea leaf disease classification based on the teaLeafBD dataset so that anyone can detect the diseases more easily and efficiently. There are 5,278 high-resolution images in this dataset. The images are classified into seven categories. Six of them represents various diseases and the rest one represents healthy leaves. The proposed pipeline contains data preprocessing, data splitting, adversarial training, augmentation, model training, evaluation, and comprehension made possible with Explainable AI strategies. DenseNet201 and EfficientNetB3 were employed to perform the classification task. To prepare the model more robustly, we applied adversarial training so it can operate effectively even with noisy or disturbed inputs. In addition, Grad-CAM visualization was executed to analyze the model's predictions by identifying the most influential regions of each image. Our experimental outcomes revealed that EfficientNetB3 achieved the highest classification accuracy of 93%, while DenseNet201 reached 91%. The outcomes prove that the effectiveness of the proposed approach can accurately detect tea leaf diseases and provide a practical solution for advanced agricultural management.

Abstract (中文翻译): 茶叶是孟加拉国经济的重要资产，因此茶树种植在推动经济发展中发挥着关键作用。然而，这些珍贵的植物容易受到多种叶部病害的侵袭，可能导致产量减少和品质下降。人工检测这些病害既困难又耗时，且容易出错。因此，本研究旨在基于teaLeafBD数据集开发一种自动化的深度学习模型，以更轻松高效地识别茶树叶病害。该数据集包含5,278张高分辨率图像，分为七类：其中六类代表不同病害，一类为健康叶片。所提出的流程包括数据预处理、数据划分、对抗训练、数据增强、模型训练、评估以及借助可解释人工智能（Explainable AI）策略实现的模型理解。研究采用了DenseNet201和EfficientNetB3进行分类任务，并通过对抗训练增强模型鲁棒性，使其在面对噪声或扰动输入时仍能有效运行。此外，还应用Grad-CAM可视化技术，通过识别每张图像中最具影响力的区域来分析模型预测。实验结果表明，EfficientNetB3取得了93%的最高分类准确率，而DenseNet201达到了91%。结果证明，所提出的方法能够准确检测茶树叶病害，为先进农业管理提供了一种实用解决方案。

</details>


### [4] [Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration](https://arxiv.org/abs/2602.11241)
*Jinghan He,Junfeng Fang,Feng Xiong,Zijun Yao,Fei Shen,Haiyun Guo,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出Active-Zero框架，通过三个协同演化的智能体（Searcher、Questioner、Solver）实现视觉语言模型在开放世界中的主动探索与自生成课程学习，显著优于现有基于静态图像的自博弈方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的自博弈方法依赖于静态图像集，缺乏主动获取与其能力匹配的视觉数据的能力，导致学习效率低下且严重依赖初始数据集。

Method: 提出Active-Zero框架，包含三个协同演化的智能体：Searcher根据模型能力前沿从开放世界中检索图像，Questioner生成校准后的推理任务，Solver则通过准确率奖励进行优化，形成闭环自生成课程。

Result: 在Qwen2.5-VL-7B-Instruct模型上，Active-Zero在12个基准测试中推理任务平均准确率达53.97%（提升5.7%），通用理解达59.77%（提升3.9%），持续优于现有自博弈基线。

Conclusion: 主动探索是构建可扩展、自适应的自进化视觉语言系统的关键要素。

Abstract: Self-play has enabled large language models to autonomously improve through self-generated challenges. However, existing self-play methods for vision-language models rely on passive interaction with static image collections, resulting in strong dependence on initial datasets and inefficient learning. Without the ability to actively seek visual data tailored to their evolving capabilities, agents waste computational effort on samples that are either trivial or beyond their current skill level. To address these limitations, we propose Active-Zero, a framework that shifts from passive interaction to active exploration of visual environments. Active-Zero employs three co-evolving agents: a Searcher that retrieves images from open-world repositories based on the model's capability frontier, a Questioner that synthesizes calibrated reasoning tasks, and a Solver refined through accuracy rewards. This closed loop enables self-scaffolding auto-curricula where the model autonomously constructs its learning trajectory. On Qwen2.5-VL-7B-Instruct across 12 benchmarks, Active-Zero achieves 53.97 average accuracy on reasoning tasks (5.7% improvement) and 59.77 on general understanding (3.9% improvement), consistently outperforming existing self-play baselines. These results highlight active exploration as a key ingredient for scalable and adaptive self-evolving vision-language systems.

Abstract (中文翻译): 自博弈使大语言模型能够通过自我生成的挑战实现自主提升。然而，现有的视觉语言模型自博弈方法依赖于与静态图像集合的被动交互，导致对初始数据集的强依赖和低效学习。由于无法主动获取与其不断演进能力相匹配的视觉数据，智能体在过于简单或超出当前技能水平的样本上浪费了大量计算资源。为解决这些问题，我们提出了Active-Zero框架，将交互方式从被动转为主动探索视觉环境。Active-Zero采用三个协同演化的智能体：Searcher根据模型的能力前沿从开放世界资源库中检索图像，Questioner合成经过校准的推理任务，而Solver则通过准确率奖励进行优化。这一闭环机制实现了自我引导的自动课程学习，使模型能够自主构建其学习轨迹。在Qwen2.5-VL-7B-Instruct模型上，Active-Zero在12个基准测试中推理任务平均准确率达到53.97%（提升5.7%），通用理解达到59.77%（提升3.9%），始终优于现有的自博弈基线方法。这些结果表明，主动探索是构建可扩展、自适应的自进化视觉语言系统的关键要素。

</details>


### [5] [ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems](https://arxiv.org/abs/2602.11242)
*Yitong Wang,Yue Yao*

Main category: cs.CV

TL;DR: ReTracing 是一个多智能体具身表演艺术项目，通过考古学方法揭示生成式AI如何通过编舞动作编码社会文化偏见，并探讨“在也能移动、思考并留下痕迹的AI中，何以为人”的问题。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能如何塑造、限制并生成人类身体动作，揭示生成式系统中潜藏的社会文化偏见，并反思人类在具备行动与认知能力的AI环境中的身份与意义。

Method: 从科幻小说中提取描述人机交互的句子，利用大语言模型（LLMs）为每段文本生成“应做”与“不应做”的配对提示；再通过基于扩散的文本到视频模型将这些提示转化为人类表演者的编舞指导和四足机器人的运动指令。人类与机器人在镜面地板上同步执行动作，由多摄像头运动捕捉系统记录，并重建为3D点云与运动轨迹，形成数字动作档案。

Result: 成功构建了一个融合AI、人类与机器人互动的沉浸式表演系统，生成了可追溯的数字动作档案，并可视化了生成式AI在动作编排中所嵌入的偏见。

Conclusion: ReTracing 不仅是一种新型的具身艺术实践，也提供了一种批判性工具，用于揭示AI系统如何通过身体动作反映并强化社会文化规范，进而引发对人类主体性在AI时代处境的深层思考。

Abstract: We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts "what to do" and "what not to do" for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?

Abstract (中文翻译): 我们提出了 ReTracing，这是一个多智能体具身表演艺术项目，采用考古学方法，考察人工智能如何塑造、限制并生成身体动作。该项目从科幻小说中提取描述人机交互的句子，利用大语言模型（LLMs）为每段摘录生成“应做”与“不应做”的配对提示。随后，基于扩散的文本到视频模型将这些提示转化为人类表演者的编舞指南和四足机器人的电机指令。人类与机器人在镜面地板上同步执行这些动作，由多摄像头运动追踪系统捕捉，并重建为三维点云与运动轨迹，形成一个动作痕迹的数字档案。通过这一过程，ReTracing 提供了一种新颖的方法，揭示生成式系统如何通过编排动作编码社会文化偏见。在人工智能、人类与机器人的沉浸式互动中，ReTracing 直面当代一个关键问题：在那些同样能移动、思考并留下痕迹的人工智能之中，何以为人？

</details>


### [6] [Stress Tests REVEAL Fragile Temporal and Visual Grounding in Video-Language Models](https://arxiv.org/abs/2602.11244)
*Sethuraman T,Savya Khosla,Aditi Tiwari,Vidya Ganesh,Rakshana Jayaprakash,Aditya Jain,Vignesh Srinivasakumar,Onkar Kishor Susladkar,Srinidhi Sunkara,Aditya Shanmugham,Rakesh Vaideeswaran,Abbaas Alif Mohamed Nishar,Simon Jenni,Derek Hoiem*

Main category: cs.CV

TL;DR: 本文发现当前视频-语言模型（VidLMs）在处理视频内容、时序和运动方面存在严重缺陷，并提出了名为REVEAL的诊断基准，通过五项压力测试揭示这些弱点。


<details>
  <summary>Details</summary>
Motivation: 评估当前视频-语言模型是否真正理解视频中的内容、时间顺序和运动，而非依赖语言捷径或表面线索。

Method: 构建REVEAL诊断基准，包含五个受控压力测试：时间预期偏差、仅依赖语言的捷径、视频谄媚行为、对摄像机运动的敏感性，以及对时空遮挡的鲁棒性；同时提供自动生成诊断样本的数据管道。

Result: 主流开源与闭源VidLMs在多项测试中表现不佳：将倒放视频误认为正向、忽略视频内容作答、认同错误陈述、难以处理基本摄像机运动、无法在简单时空遮挡下整合时序信息；而人类轻松完成这些任务。

Conclusion: 当前VidLMs在基础视频理解能力上存在显著不足，REVEAL基准及配套工具可推动更全面、可扩展的模型评估，作者将公开发布相关资源以支持后续研究。

Abstract: This work investigates a fundamental question: Do Video-Language Models (VidLMs) robustly account for video content, temporal sequence, and motion? Our investigation shows that, surprisingly, they often do not. We introduce REVEAL{}, a diagnostic benchmark that probes fundamental weaknesses of contemporary VidLMs through five controlled stress tests; assessing temporal expectation bias, reliance on language-only shortcuts, video sycophancy, camera motion sensitivity, and robustness to spatiotemporal occlusion. We test leading open- and closed-source VidLMs and find that these models confidently describe reversed scenes as forward, answer questions while neglecting video content, agree with false claims, struggle with basic camera motion, and fail to aggregate temporal information amidst simple spatiotemporal masking. Humans, on the other hand, succeed at these tasks with ease. Alongside our benchmark, we provide a data pipeline that automatically generates diagnostic examples for our stress tests, enabling broader and more scalable evaluation. We will release our benchmark and code to support future research.

Abstract (中文翻译): 本研究探讨了一个基本问题：视频-语言模型（VidLMs）是否能稳健地处理视频内容、时间序列和运动？我们的研究表明，令人惊讶的是，它们往往不能。我们提出了REVEAL{}这一诊断基准，通过五项受控的压力测试来探测当前VidLMs的根本弱点，包括时间预期偏差、仅依赖语言的捷径、视频谄媚行为、对摄像机运动的敏感性，以及对时空遮挡的鲁棒性。我们测试了领先的开源和闭源VidLMs，发现这些模型会自信地将倒放场景描述为正向播放，在回答问题时忽略视频内容，认同错误陈述，难以应对基本的摄像机运动，并且在简单的时空遮挡下无法有效整合时间信息。相比之下，人类能轻松完成这些任务。除基准外，我们还提供了一个数据管道，可自动生成用于压力测试的诊断样例，从而实现更广泛、更具可扩展性的评估。我们将公开发布该基准和代码，以支持未来的研究。

</details>


### [7] [Advancing Digital Twin Generation Through a Novel Simulation Framework and Quantitative Benchmarking](https://arxiv.org/abs/2602.11314)
*Jacob Rubinstein,Avi Donaty,Don Engel*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过高质量3D模型和程序生成的相机姿态合成图像，用于可重复、可量化的数字孪生重建评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于摄影测量的3D建模方法在生成数字孪生时存在多种设计选择，但其效果多依赖主观定性评价，缺乏可重复、可量化的评估手段。

Method: 构建一个新流程：从高质量3D模型和程序化生成的虚拟相机姿态出发，合成逼真2D图像；利用这些图像进行重建，并将重建结果与已知的虚拟相机参数和物体真实信息进行对比。

Result: 该方法支持大量可重复且可量化的实验，能够精确评估不同重建方法在视角和对象重建上的性能。

Conclusion: 所提出的合成图像生成与评估框架为数字孪生重建提供了客观、可量化的基准测试手段。

Abstract: The generation of 3D models from real-world objects has often been accomplished through photogrammetry, i.e., by taking 2D photos from a variety of perspectives and then triangulating matched point-based features to create a textured mesh. Many design choices exist within this framework for the generation of digital twins, and differences between such approaches are largely judged qualitatively. Here, we present and test a novel pipeline for generating synthetic images from high-quality 3D models and programmatically generated camera poses. This enables a wide variety of repeatable, quantifiable experiments which can compare ground-truth knowledge of virtual camera parameters and of virtual objects against the reconstructed estimations of those perspectives and subjects.

Abstract (中文翻译): 从现实世界物体生成3D模型通常通过摄影测量法实现，即从多个视角拍摄2D照片，然后通过三角化匹配的点特征来创建带纹理的网格。在此框架下，生成数字孪生体存在多种设计选择，而这些方法之间的差异主要依靠定性判断。本文提出并测试了一种新颖的流程，该流程利用高质量3D模型和程序化生成的相机姿态来生成合成图像。这使得可以开展大量可重复、可量化的实验，将重建所得的视角和对象估计结果与虚拟相机参数及虚拟物体的真实信息（ground truth）进行对比。

</details>


### [8] [Selective Prior Synchronization via SYNC Loss](https://arxiv.org/abs/2602.11316)
*Ishan Mishra,Jiajie Li,Deepak Mishra,Jinjun Xiong*

Main category: cs.CV

TL;DR: 本文提出SYNC loss，将后验（post-hoc）方法中的softmax响应引入到事前（ad-hoc）方法SelectiveNet的训练过程中，利用选择先验（selective prior）提升模型在不确定情况下的选择性预测性能，在多个数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测方法分为两类：事前方法（如SelectiveNet）修改网络结构或目标函数，后验方法（如softmax响应）分析模型输出概率。作者发现后验方法隐含生成的选择先验在推理阶段被使用，但未在训练中利用，而该信息对提升选择性预测能力同样关键。

Method: 提出SYNC loss，将softmax响应（一种后验方法）所生成的选择先验整合进SelectiveNet（一种事前方法）的训练过程，从而在训练阶段利用选择先验来增强模型的选择性预测能力。

Result: 在CIFAR-100、ImageNet-100和Stanford Cars等多个数据集上，所提方法不仅提升了模型泛化能力，还在选择性预测任务上超越了先前工作，达到新的SOTA水平。

Conclusion: 将后验方法中的选择先验引入事前方法的训练过程是有效且必要的，SYNC loss成功融合两类方法优势，显著提升了选择性预测性能。

Abstract: Prediction under uncertainty is a critical requirement for the deep neural network to succeed responsibly. This paper focuses on selective prediction, which allows DNNs to make informed decisions about when to predict or abstain based on the uncertainty level of their predictions. Current methods are either ad-hoc such as SelectiveNet, focusing on how to modify the network architecture or objective function, or post-hoc such as softmax response, achieving selective prediction through analyzing the model's probabilistic outputs. We observe that post-hoc methods implicitly generate uncertainty information, termed the selective prior, which has traditionally been used only during inference. We argue that the selective prior provided by the selection mechanism is equally vital during the training stage. Therefore, we propose the SYNC loss which introduces a novel integration of ad-hoc and post-hoc method. Specifically, our approach incorporates the softmax response into the training process of SelectiveNet, enhancing its selective prediction capabilities by examining the selective prior. Evaluated across various datasets, including CIFAR-100, ImageNet-100, and Stanford Cars, our method not only enhances the model's generalization capabilities but also surpasses previous works in selective prediction performance, and sets new benchmarks for state-of-the-art performance.

Abstract (中文翻译): 在不确定性下进行预测是深度神经网络负责任地取得成功的关键要求。本文聚焦于选择性预测，即允许深度神经网络根据其预测的不确定性水平，自主决定是否进行预测或放弃预测。当前的方法要么是事前（ad-hoc）的，例如SelectiveNet，侧重于修改网络架构或目标函数；要么是后验（post-hoc）的，例如softmax响应，通过分析模型的概率输出来实现选择性预测。我们观察到，后验方法隐式地生成了一种称为“选择先验”（selective prior）的不确定性信息，传统上仅在推理阶段使用。我们认为，由选择机制提供的选择先验在训练阶段同样至关重要。因此，我们提出了SYNC loss，这是一种新颖的事前与后验方法的结合方式。具体而言，我们的方法将softmax响应整合进SelectiveNet的训练过程中，通过考察选择先验来增强其选择性预测能力。在包括CIFAR-100、ImageNet-100和Stanford Cars在内的多个数据集上的评估表明，我们的方法不仅增强了模型的泛化能力，还在选择性预测性能上超越了以往的工作，树立了新的最先进性能基准。

</details>


### [9] [MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors](https://arxiv.org/abs/2602.11323)
*Arda Alniak,Sinan Kalkan,Mustafa Mert Ankarali,Afsar Saranli,Abdullah Aydin Alatan*

Main category: cs.CV

TL;DR: 本文提出一种将基于Vision Transformer的稠密深度估计集成到VINS-Mono后端的新框架，在保持边缘设备计算限制的同时，通过仿射不变深度一致性和序数约束显著提升低纹理环境下的VIO精度。


<details>
  <summary>Details</summary>
Motivation: 传统单目视觉惯性里程计（VIO）在低纹理环境中因稀疏视觉特征不足而难以准确估计位姿；虽然基于ViT的稠密深度估计模型能提供几何一致的深度信息，但其高计算开销阻碍了在边缘设备上的实时部署。

Method: 将学习到的深度先验直接集成到VINS-Mono优化后端中，引入仿射不变深度一致性约束和成对序数约束，并通过基于方差的门控机制显式过滤不稳定伪影。

Result: 在TartanGround和M3ED数据集上的实验表明，该方法在挑战性场景中有效防止轨迹发散，绝对轨迹误差（ATE）最多降低28.3%。

Conclusion: 所提方法在满足边缘设备计算限制的前提下，显著提升了单目VIO在低纹理环境中的鲁棒性和精度，成功弥合了先进深度估计模型与实时VIO系统之间的鸿沟。

Abstract: Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.

Abstract (中文翻译): 传统的单目视觉惯性里程计（VIO）系统在低纹理环境中表现不佳，因为稀疏的视觉特征不足以实现精确的位姿估计。为解决这一问题，稠密单目深度估计（MDE）被广泛探索作为补充信息源。尽管近期基于视觉Transformer（ViT）的复杂基础模型能够提供稠密且几何一致的深度信息，但其高昂的计算需求通常使其无法在边缘设备上实现实时部署。我们的工作通过将学习到的深度先验直接集成到VINS-Mono优化后端中，弥合了这一差距。我们提出了一种新颖的框架，该框架施加仿射不变的深度一致性约束和成对序数约束，并通过基于方差的门控机制显式滤除不稳定的伪影。该方法严格遵守边缘设备的计算限制，同时稳健地恢复度量尺度。在TartanGround和M3ED数据集上的大量实验表明，我们的方法在具有挑战性的场景中有效防止了轨迹发散，并显著提升了精度，绝对轨迹误差（ATE）最多降低了28.3%。代码将公开发布。

</details>


### [10] [Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content](https://arxiv.org/abs/2602.11339)
*Evgeney Bogatyrev,Khaled Abud,Ivan Molodetskikh,Nikita Alutis,Dmitry Vatolin*

Main category: cs.CV

TL;DR: 该论文提出了一个面向流媒体视频的超分辨率新数据集StreamSR，并引入了一个高效实时模型EfRLFN，通过注意力机制和新型激活函数提升性能；同时证明在该数据集上微调可显著提升其他模型在多个基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有实时超分辨率方法在压缩视频内容上表现不佳，且常用数据集无法准确反映流媒体视频的真实特性，导致当前基准缺乏现实相关性。

Method: 作者构建了源自YouTube的StreamSR数据集，涵盖多种视频类型和分辨率；提出EfRLFN模型，融合高效通道注意力机制与双曲正切激活函数，并设计复合损失函数优化训练；同时对11种SOTA模型进行基准测试，并在StreamSR上微调以评估泛化能力。

Result: EfRLFN在视觉质量和运行效率上优于现有方法；在StreamSR上微调其他模型能带来显著且泛化良好的性能提升。

Conclusion: StreamSR数据集更贴近真实流媒体场景，EfRLFN是一种高效且有效的实时超分辨率模型，所提出的训练策略和数据集对推动该领域发展具有重要意义。

Abstract: Recent advancements in real-time super-resolution have enabled higher-quality video streaming, yet existing methods struggle with the unique challenges of compressed video content. Commonly used datasets do not accurately reflect the characteristics of streaming media, limiting the relevance of current benchmarks. To address this gap, we introduce a comprehensive dataset - StreamSR - sourced from YouTube, covering a wide range of video genres and resolutions representative of real-world streaming scenarios. We benchmark 11 state-of-the-art real-time super-resolution models to evaluate their performance for the streaming use-case.
  Furthermore, we propose EfRLFN, an efficient real-time model that integrates Efficient Channel Attention and a hyperbolic tangent activation function - a novel design choice in the context of real-time super-resolution. We extensively optimized the architecture to maximize efficiency and designed a composite loss function that improves training convergence. EfRLFN combines the strengths of existing architectures while improving both visual quality and runtime performance.
  Finally, we show that fine-tuning other models on our dataset results in significant performance gains that generalize well across various standard benchmarks. We made the dataset, the code, and the benchmark available at https://github.com/EvgeneyBogatyrev/EfRLFN.

Abstract (中文翻译): 近期实时超分辨率技术的进步使得高质量视频流成为可能，但现有方法在处理压缩视频内容时仍面临独特挑战。常用数据集无法准确反映流媒体视频的特性，限制了当前基准的相关性。为弥补这一差距，我们提出了一个全面的数据集——StreamSR，其来源于YouTube，涵盖广泛的真实流媒体视频类型和分辨率。我们对11种最先进的实时超分辨率模型进行了基准测试，以评估它们在流媒体应用场景中的表现。此外，我们提出了EfRLFN，一种高效的实时模型，它结合了高效通道注意力机制和双曲正切激活函数——这是实时超分辨率领域的一项新颖设计。我们对架构进行了大量优化以最大化效率，并设计了一种复合损失函数以改善训练收敛性。EfRLFN融合了现有架构的优点，在视觉质量和运行时性能方面均有提升。最后，我们证明在我们的数据集上微调其他模型可带来显著的性能提升，且这种提升在多个标准基准上具有良好的泛化能力。我们已在https://github.com/EvgeneyBogatyrev/EfRLFN公开了数据集、代码和基准。

</details>
