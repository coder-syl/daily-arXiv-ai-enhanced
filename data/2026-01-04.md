<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments](https://arxiv.org/abs/2512.23786)
*Ankan Aich,Yangming Lee*

Main category: cs.CV

TL;DR: 本文提出一种结合Depth Anything V2高保真合成先验与动态向量低秩适配（DV-LORA）的方法，在内窥镜手术场景中实现更鲁棒的单目深度估计，尤其在高反光区域表现优异，并在SCARED数据集上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自监督单目深度估计方法在内窥镜手术环境中因依赖含噪真实世界伪标签，难以准确处理细长器械和透明表面，且在高反光条件下性能下降。

Method: 利用Depth Anything V2架构提供的高保真合成先验，通过动态向量低秩适配（DV-LORA）高效迁移到医疗领域，并设计物理分层评估协议以更细致地评测高反光区域性能。

Result: 在SCARED数据集上达到98.1%的准确率（δ < 1.25），相对平方误差降低超17%，显著优于现有基线方法。

Conclusion: 所提方法有效缓解了手术场景中反光和透明结构带来的深度估计难题，展现出在恶劣光照条件下的卓越鲁棒性，为机器人手术提供了更可靠的深度感知能力。

Abstract: Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.

Abstract (中文翻译): 精确的单目深度估计（MDE）对机器人手术至关重要，但在充满液体且具有镜面反射的内窥镜环境中仍十分脆弱。现有的自监督方法通常依赖于使用含噪的真实世界伪标签训练的基础模型，往往在细长手术器械和透明表面上出现边界塌陷问题。本文通过利用Depth Anything V2架构所提供的高保真合成先验来解决这一问题，该先验能够天然捕捉细小结构的精确几何细节。我们采用动态向量低秩适配（DV-LORA）技术，高效地将这些先验知识迁移到医学领域，在最小化参数开销的同时弥合合成到真实之间的差距。此外，我们在SCARED数据集上引入了一种物理分层的评估协议，以严格量化在常被整体指标掩盖的高反光区域中的性能表现。我们的方法建立了新的最先进水平，在准确率（< 1.25）上达到98.1%，并将平方相对误差降低了超过17%，相较于现有基线方法展现出在恶劣手术光照条件下的卓越鲁棒性。

</details>


### [2] [Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments](https://arxiv.org/abs/2512.23819)
*Surya Rayala,Marcos Quinones-Grueiro,Naveeduddin Mohammed,Ashwin T S,Benjamin Goldberg,Randall Spain,Paige Lawton,Gautam Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的自动评估系统，用于城市作战训练中的“进入并清房”（ECR）演练，通过计算机视觉技术从训练视频中提取人体姿态、视线方向和运动轨迹，构建任务特定指标以量化认知、心理运动技能和团队协作表现，并整合到扩展的认知任务分析（CTA）框架中生成综合评分，支持在GIFT等平台中进行可操作的复盘反馈。


<details>
  <summary>Details</summary>
Motivation: 当前军事合成训练环境中缺乏客观、可扩展的自动绩效评估手段，传统方法依赖昂贵侵入式传感器或主观人工观察，难以准确衡量认知、心理运动和团队协作能力。

Method: 利用计算机视觉模型从训练视频中提取2D骨架、视线向量和运动轨迹，构建任务特定指标（如心理运动流畅性、态势感知、团队协调），并将其整合到加权的扩展认知任务分析（CTA）层次结构中，生成团队与认知维度的综合绩效评分。

Result: 在真实ECR演练案例中验证了该方法，生成了可操作的领域特定指标，能有效捕捉个体与团队表现，并通过Gamemaster和GIFT平台的交互式仪表板支持行动后复盘。

Conclusion: 该视频分析方法为合成训练环境提供了无需额外硬件的可扩展评估方案，但仍面临追踪精度、真值验证和泛化能力等挑战；未来工作将拓展至3D视频分析并进一步提升在STE中的评估能力。

Abstract: Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.

Abstract (中文翻译): 有效的城市作战训练需要通过在逼真且受控的环境中反复练习来培养态势感知能力和肌肉记忆。其中一项关键训练科目是“进入并清房”（Enter and Clear the Room, ECR），要求参训人员进行威胁评估、协同配合并控制密闭空间。军方采用合成训练环境（Synthetic Training Environments, STEs）来提供可扩展、受控的场景以支持重复训练。然而，实现自动化的绩效评估仍具挑战性，尤其是在客观评价认知能力、心理运动技能和团队协作方面。传统方法通常依赖昂贵且侵入式的传感器或主观的人工观察，限制了评估的可扩展性和准确性。本文提出了一种基于视频的评估流程，仅利用训练视频即可生成绩效分析结果，无需额外硬件。该系统利用计算机视觉模型提取2D人体骨架、视线向量和运动轨迹，并基于这些数据开发出针对任务的指标，用以衡量心理运动流畅性、态势感知和团队协调能力。这些指标被整合进一个扩展的认知任务分析（Cognitive Task Analysis, CTA）层级结构中，通过加权组合生成团队协作与认知能力的总体绩效评分。我们通过真实世界ECR演练的案例研究展示了该方法的有效性，提供了可操作的、领域特定的指标以捕捉个体与团队表现。此外，我们还探讨了如何将这些洞察集成到Gamemaster和通用智能辅导框架（GIFT）的交互式仪表板中，以提供直观易懂的行动后复盘反馈。最后，我们讨论了当前方法的局限性，包括目标追踪困难、真实标签验证以及方法的广泛适用性问题。未来工作将扩展至3D视频数据分析，并进一步利用视频分析技术实现合成训练环境中可扩展的绩效评估。

</details>


### [3] [Pretraining Frame Preservation in Autoregressive Video Memory Compression](https://arxiv.org/abs/2512.23851)
*Lvmin Zhang,Shengqu Cai,Muyang Li,Chong Zeng,Beijia Lu,Anyi Rao,Song Han,Gordon Wetzstein,Maneesh Agrawala*

Main category: cs.CV

TL;DR: 提出PFP神经网络结构，用于将长视频压缩为短上下文，保留任意时间位置的高频率细节，可作为自回归视频模型的记忆编码器。


<details>
  <summary>Details</summary>
Motivation: 现有视频压缩方法在保留高频率细节和实现高效记忆方面存在不足，需要一种能有效压缩长视频同时保持帧级细节的方法。

Method: 设计PFP神经网络结构，通过显式的预训练目标将长视频压缩为短上下文，并保留任意时间点的高频率细节；预训练后的模型可直接微调为自回归视频模型的记忆编码器。

Result: 基线模型可将20秒视频压缩至约5k长度的上下文，并能以感知上一致的外观随机检索帧；在降低上下文成本的同时保持较低的保真度损失。

Conclusion: PFP框架在长视频压缩与细节保留之间取得良好平衡，适合作为自回归视频模型的高效记忆模块，并对不同神经架构设计的权衡进行了讨论。

Abstract: We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.

Abstract (中文翻译): 我们提出了PFP，一种用于将长视频压缩为短上下文的神经网络结构，其显式的预训练目标旨在保留任意时间位置单帧的高频细节。基础模型能够将一段20秒的视频压缩为长度约为5k的上下文，并可在其中以感知上一致的外观随机检索任意帧。此类预训练模型可直接微调为自回归视频模型的记忆编码器，在显著降低上下文开销的同时，仅带来相对较小的保真度损失。我们通过消融实验评估了该框架，并讨论了不同神经架构设计之间的权衡。

</details>


### [4] [Lifelong Domain Adaptive 3D Human Pose Estimation](https://arxiv.org/abs/2512.23860)
*Qucheng Peng,Hongfei Xue,Pu Wang,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种面向3D人体姿态估计（3D HPE）的终身域自适应新任务，并设计了一个结合3D姿态生成器、2D姿态判别器和3D姿态估计器的GAN框架，以应对目标域非平稳性和灾难性遗忘问题，在多个数据集上验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计方法依赖于受控环境中采集的标注3D数据，难以泛化到多样化的野外场景；而当前的域自适应方法（如通用DA和无源DA）忽略了目标姿态数据集的非平稳性问题。

Method: 提出终身域自适应3D HPE任务，并构建一个创新的GAN框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器；其中3D姿态生成器融合了姿态感知、时序感知和域感知知识，用于缓解域偏移、增强当前域适应能力并减轻灾难性遗忘。

Result: 在多个域自适应3D HPE数据集上的大量实验表明，所提方法性能优越。

Conclusion: 该工作首次将终身域自适应引入3D人体姿态估计任务，有效解决了目标域非平稳性和知识遗忘问题，为实际应用中的持续学习提供了新思路。

Abstract: 3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.

Abstract (中文翻译): 3D人体姿态估计（3D HPE）在人员重识别、动作识别到虚拟现实等多种应用中至关重要。然而，现有方法依赖于在受控环境中收集的标注3D数据，这使其难以泛化到多样化的野外场景。当前针对3D HPE的域自适应（DA）范式（如通用DA和无源DA）忽视了目标姿态数据集非平稳性的问题。为应对这些挑战，我们提出了一项名为“终身域自适应3D HPE”的新任务。据我们所知，这是首次将终身域自适应引入3D HPE任务。在此设定下，姿态估计器首先在源域上预训练，随后依次适应到不同的目标域；在适应当前目标域时，模型无法访问源域及所有先前的目标域。该任务需同时解决对当前域姿态的适应以及保留先前域知识（尤其是对抗灾难性遗忘）两大挑战。我们提出了一种创新的生成对抗网络（GAN）框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器，有效缓解域偏移并对齐原始与增强姿态。此外，我们构建了一种新颖的3D姿态生成器范式，融合了姿态感知、时序感知和域感知知识，以增强当前域的适应能力并缓解对先前域的灾难性遗忘。大量实验在多个域自适应3D HPE数据集上验证了本方法的优越性能。

</details>


### [5] [MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework](https://arxiv.org/abs/2512.23894)
*Krithika Iyer,Austin Tapp,Athelia Paulli,Gabrielle Dickerson,Syed Muhammad Anwar,Natasha Lepore,Marius George Linguraru*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的方法，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），从而实现颅骨分割和颅缝分割，克服了MRI无法清晰显示颅缝和骨密度的局限性。


<details>
  <summary>Details</summary>
Motivation: 量化儿童颅骨发育和颅缝骨化对诊断和治疗头颅生长相关疾病至关重要。虽然CT常用于评估颅骨和颅缝畸形，但其电离辐射不适合无明显异常的儿童；而MRI虽无辐射且软组织对比度高，却难以显示颅缝、估计骨密度或评估颅顶生长。

Method: 采用深度学习驱动的流程，利用变分自编码器将0.2–2岁儿童的T1加权MRI转化为合成CT（sCT），并在此基础上进行颅骨分割、生成颅缝概率热图，并从中提取颅缝分割结果。

Result: 在内部儿科数据集上，合成CT与真实CT的结构相似度达99%，Frechet inception distance为1.01；七块颅骨的平均Dice系数为85%，颅缝Dice系数为80%；通过TOST检验（p < 0.05）确认了sCT与真实CT在颅骨和颅缝分割上的等效性。

Conclusion: 这是首个能在MRI衍生的合成CT上实现颅缝分割的儿科颅骨CT合成框架，通过结合领域特定的变分自编码器，从常规儿科MRI中生成视觉上难以区分的颅骨合成CT，填补了无创颅骨评估的关键空白。

Abstract: Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.

Abstract (中文翻译): 量化规范化的儿童颅骨发育和颅缝骨化对于诊断和治疗与生长相关的头颅疾病至关重要。计算机断层扫描（CT）广泛用于评估颅骨和颅缝畸形，但其电离辐射不适用于无明显异常的儿童。磁共振成像（MRI）提供无辐射扫描且具有优异的软组织对比度，但与CT不同，MRI无法清晰显示颅缝、估算颅骨密度或评估颅顶生长。本研究提出了一种基于深度学习的流程，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），预测详细的颅骨分割结果，生成颅缝概率热图，并从中直接获得颅缝分割。在我们内部的儿科数据集上，合成CT与真实CT相比达到了99%的结构相似度，Fréchet Inception Distance为1.01；七块颅骨的平均Dice系数为85%，颅缝Dice系数为80%；通过双单侧检验（TOST，p < 0.05）确认了合成CT与真实CT在颅骨和颅缝分割上的等效性。据我们所知，这是首个能够在MRI衍生的合成CT上实现颅缝分割的儿科颅骨CT合成框架，尽管MRI对骨骼和颅缝的描绘能力有限。通过结合鲁棒的、领域特定的变分自编码器，我们的方法能够从常规儿科MRI中生成视觉上难以区分的颅骨合成CT，弥合了无创颅骨评估中的关键缺口。

</details>


### [6] [Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale](https://arxiv.org/abs/2512.23903)
*Charith Wickrema,Eliza Mace,Hunter Brown,Heidys Cabrera,Nick Krall,Matthew O'Neill,Shivangi Sarkar,Lowell Weissman,Eric Hughes,Guido Zarrella*

Main category: cs.CV

TL;DR: 该论文研究了在遥感领域使用超大规模高分辨率电光（EO）数据训练基础模型的缩放行为，发现即使在千万亿像素级别，模型性能仍受数据限制而非模型参数限制，为未来遥感基础模型的数据采集、算力预算和优化策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 当前多模态机器学习（如生成式AI）依赖于针对非文本模态的专用编码器。虽然自然图像领域已有成熟的缩放定律，但在遥感等高价值领域，模型容量、计算资源与数据规模之间的关系尚不明确，亟需探索适用于遥感数据的缩放规律。

Method: 利用超过千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练更大规模的视觉Transformer（ViT）骨干网络，在拍字节级（petascale）尺度上观察其成功与失败模式，并分析跨遥感模态的领域差距问题。

Result: 实验表明，即使在如此大规模下，模型性能仍处于数据受限状态，而非模型参数受限状态。

Conclusion: 研究结果为遥感领域基础模型的未来发展提供了实用指导，包括数据采集策略、算力预算分配和优化调度方案。

Abstract: We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.

Abstract (中文翻译): 我们探索了人工智能的缩放行为，以建立在高分辨率电光（EO）数据集上训练基础模型的实用技术，这些数据集的规模比当前最先进的水平高出数个数量级。现代多模态机器学习（ML）应用，例如用于图像描述、搜索和推理的生成式人工智能（GenAI）系统，依赖于针对非文本模态的鲁棒且领域专用的编码器。在互联网规模数据丰富的自然图像领域，已有成熟的缩放定律可用于优化模型容量、训练计算量和数据集规模的联合缩放。然而，在遥感（RS）等高价值领域，这些关系远未被充分理解。我们利用超过千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练越来越大的视觉Transformer（ViT）骨干网络，报告在拍字节级规模下观察到的成功与失败模式，并分析其对弥合其他遥感模态之间领域差距的启示。我们观察到，即使在此规模下，模型性能仍符合数据受限状态，而非模型参数受限状态。这些实践洞见旨在为数据采集策略、算力预算和优化调度提供参考，以推动前沿遥感基础模型的未来发展。

</details>


### [7] [Learning to learn skill assessment for fetal ultrasound scanning](https://arxiv.org/abs/2512.23920)
*Yipei Wang,Qianye Yang,Lior Drukker,Aris T. Papageorghiou,Yipeng Hu,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出一种无需人工标注技能等级的双层优化框架，通过评估胎儿超声图像上临床任务的完成质量来自动量化操作者技能水平。


<details>
  <summary>Details</summary>
Motivation: 传统超声技能评估依赖专家主观判断，耗时且缺乏客观标准；现有自动化方法多采用监督学习，局限于预设的技能影响因素。

Method: 提出一种双层优化框架，包含临床任务预测器和技能预测器，两者联合优化，通过任务执行效果间接评估技能，无需人工标注技能等级。

Result: 在真实胎儿头部超声扫描视频上验证了该方法的有效性，结果表明可通过优化后的任务表现指标有效预测操作者技能。

Conclusion: 所提框架能以任务完成质量作为技能指标，实现对胎儿超声操作技能的自动化、客观化评估。

Abstract: Traditionally, ultrasound skill assessment has relied on expert supervision and feedback, a process known for its subjectivity and time-intensive nature. Previous works on quantitative and automated skill assessment have predominantly employed supervised learning methods, often limiting the analysis to predetermined or assumed factors considered influential in determining skill levels. In this work, we propose a novel bi-level optimisation framework that assesses fetal ultrasound skills by how well a task is performed on the acquired fetal ultrasound images, without using manually predefined skill ratings. The framework consists of a clinical task predictor and a skill predictor, which are optimised jointly by refining the two networks simultaneously. We validate the proposed method on real-world clinical ultrasound videos of scanning the fetal head. The results demonstrate the feasibility of predicting ultrasound skills by the proposed framework, which quantifies optimised task performance as a skill indicator.

Abstract (中文翻译): 传统上，超声技能评估依赖专家监督与反馈，这一过程具有主观性强且耗时的特点。以往关于定量和自动化技能评估的研究主要采用监督学习方法，通常将分析局限于预先设定或假设的影响技能水平的因素。本文提出一种新颖的双层优化框架，通过评估所获取胎儿超声图像上任务的执行效果来评估胎儿超声技能，而无需使用人工预定义的技能评分。该框架包含一个临床任务预测器和一个技能预测器，通过同时优化两个网络进行联合训练。我们在真实的胎儿头部扫描临床超声视频上验证了所提出的方法。结果表明，该框架可通过量化优化后的任务表现作为技能指标，有效预测超声操作技能。

</details>


### [8] [MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation](https://arxiv.org/abs/2512.23936)
*Yulong Zou,Bo Liu,Cun-Jing Zheng,Yuan-ming Geng,Siyue Li,Qiankun Zuo,Shuihua Wang,Yudong Zhang,Jin Hong*

Main category: cs.CV

TL;DR: 本文提出了一种名为MGML的新框架，用于在多模态MRI数据缺失情况下提升脑肿瘤分割性能。该方法包含元参数化自适应模态融合（Meta-AMF）和一致性正则化模块，无需修改原始模型结构即可端到端训练，在BraTS2020和BraTS2023数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床中多模态MRI数据常不完整，限制了其在病灶分割（尤其是脑肿瘤）中的充分利用，因此如何最大化利用不完整的多模态信息成为关键挑战。

Method: 提出元引导多模态学习（MGML）框架，包括：1）元参数化自适应模态融合（Meta-AMF），根据可用模态生成自适应软标签监督信号以促进模态融合；2）一致性正则化模块，提升分割性能并增强模型鲁棒性与泛化能力。该方法不改变原模型架构，可端到端集成训练。

Result: 在BraTS2020和BraTS2023数据集上的实验表明，该方法优于多种SOTA方法。在BraTS2020上，针对15种缺失模态组合的平均Dice分数，WT、TC和ET分别达到87.55、79.36和62.67。

Conclusion: 所提出的MGML框架有效解决了多模态MRI缺失下的脑肿瘤分割问题，具有良好的性能、鲁棒性和通用性，且易于集成到现有训练流程中。

Abstract: Leveraging multimodal information from Magnetic Resonance Imaging (MRI) plays a vital role in lesion segmentation, especially for brain tumors. However, in clinical practice, multimodal MRI data are often incomplete, making it challenging to fully utilize the available information. Therefore, maximizing the utilization of this incomplete multimodal information presents a crucial research challenge. We present a novel meta-guided multi-modal learning (MGML) framework that comprises two components: meta-parameterized adaptive modality fusion and consistency regularization module. The meta-parameterized adaptive modality fusion (Meta-AMF) enables the model to effectively integrate information from multiple modalities under varying input conditions. By generating adaptive soft-label supervision signals based on the available modalities, Meta-AMF explicitly promotes more coherent multimodal fusion. In addition, the consistency regularization module enhances segmentation performance and implicitly reinforces the robustness and generalization of the overall framework. Notably, our approach does not alter the original model architecture and can be conveniently integrated into the training pipeline for end-to-end model optimization. We conducted extensive experiments on the public BraTS2020 and BraTS2023 datasets. Compared to multiple state-of-the-art methods from previous years, our method achieved superior performance. On BraTS2020, for the average Dice scores across fifteen missing modality combinations, building upon the baseline, our method obtained scores of 87.55, 79.36, and 62.67 for the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET), respectively. We have made our source code publicly available at https://github.com/worldlikerr/MGML.

Abstract (中文翻译): 利用磁共振成像（MRI）的多模态信息在病灶分割（尤其是脑肿瘤）中起着至关重要的作用。然而在临床实践中，多模态MRI数据常常不完整，使得难以充分利用已有信息。因此，如何最大化利用这种不完整的多模态信息成为一个关键的研究挑战。我们提出了一种新颖的元引导多模态学习（MGML）框架，该框架包含两个组成部分：元参数化自适应模态融合模块和一致性正则化模块。元参数化自适应模态融合（Meta-AMF）使模型能够在不同输入条件下有效整合多模态信息。通过根据可用模态生成自适应的软标签监督信号，Meta-AMF显式地促进更一致的多模态融合。此外，一致性正则化模块提升了分割性能，并隐式增强了整个框架的鲁棒性与泛化能力。值得注意的是，我们的方法不改变原始模型架构，可方便地集成到端到端模型优化的训练流程中。我们在公开的BraTS2020和BraTS2023数据集上进行了大量实验。与往年多种先进方法相比，我们的方法取得了更优的性能。在BraTS2020上，针对十五种缺失模态组合的平均Dice分数，相较于基线方法，我们的方法在全肿瘤（WT）、肿瘤核心（TC）和增强肿瘤（ET）上分别获得了87.55、79.36和62.67的得分。我们已在https://github.com/worldlikerr/MGML公开源代码。

</details>


### [9] [Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation](https://arxiv.org/abs/2512.23938)
*Hualin Ye,Bingxi Liu,Jixiang Du,Yu Qin,Ziyi Chen,Hong Zhang*

Main category: cs.CV

TL;DR: 本文提出一种新型跨视角地理定位（CVGL）系统，通过DINOv2骨干网络、多尺度通道重分配模块和基于MoE的聚合模块，在减少参数量的同时取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 跨视角地理定位中，由于查询图像与数据库图像之间存在显著视角差异，导致特征聚合与对齐困难，影响定位精度。

Method: 方法包含三点改进：1）采用DINOv2骨干网络并结合卷积适配器进行微调；2）设计多尺度通道重分配模块以增强空间表示的多样性与稳定性；3）在特征聚合中引入混合专家（MoE）机制，动态选择专家子空间处理键值对。

Result: 在University-1652和SUES-200数据集上的实验表明，该方法以更少的训练参数实现了具有竞争力的性能。

Conclusion: 所提出的CVGL系统有效缓解了视角差异带来的挑战，在保持模型轻量化的同时提升了跨视角地理定位的准确性。

Abstract: Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.

Abstract (中文翻译): 跨视角地理定位（CVGL）旨在通过将查询图像与大规模数据库中的图像进行匹配，来估计其地理位置。然而，显著的视角差异给有效的特征聚合与对齐带来了巨大挑战。为应对这些挑战，我们提出了一种新颖的CVGL系统，包含三项关键改进：首先，利用DINOv2骨干网络并结合卷积适配器进行微调，以增强模型对跨视角变化的适应能力；其次，提出一个多尺度通道重分配模块，以增强空间表示的多样性与稳定性；最后，设计了一个改进的聚合模块，将混合专家（MoE）路由机制融入特征聚合过程，具体而言，该模块在交叉注意力框架中动态为键和值选择专家子空间，从而实现对异构输入域的自适应处理。在University-1652和SUES-200数据集上的大量实验表明，我们的方法在使用更少训练参数的情况下取得了具有竞争力的性能。

</details>


### [10] [Kinematic-Based Assessment of Surgical Actions in Microanastomosis](https://arxiv.org/abs/2512.23942)
*Yan Meng,Daniel Donoho,Marcelle Altshuler,Omar Arnaout*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化框架，用于显微吻合术中的动作分割与技能评估，可在边缘计算设备上高效运行，实现了92.4%的动作分割准确率和85.5%的技能分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统显微外科手术评估依赖专家人工评分，存在主观性强、评分者间差异大、耗时长等问题，亟需客观、可扩展的自动化评估方法。

Method: 该系统包含三个模块：(1) 基于YOLO和DeepSORT的器械尖端追踪定位模块；(2) 利用自相似矩阵进行动作边界检测与无监督聚类的动作分割模块；(3) 用于评估手术动作熟练度的有监督分类模块。

Result: 在58个专家评分的显微吻合视频数据集上验证，该方法实现了92.4%的帧级动作分割准确率和85.5%的整体技能分类准确率。

Conclusion: 所提方法有望为显微外科教学提供客观、实时的反馈，推动标准化、数据驱动的培训方案，并提升高风险手术环境中的能力评估水平。

Abstract: Proficiency in microanastomosis is a critical surgical skill in neurosurgery, where the ability to precisely manipulate fine instruments is crucial to successful outcomes. These procedures require sustained attention, coordinated hand movements, and highly refined motor skills, underscoring the need for objective and systematic methods to evaluate and enhance microsurgical training. Conventional assessment approaches typically rely on expert raters supervising the procedures or reviewing surgical videos, which is an inherently subjective process prone to inter-rater variability, inconsistency, and significant time investment. These limitations highlight the necessity for automated and scalable solutions. To address this challenge, we introduce a novel AI-driven framework for automated action segmentation and performance assessment in microanastomosis procedures, designed to operate efficiently on edge computing platforms. The proposed system comprises three main components: (1) an object tip tracking and localization module based on YOLO and DeepSORT; (2) an action segmentation module leveraging self-similarity matrix for action boundary detection and unsupervised clustering; and (3) a supervised classification module designed to evaluate surgical gesture proficiency. Experimental validation on a dataset of 58 expert-rated microanastomosis videos demonstrates the effectiveness of our approach, achieving a frame-level action segmentation accuracy of 92.4% and an overall skill classification accuracy of 85.5% in replicating expert evaluations. These findings demonstrate the potential of the proposed method to provide objective, real-time feedback in microsurgical education, thereby enabling more standardized, data-driven training protocols and advancing competency assessment in high-stakes surgical environments.

Abstract (中文翻译): 显微吻合术的熟练程度是神经外科中一项关键的手术技能，精准操控精细器械的能力对成功结果至关重要。这类手术需要持续的注意力、协调的手部动作以及高度精细化的运动技能，因此迫切需要客观且系统化的方法来评估并提升显微外科培训效果。传统的评估方法通常依赖专家现场监督或回看手术视频进行评分，这一过程本质上具有主观性，易受评分者间差异、不一致性及大量时间投入的影响。这些局限性凸显了对自动化、可扩展解决方案的需求。为应对这一挑战，我们提出了一种新颖的、基于人工智能的框架，用于显微吻合术中的自动化动作分割与表现评估，该框架专为在边缘计算平台上高效运行而设计。所提出的系统包含三个主要组成部分：(1) 基于YOLO和DeepSORT的器械尖端追踪与定位模块；(2) 利用自相似矩阵进行动作边界检测和无监督聚类的动作分割模块；(3) 用于评估手术动作熟练度的有监督分类模块。在包含58个经专家评分的显微吻合视频数据集上的实验验证表明，该方法具有良好的有效性，达到了92.4%的帧级动作分割准确率和85.5%的整体技能分类准确率。这些结果展示了所提方法在显微外科教育中提供客观、实时反馈的潜力，从而实现更标准化、数据驱动的培训方案，并推进高风险手术环境中能力评估的发展。

</details>
