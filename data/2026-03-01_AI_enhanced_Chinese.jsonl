{"id": "2602.22347", "pdf": "https://arxiv.org/pdf/2602.22347", "abs": "https://arxiv.org/abs/2602.22347", "authors": ["Audun L. Henriksen", "Ole-Johan Skrede", "Lisa van der Schee", "Enric Domingo", "Sepp De Raedt", "Ily\u00e1 Kostolomov", "Jennifer Hay", "Karolina Cyll", "Wanja Kildal", "Joakim Kalsnes", "Robert W. Williams", "Manohar Pradhan", "John Arne Nesheim", "Hanne A. Askautrud", "Maria X. Isaksen", "Karmele Saez de Gordoa", "Miriam Cuatrecasas", "Joanne Edwards", "TransSCOT group", "Arild Nesbakken", "Neil A. Shepherd", "Ian Tomlinson", "Daniel-Christoph Wagner", "Rachel S. Kerr", "Tarjei Sveinsgjerd Hveem", "Knut Liest\u00f8l", "Yoshiaki Nakamura", "Marco Novelli", "Masaaki Miyo", "Sebastian Foersch", "David N. Church", "Miangela M. Lacle", "David J. Kerr", "Andreas Kleppe"], "title": "Enabling clinical use of foundation models in histopathology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u8bad\u7ec3\u4e2d\u5f15\u5165\u65b0\u578b\u9c81\u68d2\u6027\u635f\u5931\uff0c\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5bf9\u6280\u672f\u53d8\u5f02\u7684\u654f\u611f\u6027\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u4e0d\u4ec5\u5b66\u4e60\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\uff0c\u8fd8\u6355\u83b7\u4e86\u524d\u5904\u7406\u548c\u626b\u63cf\u4eea\u76f8\u5173\u7684\u6280\u672f\u53d8\u5f02\uff0c\u5bfc\u81f4\u4e0b\u6e38\u4efb\u52a1\u6a21\u578b\u9884\u6d4b\u5b58\u5728\u504f\u5dee\uff0c\u5f71\u54cd\u5176\u5728\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5728\u4e0b\u6e38\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u65b0\u578b\u9c81\u68d2\u6027\u635f\u5931\uff1b\u57fa\u4e8e\u6765\u81ea6155\u540d\u60a3\u8005\u768427,042\u5f20\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\uff0c\u5229\u75288\u79cd\u4e3b\u6d41\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u8bad\u7ec3\u6570\u5343\u4e2a\u4e0b\u6e38\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u6280\u672f\u53d8\u5f02\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u805a\u7126\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5373\u53ef\u6709\u6548\u7f13\u89e3\u5176\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u9002\u7528\u4e8e\u5e38\u89c4\u4e34\u5e8a\u5b9e\u8df5\u7684\u53ef\u9760\u8ba1\u7b97\u75c5\u7406\u6a21\u578b\u3002", "summary_cn": "\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u6709\u671b\u4fc3\u8fdb\u9ad8\u6027\u80fd\u4e14\u53ef\u6cdb\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u53d1\u5c55\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u6a21\u578b\u4e0d\u4ec5\u6355\u6349\u4e86\u751f\u7269\u5b66\u76f8\u5173\u7684\u7279\u5f81\uff0c\u8fd8\u5305\u542b\u4e86\u524d\u5904\u7406\u548c\u626b\u63cf\u4eea\u7279\u5f02\u6027\u5e26\u6765\u7684\u6280\u672f\u53d8\u5f02\uff0c\u4ece\u800c\u5bfc\u81f4\u57fa\u4e8e\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8bad\u7ec3\u7684\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u4ea7\u751f\u6709\u504f\u9884\u6d4b\u3002\u672c\u6587\u8868\u660e\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u65b0\u9896\u7684\u9c81\u68d2\u6027\u635f\u5931\uff0c\u53ef\u4ee5\u964d\u4f4e\u6a21\u578b\u5bf9\u6280\u672f\u53d8\u5f02\u7684\u654f\u611f\u6027\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5b9e\u9a8c\u4f53\u7cfb\uff0c\u5305\u542b\u6765\u81ea6155\u540d\u60a3\u8005\u768427,042\u5f20\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\uff0c\u5e76\u57fa\u4e8e\u516b\u79cd\u6d41\u884c\u7684\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u8bad\u7ec3\u4e86\u6570\u5343\u4e2a\u6a21\u578b\u3002\u9664\u4e86\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u5916\uff0c\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\uff0c\u901a\u8fc7\u805a\u7126\u4e8e\u751f\u7269\u5b66\u76f8\u5173\u7279\u5f81\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u4e5f\u5f97\u5230\u4e86\u63d0\u9ad8\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u6210\u529f\u7f13\u89e3\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u672c\u8eab\uff0c\u4ece\u800c\u63a8\u52a8\u4e86\u9002\u7528\u4e8e\u5e38\u89c4\u4e34\u5e8a\u5b9e\u8df5\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u9c81\u68d2\u8ba1\u7b97\u75c5\u7406\u6a21\u578b\u7684\u5f00\u53d1\u3002"}}
{"id": "2602.22361", "pdf": "https://arxiv.org/pdf/2602.22361", "abs": "https://arxiv.org/abs/2602.22361", "authors": ["Liping Meng", "Fan Nie", "Yunyun Zhang", "Chao Han"], "title": "Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.", "AI": {"tldr": "MNAS-Unet \u662f\u4e00\u79cd\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4e0e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u7684\u65b0\u578b\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7 MCTS \u52a8\u6001\u63a2\u7d22\u9ad8\u6548\u7f51\u7edc\u7ed3\u6784\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u641c\u7d22\u6210\u672c\u548c\u6a21\u578b\u53c2\u6570\u91cf\u3002", "motivation": "\u4f20\u7edf NAS \u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5b58\u5728\u641c\u7d22\u6548\u7387\u4f4e\u3001\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165 MCTS \u63d0\u9ad8\u67b6\u6784\u641c\u7d22\u6548\u7387\uff0c\u5e76\u4f18\u5316\u5355\u5143\u7ed3\u6784\u4ee5\u517c\u987e\u7cbe\u5ea6\u4e0e\u8f7b\u91cf\u5316\u3002", "method": "\u63d0\u51fa MNAS-Unet \u6846\u67b6\uff0c\u5c06 MCTS \u4e0e NAS \u76f8\u7ed3\u5408\uff0c\u52a8\u6001\u63a2\u7d22\u6709\u6f5c\u529b\u7684\u7f51\u7edc\u67b6\u6784\uff1b\u540c\u65f6\u8bbe\u8ba1\u5e76\u4f18\u5316 DownSC \u548c UpSC \u5355\u5143\u7ed3\u6784\uff0c\u4ee5\u5b9e\u73b0\u5feb\u901f\u4e14\u7cbe\u786e\u7684\u6a21\u578b\u8c03\u6574\u3002", "result": "\u5728 PROMISE12\u3001Ultrasound Nerve \u548c CHAOS \u7b49\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cMNAS-Unet \u7684\u5206\u5272\u7cbe\u5ea6\u4f18\u4e8e NAS-Unet \u53ca\u5176\u4ed6\u5148\u8fdb\u6a21\u578b\uff1b\u67b6\u6784\u641c\u7d22\u9884\u7b97\u51cf\u5c11 54%\uff08139 vs. 300 epochs\uff09\uff0c\u6a21\u578b\u4ec5\u542b 0.6M \u53c2\u6570\uff0cGPU \u5185\u5b58\u5360\u7528\u66f4\u4f4e\u3002", "conclusion": "MNAS-Unet \u5728\u4fdd\u6301\u9ad8\u5206\u5272\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "summary_cn": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6 MNAS-Unet\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4e0e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u3002MNAS-Unet \u901a\u8fc7 MCTS \u52a8\u6001\u63a2\u7d22\u6709\u524d\u666f\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67b6\u6784\u641c\u7d22\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u4f18\u5316\u4e86 DownSC \u548c UpSC \u5355\u5143\u7ed3\u6784\uff0c\u4ece\u800c\u5b9e\u73b0\u5feb\u901f\u800c\u7cbe\u786e\u7684\u6a21\u578b\u8c03\u6574\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMNAS-Unet \u5728\u5305\u62ec PROMISE12\u3001\u8d85\u58f0\u795e\u7ecf\uff08Ultrasound Nerve\uff09\u548c CHAOS \u5728\u5185\u7684\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u5176\u5206\u5272\u7cbe\u5ea6\u5747\u4f18\u4e8e NAS-Unet \u53ca\u5176\u4ed6\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4e0e NAS-Unet \u76f8\u6bd4\uff0cMNAS-Unet \u5728\u76f8\u540c\u641c\u7d22\u8bbe\u7f6e\u4e0b\u5c06\u67b6\u6784\u641c\u7d22\u9884\u7b97\u51cf\u5c11\u4e86 54%\uff08\u63d0\u524d\u5728\u7b2c 139 \u8f6e\u505c\u6b62\uff0c\u800c NAS-Unet \u9700\u8981 300 \u8f6e\uff09\uff0c\u540c\u65f6\u83b7\u5f97\u4e86\u4e00\u4e2a\u4ec5\u542b 0.6M \u53c2\u6570\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5e76\u964d\u4f4e\u4e86 GPU \u5185\u5b58\u6d88\u8017\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5176\u5b9e\u7528\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cMNAS-Unet \u80fd\u5728\u5b9e\u9645\u8d44\u6e90\u7ea6\u675f\u4e0b\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u5272\u7cbe\u5ea6\u3002"}}
{"id": "2602.22376", "pdf": "https://arxiv.org/pdf/2602.22376", "abs": "https://arxiv.org/abs/2602.22376", "authors": ["Hanyang Liu", "Rongjun Qin"], "title": "AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2026", "summary": "Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.", "AI": {"tldr": "AeroDGS is a physics-guided 4D Gaussian splatting method for monocular UAV videos that addresses depth ambiguity and unstable motion estimation in dynamic aerial scenes by combining geometry lifting and physics-based priors.", "motivation": "Existing 4D reconstruction methods struggle with monocular aerial videos due to challenges like single-view capture, wide spatial coverage, small object footprint, and large motion disparity, leading to depth ambiguity and unreliable motion estimation.", "method": "AeroDGS uses a Monocular Geometry Lifting module to reconstruct static and dynamic geometry from a single aerial sequence, and a Physics-Guided Optimization module that enforces ground-support, upright-stability, and trajectory-smoothness priors to resolve monocular ambiguity and ensure physically plausible motion.", "result": "Experiments on synthetic and real UAV datasets show that AeroDGS outperforms state-of-the-art methods in reconstruction fidelity for dynamic aerial environments.", "conclusion": "AeroDGS effectively addresses the ill-posed nature of monocular dynamic aerial reconstruction by integrating geometric inference with physical constraints, enabling robust and coherent 4D scene modeling from UAV videos.", "summary_cn": "\u8fd1\u671f\u57284D\u573a\u666f\u91cd\u5efa\u65b9\u9762\u7684\u8fdb\u5c55\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u9886\u57df\u4e2d\u7684\u52a8\u6001\u5efa\u6a21\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u822a\u62cd\u6761\u4ef6\u4e0b\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5355\u89c6\u89d2\u62cd\u6444\u3001\u5927\u7a7a\u95f4\u8303\u56f4\u3001\u4ee5\u53ca\u7a7a\u95f4\u5360\u6bd4\u8f83\u5c0f\u4f46\u8fd0\u52a8\u5dee\u5f02\u8f83\u5927\u7684\u52a8\u6001\u7269\u4f53\u573a\u666f\u4e2d\u3002\u8fd9\u4e9b\u6311\u6218\u5bfc\u81f4\u4e25\u91cd\u7684\u6df1\u5ea6\u6a21\u7cca\u548c\u4e0d\u7a33\u5b9a\u7684\u8fd0\u52a8\u4f30\u8ba1\uff0c\u4f7f\u5f97\u5355\u76ee\u822a\u62cd\u91cd\u5efa\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u75c5\u6001\u95ee\u9898\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AeroDGS\u2014\u2014\u4e00\u79cd\u9762\u5411\u5355\u76ee\u65e0\u4eba\u673a\u89c6\u9891\u7684\u7269\u7406\u5f15\u5bfc4D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u3002AeroDGS\u5f15\u5165\u4e86\u4e00\u4e2a\u5355\u76ee\u51e0\u4f55\u63d0\u5347\u6a21\u5757\uff0c\u53ef\u4ece\u5355\u6bb5\u822a\u62cd\u5e8f\u5217\u4e2d\u91cd\u5efa\u53ef\u9760\u7684\u9759\u6001\u4e0e\u52a8\u6001\u51e0\u4f55\u7ed3\u6784\uff0c\u4e3a\u52a8\u6001\u4f30\u8ba1\u63d0\u4f9b\u7a33\u5065\u57fa\u7840\u3002\u4e3a\u8fdb\u4e00\u6b65\u89e3\u51b3\u5355\u76ee\u6a21\u7cca\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u5f15\u5bfc\u4f18\u5316\u6a21\u5757\uff0c\u878d\u5408\u4e86\u53ef\u5fae\u5206\u7684\u5730\u9762\u652f\u6491\u3001\u76f4\u7acb\u7a33\u5b9a\u6027\u53ca\u8f68\u8ff9\u5e73\u6ed1\u6027\u5148\u9a8c\uff0c\u5c06\u6a21\u7cca\u7684\u56fe\u50cf\u7ebf\u7d22\u8f6c\u5316\u4e3a\u7269\u7406\u4e00\u81f4\u7684\u8fd0\u52a8\u3002\u8be5\u6846\u67b6\u8054\u5408\u4f18\u5316\u9759\u6001\u80cc\u666f\u4e0e\u52a8\u6001\u5b9e\u4f53\uff0c\u5b9e\u73b0\u7a33\u5b9a\u51e0\u4f55\u7ed3\u6784\u4e0e\u8fde\u8d2f\u7684\u65f6\u95f4\u6f14\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u8986\u76d6\u591a\u79cd\u98de\u884c\u9ad8\u5ea6\u548c\u8fd0\u52a8\u6761\u4ef6\u7684\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u822a\u62cd\u91cd\u5efa\u6027\u80fd\u3002\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u65e0\u4eba\u673a\u573a\u666f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAeroDGS\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u822a\u62cd\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2602.22381", "pdf": "https://arxiv.org/pdf/2602.22381", "abs": "https://arxiv.org/abs/2602.22381", "authors": ["Zhengkang Fan", "Chengkun Sun", "Russell Terry", "Jie Xu", "Longin Jan Latecki"], "title": "Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 2 figures, Accepted at IEEE ISBI 2026", "summary": "Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5668\u5b98\u805a\u7126\u6ce8\u610f\u529b\uff08OFA\uff09\u635f\u5931\u51fd\u6570\uff0c\u5728\u4e0d\u4f9d\u8d56\u80bf\u7624\u533a\u57df\u5206\u5272\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u80be\u80bf\u7624\u6076\u6027\u7a0b\u5ea6\u9884\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u5206\u5272\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f71\u50cf\u5b66\u65b9\u6cd5\u5728\u672f\u524d\u9884\u6d4b\u80be\u80bf\u7624\u6076\u6027\u7a0b\u5ea6\u65b9\u9762\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u800c\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u8017\u65f6\u4e14\u9700\u4e13\u5bb6\u53c2\u4e0e\u7684\u624b\u52a8\u5206\u5272\u6765\u63d0\u5347\u6027\u80fd\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5668\u5b98\u805a\u7126\u6ce8\u610f\u529b\uff08OFA\uff09\u635f\u5931\u51fd\u6570\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u56fe\u50cf\u5757\u4ec5\u5173\u6ce8\u540c\u5c5e\u5668\u5b98\u7684\u5176\u4ed6\u56fe\u50cf\u5757\uff0c\u4ece\u800c\u5728\u90e8\u7f72\u9636\u6bb5\u65e0\u9700\u5bf93D\u80beCT\u56fe\u50cf\u8fdb\u884c\u5206\u5272\u5373\u53ef\u8fdb\u884c\u6076\u6027\u9884\u6d4b\u3002", "result": "\u5728UF IDR\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8fbe\u5230AUC 0.685\u3001F1-score 0.872\uff1b\u5728KiTS21\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230AUC 0.760\u3001F1-score 0.852\uff0c\u4f18\u4e8e\u4f9d\u8d56\u5206\u5272\u88c1\u526a\u7684\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u663e\u5f0f\u5206\u5272\u5373\u53ef\u63d0\u9ad8\u6076\u6027\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u80be\u764c\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002", "summary_cn": "\u51c6\u786e\u9884\u6d4b\u80be\u80bf\u7624\u7684\u6076\u6027\u7a0b\u5ea6\u5bf9\u4e8e\u6307\u5bfc\u4e34\u5e8a\u51b3\u7b56\u548c\u4f18\u5316\u6cbb\u7597\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5f71\u50cf\u5b66\u65b9\u6cd5\u5728\u624b\u672f\u5e72\u9884\u524d\u5c1a\u65e0\u6cd5\u53ef\u9760\u5730\u9884\u6d4b\u6076\u6027\u7a0b\u5ea6\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u5229\u75283D CT\u56fe\u50cf\u8fdb\u884c\u6076\u6027\u9884\u6d4b\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u5206\u5272\u6765\u9694\u79bb\u80bf\u7624\u533a\u57df\u5e76\u51cf\u5c11\u566a\u58f0\uff0c\u4ece\u800c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002\u7136\u800c\uff0c\u4eba\u5de5\u5206\u5272\u8d39\u65f6\u8d39\u529b\u3001\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u3002\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u5668\u5b98\u805a\u7126\u6ce8\u610f\u529b\uff08OFA\uff09\u635f\u5931\u51fd\u6570\uff0c\u8c03\u6574\u56fe\u50cf\u5757\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u5176\u4ec5\u5173\u6ce8\u5176\u4ed6\u5c5e\u4e8e\u540c\u4e00\u5668\u5b98\u7684\u56fe\u50cf\u5757\u3002\u56e0\u6b64\uff0c\u5728\u90e8\u7f72\u9636\u6bb5\u65e0\u9700\u5bf93D\u80beCT\u56fe\u50cf\u8fdb\u884c\u5206\u5272\u5373\u53ef\u5b8c\u6210\u6076\u6027\u9884\u6d4b\u3002\u6240\u63d0\u6846\u67b6\u5728\u4f5b\u7f57\u91cc\u8fbe\u5927\u5b66\u7efc\u5408\u6570\u636e\u5b58\u50a8\u5e93\uff08UF IDR\uff09\u7684\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86AUC\u4e3a0.685\u3001F1\u5206\u6570\u4e3a0.872\u7684\u7ed3\u679c\uff0c\u5728\u516c\u5f00\u53ef\u7528\u7684KiTS21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86AUC\u4e3a0.760\u3001F1\u5206\u6570\u4e3a0.852\u7684\u7ed3\u679c\u3002\u8fd9\u4e9b\u7ed3\u679c\u4f18\u4e8e\u4f9d\u8d56\u57fa\u4e8e\u5206\u5272\u88c1\u526a\u8fdb\u884c\u964d\u566a\u7684\u4f20\u7edf\u6a21\u578b\uff0c\u8868\u660e\u8be5\u6846\u67b6\u5728\u65e0\u9700\u663e\u5f0f\u5206\u5272\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e3a\u80be\u764c\u6076\u6027\u7a0b\u5ea6\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u7684\u624b\u6bb5\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u80be\u764c\u8bca\u65ad\u4e2d\u7684\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2602.22394", "pdf": "https://arxiv.org/pdf/2602.22394", "abs": "https://arxiv.org/abs/2602.22394", "authors": ["Cheng Shi", "Yizhou Yu", "Sibei Yang"], "title": "Vision Transformers Need More Than Registers", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0Vision Transformer\uff08ViT\uff09\u4e2d\u7684\u4f2a\u5f71\u95ee\u9898\u6e90\u4e8e\u5176\u201c\u61d2\u60f0\u805a\u5408\u201d\u884c\u4e3a\uff0c\u5373\u5229\u7528\u8bed\u4e49\u65e0\u5173\u7684\u80cc\u666f\u5757\u4f5c\u4e3a\u6377\u5f84\u6765\u8868\u793a\u5168\u5c40\u8bed\u4e49\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u9009\u62e9\u6027\u5730\u5c06\u56fe\u50cf\u5757\u7279\u5f81\u6574\u5408\u5230CLS token\u4e2d\u4ee5\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u5728\u591a\u79cd\u76d1\u7763\u8303\u5f0f\u4e0b\u5747\u53d6\u5f97\u6027\u80fd\u63d0\u5347\u3002", "motivation": "ViT\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u540e\u867d\u80fd\u63d0\u4f9b\u901a\u7528\u8868\u5f81\uff0c\u4f46\u5728\u4e0d\u540c\u76d1\u7763\u8303\u5f0f\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u666e\u904d\u5b58\u5728\u4f2a\u5f71\uff08artifacts\uff09\u73b0\u8c61\uff0c\u5176\u6839\u672c\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u6df1\u5165\u5206\u6790\u4e0e\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790ViT\u4e2d\u7684\u4f2a\u5f71\u73b0\u8c61\uff0c\u8bc6\u522b\u5176\u6e90\u4e8e\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u548c\u7c97\u7c92\u5ea6\u8bed\u4e49\u76d1\u7763\u6240\u5bfc\u81f4\u7684\u201c\u61d2\u60f0\u805a\u5408\u201d\u884c\u4e3a\uff1b\u63d0\u51fa\u4e00\u79cd\u9009\u62e9\u6027\u6574\u5408\u56fe\u50cf\u5757\u7279\u5f81\u81f3CLS token\u7684\u65b9\u6cd5\uff0c\u4ee5\u524a\u5f31\u80cc\u666f\u4e3b\u5bfc\u7684\u6377\u5f84\u5f71\u54cd\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u572812\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65e0\u8bba\u662f\u5728\u6807\u7b7e\u76d1\u7763\u3001\u6587\u672c\u76d1\u7763\u8fd8\u662f\u81ea\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u5747\u4e00\u81f4\u63d0\u5347\u4e86ViT\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86ViT\u4f2a\u5f71\u7684\u6839\u672c\u6210\u56e0\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7f13\u89e3\u7b56\u7565\uff0c\u4e3a\u7406\u89e3ViT\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "summary_cn": "\u89c6\u89c9Transformer\uff08ViT\uff09\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\uff0c\u80fd\u591f\u4e3a\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u901a\u7528\u8868\u5f81\u3002\u7136\u800c\uff0c\u5728\u4e0d\u540c\u76d1\u7763\u8303\u5f0f\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cViT\u4e2d\u666e\u904d\u89c2\u5bdf\u5230\u4f2a\u5f71\u73b0\u8c61\u3002\u901a\u8fc7\u5bf9ViT\u4e2d\u4f2a\u5f71\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\u5176\u6839\u672c\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9610\u660e\u3002\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u5f97\u51fa\u7ed3\u8bba\uff1a\u8fd9\u4e9b\u4f2a\u5f71\u6e90\u4e8e\u4e00\u79cd\u201c\u61d2\u60f0\u805a\u5408\u201d\u884c\u4e3a\u2014\u2014ViT\u5728\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u548c\u7c97\u7c92\u5ea6\u8bed\u4e49\u76d1\u7763\u7684\u9a71\u52a8\u4e0b\uff0c\u5229\u7528\u8bed\u4e49\u65e0\u5173\u7684\u80cc\u666f\u56fe\u50cf\u5757\u4f5c\u4e3a\u6377\u5f84\u6765\u8868\u793a\u5168\u5c40\u8bed\u4e49\u3002\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u9009\u62e9\u6027\u5730\u5c06\u56fe\u50cf\u5757\u7279\u5f81\u6574\u5408\u5230CLS token\u4e2d\uff0c\u4ece\u800c\u51cf\u5c11\u7531\u80cc\u666f\u4e3b\u5bfc\u7684\u6377\u5f84\u6240\u5e26\u6765\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u6807\u7b7e\u76d1\u7763\u3001\u6587\u672c\u76d1\u7763\u548c\u81ea\u76d1\u7763\u4e09\u79cd\u8303\u5f0f\u4e0b\u768412\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u6301\u7eed\u63d0\u5347\u4e86\u6027\u80fd\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u4e3a\u7406\u89e3ViT\u7684\u884c\u4e3a\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2602.22419", "pdf": "https://arxiv.org/pdf/2602.22419", "abs": "https://arxiv.org/abs/2602.22419", "authors": ["Marc-Antoine Lavoie", "Anas Mahmoud", "Aldo Zaimi", "Arsene Fansi Tchango", "Steven L. Waslander"], "title": "CLIP Is Shortsighted: Paying Attention Beyond the First Sentence", "categories": ["cs.CV"], "comment": "19 pages, 13 figures, to be published in the CVPR 2026 proceedings", "summary": "CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.", "AI": {"tldr": "CLIP\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u957f\u6587\u672c\u63cf\u8ff0\u901a\u5e38\u4ee5\u7b80\u77ed\u6458\u8981\u5f00\u5934\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8\u5f00\u5934\u53e5\u5b50\u800c\u5ffd\u7565\u540e\u7eed\u7ec6\u8282\u3002\u672c\u6587\u63d0\u51faDeBias-CLIP\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u79fb\u9664\u6458\u8981\u53e5\u3001\u91c7\u7528\u53e5\u5b50\u5b50\u91c7\u6837\u548c\u6587\u672c\u586b\u5145\u7b56\u7565\uff0c\u4f7f\u76d1\u7763\u4fe1\u53f7\u5747\u5300\u5206\u5e03\u4e8e\u6240\u6709\u6587\u672c\u4f4d\u7f6e\uff0c\u4ece\u800c\u63d0\u5347\u957f\u6587\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u77ed\u6587\u672c\u68c0\u7d22\u548c\u53e5\u5b50\u987a\u5e8f\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u65f6\u4e3b\u8981\u4f7f\u7528\u56fe\u50cf\u4e0e\u7b80\u77ed\u6807\u9898\u914d\u5bf9\u7684\u6570\u636e\uff0c\u4f7f\u5176\u504f\u5411\u4e8e\u5b66\u4e60\u663e\u8457\u7269\u4f53\u7684\u7b80\u5355\u63cf\u8ff0\uff0c\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u548c\u5bc6\u96c6\u63cf\u8ff0\u65f6\u5bf9\u9f50\u6548\u679c\u8f83\u5dee\u3002\u5c3d\u7ba1\u8fd1\u671f\u5de5\u4f5c\u901a\u8fc7\u5728\u5c0f\u89c4\u6a21\u957f\u6807\u9898\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6765\u7f13\u89e3\u8be5\u95ee\u9898\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e9b\u957f\u6807\u9898\uff08\u65e0\u8bba\u662f\u4eba\u5de5\u8fd8\u662fLLM\u751f\u6210\uff09\u666e\u904d\u5b58\u5728\u201c\u5148\u6458\u8981\u540e\u7ec6\u8282\u201d\u7684\u7ed3\u6784\u504f\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u4f9d\u8d56\u5f00\u5934\u53e5\u5b50\u4f5c\u4e3a\u6377\u5f84\uff0c\u524a\u5f31\u4e86\u5bf9\u5168\u6587\u7684\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u63d0\u51faDeBias-CLIP\u65b9\u6cd5\uff1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u79fb\u9664\u957f\u6807\u9898\u4e2d\u7684\u9996\u53e5\u6458\u8981\uff1b\u5f15\u5165\u53e5\u5b50\u5b50\u91c7\u6837\uff08sentence sub-sampling\uff09\u548c\u6587\u672ctoken\u586b\u5145\uff08text token padding\uff09\u673a\u5236\uff0c\u4f7f\u76d1\u7763\u4fe1\u53f7\u5747\u5300\u5206\u5e03\u5728\u6240\u6709\u6587\u672ctoken\u4f4d\u7f6e\uff0c\u4ece\u800c\u907f\u514d\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8\u5f00\u5934\u90e8\u5206\u3002", "result": "DeBias-CLIP\u5728\u957f\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u77ed\u6587\u672c\u68c0\u7d22\u6548\u679c\uff0c\u5e76\u5bf9\u53e5\u5b50\u987a\u5e8f\u7684\u6392\u5217\u53d8\u5316\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u76f4\u63a5\u66ff\u4ee3Long-CLIP\uff0c\u4e14\u65e0\u9700\u589e\u52a0\u4efb\u4f55\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u901a\u8fc7\u6d88\u9664\u957f\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u6458\u8981\u5148\u884c\u504f\u5dee\u5e76\u91cd\u65b0\u5206\u914d\u76d1\u7763\u4fe1\u53f7\uff0cDeBias-CLIP\u6709\u6548\u63d0\u5347\u4e86CLIP\u6a21\u578b\u5bf9\u590d\u6742\u3001\u5bc6\u96c6\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5373\u63d2\u5373\u7528\u7684\u6539\u8fdb\u65b9\u6848\u3002", "summary_cn": "CLIP\u6a21\u578b\u901a\u8fc7\u5728\u4e92\u8054\u7f51\u89c4\u6a21\u6570\u636e\u4e0a\u8fdb\u884c\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u591a\u6a21\u6001\u7279\u5f81\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u96f6\u6837\u672c\u5206\u7c7b\u3001\u591a\u6a21\u6001\u68c0\u7d22\u3001\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4ee5\u53ca\u4f5c\u4e3a\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u56fe\u50cf\u7f16\u7801\u5668\u3002\u7136\u800c\uff0cCLIP\u7684\u9884\u8bad\u7ec3\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e0e\u7b80\u77ed\u6807\u9898\u914d\u5bf9\u7684\u56fe\u50cf\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u503e\u5411\u4e8e\u7f16\u7801\u663e\u8457\u7269\u4f53\u7684\u7b80\u5355\u63cf\u8ff0\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u573a\u666f\u548c\u5bc6\u96c6\u63cf\u8ff0\u4e0a\u7684\u5bf9\u9f50\u8f83\u4e3a\u7c97\u7cd9\u3002\u5c3d\u7ba1\u8fd1\u671f\u5de5\u4f5c\u901a\u8fc7\u5728\u5c0f\u89c4\u6a21\u957f\u6807\u9898\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u91cd\u8981\u7684\u5171\u6027\u504f\u5dee\uff1a\u65e0\u8bba\u662f\u4eba\u5de5\u64b0\u5199\u8fd8\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u957f\u6807\u9898\uff0c\u901a\u5e38\u90fd\u4ee5\u4e00\u53e5\u6982\u62ec\u6027\u6458\u8981\u5f00\u5934\uff0c\u968f\u540e\u624d\u662f\u8be6\u7ec6\u63cf\u8ff0\u3002\u6211\u4eec\u8bc1\u660e\u8fd9\u79cd\u7ed3\u6784\u5728\u8bad\u7ec3\u4e2d\u4f1a\u5f62\u6210\u201c\u6377\u5f84\u201d\uff0c\u4f7f\u6a21\u578b\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u5f00\u5934\u53e5\u5b50\u548c\u65e9\u671ftoken\u4e0a\uff0c\u524a\u5f31\u4e86\u5bf9\u6807\u9898\u5176\u4f59\u90e8\u5206\u7684\u5bf9\u9f50\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DeBias-CLIP\uff1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u79fb\u9664\u6458\u8981\u53e5\uff0c\u5e76\u91c7\u7528\u53e5\u5b50\u5b50\u91c7\u6837\u548c\u6587\u672ctoken\u586b\u5145\u7b56\u7565\uff0c\u5c06\u76d1\u7763\u4fe1\u53f7\u5747\u5300\u5206\u5e03\u5230\u6240\u6709token\u4f4d\u7f6e\u3002DeBias-CLIP\u5728\u957f\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u77ed\u6587\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5bf9\u53e5\u5b50\u987a\u5e8f\u7684\u6392\u5217\u53d8\u5316\u66f4\u5177\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3aLong-CLIP\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u4e14\u4e0d\u5f15\u5165\u989d\u5916\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002"}}
{"id": "2602.22426", "pdf": "https://arxiv.org/pdf/2602.22426", "abs": "https://arxiv.org/abs/2602.22426", "authors": ["Yibo Peng", "Peng Xia", "Ding Zhong", "Kaide Zeng", "Siwei Han", "Yiyang Zhou", "Jiaqi Liu", "Ruiyi Zhang", "Huaxiu Yao"], "title": "SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5904\u7406\u56fe\u50cf\u4e2d\u6587\u672c\u65f6\u5b58\u5728\u201c\u6a21\u6001\u60f0\u6027\u201d\u95ee\u9898\uff0c\u5373\u503e\u5411\u4e8e\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u4e2d\u7684\u53c2\u6570\u6377\u5f84\u800c\u975e\u771f\u6b63\u8bfb\u53d6\u56fe\u50cf\u4e2d\u7684\u6587\u5b57\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u53ef\u89c6\u5316\u95ee\u9898\uff08VQ\uff09\u8bbe\u7f6e\u8fdb\u884c\u8bca\u65ad\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aSimpleOCR\u7684\u5373\u63d2\u5373\u7528\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u8bad\u7ec3\u6837\u672c\u8f6c\u6362\u4e3aVQ\u683c\u5f0f\u5e76\u5f15\u5165\u968f\u673a\u6837\u5f0f\uff0c\u5f3a\u5236\u6a21\u578b\u5229\u7528\u89c6\u89c9\u6587\u672c\u63d0\u53d6\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aOOD\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u6570\u636e\u6548\u7387\u6781\u9ad8\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u662f\u5426\u771f\u6b63\u201c\u9605\u8bfb\u201d\u56fe\u50cf\u4e2d\u7684\u5d4c\u5165\u6587\u672c\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u4e2d\u7684\u53c2\u6570\u6377\u5f84\uff0c\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u89e3\u7b54\u3002\u4f5c\u8005\u65e8\u5728\u8bca\u65ad\u5e76\u89e3\u51b3MLLMs\u5728\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u201c\u6a21\u6001\u60f0\u6027\u201d\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u53ef\u89c6\u5316\u95ee\u9898\uff08Visualized-Question, VQ\uff09\u8bbe\u7f6e\uff0c\u5c06\u6587\u672c\u67e5\u8be2\u76f4\u63a5\u6e32\u67d3\u5230\u56fe\u50cf\u4e0a\uff0c\u4ee5\u7ed3\u6784\u6027\u5730\u5f3a\u5236\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u4ea4\u4e92\u3002\u57fa\u4e8e\u6b64\uff0c\u8bbe\u8ba1\u4e86SimpleOCR\u8bad\u7ec3\u7b56\u7565\uff1a\u5c06\u8bad\u7ec3\u6837\u672c\u8f6c\u6362\u4e3aVQ\u683c\u5f0f\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u6837\u5f0f\uff0c\u4ece\u800c\u65e0\u6548\u5316\u57fa\u4e8e\u7eaf\u6587\u672c\u7684\u6377\u5f84\uff0c\u8feb\u4f7f\u6a21\u578b\u6fc0\u6d3b\u548c\u4f18\u5316\u5176\u89c6\u89c9\u6587\u672c\u63d0\u53d6\u901a\u8def\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\uff0c\u5373\u53ef\u5373\u63d2\u5373\u7528\u3002", "result": "\u5728Qwen2.5-VL\u4e0a\u7684\u8bca\u65ad\u5b9e\u9a8c\u663e\u793a\uff0c\u5c3d\u7ba1\u6a21\u578b\u5177\u5907\u5f3a\u5927\u7684OCR\u80fd\u529b\uff0c\u4f46\u5728VQ\u8bbe\u7f6e\u4e0b\u6027\u80fd\u6700\u591a\u4e0b\u964d12.7%\u3002\u5e94\u7528SimpleOCR\u540e\uff0c\u5728\u56db\u4e2a\u4ee3\u8868\u6027OOD\u57fa\u51c6\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u53475.4%\uff0c\u6bd4\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\u7684GRPO\u65b9\u6cd5\u63d0\u53472.7%\u3002\u6b64\u5916\uff0cSimpleOCR\u5177\u6709\u6781\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u4ec5\u75288.5K\u6837\u672c\uff08\u4e3a\u8fd1\u671fRL\u65b9\u6cd5\u76841/30\uff09\u5c31\u53d6\u5f97\u4e86\u66f4\u4f18\u6027\u80fd\uff0c\u5e76\u80fd\u4e0eNoisyRollout\u7b49\u9ad8\u7ea7RL\u7b56\u7565\u65e0\u7f1d\u7ed3\u5408\uff0c\u5e26\u6765\u4e92\u8865\u6027\u63d0\u5347\u3002", "conclusion": "MLLMs\u666e\u904d\u5b58\u5728\u201c\u6a21\u6001\u60f0\u6027\u201d\uff0c\u5373\u5728\u53ef\u9009\u60c5\u51b5\u4e0b\u4f1a\u56de\u907f\u4f7f\u7528\u89c6\u89c9\u4fe1\u606f\u3002\u901a\u8fc7\u5f15\u5165VQ\u8bbe\u7f6e\u548cSimpleOCR\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u5f3a\u5236\u6a21\u578b\u5229\u7528\u5176\u5185\u5728\u7684\u89c6\u89c9\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u9700\u8981\u771f\u5b9e\u89c6\u89c9\u63a5\u5730\u7684\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u901a\u7528\u3001\u6613\u4e8e\u96c6\u6210\u3002", "summary_cn": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u89e3\u7b54\uff1a\u8fd9\u4e9b\u6a21\u578b\u7a76\u7adf\u662f\u771f\u6b63\u201c\u9605\u8bfb\u201d\u56fe\u50cf\u4e2d\u5d4c\u5165\u7684\u6587\u672c\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u6587\u672c\u63d0\u793a\u4e2d\u7684\u53c2\u6570\u6377\u5f84\uff1f\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u201c\u53ef\u89c6\u5316\u95ee\u9898\u201d\uff08Visualized-Question, VQ\uff09\u8bbe\u7f6e\u6765\u8bca\u65ad\u8fd9\u4e00\u95ee\u9898\uff0c\u8be5\u8bbe\u7f6e\u5c06\u6587\u672c\u67e5\u8be2\u76f4\u63a5\u6e32\u67d3\u5230\u56fe\u50cf\u4e0a\uff0c\u4ece\u7ed3\u6784\u4e0a\u5f3a\u5236\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u4ea4\u4e92\u3002\u6211\u4eec\u5bf9Qwen2.5-VL\u7684\u8bca\u65ad\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e00\u4e2a\u60ca\u4eba\u7684\u80fd\u529b-\u5229\u7528\u7387\u5dee\u8ddd\uff1a\u5c3d\u7ba1\u6a21\u578b\u5177\u5907\u5f3a\u5927\u7684OCR\u80fd\u529b\uff0c\u4f46\u5728VQ\u8bbe\u7f6e\u4e0b\u6027\u80fd\u6700\u591a\u4e0b\u964d12.7%\uff0c\u66b4\u9732\u4e86\u5176\u6839\u6df1\u8482\u56fa\u7684\u201c\u6a21\u6001\u60f0\u6027\u201d\u3002\u4e3a\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SimpleOCR\u2014\u2014\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5b83\u901a\u8fc7\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u65bd\u52a0\u7ed3\u6784\u6027\u7ea6\u675f\u6765\u89e3\u51b3\u95ee\u9898\u3002\u901a\u8fc7\u5c06\u8bad\u7ec3\u6837\u672c\u8f6c\u6362\u4e3aVQ\u683c\u5f0f\u5e76\u91c7\u7528\u968f\u673a\u6837\u5f0f\uff0cSimpleOCR\u6709\u6548\u5730\u6d88\u9664\u4e86\u57fa\u4e8e\u6587\u672c\u7684\u6377\u5f84\uff0c\u8feb\u4f7f\u6a21\u578b\u6fc0\u6d3b\u5e76\u4f18\u5316\u5176\u89c6\u89c9\u6587\u672c\u63d0\u53d6\u901a\u8def\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cSimpleOCR\u5728\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u5e26\u6765\u4e86\u7a33\u5065\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728\u56db\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u57fa\u51c6\u4e0a\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u57fa\u7ebf\u6a21\u578b5.4%\uff0c\u4e5f\u4f18\u4e8e\u57fa\u4e8e\u539f\u59cb\u56fe\u50cf\u7684GRPO\u65b9\u6cd52.7%\u3002\u540c\u65f6\uff0c\u5b83\u5c55\u73b0\u51fa\u6781\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u4ec5\u75288.5K\u4e2a\u6837\u672c\uff08\u4e3a\u8fd1\u671f\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6240\u9700\u6837\u672c\u91cf\u76841/30\uff09\u5c31\u5b9e\u73b0\u4e86\u66f4\u4f18\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5176\u5373\u63d2\u5373\u7528\u7684\u7279\u6027\u4f7f\u5176\u80fd\u591f\u4e0eNoisyRollout\u7b49\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u65e0\u7f1d\u96c6\u6210\uff0c\u4ea7\u751f\u4e92\u8865\u6027\u7684\u6539\u8fdb\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/aiming-lab/SimpleOCR\u3002"}}
{"id": "2602.22455", "pdf": "https://arxiv.org/pdf/2602.22455", "abs": "https://arxiv.org/abs/2602.22455", "authors": ["Giuseppe Lando", "Rosario Forte", "Antonino Furnari"], "title": "Exploring Multimodal LMMs for Online Episodic Memory Question Answering on the Edge", "categories": ["cs.CV"], "comment": null, "summary": "We investigate the feasibility of using Multimodal Large Language Models (MLLMs) for real-time online episodic memory question answering. While cloud offloading is common, it raises privacy and latency concerns for wearable assistants, hence we investigate implementation on the edge. We integrated streaming constraints into our question answering pipeline, which is structured into two asynchronous threads: a Descriptor Thread that continuously converts video into a lightweight textual memory, and a Question Answering (QA) Thread that reasons over the textual memory to answer queries. Experiments on the QAEgo4D-Closed benchmark analyze the performance of Multimodal Large Language Models (MLLMs) within strict resource boundaries, showing promising results also when compared to clound-based solutions. Specifically, an end-to-end configuration running on a consumer-grade 8GB GPU achieves 51.76% accuracy with a Time-To-First-Token (TTFT) of 0.41s. Scaling to a local enterprise-grade server yields 54.40% accuracy with a TTFT of 0.88s. In comparison, a cloud-based solution obtains an accuracy of 56.00%. These competitive results highlight the potential of edge-based solutions for privacy-preserving episodic memory retrieval.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5b9e\u73b0\u5b9e\u65f6\u5728\u7ebf\u60c5\u666f\u8bb0\u5fc6\u95ee\u7b54\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7ebf\u7a0b\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u5728\u6d88\u8d39\u7ea7\u548c\u4f01\u4e1a\u7ea7GPU\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8651.76%\u548c54.40%\u7684\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u4e91\u7aef\u65b9\u6848\u768456.00%\uff0c\u540c\u65f6\u517c\u987e\u9690\u79c1\u4e0e\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4e91\u7aef\u5378\u8f7d\u867d\u5e38\u89c1\uff0c\u4f46\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u548c\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5bf9\u53ef\u7a7f\u6234\u52a9\u624b\u800c\u8a00\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72MLLM\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u60c5\u666f\u8bb0\u5fc6\u95ee\u7b54\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u5f02\u6b65\u7ebf\u7a0b\u7684\u95ee\u7b54\u6d41\u6c34\u7ebf\uff1aDescriptor Thread\u6301\u7eed\u5c06\u89c6\u9891\u6d41\u8f6c\u6362\u4e3a\u8f7b\u91cf\u7ea7\u6587\u672c\u8bb0\u5fc6\uff1bQA Thread\u57fa\u4e8e\u8be5\u6587\u672c\u8bb0\u5fc6\u8fdb\u884c\u63a8\u7406\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u5728QAEgo4D-Closed\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e0d\u540c\u8d44\u6e90\u9650\u5236\u4e0b\u7684MLLM\u6027\u80fd\u3002", "result": "\u5728\u6d88\u8d39\u7ea78GB GPU\u4e0a\u8fbe\u523051.76%\u51c6\u786e\u7387\uff0cTTFT\u4e3a0.41\u79d2\uff1b\u5728\u672c\u5730\u4f01\u4e1a\u7ea7\u670d\u52a1\u5668\u4e0a\u8fbe\u523054.40%\u51c6\u786e\u7387\uff0cTTFT\u4e3a0.88\u79d2\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e91\u7aef\u65b9\u6848\u51c6\u786e\u7387\u4e3a56.00%\u3002", "conclusion": "\u8fb9\u7f18\u90e8\u7f72MLLM\u7528\u4e8e\u60c5\u666f\u8bb0\u5fc6\u95ee\u7b54\u5177\u6709\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u5728\u6027\u80fd\u63a5\u8fd1\u4e91\u7aef\u7684\u540c\u65f6\u4fdd\u969c\u4e86\u9690\u79c1\u548c\u4f4e\u5ef6\u8fdf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u7b49\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "summary_cn": "\u6211\u4eec\u7814\u7a76\u4e86\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8fdb\u884c\u5b9e\u65f6\u5728\u7ebf\u60c5\u666f\u8bb0\u5fc6\u95ee\u7b54\u7684\u53ef\u884c\u6027\u3002\u5c3d\u7ba1\u4e91\u7aef\u5378\u8f7d\u5f88\u5e38\u89c1\uff0c\u4f46\u5b83\u5728\u53ef\u7a7f\u6234\u52a9\u624b\u4e2d\u5f15\u53d1\u4e86\u9690\u79c1\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u56e0\u6b64\u6211\u4eec\u63a2\u7d22\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u73b0\u65b9\u6848\u3002\u6211\u4eec\u5c06\u6d41\u5f0f\u5904\u7406\u7ea6\u675f\u6574\u5408\u5230\u95ee\u7b54\u6d41\u6c34\u7ebf\u4e2d\uff0c\u8be5\u6d41\u6c34\u7ebf\u7531\u4e24\u4e2a\u5f02\u6b65\u7ebf\u7a0b\u7ec4\u6210\uff1a\u4e00\u4e2a\u63cf\u8ff0\u7b26\u7ebf\u7a0b\uff08Descriptor Thread\uff09\u6301\u7eed\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u8f7b\u91cf\u7ea7\u6587\u672c\u8bb0\u5fc6\uff0c\u53e6\u4e00\u4e2a\u95ee\u7b54\u7ebf\u7a0b\uff08QA Thread\uff09\u5219\u57fa\u4e8e\u8be5\u6587\u672c\u8bb0\u5fc6\u8fdb\u884c\u63a8\u7406\u4ee5\u56de\u7b54\u67e5\u8be2\u3002\u6211\u4eec\u5728QAEgo4D-Closed\u57fa\u51c6\u4e0a\u5206\u6790\u4e86MLLM\u5728\u4e25\u683c\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660e\u5176\u8868\u73b0\u9887\u5177\u524d\u666f\uff0c\u751a\u81f3\u53ef\u4e0e\u4e91\u7aef\u89e3\u51b3\u65b9\u6848\u76f8\u5ab2\u7f8e\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u6d88\u8d39\u7ea78GB GPU\u4e0a\u8fd0\u884c\u7684\u7aef\u5230\u7aef\u914d\u7f6e\u5b9e\u73b0\u4e8651.76%\u7684\u51c6\u786e\u7387\uff0c\u9996\u5b57\u751f\u6210\u65f6\u95f4\uff08TTFT\uff09\u4e3a0.41\u79d2\uff1b\u800c\u5728\u672c\u5730\u4f01\u4e1a\u7ea7\u670d\u52a1\u5668\u4e0a\u6269\u5c55\u90e8\u7f72\u65f6\uff0c\u51c6\u786e\u7387\u8fbe\u523054.40%\uff0cTTFT\u4e3a0.88\u79d2\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e91\u7aef\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u7387\u4e3a56.00%\u3002\u8fd9\u4e9b\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u51f8\u663e\u4e86\u57fa\u4e8e\u8fb9\u7f18\u7684\u65b9\u6848\u5728\u9690\u79c1\u4fdd\u62a4\u578b\u60c5\u666f\u8bb0\u5fc6\u68c0\u7d22\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.22462", "pdf": "https://arxiv.org/pdf/2602.22462", "abs": "https://arxiv.org/abs/2602.22462", "authors": ["Raiyan Jahangir", "Nafiz Imtiaz Khan", "Amritanand Sudheerkumar", "Vladimir Filkov"], "title": "MammoWise: Multi-Model Local RAG Pipeline for Mammography Report Generation", "categories": ["cs.CV", "cs.IR"], "comment": "arXiv preprint (submitted 25 Feb 2026). Local multi-model pipeline for mammography report generation + classification using prompting, multimodal RAG (ChromaDB), and QLoRA fine-tuning; evaluates MedGemma, LLaVA-Med, Qwen2.5-VL on VinDr-Mammo and DMID; reports BERTScore/ROUGE-L and classification metrics", "summary": "Screening mammography is high volume, time sensitive, and documentation heavy. Radiologists must translate subtle visual findings into consistent BI-RADS assessments, breast density categories, and structured narrative reports. While recent Vision Language Models (VLMs) enable image-to-text reporting, many rely on closed cloud systems or tightly coupled architectures that limit privacy, reproducibility, and adaptability. We present MammoWise, a local multi-model pipeline that transforms open source VLMs into mammogram report generators and multi-task classifiers. MammoWise supports any Ollama-hosted VLM and mammography dataset, and enables zero-shot, few-shot, and Chain-of-Thought prompting, with optional multimodal Retrieval Augmented Generation (RAG) using a vector database for case-specific context. We evaluate MedGemma, LLaVA-Med, and Qwen2.5-VL on VinDr-Mammo and DMID datasets, assessing report quality (BERTScore, ROUGE-L), BI-RADS classification, breast density, and key findings. Report generation is consistently strong and improves with few-shot prompting and RAG. Classification is feasible but sensitive to model and dataset choice. Parameter-efficient fine-tuning (QLoRA) of MedGemma improves reliability, achieving BI-RADS accuracy of 0.7545, density accuracy of 0.8840, and calcification accuracy of 0.9341 while preserving report quality. MammoWise provides a practical and extensible framework for deploying local VLMs for mammography reporting within a unified and reproducible workflow.", "AI": {"tldr": "MammoWise is a local, open-source multi-model pipeline that leverages Vision Language Models (VLMs) for mammography report generation and multi-task classification, supporting flexible prompting strategies and optional RAG, with demonstrated improvements via fine-tuning.", "motivation": "Current VLM-based solutions for mammography reporting often rely on closed or tightly coupled systems, limiting privacy, reproducibility, and adaptability; there is a need for an open, local, and flexible framework.", "method": "The authors propose MammoWise, a local pipeline using open-source VLMs (e.g., MedGemma, LLaVA-Med, Qwen2.5-VL) that supports zero-shot, few-shot, and Chain-of-Thought prompting, optionally enhanced with multimodal Retrieval Augmented Generation (RAG). They evaluate performance on VinDr-Mammo and DMID datasets across report quality and classification tasks, and apply QLoRA fine-tuning to improve reliability.", "result": "Report generation is consistently strong and improves with few-shot prompting and RAG. Classification performance varies by model and dataset. QLoRA fine-tuning of MedGemma yields high accuracy: BI-RADS (0.7545), breast density (0.8840), and calcification (0.9341), while maintaining report quality.", "conclusion": "MammoWise offers a practical, extensible, and reproducible local framework for deploying VLMs in mammography reporting and classification, balancing performance, privacy, and adaptability.", "summary_cn": "\u4e73\u817aX\u7ebf\u6444\u5f71\u7b5b\u67e5\u5177\u6709\u9ad8\u5de5\u4f5c\u91cf\u3001\u65f6\u95f4\u654f\u611f\u6027\u548c\u7e41\u91cd\u7684\u6587\u6863\u8bb0\u5f55\u9700\u6c42\u3002\u653e\u5c04\u79d1\u533b\u751f\u5fc5\u987b\u5c06\u7ec6\u5fae\u7684\u89c6\u89c9\u53d1\u73b0\u8f6c\u5316\u4e3a\u4e00\u81f4\u7684BI-RADS\u8bc4\u4f30\u3001\u4e73\u817a\u5bc6\u5ea6\u5206\u7c7b\u548c\u7ed3\u6784\u5316\u53d9\u8ff0\u62a5\u544a\u3002\u5c3d\u7ba1\u8fd1\u671f\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5df2\u5b9e\u73b0\u56fe\u50cf\u5230\u6587\u672c\u7684\u62a5\u544a\u751f\u6210\uff0c\u4f46\u8bb8\u591a\u7cfb\u7edf\u4f9d\u8d56\u5c01\u95ed\u7684\u4e91\u5e73\u53f0\u6216\u9ad8\u5ea6\u8026\u5408\u7684\u67b6\u6784\uff0c\u9650\u5236\u4e86\u9690\u79c1\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u9002\u5e94\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86MammoWise\u2014\u2014\u4e00\u4e2a\u672c\u5730\u591a\u6a21\u578b\u6d41\u6c34\u7ebf\uff0c\u5c06\u5f00\u6e90VLM\u8f6c\u5316\u4e3a\u4e73\u817aX\u7ebf\u6444\u5f71\u62a5\u544a\u751f\u6210\u5668\u548c\u591a\u4efb\u52a1\u5206\u7c7b\u5668\u3002MammoWise\u652f\u6301\u4efb\u4f55\u7531Ollama\u6258\u7ba1\u7684VLM\u548c\u4e73\u817aX\u7ebf\u6444\u5f71\u6570\u636e\u96c6\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u8fd8\u53ef\u9009\u5730\u7ed3\u5408\u5411\u91cf\u6570\u636e\u5e93\u5b9e\u73b0\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4ee5\u63d0\u4f9b\u75c5\u4f8b\u7279\u5b9a\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u5728VinDr-Mammo\u548cDMID\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86MedGemma\u3001LLaVA-Med\u548cQwen2.5-VL\uff0c\u8861\u91cf\u6307\u6807\u5305\u62ec\u62a5\u544a\u8d28\u91cf\uff08BERTScore\u3001ROUGE-L\uff09\u3001BI-RADS\u5206\u7c7b\u3001\u4e73\u817a\u5bc6\u5ea6\u548c\u5173\u952e\u53d1\u73b0\u8bc6\u522b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u62a5\u544a\u751f\u6210\u6548\u679c\u7a33\u5b9a\uff0c\u5e76\u5728\u5c11\u6837\u672c\u63d0\u793a\u548cRAG\u52a0\u6301\u4e0b\u8fdb\u4e00\u6b65\u63d0\u5347\uff1b\u5206\u7c7b\u4efb\u52a1\u53ef\u884c\u4f46\u5bf9\u6a21\u578b\u548c\u6570\u636e\u96c6\u9009\u62e9\u654f\u611f\u3002\u901a\u8fc7\u5bf9MedGemma\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08QLoRA\uff09\uff0c\u5728\u4fdd\u6301\u62a5\u544a\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86BI-RADS\u51c6\u786e\u73870.7545\u3001\u4e73\u817a\u5bc6\u5ea6\u51c6\u786e\u73870.8840\u548c\u9499\u5316\u51c6\u786e\u73870.9341\u3002MammoWise\u4e3a\u5728\u7edf\u4e00\u4e14\u53ef\u590d\u73b0\u7684\u5de5\u4f5c\u6d41\u4e2d\u90e8\u7f72\u672c\u5730VLM\u7528\u4e8e\u4e73\u817aX\u7ebf\u6444\u5f71\u62a5\u544a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2602.22469", "pdf": "https://arxiv.org/pdf/2602.22469", "abs": "https://arxiv.org/abs/2602.22469", "authors": ["Niamul Hassan Samin", "Md Arifur Rahman", "Abdullah Ibne Hanif", "Juena Ahmed Noshin", "Md Ashikur Rahman"], "title": "Beyond Dominant Patches: Spatial Credit Redistribution For Grounded Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) frequently hallucinate objects absent from the input image. We trace this failure to spatial credit collapse: activation credit concentrating on sparse visual patches in early transformer layers, which suppresses contextual evidence and increases reliance on language priors. We introduce Spatial Credit Redistribution (SCR), a training-free inference-time intervention that redistributes hidden-state activation from high-attention source patches to their context, guided by low-entropy inputs. We evaluate six model families (Chameleon, LLaVA, and Qwen, including both Qwen-VL and Qwen2-VL) at scales of 7B, 13B, and 30B, on POPE and CHAIR benchmarks. SCR reduces hallucination by ~4.7-6.0 percentage points on POPE-Adversarial, cuts CHAIR-s by 3.7-5.2 percentage points (42-51 percent relative), and CHAIR-i by 2.7-4.4 percentage points (44-58 percent relative), and preserves CIDEr within 0.8 percentage points. Gains are largest for low-entropy inputs, consistent with the theoretical framework. SCR incurs only 43-56 ms overhead (small models: +43-46 ms; large models: +54-56 ms), roughly 3-6 times lower than OPERA and VCD and 1.3-1.7 times lower than OVCD (+72 ms), while Pareto-dominating all three on both hallucination rate and CIDEr, making it practical for real-time settings. A controlled ablation confirms that attention-guided source selection is essential: replacing it with uniform random selection reduces hallucination rate gains from ~4.7-6.0 percentage points to only ~2.6-3.4 percentage points, pointing to credit-collapse as the key driver.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\u2014\u2014\u7a7a\u95f4\u4fe1\u7528\u91cd\u5206\u914d\uff08SCR\uff09\uff0c\u901a\u8fc7\u5c06\u65e9\u671fTransformer\u5c42\u4e2d\u96c6\u4e2d\u5728\u7a00\u758f\u89c6\u89c9\u5757\u4e0a\u7684\u6fc0\u6d3b\u91cd\u65b0\u5206\u914d\u5230\u5176\u4e0a\u4e0b\u6587\uff0c\u6709\u6548\u7f13\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u540c\u65f6\u51e0\u4e4e\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5177\u6709\u8f83\u4f4e\u7684\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5e38\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\u4e0a\u4ea7\u751f\u5e7b\u89c9\u3002\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e00\u95ee\u9898\u6e90\u4e8e\u201c\u7a7a\u95f4\u4fe1\u7528\u574d\u7f29\u201d\uff1a\u5728\u65e9\u671fTransformer\u5c42\u4e2d\uff0c\u6fc0\u6d3b\u4fe1\u7528\u8fc7\u5ea6\u96c6\u4e2d\u5728\u7a00\u758f\u7684\u89c6\u89c9\u5757\u4e0a\uff0c\u6291\u5236\u4e86\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u5bfc\u81f4\u6a21\u578b\u66f4\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u3002", "method": "\u63d0\u51faSpatial Credit Redistribution (SCR)\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u65e0\u9700\u8bad\u7ec3\u5730\u5c06\u9ad8\u6ce8\u610f\u529b\u6e90\u5757\u7684\u9690\u85cf\u72b6\u6001\u6fc0\u6d3b\u91cd\u65b0\u5206\u914d\u7ed9\u5176\u4e0a\u4e0b\u6587\uff0c\u8be5\u8fc7\u7a0b\u7531\u4f4e\u71b5\u8f93\u5165\u5f15\u5bfc\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u9009\u62e9\u6e90\u5757\uff0c\u800c\u975e\u968f\u673a\u9009\u62e9\u3002", "result": "\u5728POPE\u548cCHAIR\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e866\u4e2a\u6a21\u578b\u7cfb\u5217\uff08Chameleon\u3001LLaVA\u3001Qwen-VL/Qwen2-VL\uff0c\u53c2\u6570\u89c4\u6a217B\u201330B\uff09\u3002SCR\u5728POPE-Adversarial\u4e0a\u964d\u4f4e\u5e7b\u89c94.7\u20136.0\u4e2a\u767e\u5206\u70b9\uff1bCHAIR-s\u964d\u4f4e3.7\u20135.2\u4e2a\u767e\u5206\u70b9\uff08\u76f8\u5bf942\u201351%\uff09\uff1bCHAIR-i\u964d\u4f4e2.7\u20134.4\u4e2a\u767e\u5206\u70b9\uff08\u76f8\u5bf944\u201358%\uff09\uff1bCIDEr\u5f97\u5206\u4e0b\u964d\u4e0d\u8d85\u8fc70.8\u4e2a\u767e\u5206\u70b9\u3002\u63a8\u7406\u4ec5\u589e\u52a043\u201356ms\u5ef6\u8fdf\uff0c\u4f18\u4e8eOPERAs\u3001VCD\u548cOVCD\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u6e90\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7a7a\u95f4\u4fe1\u7528\u574d\u7f29\u662fVLM\u5e7b\u89c9\u7684\u5173\u952e\u539f\u56e0\uff0c\u800cSCR\u901a\u8fc7\u91cd\u5206\u914d\u65e9\u671f\u5c42\u7684\u6fc0\u6d3b\u4fe1\u7528\uff0c\u6709\u6548\u7f13\u89e3\u8be5\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u9ad8\u6548\u3001\u5b9e\u7528\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\uff0c\u5e76\u5728\u5e7b\u89c9\u6291\u5236\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e15\u7d2f\u6258\u6700\u4f18\u3002", "summary_cn": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7ecf\u5e38\u5bf9\u8f93\u5165\u56fe\u50cf\u4e2d\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\u4ea7\u751f\u5e7b\u89c9\u3002\u6211\u4eec\u5c06\u8fd9\u4e00\u5931\u8d25\u5f52\u56e0\u4e8e\u201c\u7a7a\u95f4\u4fe1\u7528\u574d\u7f29\u201d\uff1a\u5728\u65e9\u671fTransformer\u5c42\u4e2d\uff0c\u6fc0\u6d3b\u4fe1\u7528\u96c6\u4e2d\u5728\u7a00\u758f\u7684\u89c6\u89c9\u5757\u4e0a\uff0c\u4ece\u800c\u6291\u5236\u4e86\u4e0a\u4e0b\u6587\u8bc1\u636e\u5e76\u589e\u5f3a\u4e86\u5bf9\u8bed\u8a00\u5148\u9a8c\u7684\u4f9d\u8d56\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u7a7a\u95f4\u4fe1\u7528\u91cd\u5206\u914d\u201d\uff08SCR\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u9636\u6bb5\u5e72\u9884\u65b9\u6cd5\uff0c\u5b83\u6839\u636e\u4f4e\u71b5\u8f93\u5165\u7684\u5f15\u5bfc\uff0c\u5c06\u9ad8\u6ce8\u610f\u529b\u6e90\u5757\u7684\u9690\u85cf\u72b6\u6001\u6fc0\u6d3b\u91cd\u65b0\u5206\u914d\u7ed9\u5176\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u5728POPE\u548cCHAIR\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u516d\u4e2a\u6a21\u578b\u7cfb\u5217\uff08\u5305\u62ecChameleon\u3001LLaVA\u4ee5\u53caQwen-VL\u548cQwen2-VL\uff0c\u53c2\u6570\u89c4\u6a21\u6db5\u76d67B\u300113B\u548c30B\uff09\u3002SCR\u5728POPE-Adversarial\u4e0a\u5c06\u5e7b\u89c9\u7387\u964d\u4f4e\u4e86\u7ea64.7\u20136.0\u4e2a\u767e\u5206\u70b9\uff0c\u5728CHAIR-s\u4e0a\u964d\u4f4e\u4e863.7\u20135.2\u4e2a\u767e\u5206\u70b9\uff08\u76f8\u5bf9\u964d\u4f4e42\u201351%\uff09\uff0c\u5728CHAIR-i\u4e0a\u964d\u4f4e\u4e862.7\u20134.4\u4e2a\u767e\u5206\u70b9\uff08\u76f8\u5bf9\u964d\u4f4e44\u201358%\uff09\uff0c\u540c\u65f6CIDEr\u5f97\u5206\u4e0b\u964d\u4e0d\u8d85\u8fc70.8\u4e2a\u767e\u5206\u70b9\u3002\u5728\u4f4e\u71b5\u8f93\u5165\u4e0a\u6548\u679c\u6700\u4e3a\u663e\u8457\uff0c\u7b26\u5408\u7406\u8bba\u6846\u67b6\u3002SCR\u4ec5\u5e26\u676543\u201356\u6beb\u79d2\u7684\u63a8\u7406\u5f00\u9500\uff08\u5c0f\u6a21\u578b+43\u201346ms\uff0c\u5927\u6a21\u578b+54\u201356ms\uff09\uff0c\u7ea6\u4e3aOPERAs\u548cVCD\u65b9\u6cd5\u76841/3\u20131/6\uff0c\u4e5f\u4f4e\u4e8eOVCD\uff08+72ms\uff09\u76841.3\u20131.7\u500d\uff0c\u5e76\u5728\u5e7b\u89c9\u7387\u548cCIDEr\u4e24\u4e2a\u6307\u6807\u4e0a\u5747\u5e15\u7d2f\u6258\u4f18\u4e8e\u8fd9\u4e09\u79cd\u65b9\u6cd5\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002\u63a7\u5236\u6027\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6e90\u5757\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff1a\u82e5\u66ff\u6362\u4e3a\u5747\u5300\u968f\u673a\u9009\u62e9\uff0c\u5e7b\u89c9\u7387\u6539\u5584\u5c06\u4ece\u7ea64.7\u20136.0\u4e2a\u767e\u5206\u70b9\u964d\u81f3\u4ec52.6\u20133.4\u4e2a\u767e\u5206\u70b9\uff0c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4fe1\u7528\u574d\u7f29\u662f\u5e7b\u89c9\u7684\u6838\u5fc3\u9a71\u52a8\u56e0\u7d20\u3002"}}
