<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的对抗性补丁生成、优化与评估流程，用于攻击面部生物识别系统，并结合ViT-GPT2模型生成语义描述以支持法证分析，同时利用感知哈希和分割技术有效检测对抗样本。


<details>
  <summary>Details</summary>
Motivation: 提升对人脸识别系统在对抗攻击下的脆弱性理解，并为法证分析和安全测试提供可解释、可检测的对抗样本生成与分析方法。

Method: 使用FGSM生成针对身份分类器的对抗噪声，通过扩散模型进行反向扩散，结合高斯平滑和自适应亮度校正提升不可感知性；将优化后的补丁应用于人脸图像，并利用ViT-GPT2生成身份语义描述；采用感知哈希和分割技术检测对抗样本。

Result: 所提方法在保持视觉自然性的同时成功规避人脸识别系统，SSIM达到0.95，并能有效检测和分析对抗补丁及其对身份验证与表情识别的影响。

Conclusion: 该端到端流程不仅有效生成高逼真度的对抗补丁用于攻击和测试，还提供了语义解释与检测手段，有助于提升生物识别系统的安全性与可解释性。

Abstract: This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

Abstract (中文翻译): 本研究提出了一种端到端的流程，用于生成、优化和评估对抗性补丁，以攻击面部生物识别系统，并应用于法证分析与安全测试。我们采用FGSM方法生成针对身份分类器的对抗噪声，并利用扩散模型通过反向扩散过程，结合高斯平滑与自适应亮度校正，提升对抗补丁的不可感知性，从而实现合成对抗补丁的隐蔽规避。优化后的补丁被应用于人脸图像，以测试其在保持自然视觉特征的同时规避识别系统的能力。此外，利用Vision Transformer（ViT）-GPT2模型生成图像描述，为对抗图像中人物身份提供语义解释，支持身份规避与识别攻击的法证解读与文档记录。该流程评估了在对抗条件下身份分类、图像描述结果的变化，以及面部身份验证与表情识别系统的脆弱性。我们进一步通过感知哈希与图像分割技术，实现了对对抗补丁及对抗样本的有效检测与分析，取得了0.95的SSIM指标。

</details>


### [2] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TL;DR: LCF3D是一种新型的RGB与LiDAR融合框架，通过后期融合减少LiDAR误检，并通过级联融合恢复漏检目标，在多个自动驾驶数据集上显著提升3D检测性能，尤其对行人和骑行者等挑战性类别。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，准确地定位3D物体（如行人、骑行者和其他车辆）至关重要。虽然自动驾驶车辆通常结合使用RGB摄像头和LiDAR传感器以提高检测性能，但如何有效融合这两种模态的数据仍是一个挑战。

Method: 提出LCF3D框架，结合2D RGB图像检测器和3D LiDAR点云检测器，采用两种融合策略：(i) 后期融合，通过匹配RGB 2D检测与LiDAR 3D检测来过滤未匹配的LiDAR误检；(ii) 级联融合，通过为未匹配的RGB检测生成新的3D视锥建议来恢复LiDAR漏检的目标。

Result: 实验表明，LCF3D在KITTI和nuScenes数据集上显著优于纯LiDAR方法，尤其在行人、骑行者、摩托车和自行车等类别上表现突出，并具有良好的域泛化能力，能应对训练与测试时传感器配置不同的情况。

Conclusion: LCF3D通过多模态融合有效提升了3D目标检测的准确性和鲁棒性，特别是在处理挑战性类别和不同传感器配置场景下表现出色，为自动驾驶中的感知系统提供了实用解决方案。

Abstract: Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

Abstract (中文翻译): 在自动驾驶中，准确地定位行人、骑行者及其他车辆等3D物体至关重要。为确保高检测性能，自动驾驶车辆通常将RGB摄像头与LiDAR传感器相结合，但如何有效融合这两种数据源以实现3D目标检测仍具挑战性。我们提出了LCF3D，一种新颖的传感器融合框架，将RGB图像上的2D目标检测器与LiDAR点云上的3D目标检测器相结合。通过利用多模态融合原则，我们弥补了LiDAR目标检测网络中的不准确性。该方案结合了两个关键原则：(i) 后期融合，通过将LiDAR 3D检测结果与RGB 2D检测结果进行匹配，并滤除未匹配的LiDAR检测，从而减少LiDAR的误报；(ii) 级联融合，通过为未匹配的RGB检测生成新的3D视锥区域建议，从而恢复LiDAR遗漏的目标。实验表明，LCF3D在域泛化方面具有优势，能够有效应对训练域与测试域之间传感器配置不同的情况。在KITTI数据集中，LCF3D在行人和骑行者等具有挑战性的类别上，以及在nuScenes数据集中的摩托车和自行车类别上，均显著优于基于LiDAR的方法。代码可从以下地址下载：https://github.com/CarloSgaravatti/LCF3D。

</details>


### [3] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

TL;DR: 本研究比较了DenseNet121和EfficientNet-B0两种CNN模型在儿童肺炎X光片自动诊断中的性能，结果表明EfficientNet-B0在准确率、F1分数和MCC等指标上更优，且结合Grad-CAM与LIME可解释性方法验证了模型关注区域的临床相关性。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎是全球致病和致死的主要原因之一，亟需高效准确的辅助诊断工具；深度学习在医学影像分析中展现出潜力，因此有必要评估先进CNN架构在该任务中的表现。

Method: 使用包含5,863张儿童胸部X光图像的公开数据集，对图像进行归一化、调整尺寸和数据增强；基于ImageNet预训练权重，在相同设置下微调DenseNet121和EfficientNet-B0；采用准确率、F1分数、MCC和召回率评估性能，并利用Grad-CAM和LIME进行可解释性分析。

Result: EfficientNet-B0优于DenseNet121，准确率达84.6%、F1分数为0.8899、MCC为0.6849；DenseNet121准确率为79.7%、F1分数为0.8597、MCC为0.5852；两模型召回率均超0.99；Grad-CAM与LIME显示模型聚焦于临床相关肺部区域。

Conclusion: EfficientNet-B0在性能和计算效率上更均衡，适合临床部署；结合可解释性技术可提升AI辅助诊断的透明度与可信度。

Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

Abstract (中文翻译): 背景：肺炎仍是全球儿童发病和死亡的主要原因，凸显了对准确高效诊断支持工具的需求。深度学习在医学图像分析中展现出强大潜力，尤其是在胸部X光解读方面。本研究比较了两种最先进的卷积神经网络（CNN）架构在儿童肺炎自动检测中的表现。方法：使用一个包含5,863张儿童胸部X光图像的公开数据集。图像经过归一化、尺寸调整和数据增强以提升泛化能力。在相同训练设置下，利用ImageNet预训练权重对DenseNet121和EfficientNet-B0进行微调。通过准确率、F1分数、马修斯相关系数（MCC）和召回率评估性能，并采用梯度加权类激活映射（Grad-CAM）和局部可解释模型无关解释（LIME）实现模型可解释性，以可视化影响预测的图像区域。结果：EfficientNet-B0优于DenseNet121，准确率达到84.6%，F1分数为0.8899，MCC为0.6849；而DenseNet121的准确率为79.7%，F1分数为0.8597，MCC为0.5852。两个模型的召回率均高于0.99，表明对肺炎检测具有高敏感性。Grad-CAM和LIME可视化结果显示，模型一致聚焦于临床上相关的肺部区域，支持其决策的可靠性。结论：与DenseNet121相比，EfficientNet-B0提供了更均衡且计算效率更高的性能，是临床部署的有力候选方案。可解释性技术的整合增强了人工智能辅助儿童肺炎诊断的透明度和可信度。

</details>


### [4] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

TL;DR: 提出NanoSD，一种从Stable Diffusion 1.5蒸馏而来的轻量级扩散基础模型家族，通过联合优化U-Net与VAE结构，在保持生成先验的同时实现边缘设备上的实时推理，并在多个图像恢复任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级扩散模型主要压缩U-Net或缩短扩散轨迹，破坏了潜在流形并限制了跨任务泛化能力；同时完整扩散流程计算开销大，难以部署于边缘设备。

Method: 采用网络剪枝、逐特征生成式蒸馏和结构化架构缩放，对U-Net与VAE编解码器进行全流水线协同设计，构建Pareto最优的轻量扩散模型家族NanoSD。

Result: NanoSD模型参数量为130M–315M，在移动端NPU上可实现实时推理（低至20ms），在超分、去模糊、人脸修复和单目深度估计等任务上优于现有轻量扩散模型，在感知质量和实际部署性方面均表现优异。

Conclusion: NanoSD提供了一种通用、高效且可部署的扩散基础模型家族，适用于边缘设备上的实时视觉生成与恢复任务。

Abstract: Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

Abstract (中文翻译): 诸如Stable Diffusion 1.5之类的潜在扩散模型提供了强大的生成先验，在图像恢复任务中极具价值，但其完整流程计算开销过大，难以部署于边缘设备。现有的轻量级变体主要通过压缩去噪U-Net或缩短扩散轨迹来减小模型规模，但这会破坏底层潜在流形，限制模型在单一任务之外的泛化能力。我们提出了NanoSD——一个从Stable Diffusion 1.5蒸馏而来的帕累托最优扩散基础模型家族，通过网络剪枝、逐特征生成式蒸馏以及结构化架构缩放，对U-Net和VAE编码器-解码器进行联合优化。这种全流水线协同设计在保留生成先验的同时，产生了在精度-延迟-模型大小前沿上占据不同运行点的模型（例如参数量为1.3亿至3.15亿，在移动级NPU上推理时间可低至20毫秒）。我们发现仅减少参数量并不一定提升硬件效率，并通过分析揭示了架构平衡、特征路由和潜在空间保留如何共同影响真实设备上的延迟。当作为即插即用骨干网络使用时，NanoSD在图像超分辨率、图像去模糊、人脸修复和单目深度估计等多个任务上实现了最先进的性能，在感知质量和实际部署性方面均优于以往的轻量级扩散模型。NanoSD建立了一个适用于边缘设备实时视觉生成与恢复的通用扩散基础模型家族。

</details>


### [5] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

TL;DR: 本文提出Unified Hashing（UniHash），一种结合pointwise和pairwise训练范式的双分支深度哈希框架，通过双向知识迁移提升在已见和未见类别上的图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度哈希方法通常仅采用单一训练范式（pointwise或pairwise），前者在已见类别上表现好，后者在未见类别上泛化能力强，但难以兼顾两者。为解决这一局限，需设计能融合两种范式优势的方法。

Method: 提出UniHash双分支框架：一个基于中心的pointwise分支和一个pairwise分支；引入互学习损失对齐哈希表示，并设计Split-Merge Mixture of Hash Experts（SM-MoH）模块增强跨分支哈希表示交换，实现双向知识迁移。

Result: 在CIFAR-10、MSCOCO和ImageNet上的实验表明，UniHash在已见和未见类别的图像检索任务中均达到最先进的性能。

Conclusion: UniHash通过统一两种训练范式，有效提升了哈希码的判别性和泛化能力，在跨已见/未见类别的图像检索中表现优异。

Abstract: Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

Abstract (中文翻译): 在现代图像检索系统中，实现对已见类别和未见类别的有效检索至关重要。已见类别的检索确保对已知类别的精确识别，而未见类别的检索则促进模型在有限监督下对新类别的泛化能力。然而，大多数现有的深度哈希方法局限于单一的训练范式——要么是逐点（pointwise）范式，要么是成对（pairwise）范式，其中前者在已见类别上表现优异，后者在未见类别上具有更好的泛化能力。为克服这一局限，我们提出了统一哈希（Unified Hashing, UniHash），这是一种双分支框架，融合了两种范式的优点，以在已见和未见类别上实现均衡的检索性能。UniHash包含两个互补分支：一个遵循逐点范式的基于中心的分支，以及一个遵循成对范式的分支。我们引入了一种新颖的哈希码学习方法，以实现分支间的双向知识迁移，从而提升哈希码的判别性和泛化能力。该方法采用互学习损失来对齐哈希表示，并引入Split-Merge Mixture of Hash Experts（SM-MoH）模块以增强跨分支的哈希表示交换。理论分析验证了UniHash的有效性，且在CIFAR-10、MSCOCO和ImageNet上的大量实验表明，UniHash在已见和未见图像检索场景中均持续达到最先进的性能。

</details>


### [6] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 提出了一种名为ViSIL的新指标，用于评估多模态视频摘要（关键帧+文本）的信息覆盖度，通过衡量未被摘要捕获的视频信息量，实现跨模态格式的统一比较，并在VQA任务中显著优于传统文本摘要。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如BLEU、ROUGE）无法有效衡量多模态视频摘要（包含关键帧和文本）在不同模态间的信息覆盖程度，缺乏统一的评估标准。

Method: 提出Video Summary Information Loss (ViSIL)评分，基于视觉-语言模型（VLM）推理，从信息论角度量化视频摘要未能捕捉的视频信息量，从而构建一个可跨模态结构比较的统一评估框架。

Result: ViSIL评分与人类及VLM在视频问答（VQA）任务中的表现具有统计显著相关性；利用ViSIL进行摘要选择可在不增加处理负担的前提下，在VQA准确率上比纯文本摘要提升7%，并构建出帕累托最优前沿。

Conclusion: ViSIL提供了一种有效且统一的多模态视频摘要评估方法，不仅更贴合实际语义覆盖需求，还能优化摘要效率与信息保留之间的权衡。

Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

Abstract (中文翻译): 多模态视频字幕将密集的视频内容压缩为关键帧与自然语言相结合的结构化形式。通过构建连贯的多模态摘要，该方法将生成式AI锚定于丰富的语义证据中，并作为高效检索的轻量级代理。然而，BLEU或ROUGE等传统指标无法量化不同模态（例如一段文字与一系列关键帧）之间的信息覆盖程度。为此，我们提出了视频摘要信息损失（ViSIL）评分，这是一种基于信息论的框架，通过视觉-语言模型（VLM）推理来量化摘要未能捕捉的视频信息量。通过衡量信息损失，ViSIL成为一个统一的指标，能够直接比较结构各异的多模态摘要格式。实验结果表明，ViSIL评分与人类及VLM在视频问答（VQA）任务中的表现具有统计显著的相关性。此外，ViSIL还能用于摘要选择，在信息损失与处理速度之间实现最优权衡，在不增加处理负载的情况下，使VQA准确率相比纯文本摘要提升7%，并构建出帕累托最优前沿。

</details>


### [7] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

TL;DR: 本文提出TuneCLIP，一种自监督微调框架，可在不重新训练的情况下提升开源CLIP模型在多种下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 改进CLIP模型通常需要从头开始在数十亿样本上训练，成本高昂；作者探索是否能仅用现有自监督数据集提升开源CLIP模型的通用性能。

Method: TuneCLIP包含两个关键阶段：(1) 基于理论分析的优化统计量恢复热身阶段，以减少冷启动偏差；(2) 优化新对比损失的微调阶段，以缓解对假负样本对的惩罚。

Result: 在多种模型架构和规模上一致提升性能，例如SigLIP (ViT-B/16) 在ImageNet及分布外基准上提升最多+2.5%，在DataComp基准上提升+1.2%。

Conclusion: TuneCLIP为高效地进行预训练后适应提供了新的强基线，有效克服了直接微调导致性能下降的问题。

Abstract: CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

Abstract (中文翻译): CLIP已成为多模态表征学习的基石，但提升其性能通常需要从头开始在数十亿样本上进行训练，这一过程成本极高。我们提出了一个不同的问题：能否仅利用现有的自监督数据集，在多种下游任务中提升开源权重CLIP模型的性能？与将预训练模型适配到单一任务的监督微调不同，我们的目标是提升模型在各类任务中的通用性能。然而，正如我们的实验和先前研究所揭示的那样，直接对开源CLIP模型应用标准训练协议往往会导致性能下降。本文提出了TuneCLIP——一种自监督微调框架，可有效克服性能退化问题。TuneCLIP包含两个关键组成部分：(1) 受理论分析启发的“热身”阶段，用于恢复优化统计量以减少冷启动偏差；(2) 微调阶段，通过优化一种新的对比损失来减轻对假负样本对的惩罚。大量实验表明，TuneCLIP在不同模型架构和规模下均能持续提升性能。值得注意的是，该方法显著提升了SigLIP（ViT-B/16）等领先的开源模型，在ImageNet及其相关分布外基准上最高提升+2.5%，在竞争激烈的DataComp基准上提升+1.2%，为高效的预训练后适应设立了新的强大基线。

</details>


### [8] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

TL;DR: VibrantSR 是一种基于 Sentinel-2 卫星影像的生成式超分辨率框架，可从 10 米分辨率影像生成 0.5 米冠层高度模型，在美国西部 22 个生态区评估中 MAE 为 4.39 米，优于多个现有卫星产品，支持大范围、低成本、高频次的森林监测。


<details>
  <summary>Details</summary>
Motivation: 传统基于航空影像的冠层高度建模方法受限于获取频率低、覆盖不规律，难以支持持续的大尺度森林监测与碳核算；因此需要一种利用全球可获取、高时间频率的卫星数据（如 Sentinel-2）进行高分辨率冠层高度估计的方法。

Method: 提出 VibrantSR 框架，利用 Sentinel-2 的季节性合成影像，通过生成式超分辨率技术将 10 米分辨率影像提升至 0.5 米冠层高度模型（CHM）。

Result: 在美国西部 22 个 EPA Level 3 生态区的独立空间验证中，VibrantSR 对 ≥2 米冠层高度的平均绝对误差（MAE）为 4.39 米，优于 Meta（4.83 米）、LANDFIRE（5.96 米）和 ETH（7.05 米）等卫星基准产品；虽略逊于基于航空影像的 VibrantVS（2.71 米 MAE），但具备更优的可扩展性和时效性。

Conclusion: VibrantSR 能够在不依赖昂贵且低频航空数据的前提下，实现大陆尺度的业务化森林监测与碳核算，为大范围、高频次的生态系统观测提供可行方案。

Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

Abstract (中文翻译): 我们提出了 VibrantSR（Vibrant 超分辨率），这是一种生成式超分辨率框架，用于从 10 米分辨率的 Sentinel-2 影像中估算 0.5 米分辨率的冠层高度模型（CHM）。与依赖航空影像的方法不同——后者受限于获取频率低且不规律，VibrantSR 利用全球可获取的 Sentinel-2 季节性合成影像，能够以季节至年度的频率进行持续监测。在美国西部 22 个 EPA 第三级生态区上，使用空间分离的验证集进行评估，VibrantSR 在冠层高度 ≥2 米时的平均绝对误差（MAE）为 4.39 米，优于 Meta（4.83 米）、LANDFIRE（5.96 米）和 ETH（7.05 米）等基于卫星的基准产品。尽管基于航空影像的 VibrantVS（MAE 为 2.71 米）在精度上仍具优势，但 VibrantSR 无需依赖成本高昂且时间上不连续的航空数据，即可实现大陆尺度的业务化森林监测与碳核算。

</details>


### [9] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: 本文提出 MedVL-SAM2，一个统一的 3D 医学多模态模型，同时支持报告生成、视觉问答（VQA）和多种范式的分割任务（包括语义、指代表达和交互式分割），通过结合图像级推理与像素级感知，在多个任务上达到 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型（VLMs）在图像级文本中心任务（如报告生成和 VQA）上表现良好，但在实现细粒度视觉定位和 3D 体数据空间推理方面仍存在挑战，尤其缺乏一个能统一这些能力的通用框架。

Method: 提出 MedVL-SAM2 模型，采用专为 3D 医学影像设计的统一架构，整合图像级推理与像素级感知，并引入基于 SAM2 的体分割模块以支持多粒度空间推理。模型首先在大规模 3D CT 图文对上预训练，再通过包含语言理解和分割目标的联合优化进行微调。

Result: MedVL-SAM2 在报告生成、VQA 和多种 3D 分割任务上均取得 SOTA 性能；实验表明其具备可靠的 3D 视觉定位能力、可控的交互式分割能力以及鲁棒的跨模态推理能力。

Conclusion: 高阶语义推理与精确的 3D 定位可以在统一的 3D 医学 VLM 中共同实现，MedVL-SAM2 为此提供了一个有效且通用的解决方案。

Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

Abstract (中文翻译): 近期医学视觉-语言模型（VLMs）在图像级文本中心任务（如报告生成和视觉问答（VQA））上取得了优异性能。然而，在3D医学VLM中实现细粒度视觉定位和体数据空间推理仍然具有挑战性，尤其是在试图将这些能力统一到单一、可泛化的框架中时。为解决这一问题，我们提出了 MedVL-SAM2——一个统一的3D医学多模态模型，能够同时支持报告生成、VQA 以及多范式分割任务，包括语义分割、指代表达分割和交互式分割。MedVL-SAM2 通过专为3D医学影像定制的连贯架构，整合了图像级推理与像素级感知，并引入基于 SAM2 的体分割模块，以实现精确的多粒度空间推理。该模型采用多阶段训练流程：首先在大规模3D CT图像-文本对语料库上进行预训练，以对齐体数据视觉特征与放射学语言嵌入；随后利用全面的3D CT分割数据集，对语言理解与分割目标进行联合优化。这种联合训练使得模型可通过语言、点或框提示进行灵活交互，从而统一高层视觉推理与空间精确定位。我们的统一架构在报告生成、VQA 和多种3D分割任务上均达到了最先进的性能。大量分析进一步表明，该模型提供了可靠的3D视觉定位、可控的交互式分割以及鲁棒的跨模态推理能力，证明了高层语义推理与精确3D定位可在统一的3D医学VLM中共同实现。

</details>


### [10] [Transition Matching Distillation for Fast Video Generation](https://arxiv.org/abs/2601.09881)
*Weili Nie,Julius Berner,Nanye Ma,Chao Liu,Saining Xie,Arash Vahdat*

Main category: cs.CV

TL;DR: 本文提出了一种名为Transition Matching Distillation（TMD）的新框架，用于将视频扩散模型蒸馏为高效的少步生成器，通过匹配多步去噪轨迹与少步概率转移过程，在保持视觉质量和提示一致性的同时显著提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 大型视频扩散和流模型虽然在高质量视频生成方面取得了显著成果，但由于其低效的多步采样过程，难以应用于实时交互场景。因此，亟需一种能有效压缩推理步骤、同时保留生成质量的蒸馏方法。

Method: TMD框架的核心思想是将扩散模型的多步去噪轨迹与一个少步的概率转移过程对齐，其中每一步转移由轻量级的条件流建模。作者将原始扩散主干网络分解为主干部分（提取语义表示）和流头部分（执行多次内部流更新），并引入分布匹配蒸馏策略，在每个转移步骤中对学生模型进行训练。

Result: 在Wan2.1 1.3B和14B文本到视频模型上的大量实验表明，TMD在生成速度与视觉质量之间实现了灵活且优越的权衡，在相近推理成本下，其生成结果在视觉保真度和提示遵循能力上优于现有蒸馏方法。

Conclusion: TMD是一种高效且有效的视频扩散模型蒸馏方法，能够显著加速视频生成过程，同时维持高质量输出，为实时交互式视频生成应用提供了可行路径。

Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd

Abstract (中文翻译): 大型视频扩散模型和流模型在高质量视频生成方面取得了显著成功，但由于其低效的多步采样过程，其在实时交互应用中的使用仍受限。在本研究中，我们提出了“转移匹配蒸馏”（Transition Matching Distillation, TMD），这是一种将视频扩散模型蒸馏为高效少步生成器的新框架。TMD的核心思想是将扩散模型的多步去噪轨迹与一个少步的概率转移过程相匹配，其中每一步转移被建模为一个轻量级的条件流。为了实现高效蒸馏，我们将原始扩散主干网络分解为两个组件：（1）主干部分，包含大部分早期层，在每个外部转移步骤中提取语义表示；（2）流头部分，由最后几层组成，利用这些表示执行多次内部流更新。给定一个预训练的视频扩散模型，我们首先为其引入一个流头，并将其适配为条件流映射。然后，我们在每个转移步骤中对带有流头展开的学生模型应用分布匹配蒸馏。在Wan2.1 1.3B和14B文本到视频模型上的大量实验表明，TMD在生成速度与视觉质量之间提供了灵活而强大的权衡。特别是在相近推理成本下，TMD在视觉保真度和提示遵循能力方面优于现有的蒸馏模型。项目页面：https://research.nvidia.com/labs/genair/tmd

</details>
