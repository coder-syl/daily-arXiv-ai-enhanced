<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study](https://arxiv.org/abs/2601.11612)
*Arnav S. Sonavane*

Main category: cs.CV

TL;DR: 在农业病害分类任务中，仅使用3,000张无标签农业图像进行SimCLR自监督预训练，即可带来比改进模型架构更大的准确率提升（+4.57%），且该优势在不同视觉Transformer架构中均存在。HierarchicalViT（HVT）在多个数据集上表现优异，并具有良好的校准性能。


<details>
  <summary>Details</summary>
Motivation: 探索领域特定的自监督预训练对农业病害分类任务的影响，以判断在资源有限的情况下，是应优先收集领域数据还是优化模型架构。

Method: 采用SimCLR自监督学习方法在3,000张未标注的农业图像上进行预训练，并结合层次化视觉Transformer（HierarchicalViT，HVT）进行下游分类任务。在Cotton Leaf Disease、PlantVillage和PlantDoc三个数据集上评估模型性能，并与Swin-Base、ViT-Base等模型进行对比。同时分析模型的校准性能（ECE）。

Result: SimCLR预训练在HVT上带来+4.57%的准确率提升，优于层次结构设计本身的+3.70%增益；该预训练策略在Swin-Base（+4.08%）和ViT-Base（+4.20%）上同样有效。HVT-Base（78M参数）在综合准确率上优于Swin-Base（88M参数）1.68%。HVT的ECE为3.56%（温度缩放后为1.52%）。

Conclusion: 在农业图像分类任务中，领域特定的自监督预训练比模型架构选择更为关键。建议从业者优先收集领域内无标签数据进行预训练，以获得更显著的性能提升。所提出的HierarchicalViT兼具高性能与良好校准性，适合实际部署。

Abstract: We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: https://github.com/w2sg-arnav/HierarchicalViT

Abstract (中文翻译): 我们研究了领域特定的自监督预训练对农业病害分类任务的影响，采用了层次化视觉Transformer模型。我们的主要发现是：仅使用3,000张未标注的农业图像进行SimCLR预训练，即可带来+4.57%的准确率提升，这一增益超过了层次化架构设计本身带来的+3.70%提升。更重要的是，我们证明这种自监督学习（SSL）的优势与模型架构无关：将相同的预训练应用于Swin-Base可获得+4.08%的提升，应用于ViT-Base则获得+4.20%的提升，这表明从业者应优先收集领域数据而非过度关注架构选择。我们使用一种名为HierarchicalViT（HVT）的Swin风格层次化Transformer，在三个数据集上进行了评估：棉花叶病（7类，90.24%）、PlantVillage（38类，96.3%）和PlantDoc（27类，87.1%）。在参数量相近的情况下，HVT-Base（78M）达到88.91%的准确率，优于Swin-Base（88M）的87.23%，提升了1.68%。为评估部署可靠性，我们还报告了校准分析结果：HVT的期望校准误差（ECE）为3.56%（经温度缩放后降至1.52%）。代码地址：https://github.com/w2sg-arnav/HierarchicalViT

</details>


### [2] [Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning](https://arxiv.org/abs/2601.11614)
*Jason Qiu*

Main category: cs.CV

TL;DR: 本文提出一种基于3D TransUNet的图像合成方法，从常规T1加权MRI预测扩散MRI（dMRI）的FA和MD图，从而在无dMRI数据的情况下提升阿尔茨海默病（AD）及轻度认知障碍（MCI）的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的病理变化早于临床症状多年，早期检测至关重要。虽然dMRI能捕捉早期微结构异常，但其采集耗时且易受运动伪影影响，难以在临床常规使用。而T1w MRI虽广泛使用，但仅反映较晚期的宏观结构变化。因此，亟需一种方法从常规T1w MRI中提取dMRI所包含的微结构信息。

Method: 作者提出一个3D TransUNet图像合成框架，直接从T1w MRI生成高保真度的FA和MD图，并将这些合成特征整合到多模态诊断模型中用于AD和MCI分类。

Result: 合成的FA/MD图与真实dMRI高度一致（SSIM > 0.93，Pearson相关性 > 0.94）。在多模态诊断模型中，该方法使AD分类准确率提升5%（78.75%→83.75%），MCI检测准确率提升12.5%。

Conclusion: 该研究表明，高质量的扩散微结构信息可从常规T1w MRI中有效推断，从而在缺乏dMRI数据的临床场景中实现多模态成像的优势，有望提高AD诊断的可及性、效率和准确性。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.

Abstract (中文翻译): 阿尔茨海默病（AD）是一种进行性神经退行性疾病，其病理变化在临床症状出现前多年就已开始，因此早期检测对于及时干预至关重要。T1加权（T1w）磁共振成像（MRI）在临床实践中常用于识别宏观脑部改变，但这些改变通常出现在疾病进程相对较晚的阶段。相比之下，扩散MRI（dMRI）通过探测脑组织中水分子的扩散，对更早期的微结构异常具有敏感性。dMRI指标（如各向异性分数FA和平均扩散率MD）可提供关于白质完整性和神经退行性的补充信息。然而，dMRI采集过程耗时且易受运动伪影影响，限制了其在临床人群中的常规应用。为弥合这一差距，本文提出一种3D TransUNet图像合成框架，可直接从T1w MRI预测FA和MD图。该模型生成的图像具有高保真度，结构相似性指数（SSIM）超过0.93，与真实dMRI的皮尔逊相关性高于0.94。当将这些合成特征整合到多模态诊断模型中时，AD分类准确率提升了5%（从78.75%提升至83.75%），更重要的是，轻度认知障碍（MCI）的检测准确率提高了12.5%。本研究证明，高质量的扩散微结构信息可以从常规采集的T1w MRI中推断出来，从而在缺乏扩散数据的情况下有效实现多模态成像的优势。通过在减少扫描时间的同时保留互补的结构与微结构信息，所提出的方法有望提升临床实践中AD诊断的可及性、效率和准确性。

</details>


### [3] [PointSLAM++: Robust Dense Neural Gaussian Point Cloud-based SLAM](https://arxiv.org/abs/2601.11617)
*Xu Wang,Boyao Han,Xiaojun Chen,Ying Liu,Ruihui Li*

Main category: cs.CV

TL;DR: PointSLAM++ is a novel RGB-D SLAM system using hierarchically constrained neural Gaussians and progressive pose optimization to achieve high-accuracy, photorealistic 3D reconstruction robust to depth noise.


<details>
  <summary>Details</summary>
Motivation: Existing SLAM methods often fail to maintain structural consistency and accurate pose estimation under depth sensor noise, limiting their effectiveness in real-time applications like AR and robotics.

Method: PointSLAM++ uses a hierarchically constrained neural Gaussian representation to preserve structural relationships, progressive pose optimization to reduce depth noise impact, and a dynamic neural representation graph that adapts Gaussian node distribution based on local geometry.

Result: Experiments show PointSLAM++ surpasses current 3DGS-based SLAM approaches in both reconstruction accuracy and rendering quality.

Conclusion: PointSLAM++ offers a robust and high-fidelity solution for real-time 3D mapping, making it well-suited for large-scale augmented reality and robotic applications.

Abstract: Real-time 3D reconstruction is crucial for robotics and augmented reality, yet current simultaneous localization and mapping(SLAM) approaches often struggle to maintain structural consistency and robust pose estimation in the presence of depth noise. This work introduces PointSLAM++, a novel RGB-D SLAM system that leverages a hierarchically constrained neural Gaussian representation to preserve structural relationships while generating Gaussian primitives for scene mapping. It also employs progressive pose optimization to mitigate depth sensor noise, significantly enhancing localization accuracy. Furthermore, it utilizes a dynamic neural representation graph that adjusts the distribution of Gaussian nodes based on local geometric complexity, enabling the map to adapt to intricate scene details in real time. This combination yields high-precision 3D mapping and photorealistic scene rendering. Experimental results show PointSLAM++ outperforms existing 3DGS-based SLAM methods in reconstruction accuracy and rendering quality, demonstrating its advantages for large-scale AR and robotics.

Abstract (中文翻译): 实时三维重建对于机器人和增强现实至关重要，但现有的同步定位与建图（SLAM）方法在深度噪声存在的情况下，往往难以保持结构一致性并实现鲁棒的位姿估计。本文提出了PointSLAM++，一种新颖的RGB-D SLAM系统，它利用层次约束的神经高斯表示，在生成用于场景建图的高斯基元的同时保留结构关系；同时采用渐进式位姿优化以减轻深度传感器噪声的影响，显著提升定位精度。此外，该系统还引入了一个动态神经表示图，可根据局部几何复杂度调整高斯节点的分布，使地图能够实时适应复杂的场景细节。这种组合实现了高精度的三维建图和逼真的场景渲染。实验结果表明，PointSLAM++在重建精度和渲染质量方面均优于现有的基于3DGS的SLAM方法，展现出其在大规模增强现实和机器人应用中的优势。

</details>


### [4] [Handcrafted Feature-Assisted One-Class Learning for Artist Authentication in Historical Drawings](https://arxiv.org/abs/2601.11627)
*Hassan Ugail,Jan Ritch-Frel,Irina Matuzava*

Main category: cs.CV

TL;DR: 本文提出一种基于单类自编码器的计算框架，用于在参考样本有限的情况下对历史素描进行验证式真伪鉴别，利用手工提取的可解释特征，在多个博物馆数据集上训练艺术家专属验证器，并在生物识别式协议下评估其性能。


<details>
  <summary>Details</summary>
Motivation: 在文化遗产领域，纸质作品的真伪鉴定与归属长期面临挑战，尤其当可用参考作品数量少、风格线索主要通过线条和有限明暗变化体现时，传统方法难以有效应对。

Method: 采用验证式方法，构建基于单类自编码器的系统，使用从傅里叶域能量、香农熵、全局对比度、GLCM同质性和分形复杂度等手工特征组成的特征向量；在来自大都会艺术博物馆、阿什莫林博物馆等机构的已认证素描上训练十位艺术家专属验证器，并采用生物识别式协议（含真实与冒充样本）进行评估。

Result: 在900次验证决策中（90次真实，810次冒充），系统在选定操作点达到83.3%的真实接受率（TAR）和9.5%的错误接受率（FAR）；不同艺术家表现差异显著，部分验证器几乎无误判，而另一些则存在较高混淆；错误接受的成对分析揭示了与风格接近性和共同绘图惯例一致的结构化错误路径。

Conclusion: 所提方法旨在补充而非取代鉴赏家的专业判断，为历史素描归属这类数据稀缺场景提供可复现、定量的证据支持，同时指出需更严格控制数字化伪影并优化阈值校准。

Abstract: Authentication and attribution of works on paper remain persistent challenges in cultural heritage, particularly when the available reference corpus is small and stylistic cues are primarily expressed through line and limited tonal variation. We present a verification-based computational framework for historical drawing authentication using one-class autoencoders trained on a compact set of interpretable handcrafted features. Ten artist-specific verifiers are trained using authenticated sketches from the Metropolitan Museum of Art open-access collection, the Ashmolean Collections Catalogue, the Morgan Library and Museum, the Royal Collection Trust (UK), the Victoria and Albert Museum Collections, and an online catalogue of the Casa Buonarroti collection and evaluated under a biometric-style protocol with genuine and impostor trials. Feature vectors comprise Fourier-domain energy, Shannon entropy, global contrast, GLCM-based homogeneity, and a box-counting estimate of fractal complexity. Across 900 verification decisions (90 genuine and 810 impostor trials), the pooled system achieves a True Acceptance Rate of 83.3% with a False Acceptance Rate of 9.5% at the chosen operating point. Performance varies substantially by artist, with near-zero false acceptance for some verifiers and elevated confusability for others. A pairwise attribution of false accepts indicates structured error pathways consistent with stylistic proximity and shared drawing conventions, whilst also motivating tighter control of digitisation artefacts and threshold calibration. The proposed methodology is designed to complement, rather than replace, connoisseurship by providing reproducible, quantitative evidence suitable for data-scarce settings common in historical sketch attribution.

Abstract (中文翻译): 纸质作品的真伪鉴定与归属在文化遗产领域始终是持续存在的难题，尤其是在可用参考作品数量有限、风格线索主要通过线条和有限的明暗变化体现的情况下。我们提出了一种基于验证的计算框架，利用在少量可解释手工特征上训练的单类自编码器，对历史素描进行真伪鉴别。该研究使用来自大都会艺术博物馆开放获取藏品、阿什莫林藏品目录、摩根图书馆与博物馆、英国皇家收藏信托、维多利亚与阿尔伯特博物馆藏品，以及卡萨·博纳罗蒂在线藏品目录中的已认证素描，训练了十位艺术家专属的验证器，并在仿照生物识别的评估协议下，通过真实样本与冒充样本进行测试。特征向量包括傅里叶域能量、香农熵、全局对比度、基于灰度共生矩阵（GLCM）的同质性，以及基于盒计数法的分形复杂度估计。在900次验证决策（90次真实尝试和810次冒充尝试）中，该集成系统在选定操作点实现了83.3%的真实接受率（TAR）和9.5%的错误接受率（FAR）。不同艺术家的表现差异显著：部分验证器的错误接受率接近于零，而另一些则表现出较高的混淆性。对错误接受案例的成对归属分析表明，错误路径具有结构性，与艺术家之间的风格相近性和共享的绘图惯例一致，同时也提示需加强对数字化伪影的控制和阈值校准。所提出的方法旨在补充而非取代传统的鉴赏判断，为历史素描归属这类常见于数据稀缺环境的任务提供可复现且定量的证据支持。

</details>


### [5] [A one-step generation model with a Single-Layer Transformer: Layer number re-distillation of FreeFlow](https://arxiv.org/abs/2601.11630)
*Haonan Wei,Linyuan Wang,Nuolin Sun,Zhizhong Zheng,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: 本文提出SLT（单层Transformer），通过知识蒸馏将FreeFlow的28层Transformer压缩为单个共享DiT块，在大幅减少参数量（675M→4.3M）的同时，利用其快速采样能力在相同时间内筛选大量噪声候选点，从而为FreeFlow教师模型提供更优初始点，显著提升单步生成图像的质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于ODE的单步生成流匹配方法（如FreeFlow）虽快但依赖深层网络，且单次采样易受低质量初始噪声影响，导致生成结果不稳定。作者旨在通过模型压缩和高效噪声筛选，提升单步生成的稳定性和平均质量。

Method: 将FreeFlow的28层Transformer视为ODE沿深度轴的欧拉离散化，据此蒸馏出仅含单个共享DiT块的SLT模型。训练时，SLT在多个深度片段匹配教师中间特征、融合表征，并对齐教师最终速度预测。利用SLT极小的参数量和快速采样能力，在相同时间内筛选大量噪声候选点，选择最优初始点供FreeFlow生成高质量图像。

Result: 成功将FreeFlow的参数量从675M压缩至4.3M。在与教师模型两次随机采样相当的时间内，SLT可完成上百次噪声筛选，并用选出的优质点通过教师模型生成高质量样本，有效避免了因低质量初始噪声导致的质量波动。

Conclusion: 所提出的SLT模型通过高效的蒸馏和噪声筛选机制，显著提升了基于ODE的单步生成方法的稳定性和平均生成质量，为快速、高质量的图像生成提供了一种有效方案。

Abstract: Currently, Flow matching methods aim to compress the iterative generation process of diffusion models into a few or even a single step, with MeanFlow and FreeFlow being representative achievements of one-step generation based on Ordinary Differential Equations (ODEs). We observe that the 28-layer Transformer architecture of FreeFlow can be characterized as an Euler discretization scheme for an ODE along the depth axis, where the layer index serves as the discrete time step. Therefore, we distill the number of layers of the FreeFlow model, following the same derivation logic as FreeFlow, and propose SLT (Single-Layer Transformer), which uses a single shared DiT block to approximate the depth-wise feature evolution of the 28-layer teacher. During training, it matches the teacher's intermediate features at several depth patches, fuses those patch-level representations, and simultaneously aligns the teacher's final velocity prediction. Through distillation training, we compress the 28 independent Transformer Blocks of the teacher model DiT-XL/2 into a single Transformer Block, reducing the parameter count from 675M to 4.3M. Furthermore, leveraging its minimal parameters and rapid sampling speed, SLT can screen more candidate points in the noise space within the same timeframe, thereby selecting higher-quality initial points for the teacher model FreeFlow and ultimately enhancing the quality of generated images. Experimental results demonstrate that within a time budget comparable to two random samplings of the teacher model, our method performs over 100 noise screenings and produces a high-quality sample through the teacher model using the selected points. Quality fluctuations caused by low-quality initial noise under a limited number of FreeFlow sampling calls are effectively avoided, substantially improving the stability and average generation quality of one-step generation.

Abstract (中文翻译): 目前，流匹配方法旨在将扩散模型的迭代生成过程压缩为几步甚至单步，其中MeanFlow和FreeFlow是基于常微分方程（ODE）实现单步生成的代表性成果。我们观察到，FreeFlow的28层Transformer架构可被描述为沿深度轴对ODE进行的欧拉离散化方案，其中层索引作为离散时间步。因此，我们遵循与FreeFlow相同的推导逻辑，对FreeFlow模型的层数进行蒸馏，提出了SLT（单层Transformer），它使用单个共享的DiT块来近似28层教师模型的深度特征演化。在训练过程中，SLT在若干深度片段处匹配教师模型的中间特征，融合这些片段级表征，并同时对齐教师模型的最终速度预测。通过蒸馏训练，我们将教师模型DiT-XL/2的28个独立Transformer块压缩为单个Transformer块，参数量从6.75亿降至430万。此外，凭借其极小的参数量和快速的采样速度，SLT能在相同时间内于噪声空间中筛选更多候选点，从而为教师模型FreeFlow选择更高质量的初始点，最终提升生成图像的质量。实验结果表明，在与教师模型两次随机采样相当的时间预算内，我们的方法能执行上百次噪声筛选，并利用所选点通过教师模型生成高质量样本，有效避免了在有限次FreeFlow采样调用下因低质量初始噪声导致的质量波动，大幅提高了单步生成的稳定性和平均生成质量。

</details>


### [6] [Compress to Focus: Efficient Coordinate Compression for Policy Optimization in Multi-Turn GUI Agents](https://arxiv.org/abs/2601.11631)
*Yurun Song,Jiong Yin,Rongjunchen Zhang,Ian G. Harris*

Main category: cs.CV

TL;DR: 本文提出CCPO框架，通过坐标感知的空间压缩和基于距离的优势函数，在多轮GUI智能体中实现高效策略优化，在保持性能的同时显著减少token数量并加速训练。


<details>
  <summary>Details</summary>
Motivation: 多轮GUI智能体在完成复杂任务时面临交互历史累积导致的上下文膨胀问题，现有方法要么通过截断牺牲长期上下文，要么通过token剪枝破坏空间结构。

Method: 提出Coordinate Compression Policy Optimization (CCPO)框架，包含Coordinate-Aware Spatial Compression (CASC)机制，通过聚合多轮rollout中的坐标信息，动态构建注意力边界以聚焦关键视觉区域；并设计Distance-Based Advantage提供细粒度学习信号。

Result: 在四个基准上达到SOTA性能，最高实现55%的token压缩率和3.8倍的训练加速。

Conclusion: CCPO有效解决了多轮GUI智能体中的上下文膨胀问题，在提升训练效率的同时保持甚至提升了任务性能。

Abstract: Multi-turn GUI agents enable complex task completion through sequential decision-making, but suffer from severe context inflation as interaction history accumulates. Existing strategies either sacrifice long-term context via truncation or compromise spatial structure through token pruning. In this paper, we propose Coordinate Compression Policy Optimization (CCPO), an efficient policy optimization framework that couples visual compression with policy optimization for multi-turn GUI agents. CCPO introduces Coordinate-Aware Spatial Compression (CASC), which aggregates coordinates from multiple rollouts to capture target-relevant regions and progressively narrow historical attention around key visual areas. From interactions across rollouts, CASC adaptively constructs attention boundaries that concentrate computation on the most informative regions of the scene. We further design a Distance-Based Advantage that provides fine-grained learning signals based on distance rather than binary correctness, improving both grounding accuracy and compression quality. Extensive experiments demonstrate that CCPO achieves SOTA performance across four benchmarks with up to 55% token compression and 3.8$\times$ training speedup.

Abstract (中文翻译): 多轮GUI智能体通过序列决策实现复杂任务，但随着交互历史的积累，会遭遇严重的上下文膨胀问题。现有策略要么通过截断牺牲长期上下文，要么通过token剪枝损害空间结构。本文提出坐标压缩策略优化（Coordinate Compression Policy Optimization, CCPO），这是一种高效的策略优化框架，将视觉压缩与策略优化相结合用于多轮GUI智能体。CCPO引入了坐标感知空间压缩（Coordinate-Aware Spatial Compression, CASC），通过聚合多轮rollout中的坐标信息来捕捉与目标相关的区域，并逐步将历史注意力集中在关键视觉区域周围。CASC从多轮交互中自适应地构建注意力边界，将计算资源集中在场景中最具信息量的区域。我们进一步设计了一种基于距离的优势函数（Distance-Based Advantage），该函数基于距离而非二元正确性提供细粒度的学习信号，从而同时提升定位准确性和压缩质量。大量实验表明，CCPO在四个基准上均达到最先进性能，最高可实现55%的token压缩率和3.8倍的训练加速。

</details>


### [7] [KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering](https://arxiv.org/abs/2601.11632)
*Zhiyang Li,Ao Ke,Yukun Cao,Xike Xie*

Main category: cs.CV

TL;DR: 本文提出KG-ViP框架，通过融合场景图和常识图来增强多模态大语言模型在视觉问答任务中的表现，有效缓解知识幻觉并提升细粒度视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有用于视觉问答（VQA）的多模态大语言模型（MLLMs）存在知识幻觉和细粒度视觉感知不足的问题。常识图和场景图分别能提供外部知识和细粒度视觉细节，但以往研究通常孤立使用二者，未挖掘其协同潜力。

Method: 作者提出KG-ViP统一框架，核心是一个新颖的检索-融合流程：以查询为语义桥梁，逐步整合场景图与常识图，生成统一的结构化上下文，以支持可靠的多模态推理。

Result: 在FVQA 2.0+和MVQA基准上的大量实验表明，KG-ViP显著优于现有VQA方法。

Conclusion: 融合常识图与场景图能有效提升MLLM在VQA任务中的准确性和鲁棒性，验证了两类知识图谱协同作用的重要性。

Abstract: Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.

Abstract (中文翻译): 用于视觉问答（VQA）的多模态大语言模型（MLLMs）常常面临双重局限：知识幻觉和细粒度视觉感知不足。我们关键地发现，常识图和场景图恰好能分别通过提供丰富的外部知识和捕捉细粒度视觉细节，互补地解决上述缺陷。然而，以往的工作通常孤立地使用它们，忽视了其协同潜力。为弥合这一差距，我们提出了KG-ViP，一个通过融合场景图和常识图来增强MLLMs的统一框架。KG-ViP框架的核心是一种新颖的检索与融合流程，该流程利用查询作为语义桥梁，逐步整合两种图谱，合成一个统一的结构化上下文，从而促进可靠的多模态推理。在FVQA 2.0+和MVQA基准上的大量实验表明，KG-ViP显著优于现有的VQA方法。

</details>


### [8] [Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images](https://arxiv.org/abs/2601.11633)
*Xuchen Li,Xuzhao Li,Renjie Pi,Shiyu Hu,Jian Zhao,Jiahui Gao*

Main category: cs.CV

TL;DR: 提出了ViEBench，一个可验证推理过程的视觉语言模型评测基准，通过细粒度视觉证据和双轴评估矩阵，揭示模型在多步推理中对视觉线索的真实利用情况。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）的评测主要依赖结果准确性，无法评估模型是否真正利用细粒度视觉线索进行多步推理，缺乏对推理过程真实性的验证。

Method: 构建包含200张高分辨率图像的ViEBench基准，每张图像配有专家标注的视觉证据；任务按感知与推理两个维度划分难度；引入双轴矩阵，通过四个诊断象限提供细粒度评估指标。

Result: 实验发现：(1) VLMs有时基于无关区域仍能给出正确答案；(2) 即使定位到正确证据，也可能无法据此得出正确结论。ViEBench能有效揭示这些行为。

Conclusion: ViEBench为评估具身VLMs的视觉推理能力提供了更可解释、更实用的基准，有助于推动模型在真实推理过程中的可靠性提升。

Abstract: Despite the remarkable progress of Vision-Language Models (VLMs) in adopting "Thinking-with-Images" capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: https://github.com/Xuchen-Li/ViEBench.

Abstract (中文翻译): 尽管视觉语言模型（VLMs）在具备“结合图像思考”的能力方面取得了显著进展，但如何准确评估其推理过程的真实性仍是一个关键挑战。现有的评测基准主要依赖以结果为导向的准确性，缺乏评估模型能否准确利用细粒度视觉线索进行多步推理的能力。为解决这一问题，我们提出了ViEBench——一个可验证推理过程的基准，用于评估忠实的视觉推理能力。该基准包含200张多场景高分辨率图像，并配有专家标注的视觉证据。ViEBench独特地从感知和推理两个维度对任务难度进行分类，其中推理任务需要结合局部视觉细节与先验知识。为建立全面的评估标准，我们引入了一个双轴矩阵，通过四个诊断象限提供细粒度指标，从而透明地诊断模型在不同任务复杂度下的行为。我们的实验得出了若干有趣发现：(1) VLMs有时即使基于无关区域也能产生正确的最终答案；(2) 它们可能成功定位到正确的证据，却仍无法利用该证据得出准确结论。研究结果表明，ViEBench可作为更可解释且实用的基准，全面评估具身VLMs的有效性。代码将发布于：https://github.com/Xuchen-Li/ViEBench。

</details>


### [9] [When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms](https://arxiv.org/abs/2601.11634)
*Chenghui Yu,Hongwei Wang,Junwen Chen,Zixuan Wang,Bingfeng Deng,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: 本文提出一种基于多模态大语言模型（LLM）智能体的自动新兴问题发现方法，用于快速识别短视频平台上的新内容问题，并自动生成更新的标注策略，显著提升问题发现效率与治理效果。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容趋势变化迅速，传统人工方式难以及时发现新兴内容问题，导致标注策略更新滞后，影响内容治理效果。

Method: 利用多模态LLM智能体自动召回潜在问题短视频，采用两阶段聚类策略将视频分组，每组对应一个新发现的问题，并由智能体生成对应的更新标注策略。

Result: 离线与在线实验表明，该方法在新兴问题发现上的F1分数提升超20%，问题治理效果提升（问题视频观看量减少约15%），且大幅缩短人工发现所需时间。

Conclusion: 基于多模态LLM智能体的自动问题发现方法能高效、快速地识别新兴内容问题并推动标注策略迭代，在实际系统中具有显著应用价值。

Abstract: Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.

Abstract (中文翻译): 短视频平台上的内容趋势变化迅速，每天都会出现超出当前标注策略覆盖范围的新内容问题。然而，传统依赖人工发现新兴问题的方式过于缓慢，导致标注策略更新滞后，给有效的内容治理带来重大挑战。本文提出了一种基于多模态大语言模型（LLM）智能体的自动问题发现方法。该方法能自动召回包含潜在新问题的短视频，并采用两阶段聚类策略对其进行分组，每个聚类对应一个新发现的问题。随后，智能体从这些聚类中生成更新后的标注策略，从而将新兴问题纳入治理范围。该智能体已在真实系统中部署。离线和在线实验均表明，这种基于智能体的方法显著提升了新兴问题发现的有效性（F1分数提升超过20%），并增强了后续问题治理的效果（问题视频的观看量减少了约15%）。更重要的是，与人工发现方式相比，该方法大幅降低了时间成本，显著加快了标注策略的迭代速度。

</details>


### [10] [Now You See Me, Now You Don't: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos](https://arxiv.org/abs/2601.11635)
*Anil Egin,Andrea Tangherloni,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 提出Anon-NET，一种用于面部视频匿名化的统一框架，在去除身份信息的同时保留年龄、性别、种族、姿态和表情等关键属性。


<details>
  <summary>Details</summary>
Motivation: 在保护隐私的前提下，使面部视频仍可用于表情识别、人物追踪和动作识别等计算机视觉任务，需在去身份化的同时保留关键语义信息。

Method: 使用基于扩散模型的生成方法对人脸进行修复，结合高层属性识别与运动感知的表情迁移；再通过视频驱动的动画技术将去身份化的人脸与原始视频同步动画。

Result: 在VoxCeleb2、CelebV-HQ和HDTF数据集上的实验表明，Anon-NET能有效隐藏身份，同时保持视觉真实性和时间一致性。

Conclusion: Anon-NET为面部视频匿名化提供了一个高效且统一的解决方案，兼顾隐私保护与下游任务可用性，代码将公开发布。

Abstract: Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.

Abstract (中文翻译): 面部视频匿名化旨在保护隐私的同时，仍允许视频用于多种计算机视觉下游任务，如表情识别、人物追踪和动作识别。本文提出了一种名为Anon-NET的新型统一框架，专门用于对面部视频进行去身份化处理，同时保留原始视频中的年龄、性别、种族、姿态和表情等属性。具体而言，我们采用基于扩散模型的生成方法对人脸进行修复，并由高层属性识别和运动感知的表情迁移进行引导。随后，通过视频驱动的动画技术，将去身份化后的人脸与原始视频进行同步动画生成。在包含多样化面部动态的VoxCeleb2、CelebV-HQ和HDTF数据集上进行的大量实验表明，Anon-NET在有效模糊身份信息的同时，仍能保持良好的视觉真实性和时间一致性。Anon-NET的代码将公开发布。

</details>
