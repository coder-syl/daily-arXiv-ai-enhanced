{"id": "2602.11214", "pdf": "https://arxiv.org/pdf/2602.11214", "abs": "https://arxiv.org/abs/2602.11214", "authors": ["Manuel Hetzel", "Kerim Turacan", "Hannes Reichert", "Konrad Doll", "Bernhard Sick"], "title": "DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDD-MDN\u6a21\u578b\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6982\u7387\u6027\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u9ad8\u4f4d\u7f6e\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5177\u5907\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u5bf9\u77ed\u89c2\u6d4b\u65f6\u957f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3001\u793e\u4f1a\u4ea4\u4e92\u5efa\u6a21\u548c\u591a\u6837\u6027\uff0c\u4f46\u5bf9\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u6821\u51c6\u80fd\u529b\u4ee5\u53ca\u77ed\u89c2\u6d4b\u7a97\u53e3\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u5173\u6ce8\u4e0d\u8db3\uff0c\u800c\u8fd9\u4e9b\u5bf9\u4e8e\u8def\u5f84\u89c4\u5212\u548c\u907f\u969c\u7b49\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "DD-MDN\u7ed3\u5408\u5c11\u6837\u672c\u53bb\u566a\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\u4e0e\u53cc\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff0c\u5b66\u4e60\u81ea\u6821\u51c6\u7684\u9a7b\u7559\u533a\u57df\u548c\u6309\u6982\u7387\u6392\u5e8f\u7684\u951a\u70b9\u8def\u5f84\uff0c\u4ece\u800c\u751f\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\u5047\u8bbe\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u951a\u70b9\u6216\u7ec8\u70b9\u3002", "result": "\u5728ETH/UCY\u3001SDD\u3001inD\u548cIMPTC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7cbe\u5ea6\u3001\u77ed\u89c2\u6d4b\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u9762\u5747\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "conclusion": "DD-MDN\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u4e2d\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u77ed\u89c2\u6d4b\u9c81\u68d2\u6027\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u5982\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u652f\u6301\u3002", "summary_cn": "\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\uff08HTF\uff09\u901a\u8fc7\u5386\u53f2\u8f68\u8ff9\u548c\u73af\u5883\u4e0a\u4e0b\u6587\u9884\u6d4b\u672a\u6765\u7684\u4eba\u7c7b\u8fd0\u52a8\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u3001\u667a\u80fd\u76d1\u63a7\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u3002\u5c3d\u7ba1\u4ee5\u5f80\u7684\u7814\u7a76\u805a\u7126\u4e8e\u9884\u6d4b\u51c6\u786e\u6027\u3001\u793e\u4f1a\u4ea4\u4e92\u5efa\u6a21\u548c\u591a\u6837\u6027\uff0c\u4f46\u5bf9\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u6821\u51c6\u80fd\u529b\u4ee5\u53ca\u77ed\u89c2\u6d4b\u7a97\u53e3\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u5173\u6ce8\u8f83\u5c11\uff0c\u800c\u8fd9\u4e9b\u5bf9\u4e8e\u8def\u5f84\u89c4\u5212\u548c\u78b0\u649e\u89c4\u907f\u7b49\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51faDD-MDN\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6982\u7387\u6027HTF\u6a21\u578b\uff0c\u517c\u5177\u9ad8\u4f4d\u7f6e\u7cbe\u5ea6\u3001\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4ee5\u53ca\u5bf9\u77ed\u89c2\u6d4b\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5c11\u6837\u672c\u53bb\u566a\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\u4e0e\u53cc\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff0c\u5b66\u4e60\u81ea\u6821\u51c6\u7684\u9a7b\u7559\u533a\u57df\u548c\u6309\u6982\u7387\u6392\u5e8f\u7684\u951a\u70b9\u8def\u5f84\uff0c\u4ece\u800c\u751f\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\u5047\u8bbe\uff0c\u4e14\u65e0\u9700\u9884\u5b9a\u4e49\u951a\u70b9\u6216\u7ec8\u70b9\u3002\u5728ETH/UCY\u3001SDD\u3001inD\u548cIMPTC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7cbe\u5ea6\u3001\u77ed\u89c2\u6d4b\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u9762\u5747\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/kav-institute/ddmdn\u3002"}}
{"id": "2602.11236", "pdf": "https://arxiv.org/pdf/2602.11236", "abs": "https://arxiv.org/abs/2602.11236", "authors": ["Yandan Yang", "Shuang Zeng", "Tong Lin", "Xinyuan Chang", "Dekang Qi", "Junjin Xiao", "Haoyun Liu", "Ronghan Chen", "Yuzhi Chen", "Dongjie Huo", "Feng Xiong", "Xing Wei", "Zhiheng Ma", "Mu Xu"], "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "Project website: https://amap-cvlab.github.io/ABot-Manipulation/ . Code: https://github.com/amap-cvlab/ABot-Manipulation . 22 pages, 10 figures, 10 tables", "summary": "Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faABot-M0\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6570\u636e\u5904\u7406\u3001\u7edf\u4e00\u9884\u8bad\u7ec3\u548c\u52a8\u4f5c\u6d41\u5f62\u5b66\u4e60\uff0c\u5b9e\u73b0\u8de8\u591a\u79cd\u673a\u5668\u4eba\u5f62\u6001\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u3002", "motivation": "\u6784\u5efa\u9002\u7528\u4e8e\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\uff08\u201c\u4e00\u8111\u591a\u5f62\u201d\uff09\u9762\u4e34\u6570\u636e\u788e\u7247\u5316\u3001\u8868\u5f81\u4e0d\u4e00\u81f4\u548c\u8bad\u7ec3\u76ee\u6807\u9519\u4f4d\u7b49\u6311\u6218\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86UniACT\u6570\u636e\u96c6\uff08\u542b600\u4e07\u8f68\u8ff9\u30019500\u5c0f\u65f6\u6570\u636e\uff09\uff0c\u5e76\u63d0\u51fa\u52a8\u4f5c\u6d41\u5f62\u5047\u8bbe\u4e0e\u52a8\u4f5c\u6d41\u5f62\u5b66\u4e60\uff08AML\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408DiT\u9aa8\u5e72\u7f51\u7edc\u76f4\u63a5\u9884\u6d4b\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\uff1b\u540c\u65f6\u91c7\u7528\u53cc\u6d41\u673a\u5236\u878d\u5408VLM\u8bed\u4e49\u4e0e\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u96c6\u6210\u5373\u63d2\u5373\u7528\u76843D\u6a21\u5757\u4ee5\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5404\u7ec4\u4ef6\u53ef\u72ec\u7acb\u8fd0\u884c\u4e14\u6548\u679c\u53e0\u52a0\uff0c\u663e\u8457\u63d0\u5347\u8de8\u5e73\u53f0\u77e5\u8bc6\u8fc1\u79fb\u3001\u6cdb\u5316\u80fd\u529b\u3001\u52a8\u4f5c\u9884\u6d4b\u6548\u7387\u4e0e\u7b56\u7565\u7a33\u5b9a\u6027\u3002", "conclusion": "ABot-M0\u4e3a\u901a\u7528\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u7ba1\u9053\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002", "summary_cn": "\u5728\u673a\u5668\u4eba\u9886\u57df\uff0c\u6784\u5efa\u9002\u7528\u4e8e\u5404\u79cd\u786c\u4ef6\u5e73\u53f0\u7684\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u59cb\u7ec8\u662f\u4e00\u9879\u6838\u5fc3\u6311\u6218\uff0c\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u4e00\u8111\u591a\u5f62\u201d\u8303\u5f0f\u3002\u5f53\u524d\u8fdb\u5c55\u53d7\u9650\u4e8e\u6570\u636e\u788e\u7247\u5316\u3001\u8868\u5f81\u4e0d\u4e00\u81f4\u4ee5\u53ca\u8bad\u7ec3\u76ee\u6807\u9519\u4f4d\u7b49\u95ee\u9898\u3002\u6211\u4eec\u63d0\u51fa\u4e86ABot-M0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u5957\u7cfb\u7edf\u5316\u7684\u6570\u636e\u6574\u7406\u6d41\u7a0b\uff0c\u5e76\u8054\u5408\u4f18\u5316\u6a21\u578b\u67b6\u6784\u4e0e\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u5f02\u6784\u539f\u59cb\u6570\u636e\u5230\u7edf\u4e00\u9ad8\u6548\u8868\u5f81\u7684\u7aef\u5230\u7aef\u8f6c\u6362\u3002\u57fa\u4e8e\u516d\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6211\u4eec\u6e05\u6d17\u3001\u6807\u51c6\u5316\u5e76\u5e73\u8861\u6837\u672c\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21UniACT\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7600\u4e07\u6761\u8f68\u8ff9\u548c9500\u5c0f\u65f6\u7684\u6570\u636e\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u5f62\u6001\u4e0e\u4efb\u52a1\u573a\u666f\u3002\u7edf\u4e00\u9884\u8bad\u7ec3\u589e\u5f3a\u4e86\u8de8\u5e73\u53f0\u548c\u8de8\u4efb\u52a1\u7684\u77e5\u8bc6\u8fc1\u79fb\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u901a\u7528\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002\u4e3a\u63d0\u5347\u52a8\u4f5c\u9884\u6d4b\u7684\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u201c\u52a8\u4f5c\u6d41\u5f62\u5047\u8bbe\u201d\uff1a\u6709\u6548\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u5e76\u975e\u5206\u5e03\u5728\u6574\u4e2a\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u800c\u662f\u4f4d\u4e8e\u4e00\u4e2a\u7531\u7269\u7406\u89c4\u5f8b\u548c\u4efb\u52a1\u7ea6\u675f\u6240\u51b3\u5b9a\u7684\u4f4e\u7ef4\u5149\u6ed1\u6d41\u5f62\u4e0a\u3002\u57fa\u4e8e\u6b64\u5047\u8bbe\uff0c\u6211\u4eec\u5f15\u5165\u52a8\u4f5c\u6d41\u5f62\u5b66\u4e60\uff08Action Manifold Learning, AML\uff09\uff0c\u5229\u7528DiT\u9aa8\u5e72\u7f51\u7edc\u76f4\u63a5\u9884\u6d4b\u5e72\u51c0\u3001\u8fde\u7eed\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u4ece\u53bb\u566a\u8f6c\u53d8\u4e3a\u5411\u53ef\u884c\u6d41\u5f62\u4e0a\u7684\u6295\u5f71\uff0c\u4ece\u800c\u63d0\u5347\u89e3\u7801\u901f\u5ea6\u4e0e\u7b56\u7565\u7a33\u5b9a\u6027\u3002ABot-M0\u8fd8\u901a\u8fc7\u53cc\u6d41\u673a\u5236\u652f\u6301\u6a21\u5757\u5316\u611f\u77e5\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8bed\u4e49\u4fe1\u606f\u4e0e\u51e0\u4f55\u5148\u9a8c\u76f8\u7ed3\u5408\uff0c\u5e76\u6574\u5408\u6765\u81ea\u5373\u63d2\u5373\u75283D\u6a21\u5757\uff08\u5982VGGT\u548cQwen-Image-Edit\uff09\u7684\u591a\u89c6\u89d2\u8f93\u5165\uff0c\u5728\u4e0d\u4fee\u6539\u4e3b\u5e72\u7f51\u7edc\u7684\u524d\u63d0\u4e0b\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u7f13\u89e3\u6807\u51c6VLM\u5728\u4e09\u7ef4\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5404\u7ec4\u4ef6\u53ef\u72ec\u7acb\u8fd0\u4f5c\u4e14\u5177\u6709\u53e0\u52a0\u589e\u76ca\u3002\u6211\u4eec\u5c06\u5f00\u6e90\u5168\u90e8\u4ee3\u7801\u4e0e\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u5e76\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.11239", "pdf": "https://arxiv.org/pdf/2602.11239", "abs": "https://arxiv.org/abs/2602.11239", "authors": ["Samanta Ghosh", "Jannatul Adan Mahi", "Shayan Abrar", "Md Parvez Mia", "Asaduzzaman Rayhan", "Abdul Awal Yasir", "Asaduzzaman Hridoy"], "title": "Toward Reliable Tea Leaf Disease Diagnosis Using Deep Learning Model: Enhancing Robustness With Explainable AI and Adversarial Training", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages,9 figures, 2025 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)", "summary": "Tea is a valuable asset for the economy of Bangladesh. So, tea cultivation plays an important role to boost the economy. These valuable plants are vulnerable to various kinds of leaf infections which may cause less production and low quality. It is not so easy to detect these diseases manually. It may take time and there could be some errors in the detection.Therefore, the purpose of the study is to develop an automated deep learning model for tea leaf disease classification based on the teaLeafBD dataset so that anyone can detect the diseases more easily and efficiently. There are 5,278 high-resolution images in this dataset. The images are classified into seven categories. Six of them represents various diseases and the rest one represents healthy leaves. The proposed pipeline contains data preprocessing, data splitting, adversarial training, augmentation, model training, evaluation, and comprehension made possible with Explainable AI strategies. DenseNet201 and EfficientNetB3 were employed to perform the classification task. To prepare the model more robustly, we applied adversarial training so it can operate effectively even with noisy or disturbed inputs. In addition, Grad-CAM visualization was executed to analyze the model's predictions by identifying the most influential regions of each image. Our experimental outcomes revealed that EfficientNetB3 achieved the highest classification accuracy of 93%, while DenseNet201 reached 91%. The outcomes prove that the effectiveness of the proposed approach can accurately detect tea leaf diseases and provide a practical solution for advanced agricultural management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u8336\u6811\u53f6\u75c5\u5bb3\u5206\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u8336LeafBD\u6570\u636e\u96c6\uff08\u5305\u542b5278\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5206\u4e3a7\u7c7b\uff09\u8bad\u7ec3DenseNet201\u548cEfficientNetB3\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u4e0eGrad-CAM\u53ef\u89c6\u5316\u6280\u672f\uff0c\u6700\u7ec8EfficientNetB3\u8fbe\u523093%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u7ecf\u6d4e\u9ad8\u5ea6\u4f9d\u8d56\u8336\u53f6\u4ea7\u4e1a\uff0c\u4f46\u8336\u6811\u6613\u53d7\u591a\u79cd\u53f6\u90e8\u75c5\u5bb3\u5f71\u54cd\uff0c\u5bfc\u81f4\u4ea7\u91cf\u4e0b\u964d\u548c\u54c1\u8d28\u964d\u4f4e\u3002\u4f20\u7edf\u4eba\u5de5\u68c0\u6d4b\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u7684\u81ea\u52a8\u75c5\u5bb3\u8bc6\u522b\u65b9\u6cd5\u4ee5\u652f\u6301\u519c\u4e1a\u7ba1\u7406\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u5212\u5206\u3001\u5bf9\u6297\u8bad\u7ec3\u3001\u6570\u636e\u589e\u5f3a\u3001\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\uff0c\u5e76\u5f15\u5165\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7b56\u7565\u3002\u91c7\u7528DenseNet201\u548cEfficientNetB3\u8fdb\u884c\u5206\u7c7b\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u4f7f\u7528Grad-CAM\u53ef\u89c6\u5316\u6a21\u578b\u51b3\u7b56\u4f9d\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEfficientNetB3\u5728\u8336\u53f6\u75c5\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8693%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0cDenseNet201\u8fbe\u523091%\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u5177\u5907\u53ef\u89e3\u91ca\u6027\u7684\u8336\u6811\u53f6\u75c5\u5bb3\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf\uff0c\u4e3a\u73b0\u4ee3\u519c\u4e1a\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8336\u53f6\u751f\u4ea7\u7684\u8d28\u91cf\u4e0e\u6548\u7387\u3002", "summary_cn": "\u8336\u53f6\u662f\u5b5f\u52a0\u62c9\u56fd\u7ecf\u6d4e\u7684\u91cd\u8981\u8d44\u4ea7\uff0c\u56e0\u6b64\u8336\u6811\u79cd\u690d\u5728\u63a8\u52a8\u7ecf\u6d4e\u53d1\u5c55\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u73cd\u8d35\u7684\u690d\u7269\u5bb9\u6613\u53d7\u5230\u591a\u79cd\u53f6\u90e8\u75c5\u5bb3\u7684\u4fb5\u88ad\uff0c\u53ef\u80fd\u5bfc\u81f4\u4ea7\u91cf\u51cf\u5c11\u548c\u54c1\u8d28\u4e0b\u964d\u3002\u4eba\u5de5\u68c0\u6d4b\u8fd9\u4e9b\u75c5\u5bb3\u65e2\u56f0\u96be\u53c8\u8017\u65f6\uff0c\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u57fa\u4e8eteaLeafBD\u6570\u636e\u96c6\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u66f4\u8f7b\u677e\u9ad8\u6548\u5730\u8bc6\u522b\u8336\u6811\u53f6\u75c5\u5bb3\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b5,278\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5206\u4e3a\u4e03\u7c7b\uff1a\u5176\u4e2d\u516d\u7c7b\u4ee3\u8868\u4e0d\u540c\u75c5\u5bb3\uff0c\u4e00\u7c7b\u4e3a\u5065\u5eb7\u53f6\u7247\u3002\u6240\u63d0\u51fa\u7684\u6d41\u7a0b\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u5212\u5206\u3001\u5bf9\u6297\u8bad\u7ec3\u3001\u6570\u636e\u589e\u5f3a\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u4ee5\u53ca\u501f\u52a9\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08Explainable AI\uff09\u7b56\u7565\u5b9e\u73b0\u7684\u6a21\u578b\u7406\u89e3\u3002\u7814\u7a76\u91c7\u7528\u4e86DenseNet201\u548cEfficientNetB3\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f7f\u5176\u5728\u9762\u5bf9\u566a\u58f0\u6216\u6270\u52a8\u8f93\u5165\u65f6\u4ecd\u80fd\u6709\u6548\u8fd0\u884c\u3002\u6b64\u5916\uff0c\u8fd8\u5e94\u7528Grad-CAM\u53ef\u89c6\u5316\u6280\u672f\uff0c\u901a\u8fc7\u8bc6\u522b\u6bcf\u5f20\u56fe\u50cf\u4e2d\u6700\u5177\u5f71\u54cd\u529b\u7684\u533a\u57df\u6765\u5206\u6790\u6a21\u578b\u9884\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEfficientNetB3\u53d6\u5f97\u4e8693%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u800cDenseNet201\u8fbe\u5230\u4e8691%\u3002\u7ed3\u679c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u8336\u6811\u53f6\u75c5\u5bb3\uff0c\u4e3a\u5148\u8fdb\u519c\u4e1a\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11241", "pdf": "https://arxiv.org/pdf/2602.11241", "abs": "https://arxiv.org/abs/2602.11241", "authors": ["Jinghan He", "Junfeng Fang", "Feng Xiong", "Zijun Yao", "Fei Shen", "Haiyun Guo", "Jinqiao Wang", "Tat-Seng Chua"], "title": "Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Self-play has enabled large language models to autonomously improve through self-generated challenges. However, existing self-play methods for vision-language models rely on passive interaction with static image collections, resulting in strong dependence on initial datasets and inefficient learning. Without the ability to actively seek visual data tailored to their evolving capabilities, agents waste computational effort on samples that are either trivial or beyond their current skill level. To address these limitations, we propose Active-Zero, a framework that shifts from passive interaction to active exploration of visual environments. Active-Zero employs three co-evolving agents: a Searcher that retrieves images from open-world repositories based on the model's capability frontier, a Questioner that synthesizes calibrated reasoning tasks, and a Solver refined through accuracy rewards. This closed loop enables self-scaffolding auto-curricula where the model autonomously constructs its learning trajectory. On Qwen2.5-VL-7B-Instruct across 12 benchmarks, Active-Zero achieves 53.97 average accuracy on reasoning tasks (5.7% improvement) and 59.77 on general understanding (3.9% improvement), consistently outperforming existing self-play baselines. These results highlight active exploration as a key ingredient for scalable and adaptive self-evolving vision-language systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faActive-Zero\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u540c\u6f14\u5316\u7684\u667a\u80fd\u4f53\uff08Searcher\u3001Questioner\u3001Solver\uff09\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u4e3b\u52a8\u63a2\u7d22\u4e0e\u81ea\u751f\u6210\u8bfe\u7a0b\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u9759\u6001\u56fe\u50cf\u7684\u81ea\u535a\u5f08\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u535a\u5f08\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u56fe\u50cf\u96c6\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u83b7\u53d6\u4e0e\u5176\u80fd\u529b\u5339\u914d\u7684\u89c6\u89c9\u6570\u636e\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u4e14\u4e25\u91cd\u4f9d\u8d56\u521d\u59cb\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faActive-Zero\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u6f14\u5316\u7684\u667a\u80fd\u4f53\uff1aSearcher\u6839\u636e\u6a21\u578b\u80fd\u529b\u524d\u6cbf\u4ece\u5f00\u653e\u4e16\u754c\u4e2d\u68c0\u7d22\u56fe\u50cf\uff0cQuestioner\u751f\u6210\u6821\u51c6\u540e\u7684\u63a8\u7406\u4efb\u52a1\uff0cSolver\u5219\u901a\u8fc7\u51c6\u786e\u7387\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff0c\u5f62\u6210\u95ed\u73af\u81ea\u751f\u6210\u8bfe\u7a0b\u3002", "result": "\u5728Qwen2.5-VL-7B-Instruct\u6a21\u578b\u4e0a\uff0cActive-Zero\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a8\u7406\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u7387\u8fbe53.97%\uff08\u63d0\u53475.7%\uff09\uff0c\u901a\u7528\u7406\u89e3\u8fbe59.77%\uff08\u63d0\u53473.9%\uff09\uff0c\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u81ea\u535a\u5f08\u57fa\u7ebf\u3002", "conclusion": "\u4e3b\u52a8\u63a2\u7d22\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u7684\u81ea\u8fdb\u5316\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u7684\u5173\u952e\u8981\u7d20\u3002", "summary_cn": "\u81ea\u535a\u5f08\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u81ea\u6211\u751f\u6210\u7684\u6311\u6218\u5b9e\u73b0\u81ea\u4e3b\u63d0\u5347\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u535a\u5f08\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e0e\u9759\u6001\u56fe\u50cf\u96c6\u5408\u7684\u88ab\u52a8\u4ea4\u4e92\uff0c\u5bfc\u81f4\u5bf9\u521d\u59cb\u6570\u636e\u96c6\u7684\u5f3a\u4f9d\u8d56\u548c\u4f4e\u6548\u5b66\u4e60\u3002\u7531\u4e8e\u65e0\u6cd5\u4e3b\u52a8\u83b7\u53d6\u4e0e\u5176\u4e0d\u65ad\u6f14\u8fdb\u80fd\u529b\u76f8\u5339\u914d\u7684\u89c6\u89c9\u6570\u636e\uff0c\u667a\u80fd\u4f53\u5728\u8fc7\u4e8e\u7b80\u5355\u6216\u8d85\u51fa\u5f53\u524d\u6280\u80fd\u6c34\u5e73\u7684\u6837\u672c\u4e0a\u6d6a\u8d39\u4e86\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Active-Zero\u6846\u67b6\uff0c\u5c06\u4ea4\u4e92\u65b9\u5f0f\u4ece\u88ab\u52a8\u8f6c\u4e3a\u4e3b\u52a8\u63a2\u7d22\u89c6\u89c9\u73af\u5883\u3002Active-Zero\u91c7\u7528\u4e09\u4e2a\u534f\u540c\u6f14\u5316\u7684\u667a\u80fd\u4f53\uff1aSearcher\u6839\u636e\u6a21\u578b\u7684\u80fd\u529b\u524d\u6cbf\u4ece\u5f00\u653e\u4e16\u754c\u8d44\u6e90\u5e93\u4e2d\u68c0\u7d22\u56fe\u50cf\uff0cQuestioner\u5408\u6210\u7ecf\u8fc7\u6821\u51c6\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u800cSolver\u5219\u901a\u8fc7\u51c6\u786e\u7387\u5956\u52b1\u8fdb\u884c\u4f18\u5316\u3002\u8fd9\u4e00\u95ed\u73af\u673a\u5236\u5b9e\u73b0\u4e86\u81ea\u6211\u5f15\u5bfc\u7684\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u6784\u5efa\u5176\u5b66\u4e60\u8f68\u8ff9\u3002\u5728Qwen2.5-VL-7B-Instruct\u6a21\u578b\u4e0a\uff0cActive-Zero\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a8\u7406\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523053.97%\uff08\u63d0\u53475.7%\uff09\uff0c\u901a\u7528\u7406\u89e3\u8fbe\u523059.77%\uff08\u63d0\u53473.9%\uff09\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u535a\u5f08\u57fa\u7ebf\u65b9\u6cd5\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u4e3b\u52a8\u63a2\u7d22\u662f\u6784\u5efa\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u7684\u81ea\u8fdb\u5316\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u7684\u5173\u952e\u8981\u7d20\u3002"}}
{"id": "2602.11242", "pdf": "https://arxiv.org/pdf/2602.11242", "abs": "https://arxiv.org/abs/2602.11242", "authors": ["Yitong Wang", "Yue Yao"], "title": "ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems", "categories": ["cs.CV"], "comment": null, "summary": "We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts \"what to do\" and \"what not to do\" for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?", "AI": {"tldr": "ReTracing \u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5177\u8eab\u8868\u6f14\u827a\u672f\u9879\u76ee\uff0c\u901a\u8fc7\u8003\u53e4\u5b66\u65b9\u6cd5\u63ed\u793a\u751f\u6210\u5f0fAI\u5982\u4f55\u901a\u8fc7\u7f16\u821e\u52a8\u4f5c\u7f16\u7801\u793e\u4f1a\u6587\u5316\u504f\u89c1\uff0c\u5e76\u63a2\u8ba8\u201c\u5728\u4e5f\u80fd\u79fb\u52a8\u3001\u601d\u8003\u5e76\u7559\u4e0b\u75d5\u8ff9\u7684AI\u4e2d\uff0c\u4f55\u4ee5\u4e3a\u4eba\u201d\u7684\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u5851\u9020\u3001\u9650\u5236\u5e76\u751f\u6210\u4eba\u7c7b\u8eab\u4f53\u52a8\u4f5c\uff0c\u63ed\u793a\u751f\u6210\u5f0f\u7cfb\u7edf\u4e2d\u6f5c\u85cf\u7684\u793e\u4f1a\u6587\u5316\u504f\u89c1\uff0c\u5e76\u53cd\u601d\u4eba\u7c7b\u5728\u5177\u5907\u884c\u52a8\u4e0e\u8ba4\u77e5\u80fd\u529b\u7684AI\u73af\u5883\u4e2d\u7684\u8eab\u4efd\u4e0e\u610f\u4e49\u3002", "method": "\u4ece\u79d1\u5e7b\u5c0f\u8bf4\u4e2d\u63d0\u53d6\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u7684\u53e5\u5b50\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u6bcf\u6bb5\u6587\u672c\u751f\u6210\u201c\u5e94\u505a\u201d\u4e0e\u201c\u4e0d\u5e94\u505a\u201d\u7684\u914d\u5bf9\u63d0\u793a\uff1b\u518d\u901a\u8fc7\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5c06\u8fd9\u4e9b\u63d0\u793a\u8f6c\u5316\u4e3a\u4eba\u7c7b\u8868\u6f14\u8005\u7684\u7f16\u821e\u6307\u5bfc\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6307\u4ee4\u3002\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u955c\u9762\u5730\u677f\u4e0a\u540c\u6b65\u6267\u884c\u52a8\u4f5c\uff0c\u7531\u591a\u6444\u50cf\u5934\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u8bb0\u5f55\uff0c\u5e76\u91cd\u5efa\u4e3a3D\u70b9\u4e91\u4e0e\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5f62\u6210\u6570\u5b57\u52a8\u4f5c\u6863\u6848\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u878d\u5408AI\u3001\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u7684\u6c89\u6d78\u5f0f\u8868\u6f14\u7cfb\u7edf\uff0c\u751f\u6210\u4e86\u53ef\u8ffd\u6eaf\u7684\u6570\u5b57\u52a8\u4f5c\u6863\u6848\uff0c\u5e76\u53ef\u89c6\u5316\u4e86\u751f\u6210\u5f0fAI\u5728\u52a8\u4f5c\u7f16\u6392\u4e2d\u6240\u5d4c\u5165\u7684\u504f\u89c1\u3002", "conclusion": "ReTracing \u4e0d\u4ec5\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5177\u8eab\u827a\u672f\u5b9e\u8df5\uff0c\u4e5f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6279\u5224\u6027\u5de5\u5177\uff0c\u7528\u4e8e\u63ed\u793aAI\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u8eab\u4f53\u52a8\u4f5c\u53cd\u6620\u5e76\u5f3a\u5316\u793e\u4f1a\u6587\u5316\u89c4\u8303\uff0c\u8fdb\u800c\u5f15\u53d1\u5bf9\u4eba\u7c7b\u4e3b\u4f53\u6027\u5728AI\u65f6\u4ee3\u5904\u5883\u7684\u6df1\u5c42\u601d\u8003\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86 ReTracing\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5177\u8eab\u8868\u6f14\u827a\u672f\u9879\u76ee\uff0c\u91c7\u7528\u8003\u53e4\u5b66\u65b9\u6cd5\uff0c\u8003\u5bdf\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u5851\u9020\u3001\u9650\u5236\u5e76\u751f\u6210\u8eab\u4f53\u52a8\u4f5c\u3002\u8be5\u9879\u76ee\u4ece\u79d1\u5e7b\u5c0f\u8bf4\u4e2d\u63d0\u53d6\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u7684\u53e5\u5b50\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u6bcf\u6bb5\u6458\u5f55\u751f\u6210\u201c\u5e94\u505a\u201d\u4e0e\u201c\u4e0d\u5e94\u505a\u201d\u7684\u914d\u5bf9\u63d0\u793a\u3002\u968f\u540e\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5c06\u8fd9\u4e9b\u63d0\u793a\u8f6c\u5316\u4e3a\u4eba\u7c7b\u8868\u6f14\u8005\u7684\u7f16\u821e\u6307\u5357\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684\u7535\u673a\u6307\u4ee4\u3002\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u955c\u9762\u5730\u677f\u4e0a\u540c\u6b65\u6267\u884c\u8fd9\u4e9b\u52a8\u4f5c\uff0c\u7531\u591a\u6444\u50cf\u5934\u8fd0\u52a8\u8ffd\u8e2a\u7cfb\u7edf\u6355\u6349\uff0c\u5e76\u91cd\u5efa\u4e3a\u4e09\u7ef4\u70b9\u4e91\u4e0e\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5f62\u6210\u4e00\u4e2a\u52a8\u4f5c\u75d5\u8ff9\u7684\u6570\u5b57\u6863\u6848\u3002\u901a\u8fc7\u8fd9\u4e00\u8fc7\u7a0b\uff0cReTracing \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u751f\u6210\u5f0f\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u7f16\u6392\u52a8\u4f5c\u7f16\u7801\u793e\u4f1a\u6587\u5316\u504f\u89c1\u3002\u5728\u4eba\u5de5\u667a\u80fd\u3001\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u7684\u6c89\u6d78\u5f0f\u4e92\u52a8\u4e2d\uff0cReTracing \u76f4\u9762\u5f53\u4ee3\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5728\u90a3\u4e9b\u540c\u6837\u80fd\u79fb\u52a8\u3001\u601d\u8003\u5e76\u7559\u4e0b\u75d5\u8ff9\u7684\u4eba\u5de5\u667a\u80fd\u4e4b\u4e2d\uff0c\u4f55\u4ee5\u4e3a\u4eba\uff1f"}}
{"id": "2602.11244", "pdf": "https://arxiv.org/pdf/2602.11244", "abs": "https://arxiv.org/abs/2602.11244", "authors": ["Sethuraman T", "Savya Khosla", "Aditi Tiwari", "Vidya Ganesh", "Rakshana Jayaprakash", "Aditya Jain", "Vignesh Srinivasakumar", "Onkar Kishor Susladkar", "Srinidhi Sunkara", "Aditya Shanmugham", "Rakesh Vaideeswaran", "Abbaas Alif Mohamed Nishar", "Simon Jenni", "Derek Hoiem"], "title": "Stress Tests REVEAL Fragile Temporal and Visual Grounding in Video-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This work investigates a fundamental question: Do Video-Language Models (VidLMs) robustly account for video content, temporal sequence, and motion? Our investigation shows that, surprisingly, they often do not. We introduce REVEAL{}, a diagnostic benchmark that probes fundamental weaknesses of contemporary VidLMs through five controlled stress tests; assessing temporal expectation bias, reliance on language-only shortcuts, video sycophancy, camera motion sensitivity, and robustness to spatiotemporal occlusion. We test leading open- and closed-source VidLMs and find that these models confidently describe reversed scenes as forward, answer questions while neglecting video content, agree with false claims, struggle with basic camera motion, and fail to aggregate temporal information amidst simple spatiotemporal masking. Humans, on the other hand, succeed at these tasks with ease. Alongside our benchmark, we provide a data pipeline that automatically generates diagnostic examples for our stress tests, enabling broader and more scalable evaluation. We will release our benchmark and code to support future research.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5f53\u524d\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff08VidLMs\uff09\u5728\u5904\u7406\u89c6\u9891\u5185\u5bb9\u3001\u65f6\u5e8f\u548c\u8fd0\u52a8\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u540d\u4e3aREVEAL\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u901a\u8fc7\u4e94\u9879\u538b\u529b\u6d4b\u8bd5\u63ed\u793a\u8fd9\u4e9b\u5f31\u70b9\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524d\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u89c6\u9891\u4e2d\u7684\u5185\u5bb9\u3001\u65f6\u95f4\u987a\u5e8f\u548c\u8fd0\u52a8\uff0c\u800c\u975e\u4f9d\u8d56\u8bed\u8a00\u6377\u5f84\u6216\u8868\u9762\u7ebf\u7d22\u3002", "method": "\u6784\u5efaREVEAL\u8bca\u65ad\u57fa\u51c6\uff0c\u5305\u542b\u4e94\u4e2a\u53d7\u63a7\u538b\u529b\u6d4b\u8bd5\uff1a\u65f6\u95f4\u9884\u671f\u504f\u5dee\u3001\u4ec5\u4f9d\u8d56\u8bed\u8a00\u7684\u6377\u5f84\u3001\u89c6\u9891\u8c04\u5a9a\u884c\u4e3a\u3001\u5bf9\u6444\u50cf\u673a\u8fd0\u52a8\u7684\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u5bf9\u65f6\u7a7a\u906e\u6321\u7684\u9c81\u68d2\u6027\uff1b\u540c\u65f6\u63d0\u4f9b\u81ea\u52a8\u751f\u6210\u8bca\u65ad\u6837\u672c\u7684\u6570\u636e\u7ba1\u9053\u3002", "result": "\u4e3b\u6d41\u5f00\u6e90\u4e0e\u95ed\u6e90VidLMs\u5728\u591a\u9879\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1a\u5c06\u5012\u653e\u89c6\u9891\u8bef\u8ba4\u4e3a\u6b63\u5411\u3001\u5ffd\u7565\u89c6\u9891\u5185\u5bb9\u4f5c\u7b54\u3001\u8ba4\u540c\u9519\u8bef\u9648\u8ff0\u3001\u96be\u4ee5\u5904\u7406\u57fa\u672c\u6444\u50cf\u673a\u8fd0\u52a8\u3001\u65e0\u6cd5\u5728\u7b80\u5355\u65f6\u7a7a\u906e\u6321\u4e0b\u6574\u5408\u65f6\u5e8f\u4fe1\u606f\uff1b\u800c\u4eba\u7c7b\u8f7b\u677e\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002", "conclusion": "\u5f53\u524dVidLMs\u5728\u57fa\u7840\u89c6\u9891\u7406\u89e3\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0cREVEAL\u57fa\u51c6\u53ca\u914d\u5957\u5de5\u5177\u53ef\u63a8\u52a8\u66f4\u5168\u9762\u3001\u53ef\u6269\u5c55\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u4f5c\u8005\u5c06\u516c\u5f00\u53d1\u5e03\u76f8\u5173\u8d44\u6e90\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002", "summary_cn": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\uff08VidLMs\uff09\u662f\u5426\u80fd\u7a33\u5065\u5730\u5904\u7406\u89c6\u9891\u5185\u5bb9\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u8fd0\u52a8\uff1f\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5b83\u4eec\u5f80\u5f80\u4e0d\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u4e86REVEAL{}\u8fd9\u4e00\u8bca\u65ad\u57fa\u51c6\uff0c\u901a\u8fc7\u4e94\u9879\u53d7\u63a7\u7684\u538b\u529b\u6d4b\u8bd5\u6765\u63a2\u6d4b\u5f53\u524dVidLMs\u7684\u6839\u672c\u5f31\u70b9\uff0c\u5305\u62ec\u65f6\u95f4\u9884\u671f\u504f\u5dee\u3001\u4ec5\u4f9d\u8d56\u8bed\u8a00\u7684\u6377\u5f84\u3001\u89c6\u9891\u8c04\u5a9a\u884c\u4e3a\u3001\u5bf9\u6444\u50cf\u673a\u8fd0\u52a8\u7684\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u5bf9\u65f6\u7a7a\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u6d4b\u8bd5\u4e86\u9886\u5148\u7684\u5f00\u6e90\u548c\u95ed\u6e90VidLMs\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u4f1a\u81ea\u4fe1\u5730\u5c06\u5012\u653e\u573a\u666f\u63cf\u8ff0\u4e3a\u6b63\u5411\u64ad\u653e\uff0c\u5728\u56de\u7b54\u95ee\u9898\u65f6\u5ffd\u7565\u89c6\u9891\u5185\u5bb9\uff0c\u8ba4\u540c\u9519\u8bef\u9648\u8ff0\uff0c\u96be\u4ee5\u5e94\u5bf9\u57fa\u672c\u7684\u6444\u50cf\u673a\u8fd0\u52a8\uff0c\u5e76\u4e14\u5728\u7b80\u5355\u7684\u65f6\u7a7a\u906e\u6321\u4e0b\u65e0\u6cd5\u6709\u6548\u6574\u5408\u65f6\u95f4\u4fe1\u606f\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4eba\u7c7b\u80fd\u8f7b\u677e\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002\u9664\u57fa\u51c6\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6570\u636e\u7ba1\u9053\uff0c\u53ef\u81ea\u52a8\u751f\u6210\u7528\u4e8e\u538b\u529b\u6d4b\u8bd5\u7684\u8bca\u65ad\u6837\u4f8b\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u3001\u66f4\u5177\u53ef\u6269\u5c55\u6027\u7684\u8bc4\u4f30\u3002\u6211\u4eec\u5c06\u516c\u5f00\u53d1\u5e03\u8be5\u57fa\u51c6\u548c\u4ee3\u7801\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2602.11314", "pdf": "https://arxiv.org/pdf/2602.11314", "abs": "https://arxiv.org/abs/2602.11314", "authors": ["Jacob Rubinstein", "Avi Donaty", "Don Engel"], "title": "Advancing Digital Twin Generation Through a Novel Simulation Framework and Quantitative Benchmarking", "categories": ["cs.CV", "cs.GR"], "comment": "9 pages, 10 figures. Preprint", "summary": "The generation of 3D models from real-world objects has often been accomplished through photogrammetry, i.e., by taking 2D photos from a variety of perspectives and then triangulating matched point-based features to create a textured mesh. Many design choices exist within this framework for the generation of digital twins, and differences between such approaches are largely judged qualitatively. Here, we present and test a novel pipeline for generating synthetic images from high-quality 3D models and programmatically generated camera poses. This enables a wide variety of repeatable, quantifiable experiments which can compare ground-truth knowledge of virtual camera parameters and of virtual objects against the reconstructed estimations of those perspectives and subjects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf3D\u6a21\u578b\u548c\u7a0b\u5e8f\u751f\u6210\u7684\u76f8\u673a\u59ff\u6001\u5408\u6210\u56fe\u50cf\uff0c\u7528\u4e8e\u53ef\u91cd\u590d\u3001\u53ef\u91cf\u5316\u7684\u6570\u5b57\u5b6a\u751f\u91cd\u5efa\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6444\u5f71\u6d4b\u91cf\u76843D\u5efa\u6a21\u65b9\u6cd5\u5728\u751f\u6210\u6570\u5b57\u5b6a\u751f\u65f6\u5b58\u5728\u591a\u79cd\u8bbe\u8ba1\u9009\u62e9\uff0c\u4f46\u5176\u6548\u679c\u591a\u4f9d\u8d56\u4e3b\u89c2\u5b9a\u6027\u8bc4\u4ef7\uff0c\u7f3a\u4e4f\u53ef\u91cd\u590d\u3001\u53ef\u91cf\u5316\u7684\u8bc4\u4f30\u624b\u6bb5\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u65b0\u6d41\u7a0b\uff1a\u4ece\u9ad8\u8d28\u91cf3D\u6a21\u578b\u548c\u7a0b\u5e8f\u5316\u751f\u6210\u7684\u865a\u62df\u76f8\u673a\u59ff\u6001\u51fa\u53d1\uff0c\u5408\u6210\u903c\u771f2D\u56fe\u50cf\uff1b\u5229\u7528\u8fd9\u4e9b\u56fe\u50cf\u8fdb\u884c\u91cd\u5efa\uff0c\u5e76\u5c06\u91cd\u5efa\u7ed3\u679c\u4e0e\u5df2\u77e5\u7684\u865a\u62df\u76f8\u673a\u53c2\u6570\u548c\u7269\u4f53\u771f\u5b9e\u4fe1\u606f\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u5927\u91cf\u53ef\u91cd\u590d\u4e14\u53ef\u91cf\u5316\u7684\u5b9e\u9a8c\uff0c\u80fd\u591f\u7cbe\u786e\u8bc4\u4f30\u4e0d\u540c\u91cd\u5efa\u65b9\u6cd5\u5728\u89c6\u89d2\u548c\u5bf9\u8c61\u91cd\u5efa\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5408\u6210\u56fe\u50cf\u751f\u6210\u4e0e\u8bc4\u4f30\u6846\u67b6\u4e3a\u6570\u5b57\u5b6a\u751f\u91cd\u5efa\u63d0\u4f9b\u4e86\u5ba2\u89c2\u3001\u53ef\u91cf\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u624b\u6bb5\u3002", "summary_cn": "\u4ece\u73b0\u5b9e\u4e16\u754c\u7269\u4f53\u751f\u62103D\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u6444\u5f71\u6d4b\u91cf\u6cd5\u5b9e\u73b0\uff0c\u5373\u4ece\u591a\u4e2a\u89c6\u89d2\u62cd\u64442D\u7167\u7247\uff0c\u7136\u540e\u901a\u8fc7\u4e09\u89d2\u5316\u5339\u914d\u7684\u70b9\u7279\u5f81\u6765\u521b\u5efa\u5e26\u7eb9\u7406\u7684\u7f51\u683c\u3002\u5728\u6b64\u6846\u67b6\u4e0b\uff0c\u751f\u6210\u6570\u5b57\u5b6a\u751f\u4f53\u5b58\u5728\u591a\u79cd\u8bbe\u8ba1\u9009\u62e9\uff0c\u800c\u8fd9\u4e9b\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u5f02\u4e3b\u8981\u4f9d\u9760\u5b9a\u6027\u5224\u65ad\u3002\u672c\u6587\u63d0\u51fa\u5e76\u6d4b\u8bd5\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u5229\u7528\u9ad8\u8d28\u91cf3D\u6a21\u578b\u548c\u7a0b\u5e8f\u5316\u751f\u6210\u7684\u76f8\u673a\u59ff\u6001\u6765\u751f\u6210\u5408\u6210\u56fe\u50cf\u3002\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u5f00\u5c55\u5927\u91cf\u53ef\u91cd\u590d\u3001\u53ef\u91cf\u5316\u7684\u5b9e\u9a8c\uff0c\u5c06\u91cd\u5efa\u6240\u5f97\u7684\u89c6\u89d2\u548c\u5bf9\u8c61\u4f30\u8ba1\u7ed3\u679c\u4e0e\u865a\u62df\u76f8\u673a\u53c2\u6570\u53ca\u865a\u62df\u7269\u4f53\u7684\u771f\u5b9e\u4fe1\u606f\uff08ground truth\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002"}}
{"id": "2602.11316", "pdf": "https://arxiv.org/pdf/2602.11316", "abs": "https://arxiv.org/abs/2602.11316", "authors": ["Ishan Mishra", "Jiajie Li", "Deepak Mishra", "Jinjun Xiong"], "title": "Selective Prior Synchronization via SYNC Loss", "categories": ["cs.CV"], "comment": null, "summary": "Prediction under uncertainty is a critical requirement for the deep neural network to succeed responsibly. This paper focuses on selective prediction, which allows DNNs to make informed decisions about when to predict or abstain based on the uncertainty level of their predictions. Current methods are either ad-hoc such as SelectiveNet, focusing on how to modify the network architecture or objective function, or post-hoc such as softmax response, achieving selective prediction through analyzing the model's probabilistic outputs. We observe that post-hoc methods implicitly generate uncertainty information, termed the selective prior, which has traditionally been used only during inference. We argue that the selective prior provided by the selection mechanism is equally vital during the training stage. Therefore, we propose the SYNC loss which introduces a novel integration of ad-hoc and post-hoc method. Specifically, our approach incorporates the softmax response into the training process of SelectiveNet, enhancing its selective prediction capabilities by examining the selective prior. Evaluated across various datasets, including CIFAR-100, ImageNet-100, and Stanford Cars, our method not only enhances the model's generalization capabilities but also surpasses previous works in selective prediction performance, and sets new benchmarks for state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSYNC loss\uff0c\u5c06\u540e\u9a8c\uff08post-hoc\uff09\u65b9\u6cd5\u4e2d\u7684softmax\u54cd\u5e94\u5f15\u5165\u5230\u4e8b\u524d\uff08ad-hoc\uff09\u65b9\u6cd5SelectiveNet\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u9009\u62e9\u5148\u9a8c\uff08selective prior\uff09\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u9009\u62e9\u6027\u9884\u6d4b\u65b9\u6cd5\u5206\u4e3a\u4e24\u7c7b\uff1a\u4e8b\u524d\u65b9\u6cd5\uff08\u5982SelectiveNet\uff09\u4fee\u6539\u7f51\u7edc\u7ed3\u6784\u6216\u76ee\u6807\u51fd\u6570\uff0c\u540e\u9a8c\u65b9\u6cd5\uff08\u5982softmax\u54cd\u5e94\uff09\u5206\u6790\u6a21\u578b\u8f93\u51fa\u6982\u7387\u3002\u4f5c\u8005\u53d1\u73b0\u540e\u9a8c\u65b9\u6cd5\u9690\u542b\u751f\u6210\u7684\u9009\u62e9\u5148\u9a8c\u5728\u63a8\u7406\u9636\u6bb5\u88ab\u4f7f\u7528\uff0c\u4f46\u672a\u5728\u8bad\u7ec3\u4e2d\u5229\u7528\uff0c\u800c\u8be5\u4fe1\u606f\u5bf9\u63d0\u5347\u9009\u62e9\u6027\u9884\u6d4b\u80fd\u529b\u540c\u6837\u5173\u952e\u3002", "method": "\u63d0\u51faSYNC loss\uff0c\u5c06softmax\u54cd\u5e94\uff08\u4e00\u79cd\u540e\u9a8c\u65b9\u6cd5\uff09\u6240\u751f\u6210\u7684\u9009\u62e9\u5148\u9a8c\u6574\u5408\u8fdbSelectiveNet\uff08\u4e00\u79cd\u4e8b\u524d\u65b9\u6cd5\uff09\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u9009\u62e9\u5148\u9a8c\u6765\u589e\u5f3a\u6a21\u578b\u7684\u9009\u62e9\u6027\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728CIFAR-100\u3001ImageNet-100\u548cStanford Cars\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u5728\u9009\u62e9\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u5de5\u4f5c\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u6c34\u5e73\u3002", "conclusion": "\u5c06\u540e\u9a8c\u65b9\u6cd5\u4e2d\u7684\u9009\u62e9\u5148\u9a8c\u5f15\u5165\u4e8b\u524d\u65b9\u6cd5\u7684\u8bad\u7ec3\u8fc7\u7a0b\u662f\u6709\u6548\u4e14\u5fc5\u8981\u7684\uff0cSYNC loss\u6210\u529f\u878d\u5408\u4e24\u7c7b\u65b9\u6cd5\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\u3002", "summary_cn": "\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u9884\u6d4b\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8d1f\u8d23\u4efb\u5730\u53d6\u5f97\u6210\u529f\u7684\u5173\u952e\u8981\u6c42\u3002\u672c\u6587\u805a\u7126\u4e8e\u9009\u62e9\u6027\u9884\u6d4b\uff0c\u5373\u5141\u8bb8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6839\u636e\u5176\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\uff0c\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u8fdb\u884c\u9884\u6d4b\u6216\u653e\u5f03\u9884\u6d4b\u3002\u5f53\u524d\u7684\u65b9\u6cd5\u8981\u4e48\u662f\u4e8b\u524d\uff08ad-hoc\uff09\u7684\uff0c\u4f8b\u5982SelectiveNet\uff0c\u4fa7\u91cd\u4e8e\u4fee\u6539\u7f51\u7edc\u67b6\u6784\u6216\u76ee\u6807\u51fd\u6570\uff1b\u8981\u4e48\u662f\u540e\u9a8c\uff08post-hoc\uff09\u7684\uff0c\u4f8b\u5982softmax\u54cd\u5e94\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u7684\u6982\u7387\u8f93\u51fa\u6765\u5b9e\u73b0\u9009\u62e9\u6027\u9884\u6d4b\u3002\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u540e\u9a8c\u65b9\u6cd5\u9690\u5f0f\u5730\u751f\u6210\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u9009\u62e9\u5148\u9a8c\u201d\uff08selective prior\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u4f20\u7edf\u4e0a\u4ec5\u5728\u63a8\u7406\u9636\u6bb5\u4f7f\u7528\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u7531\u9009\u62e9\u673a\u5236\u63d0\u4f9b\u7684\u9009\u62e9\u5148\u9a8c\u5728\u8bad\u7ec3\u9636\u6bb5\u540c\u6837\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SYNC loss\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4e8b\u524d\u4e0e\u540e\u9a8c\u65b9\u6cd5\u7684\u7ed3\u5408\u65b9\u5f0f\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06softmax\u54cd\u5e94\u6574\u5408\u8fdbSelectiveNet\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u8003\u5bdf\u9009\u62e9\u5148\u9a8c\u6765\u589e\u5f3a\u5176\u9009\u62e9\u6027\u9884\u6d4b\u80fd\u529b\u3002\u5728\u5305\u62ecCIFAR-100\u3001ImageNet-100\u548cStanford Cars\u5728\u5185\u7684\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u5728\u9009\u62e9\u6027\u9884\u6d4b\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u5de5\u4f5c\uff0c\u6811\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2602.11323", "pdf": "https://arxiv.org/pdf/2602.11323", "abs": "https://arxiv.org/abs/2602.11323", "authors": ["Arda Alniak", "Sinan Kalkan", "Mustafa Mert Ankarali", "Afsar Saranli", "Abdullah Aydin Alatan"], "title": "MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors", "categories": ["cs.CV"], "comment": "6 pages, 2 figures, 3 tables. Submitted to ICIP 2026", "summary": "Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06\u57fa\u4e8eVision Transformer\u7684\u7a20\u5bc6\u6df1\u5ea6\u4f30\u8ba1\u96c6\u6210\u5230VINS-Mono\u540e\u7aef\u7684\u65b0\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u9650\u5236\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u4eff\u5c04\u4e0d\u53d8\u6df1\u5ea6\u4e00\u81f4\u6027\u548c\u5e8f\u6570\u7ea6\u675f\u663e\u8457\u63d0\u5347\u4f4e\u7eb9\u7406\u73af\u5883\u4e0b\u7684VIO\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5355\u76ee\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u5728\u4f4e\u7eb9\u7406\u73af\u5883\u4e2d\u56e0\u7a00\u758f\u89c6\u89c9\u7279\u5f81\u4e0d\u8db3\u800c\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u4f4d\u59ff\uff1b\u867d\u7136\u57fa\u4e8eViT\u7684\u7a20\u5bc6\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u80fd\u63d0\u4f9b\u51e0\u4f55\u4e00\u81f4\u7684\u6df1\u5ea6\u4fe1\u606f\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u5f00\u9500\u963b\u788d\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u90e8\u7f72\u3002", "method": "\u5c06\u5b66\u4e60\u5230\u7684\u6df1\u5ea6\u5148\u9a8c\u76f4\u63a5\u96c6\u6210\u5230VINS-Mono\u4f18\u5316\u540e\u7aef\u4e2d\uff0c\u5f15\u5165\u4eff\u5c04\u4e0d\u53d8\u6df1\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u6210\u5bf9\u5e8f\u6570\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u65b9\u5dee\u7684\u95e8\u63a7\u673a\u5236\u663e\u5f0f\u8fc7\u6ee4\u4e0d\u7a33\u5b9a\u4f2a\u5f71\u3002", "result": "\u5728TartanGround\u548cM3ED\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u6709\u6548\u9632\u6b62\u8f68\u8ff9\u53d1\u6563\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\uff08ATE\uff09\u6700\u591a\u964d\u4f4e28.3%\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u6ee1\u8db3\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u9650\u5236\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76eeVIO\u5728\u4f4e\u7eb9\u7406\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\uff0c\u6210\u529f\u5f25\u5408\u4e86\u5148\u8fdb\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u4e0e\u5b9e\u65f6VIO\u7cfb\u7edf\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "summary_cn": "\u4f20\u7edf\u7684\u5355\u76ee\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u7cfb\u7edf\u5728\u4f4e\u7eb9\u7406\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u7a00\u758f\u7684\u89c6\u89c9\u7279\u5f81\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7a20\u5bc6\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u88ab\u5e7f\u6cdb\u63a2\u7d22\u4f5c\u4e3a\u8865\u5145\u4fe1\u606f\u6e90\u3002\u5c3d\u7ba1\u8fd1\u671f\u57fa\u4e8e\u89c6\u89c9Transformer\uff08ViT\uff09\u7684\u590d\u6742\u57fa\u7840\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u7a20\u5bc6\u4e14\u51e0\u4f55\u4e00\u81f4\u7684\u6df1\u5ea6\u4fe1\u606f\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u9700\u6c42\u901a\u5e38\u4f7f\u5176\u65e0\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u901a\u8fc7\u5c06\u5b66\u4e60\u5230\u7684\u6df1\u5ea6\u5148\u9a8c\u76f4\u63a5\u96c6\u6210\u5230VINS-Mono\u4f18\u5316\u540e\u7aef\u4e2d\uff0c\u5f25\u5408\u4e86\u8fd9\u4e00\u5dee\u8ddd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u65bd\u52a0\u4eff\u5c04\u4e0d\u53d8\u7684\u6df1\u5ea6\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u6210\u5bf9\u5e8f\u6570\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u65b9\u5dee\u7684\u95e8\u63a7\u673a\u5236\u663e\u5f0f\u6ee4\u9664\u4e0d\u7a33\u5b9a\u7684\u4f2a\u5f71\u3002\u8be5\u65b9\u6cd5\u4e25\u683c\u9075\u5b88\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u540c\u65f6\u7a33\u5065\u5730\u6062\u590d\u5ea6\u91cf\u5c3a\u5ea6\u3002\u5728TartanGround\u548cM3ED\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u6709\u6548\u9632\u6b62\u4e86\u8f68\u8ff9\u53d1\u6563\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\uff08ATE\uff09\u6700\u591a\u964d\u4f4e\u4e8628.3%\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2602.11339", "pdf": "https://arxiv.org/pdf/2602.11339", "abs": "https://arxiv.org/abs/2602.11339", "authors": ["Evgeney Bogatyrev", "Khaled Abud", "Ivan Molodetskikh", "Nikita Alutis", "Dmitry Vatolin"], "title": "Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in real-time super-resolution have enabled higher-quality video streaming, yet existing methods struggle with the unique challenges of compressed video content. Commonly used datasets do not accurately reflect the characteristics of streaming media, limiting the relevance of current benchmarks. To address this gap, we introduce a comprehensive dataset - StreamSR - sourced from YouTube, covering a wide range of video genres and resolutions representative of real-world streaming scenarios. We benchmark 11 state-of-the-art real-time super-resolution models to evaluate their performance for the streaming use-case.\n  Furthermore, we propose EfRLFN, an efficient real-time model that integrates Efficient Channel Attention and a hyperbolic tangent activation function - a novel design choice in the context of real-time super-resolution. We extensively optimized the architecture to maximize efficiency and designed a composite loss function that improves training convergence. EfRLFN combines the strengths of existing architectures while improving both visual quality and runtime performance.\n  Finally, we show that fine-tuning other models on our dataset results in significant performance gains that generalize well across various standard benchmarks. We made the dataset, the code, and the benchmark available at https://github.com/EvgeneyBogatyrev/EfRLFN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u6d41\u5a92\u4f53\u89c6\u9891\u7684\u8d85\u5206\u8fa8\u7387\u65b0\u6570\u636e\u96c6StreamSR\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u6548\u5b9e\u65f6\u6a21\u578bEfRLFN\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u65b0\u578b\u6fc0\u6d3b\u51fd\u6570\u63d0\u5347\u6027\u80fd\uff1b\u540c\u65f6\u8bc1\u660e\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u5176\u4ed6\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5b9e\u65f6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u538b\u7f29\u89c6\u9891\u5185\u5bb9\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5e38\u7528\u6570\u636e\u96c6\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6d41\u5a92\u4f53\u89c6\u9891\u7684\u771f\u5b9e\u7279\u6027\uff0c\u5bfc\u81f4\u5f53\u524d\u57fa\u51c6\u7f3a\u4e4f\u73b0\u5b9e\u76f8\u5173\u6027\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u6e90\u81eaYouTube\u7684StreamSR\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u89c6\u9891\u7c7b\u578b\u548c\u5206\u8fa8\u7387\uff1b\u63d0\u51faEfRLFN\u6a21\u578b\uff0c\u878d\u5408\u9ad8\u6548\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u53cc\u66f2\u6b63\u5207\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u8bbe\u8ba1\u590d\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u8bad\u7ec3\uff1b\u540c\u65f6\u5bf911\u79cdSOTA\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5728StreamSR\u4e0a\u5fae\u8c03\u4ee5\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u3002", "result": "EfRLFN\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8fd0\u884c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u5728StreamSR\u4e0a\u5fae\u8c03\u5176\u4ed6\u6a21\u578b\u80fd\u5e26\u6765\u663e\u8457\u4e14\u6cdb\u5316\u826f\u597d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "StreamSR\u6570\u636e\u96c6\u66f4\u8d34\u8fd1\u771f\u5b9e\u6d41\u5a92\u4f53\u573a\u666f\uff0cEfRLFN\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u5b9e\u65f6\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff0c\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6570\u636e\u96c6\u5bf9\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "summary_cn": "\u8fd1\u671f\u5b9e\u65f6\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97\u9ad8\u8d28\u91cf\u89c6\u9891\u6d41\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u538b\u7f29\u89c6\u9891\u5185\u5bb9\u65f6\u4ecd\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002\u5e38\u7528\u6570\u636e\u96c6\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6d41\u5a92\u4f53\u89c6\u9891\u7684\u7279\u6027\uff0c\u9650\u5236\u4e86\u5f53\u524d\u57fa\u51c6\u7684\u76f8\u5173\u6027\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\u2014\u2014StreamSR\uff0c\u5176\u6765\u6e90\u4e8eYouTube\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u771f\u5b9e\u6d41\u5a92\u4f53\u89c6\u9891\u7c7b\u578b\u548c\u5206\u8fa8\u7387\u3002\u6211\u4eec\u5bf911\u79cd\u6700\u5148\u8fdb\u7684\u5b9e\u65f6\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5b83\u4eec\u5728\u6d41\u5a92\u4f53\u5e94\u7528\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86EfRLFN\uff0c\u4e00\u79cd\u9ad8\u6548\u7684\u5b9e\u65f6\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u9ad8\u6548\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cc\u66f2\u6b63\u5207\u6fc0\u6d3b\u51fd\u6570\u2014\u2014\u8fd9\u662f\u5b9e\u65f6\u8d85\u5206\u8fa8\u7387\u9886\u57df\u7684\u4e00\u9879\u65b0\u9896\u8bbe\u8ba1\u3002\u6211\u4eec\u5bf9\u67b6\u6784\u8fdb\u884c\u4e86\u5927\u91cf\u4f18\u5316\u4ee5\u6700\u5927\u5316\u6548\u7387\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u590d\u5408\u635f\u5931\u51fd\u6570\u4ee5\u6539\u5584\u8bad\u7ec3\u6536\u655b\u6027\u3002EfRLFN\u878d\u5408\u4e86\u73b0\u6709\u67b6\u6784\u7684\u4f18\u70b9\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5176\u4ed6\u6a21\u578b\u53ef\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u8fd9\u79cd\u63d0\u5347\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u5df2\u5728https://github.com/EvgeneyBogatyrev/EfRLFN\u516c\u5f00\u4e86\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u57fa\u51c6\u3002"}}
