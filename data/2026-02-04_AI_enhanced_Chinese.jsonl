{"id": "2602.02537", "pdf": "https://arxiv.org/pdf/2602.02537", "abs": "https://arxiv.org/abs/2602.02537", "authors": ["Runjie Zhou", "Youbo Shao", "Haoyu Lu", "Bowei Xing", "Tongtong Bai", "Yujie Chen", "Jie Zhao", "Lin Sui", "Haotian Yao", "Zijia Zhao", "Hao Yang", "Haoning Wu", "Zaida Zhou", "Jinguo Zhu", "Zhiqi Huang", "Yiping Bao", "Yangyang Liu", "Y. Charles", "Xinyu Zhou"], "title": "WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure \"what the model memorizes.\" The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.", "AI": {"tldr": "WorldVQA is a new benchmark that isolates and evaluates the memorized visual world knowledge of Multimodal Large Language Models (MLLMs), focusing on their ability to correctly name and ground visual entities without conflating it with reasoning.", "motivation": "Current evaluations often mix visual knowledge retrieval with reasoning, making it hard to assess what visual facts MLLMs actually memorize. WorldVQA addresses this by decoupling these aspects to measure pure visual factual knowledge.", "method": "WorldVQA constructs a stratified taxonomy of visual entities\u2014from common to rare\u2014and tests MLLMs on their atomic capability to ground and name these entities, thereby measuring memorized visual knowledge directly.", "result": "The benchmark enables rigorous evaluation of visual factuality, providing insights into the encyclopedic breadth and hallucination rates of MLLMs.", "conclusion": "WorldVQA establishes a standard for assessing the factual visual knowledge and reliability of current and future MLLMs by focusing on memorized visual world knowledge rather than reasoning performance.", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86WorldVQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u539f\u5b50\u7ea7\u89c6\u89c9\u4e16\u754c\u77e5\u8bc6\u7684\u57fa\u51c6\u3002\u4e0e\u5f53\u524d\u5e38\u5c06\u89c6\u89c9\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u6df7\u4e3a\u4e00\u8c08\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u540c\uff0cWorldVQA\u5c06\u8fd9\u4e24\u79cd\u80fd\u529b\u89e3\u8026\uff0c\u4ee5\u4e25\u683c\u8861\u91cf\u201c\u6a21\u578b\u8bb0\u4f4f\u4e86\u4ec0\u4e48\u201d\u3002\u8be5\u57fa\u51c6\u901a\u8fc7\u4e00\u4e2a\u5206\u5c42\u5206\u7c7b\u4f53\u7cfb\u8bc4\u4f30\u6a21\u578b\u5728\u547d\u540d\u548c\u5b9a\u4f4d\u89c6\u89c9\u5b9e\u4f53\u65b9\u9762\u7684\u539f\u5b50\u80fd\u529b\uff0c\u6db5\u76d6\u4ece\u5e38\u89c1\u5934\u90e8\u7c7b\u522b\u7269\u4f53\u5230\u957f\u5c3e\u7a00\u6709\u7269\u4f53\u7684\u5e7f\u6cdb\u8303\u56f4\u3002\u6211\u4eec\u671f\u671bWorldVQA\u80fd\u6210\u4e3a\u89c6\u89c9\u4e8b\u5b9e\u6027\u7684\u4e25\u683c\u6d4b\u8bd5\u6807\u51c6\uff0c\u4ece\u800c\u4e3a\u8bc4\u4f30\u5f53\u524d\u53ca\u4e0b\u4e00\u4ee3\u524d\u6cbf\u6a21\u578b\u7684\u767e\u79d1\u5168\u4e66\u5f0f\u77e5\u8bc6\u5e7f\u5ea6\u548c\u5e7b\u89c9\u7387\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.02676", "pdf": "https://arxiv.org/pdf/2602.02676", "abs": "https://arxiv.org/abs/2602.02676", "authors": ["Xintong Zhang", "Xiaowen Zhang", "Jongrong Wu", "Zhi Gao", "Shilin Yan", "Zhenxin Diao", "Kunpeng Gao", "Xuanyan Chen", "Yuwei Wu", "Yunde Jia", "Qing Li"], "title": "AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process", "categories": ["cs.CV"], "comment": null, "summary": "Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AdaptMMBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u81ea\u9002\u5e94\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u52a8\u6001\u96be\u5ea6\u8bc6\u522b\u548c\u591a\u7ef4\u8fc7\u7a0b\u5206\u6790\uff0c\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u6a21\u5f0f\u9009\u62e9\u4e0e\u6700\u7ec8\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u89e3\u8026\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u5bf9\u81ea\u9002\u5e94\u591a\u6a21\u6001\u63a8\u7406\u7684\u8bc4\u4f30\u4f9d\u8d56\u9759\u6001\u96be\u5ea6\u6807\u7b7e\u548c\u7b80\u5355\u6307\u6807\uff0c\u65e0\u6cd5\u53cd\u6620\u4efb\u52a1\u96be\u5ea6\u968f\u6a21\u578b\u80fd\u529b\u53d8\u5316\u7684\u52a8\u6001\u7279\u6027\uff0c\u4e14\u6df7\u6dc6\u4e86\u81ea\u9002\u5e94\u6a21\u5f0f\u9009\u62e9\u80fd\u529b\u4e0e\u6574\u4f53\u6027\u80fd\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "method": "\u63d0\u51faAdaptMMBench\u57fa\u51c6\uff0c\u6db5\u76d6\u4e94\u4e2a\u9886\u57df\uff08\u73b0\u5b9e\u4e16\u754c\u3001OCR\u3001GUI\u3001\u77e5\u8bc6\u3001\u6570\u5b66\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u7684\u52a8\u6001\u96be\u5ea6\u8bc6\u522b\u673a\u5236\uff0c\u5e76\u5f15\u5165Matthews\u76f8\u5173\u7cfb\u6570\uff08MCC\uff09\u8bc4\u4f30\u63a8\u7406\u6a21\u5f0f\u9009\u62e9\u7684\u5408\u7406\u6027\uff1b\u540c\u65f6\u652f\u6301\u5bf9\u5173\u952e\u6b65\u9aa4\u8986\u76d6\u7387\u3001\u5de5\u5177\u6709\u6548\u6027\u53ca\u8ba1\u7b97\u6548\u7387\u7684\u591a\u7ef4\u8fc7\u7a0b\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff1a\u81ea\u9002\u5e94\u6a21\u5f0f\u9009\u62e9\u80fd\u529b\u968f\u6a21\u578b\u5bb9\u91cf\u63d0\u5347\u800c\u589e\u5f3a\uff0c\u4f46\u4e0e\u6700\u7ec8\u51c6\u786e\u7387\u660e\u663e\u89e3\u8026\uff1b\u5173\u952e\u6b65\u9aa4\u8986\u76d6\u7387\u4e0e\u6027\u80fd\u4e00\u81f4\uff0c\u800c\u5de5\u5177\u6709\u6548\u6027\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u95f4\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "AdaptMMBench\u80fd\u66f4\u7cbe\u51c6\u5730\u8bc4\u4f30VLMs\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u5f0f\u9009\u62e9\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u8131\u8282\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "summary_cn": "\u81ea\u9002\u5e94\u591a\u6a21\u6001\u63a8\u7406\u5df2\u6210\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u4e00\u4e2a\u524d\u666f\u5e7f\u9614\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u52a8\u6001\u8c03\u8282\u5de5\u5177\u589e\u5f3a\u7684\u89c6\u89c9\u63a8\u7406\u4e0e\u6587\u672c\u63a8\u7406\uff0c\u4ee5\u63d0\u5347\u6548\u679c\u4e0e\u6548\u7387\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u96be\u5ea6\u6807\u7b7e\u548c\u7b80\u5316\u7684\u6307\u6807\uff0c\u65e0\u6cd5\u6355\u6349\u4efb\u52a1\u96be\u5ea6\u76f8\u5bf9\u4e8e\u4e0d\u540c\u6a21\u578b\u80fd\u529b\u7684\u52a8\u6001\u7279\u6027\uff0c\u4ece\u800c\u6a21\u7cca\u4e86\u81ea\u9002\u5e94\u6a21\u5f0f\u9009\u62e9\u4e0e\u6574\u4f53\u6027\u80fd\u4e4b\u95f4\u7684\u533a\u522b\uff0c\u5e76\u5ffd\u89c6\u4e86\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u3002\u672c\u6587\u63d0\u51faAdaptMMBench\u2014\u2014\u4e00\u4e2a\u9762\u5411\u81ea\u9002\u5e94\u591a\u6a21\u6001\u63a8\u7406\u7684\u7efc\u5408\u6027\u57fa\u51c6\uff0c\u8986\u76d6\u73b0\u5b9e\u4e16\u754c\u3001OCR\u3001GUI\u3001\u77e5\u8bc6\u548c\u6570\u5b66\u4e94\u5927\u9886\u57df\uff0c\u5305\u542b\u76f4\u63a5\u611f\u77e5\u4e0e\u590d\u6742\u63a8\u7406\u4efb\u52a1\u3002AdaptMMBench\u91c7\u7528Matthews\u76f8\u5173\u7cfb\u6570\uff08MCC\uff09\u6765\u8bc4\u4f30\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u9009\u62e9\u7684\u5408\u7406\u6027\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u52a8\u6001\u8bc6\u522b\u4efb\u52a1\u96be\u5ea6\uff0c\u4ece\u800c\u5c06\u8fd9\u79cd\u5143\u8ba4\u77e5\u80fd\u529b\u72ec\u7acb\u51fa\u6765\u3002\u6b64\u5916\uff0c\u8be5\u57fa\u51c6\u8fd8\u652f\u6301\u5728\u5173\u952e\u6b65\u9aa4\u8986\u76d6\u7387\u3001\u5de5\u5177\u6709\u6548\u6027\u53ca\u8ba1\u7b97\u6548\u7387\u7b49\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u8fc7\u7a0b\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5c3d\u7ba1\u81ea\u9002\u5e94\u6a21\u5f0f\u9009\u62e9\u80fd\u529b\u968f\u6a21\u578b\u5bb9\u91cf\u589e\u957f\u800c\u63d0\u5347\uff0c\u4f46\u5b83\u4e0e\u6700\u7ec8\u51c6\u786e\u7387\u663e\u8457\u89e3\u8026\uff1b\u76f8\u53cd\uff0c\u5173\u952e\u6b65\u9aa4\u8986\u76d6\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u4e00\u81f4\uff0c\u800c\u5de5\u5177\u6709\u6548\u6027\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e4b\u95f4\u4ecd\u9ad8\u5ea6\u4e0d\u4e00\u81f4\u3002"}}
{"id": "2602.02721", "pdf": "https://arxiv.org/pdf/2602.02721", "abs": "https://arxiv.org/abs/2602.02721", "authors": ["Jinglun Yu", "Yaning Wang", "Wenhan Guo", "Yuan Gao", "Yu Sun", "Jin U. Kang"], "title": "End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning", "categories": ["cs.CV"], "comment": null, "summary": "Inverse scattering in optical coherence tomography (OCT) seeks to recover both structural images and intrinsic tissue optical properties, including refractive index, scattering coefficient, and anisotropy. This inverse problem is challenging due to attenuation, speckle noise, and strong coupling among parameters. We propose a regularized end-to-end deep learning framework that jointly reconstructs optical parameter maps and speckle-reduced OCT structural intensity for layer visualization. Trained with Monte Carlo-simulated ground truth, our network incorporates a physics-based OCT forward model that generates predicted signals from the estimated parameters, providing physics-consistent supervision for parameter recovery and artifact suppression. Experiments on the synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity. This approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6b63\u5219\u5316\u6846\u67b6\uff0c\u7528\u4e8eOCT\u4e2d\u7684\u9006\u6563\u5c04\u95ee\u9898\uff0c\u53ef\u540c\u65f6\u91cd\u5efa\u7ec4\u7ec7\u5149\u5b66\u53c2\u6570\u56fe\u548c\u53bb\u6591\u70b9\u7684\u7ed3\u6784\u56fe\u50cf\u3002", "motivation": "OCT\u4e2d\u7684\u9006\u6563\u5c04\u95ee\u9898\u56e0\u8870\u51cf\u3001\u6591\u70b9\u566a\u58f0\u53ca\u53c2\u6570\u95f4\u5f3a\u8026\u5408\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u51c6\u786e\u6062\u590d\u7ed3\u6784\u56fe\u50cf\u548c\u5185\u5728\u5149\u5b66\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684OCT\u524d\u5411\u6a21\u578b\uff0c\u5e76\u5229\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u7269\u7406\u4e00\u81f4\u7684\u76d1\u7763\u4fe1\u53f7\u8054\u5408\u91cd\u5efa\u5149\u5b66\u53c2\u6570\u56fe\u548c\u53bb\u566a\u7ed3\u6784\u56fe\u50cf\u3002", "result": "\u5728\u5408\u6210\u89d2\u819cOCT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u4ecd\u80fd\u7a33\u5065\u6062\u590d\u5149\u5b66\u53c2\u6570\u56fe\uff0c\u63d0\u5347\u5206\u8fa8\u7387\u5e76\u589e\u5f3a\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b9a\u91cf\u591a\u53c2\u6570\u7ec4\u7ec7\u8868\u5f81\uff0c\u5c55\u793a\u4e86\u5c06\u7269\u7406\u4fe1\u606f\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\u5728\u8ba1\u7b97OCT\u4e2d\u7684\u4f18\u52bf\u3002", "summary_cn": "\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\uff08OCT\uff09\u4e2d\u7684\u9006\u6563\u5c04\u65e8\u5728\u540c\u65f6\u6062\u590d\u7ec4\u7ec7\u7684\u7ed3\u6784\u56fe\u50cf\u548c\u5176\u56fa\u6709\u5149\u5b66\u7279\u6027\uff0c\u5305\u62ec\u6298\u5c04\u7387\u3001\u6563\u5c04\u7cfb\u6570\u548c\u5404\u5411\u5f02\u6027\u3002\u7531\u4e8e\u8870\u51cf\u3001\u6591\u70b9\u566a\u58f0\u4ee5\u53ca\u53c2\u6570\u95f4\u7684\u5f3a\u8026\u5408\uff0c\u8fd9\u4e00\u9006\u95ee\u9898\u6781\u5177\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6b63\u5219\u5316\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u8054\u5408\u91cd\u5efa\u5149\u5b66\u53c2\u6570\u56fe\u548c\u7ecf\u8fc7\u53bb\u6591\u70b9\u5904\u7406\u7684OCT\u7ed3\u6784\u5f3a\u5ea6\u56fe\u50cf\uff0c\u4ee5\u5b9e\u73b0\u5206\u5c42\u53ef\u89c6\u5316\u3002\u8be5\u7f51\u7edc\u5229\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u751f\u6210\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5d4c\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684OCT\u524d\u5411\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6839\u636e\u4f30\u8ba1\u7684\u53c2\u6570\u751f\u6210\u9884\u6d4b\u4fe1\u53f7\uff0c\u4ece\u800c\u4e3a\u53c2\u6570\u6062\u590d\u548c\u4f2a\u5f71\u6291\u5236\u63d0\u4f9b\u7269\u7406\u4e00\u81f4\u6027\u7684\u76d1\u7763\u3002\u5728\u5408\u6210\u89d2\u819cOCT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4ecd\u80fd\u7a33\u5065\u5730\u6062\u590d\u5149\u5b66\u53c2\u6570\u56fe\uff0c\u63d0\u9ad8\u5206\u8fa8\u7387\u5e76\u589e\u5f3a\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u7ec4\u7ec7\u7684\u5b9a\u91cf\u591a\u53c2\u6570\u8868\u5f81\uff0c\u5e76\u7a81\u663e\u4e86\u5c06\u7269\u7406\u4fe1\u606f\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\u5728\u8ba1\u7b97OCT\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.02765", "pdf": "https://arxiv.org/pdf/2602.02765", "abs": "https://arxiv.org/abs/2602.02765", "authors": ["Haruhiko Murata", "Kazuhiro Hotta"], "title": "SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\\textbf{SPC module}, \\textbf{SSVA}, and \\textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.", "AI": {"tldr": "SVD-ViT uses singular value decomposition to enhance foreground feature learning in Vision Transformers, improving classification accuracy by suppressing background noise.", "motivation": "Vision Transformers lack an explicit mechanism to distinguish foreground from background due to global self-attention, which can lead to learning irrelevant background features and degraded performance.", "method": "The proposed SVD-ViT integrates three components\u2014SPC module, SSVA, and ID-RSVD\u2014to extract and aggregate singular vectors that capture foreground information, thereby suppressing background noise and artifacts.", "result": "Experiments show that SVD-ViT improves classification accuracy and effectively learns informative foreground representations while reducing background interference.", "conclusion": "Incorporating SVD into ViT enables better foreground-background separation, leading to more robust and accurate visual classification.", "summary_cn": "\u89c6\u89c9Transformer\uff08ViT\uff09\u5df2\u88ab\u786e\u7acb\u4e3a\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u662f\u5168\u5c40\u64cd\u4f5c\u7684\uff0c\u5b83\u4eec\u7f3a\u4e4f\u663e\u5f0f\u533a\u5206\u524d\u666f\u4e0e\u80cc\u666f\u7684\u673a\u5236\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u5230\u4e0d\u5fc5\u8981\u7684\u80cc\u666f\u7279\u5f81\u548c\u4f2a\u5f71\uff0c\u4ece\u800c\u964d\u4f4e\u5206\u7c7b\u6027\u80fd\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SVD-ViT\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u6765\u4f18\u5148\u5b66\u4e60\u524d\u666f\u7279\u5f81\u3002SVD-ViT\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\u2014\u2014SPC\u6a21\u5757\u3001SSVA\u548cID-RSVD\uff0c\u901a\u8fc7\u63d0\u53d6\u5e76\u805a\u5408\u80fd\u591f\u6355\u6349\u7269\u4f53\u524d\u666f\u4fe1\u606f\u7684\u5947\u5f02\u5411\u91cf\uff0c\u6291\u5236\u80cc\u666f\u566a\u58f0\u548c\u4f2a\u5f71\u7b49\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u56e0\u7d20\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6709\u6548\u5b66\u4e60\u4e86\u5177\u6709\u4fe1\u606f\u91cf\u7684\u524d\u666f\u8868\u5f81\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u80cc\u666f\u566a\u58f0\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.02808", "pdf": "https://arxiv.org/pdf/2602.02808", "abs": "https://arxiv.org/abs/2602.02808", "authors": ["Matteo Bastico", "Pierre Onghena", "David Ryckelynck", "Beatriz Marcotegui", "Santiago Velasco-Forero", "Laurent Cort\u00e9", "Caroline Robine--Decourcelle", "Etienne Decenci\u00e8re"], "title": "LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "This paper has been accepted at International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Accurate identification of anatomical landmarks is crucial for various medical applications. Traditional manual landmarking is time-consuming and prone to inter-observer variability, while rule-based methods are often tailored to specific geometries or limited sets of landmarks. In recent years, anatomical surfaces have been effectively represented as point clouds, which are lightweight structures composed of spatial coordinates. Following this strategy and to overcome the limitations of existing landmarking techniques, we propose Landmark Point Transformer (LmPT), a method for automatic anatomical landmark detection on point clouds that can leverage homologous bones from different species for translational research. The LmPT model incorporates a conditioning mechanism that enables adaptability to different input types to conduct cross-species learning. We focus the evaluation of our approach on femoral landmarking using both human and newly annotated dog femurs, demonstrating its generalization and effectiveness across species. The code and dog femur dataset will be publicly available at: https://github.com/Pierreoo/LandmarkPointTransformer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLandmark Point Transformer\uff08LmPT\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u70b9\u4e91\u4e0a\u81ea\u52a8\u68c0\u6d4b\u89e3\u5256\u6807\u5fd7\u70b9\uff0c\u652f\u6301\u8de8\u7269\u79cd\u5b66\u4e60\uff0c\u5e76\u5728\u4eba\u7c7b\u548c\u72d7\u7684\u80a1\u9aa8\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u6807\u8bb0\u89e3\u5256\u6807\u5fd7\u70b9\u8017\u65f6\u4e14\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\uff0c\u800c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u901a\u5e38\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u51e0\u4f55\u7ed3\u6784\u6216\u6709\u9650\u7684\u6807\u5fd7\u70b9\u96c6\u5408\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u3001\u81ea\u52a8\u5316\u7684\u89e3\u5256\u6807\u5fd7\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8de8\u7269\u79cd\u7814\u7a76\u3002", "method": "\u5c06\u89e3\u5256\u8868\u9762\u8868\u793a\u4e3a\u70b9\u4e91\uff0c\u5e76\u63d0\u51faLmPT\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e00\u79cd\u6761\u4ef6\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u7269\u79cd\u7684\u89e3\u5256\u6807\u5fd7\u70b9\u81ea\u52a8\u68c0\u6d4b\u3002", "result": "\u5728\u4eba\u7c7b\u548c\u65b0\u6807\u6ce8\u7684\u72d7\u80a1\u9aa8\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86LmPT\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u79cd\u95f4\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "LmPT\u662f\u4e00\u79cd\u6709\u6548\u7684\u8de8\u7269\u79cd\u89e3\u5256\u6807\u5fd7\u70b9\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5177\u5907\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u72d7\u80a1\u9aa8\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002", "summary_cn": "\u51c6\u786e\u8bc6\u522b\u89e3\u5256\u6807\u5fd7\u70b9\u5bf9\u4e8e\u591a\u79cd\u533b\u5b66\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u6807\u8bb0\u65b9\u6cd5\u8017\u65f6\u4e14\u5bb9\u6613\u53d7\u5230\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u7684\u5f71\u54cd\uff0c\u800c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u51e0\u4f55\u7ed3\u6784\u6216\u6709\u9650\u7684\u6807\u5fd7\u70b9\u96c6\u5408\u3002\u8fd1\u5e74\u6765\uff0c\u89e3\u5256\u8868\u9762\u5df2\u88ab\u6709\u6548\u5730\u8868\u793a\u4e3a\u7531\u7a7a\u95f4\u5750\u6807\u7ec4\u6210\u7684\u8f7b\u91cf\u7ea7\u70b9\u4e91\u7ed3\u6784\u3002\u57fa\u4e8e\u8fd9\u4e00\u7b56\u7565\u5e76\u4e3a\u514b\u670d\u73b0\u6709\u6807\u8bb0\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Landmark Point Transformer\uff08LmPT\uff09\u2014\u2014\u4e00\u79cd\u7528\u4e8e\u5728\u70b9\u4e91\u4e0a\u81ea\u52a8\u68c0\u6d4b\u89e3\u5256\u6807\u5fd7\u70b9\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5229\u7528\u6765\u81ea\u4e0d\u540c\u7269\u79cd\u7684\u540c\u6e90\u9aa8\u9abc\u8fdb\u884c\u8f6c\u5316\u7814\u7a76\u3002LmPT\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u79cd\u6761\u4ef6\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u8f93\u5165\u7c7b\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u7269\u79cd\u5b66\u4e60\u3002\u6211\u4eec\u5728\u4eba\u7c7b\u548c\u65b0\u6807\u6ce8\u7684\u72d7\u80a1\u9aa8\u6570\u636e\u4e0a\u5bf9\u6240\u63d0\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u7269\u79cd\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6709\u6548\u6027\u3002\u76f8\u5173\u4ee3\u7801\u548c\u72d7\u80a1\u9aa8\u6570\u636e\u96c6\u5c06\u5728 https://github.com/Pierreoo/LandmarkPointTransformer \u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2602.02850", "pdf": "https://arxiv.org/pdf/2602.02850", "abs": "https://arxiv.org/abs/2602.02850", "authors": ["Keqi Chen", "Vinkle Srivastav", "Armine Vardazaryan", "Cindy Rolland", "Didier Mutter", "Nicolas Padoy"], "title": "Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room", "categories": ["cs.CV"], "comment": null, "summary": "Privacy preservation is a prerequisite for using video data in Operating Room (OR) research. Effective anonymization relies on the exhaustive localization of every individual; even a single missed detection necessitates extensive manual correction. However, existing approaches face two critical scalability bottlenecks: (1) they usually require manual annotations of each new clinical site for high accuracy; (2) while multi-camera setups have been widely adopted to address single-view ambiguity, camera calibration is typically required whenever cameras are repositioned. To address these problems, we propose a novel self-supervised multi-view video anonymization framework consisting of whole-body person detection and whole-body pose estimation, without annotation or camera calibration. Our core strategy is to enhance the single-view detector by \"retrieving\" false negatives using temporal and multi-view context, and conducting self-supervised domain adaptation. We first run an off-the-shelf whole-body person detector in each view with a low-score threshold to gather candidate detections. Then, we retrieve the low-score false negatives that exhibit consistency with the high-score detections via tracking and self-supervised uncalibrated multi-view association. These recovered detections serve as pseudo labels to iteratively fine-tune the whole-body detector. Finally, we apply whole-body pose estimation on each detected person, and fine-tune the pose model using its own high-score predictions. Experiments on the 4D-OR dataset of simulated surgeries and our dataset of real surgeries show the effectiveness of our approach achieving over 97% recall. Moreover, we train a real-time whole-body detector using our pseudo labels, achieving comparable performance and highlighting our method's practical applicability. Code is available at https://github.com/CAMMA-public/OR_anonymization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u548c\u76f8\u673a\u6807\u5b9a\u7684\u81ea\u76d1\u7763\u591a\u89c6\u89d2\u89c6\u9891\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u5e8f\u548c\u591a\u89c6\u89d2\u4e0a\u4e0b\u6587\u4fe1\u606f\u627e\u56de\u5355\u89c6\u89d2\u68c0\u6d4b\u4e2d\u7684\u6f0f\u68c0\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u57df\u81ea\u9002\u5e94\u63d0\u5347\u68c0\u6d4b\u4e0e\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u5728\u624b\u672f\u5ba4\u89c6\u9891\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc797%\u7684\u53ec\u56de\u7387\u3002", "motivation": "\u5728\u624b\u672f\u5ba4\uff08OR\uff09\u89c6\u9891\u7814\u7a76\u4e2d\uff0c\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u533f\u540d\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u6bcf\u4e2a\u4e2a\u4f53\u7684\u5b8c\u6574\u5b9a\u4f4d\u3002\u7136\u800c\uff0c\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff1a\u4e00\u662f\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u4e34\u5e8a\u573a\u6240\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\u4ee5\u4fdd\u8bc1\u7cbe\u5ea6\uff1b\u4e8c\u662f\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u5728\u6444\u50cf\u5934\u4f4d\u7f6e\u53d8\u52a8\u540e\u9700\u91cd\u65b0\u6807\u5b9a\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u591a\u89c6\u89d2\u89c6\u9891\u533f\u540d\u5316\u6846\u67b6\uff0c\u5305\u542b\u5168\u8eab\u4eba\u4f53\u68c0\u6d4b\u4e0e\u59ff\u6001\u4f30\u8ba1\u4e24\u4e2a\u6a21\u5757\u3002\u9996\u5148\u5728\u5404\u89c6\u89d2\u4f7f\u7528\u4f4e\u9608\u503c\u8fd0\u884c\u73b0\u6210\u68c0\u6d4b\u5668\u83b7\u53d6\u5019\u9009\u6846\uff1b\u7136\u540e\u901a\u8fc7\u8ddf\u8e2a\u548c\u65e0\u6807\u5b9a\u7684\u591a\u89c6\u89d2\u5173\u8054\uff0c\u627e\u56de\u4e0e\u9ad8\u7f6e\u4fe1\u5ea6\u68c0\u6d4b\u4e00\u81f4\u7684\u4f4e\u5206\u6f0f\u68c0\u4f5c\u4e3a\u4f2a\u6807\u7b7e\uff1b\u63a5\u7740\u7528\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u8fed\u4ee3\u5fae\u8c03\u68c0\u6d4b\u5668\uff1b\u6700\u540e\u5728\u68c0\u6d4b\u7ed3\u679c\u4e0a\u8fdb\u884c\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5229\u7528\u81ea\u8eab\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5fae\u8c03\u59ff\u6001\u6a21\u578b\u3002", "result": "\u57284D-OR\u6a21\u62df\u624b\u672f\u6570\u636e\u96c6\u548c\u771f\u5b9e\u624b\u672f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ec\u56de\u7387\u8d85\u8fc797%\u3002\u6b64\u5916\uff0c\u5229\u7528\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8bad\u7ec3\u7684\u5b9e\u65f6\u5168\u8eab\u68c0\u6d4b\u5668\u6027\u80fd\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u624b\u672f\u5ba4\u89c6\u9891\u533f\u540d\u5316\u4e2d\u5bf9\u4eba\u5de5\u6807\u6ce8\u548c\u76f8\u673a\u6807\u5b9a\u7684\u4f9d\u8d56\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u533b\u7597\u89c6\u9891\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u9690\u79c1\u4fdd\u62a4\u662f\u5c06\u89c6\u9891\u6570\u636e\u7528\u4e8e\u624b\u672f\u5ba4\uff08OR\uff09\u7814\u7a76\u7684\u524d\u63d0\u6761\u4ef6\u3002\u6709\u6548\u7684\u533f\u540d\u5316\u4f9d\u8d56\u4e8e\u5bf9\u6bcf\u4e2a\u4e2a\u4f53\u7684\u5168\u9762\u5b9a\u4f4d\uff1b\u5373\u4f7f\u6f0f\u68c0\u4e00\u4eba\uff0c\u4e5f\u9700\u8981\u5927\u91cf\u4eba\u5de5\u4fee\u6b63\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff1a\uff081\uff09\u901a\u5e38\u9700\u8981\u5bf9\u6bcf\u4e2a\u65b0\u7684\u4e34\u5e8a\u573a\u6240\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\u4ee5\u83b7\u5f97\u9ad8\u7cbe\u5ea6\uff1b\uff082\uff09\u5c3d\u7ba1\u591a\u6444\u50cf\u5934\u8bbe\u7f6e\u5df2\u88ab\u5e7f\u6cdb\u91c7\u7528\u4ee5\u89e3\u51b3\u5355\u89c6\u89d2\u6a21\u7cca\u95ee\u9898\uff0c\u4f46\u6bcf\u6b21\u6444\u50cf\u5934\u4f4d\u7f6e\u53d8\u52a8\u540e\u901a\u5e38\u90fd\u9700\u8981\u91cd\u65b0\u6807\u5b9a\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u591a\u89c6\u89d2\u89c6\u9891\u533f\u540d\u5316\u6846\u67b6\uff0c\u5305\u542b\u5168\u8eab\u4eba\u4f53\u68c0\u6d4b\u4e0e\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u76f8\u673a\u6807\u5b9a\u3002\u6211\u4eec\u7684\u6838\u5fc3\u7b56\u7565\u662f\u901a\u8fc7\u5229\u7528\u65f6\u5e8f\u548c\u591a\u89c6\u89d2\u4e0a\u4e0b\u6587\u201c\u627e\u56de\u201d\u5355\u89c6\u89d2\u68c0\u6d4b\u4e2d\u7684\u6f0f\u68c0\uff0c\u5e76\u8fdb\u884c\u81ea\u76d1\u7763\u57df\u81ea\u9002\u5e94\u3002\u6211\u4eec\u9996\u5148\u5728\u6bcf\u4e2a\u89c6\u89d2\u4e2d\u4ee5\u4f4e\u5206\u9608\u503c\u8fd0\u884c\u73b0\u6210\u7684\u5168\u8eab\u4eba\u4f53\u68c0\u6d4b\u5668\u4ee5\u6536\u96c6\u5019\u9009\u68c0\u6d4b\u7ed3\u679c\uff1b\u7136\u540e\u901a\u8fc7\u8ddf\u8e2a\u548c\u81ea\u76d1\u7763\u7684\u65e0\u6807\u5b9a\u591a\u89c6\u89d2\u5173\u8054\uff0c\u627e\u56de\u4e0e\u9ad8\u5206\u68c0\u6d4b\u7ed3\u679c\u4e00\u81f4\u7684\u4f4e\u5206\u6f0f\u68c0\u3002\u8fd9\u4e9b\u6062\u590d\u7684\u68c0\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u4f2a\u6807\u7b7e\uff0c\u7528\u4e8e\u8fed\u4ee3\u5fae\u8c03\u5168\u8eab\u68c0\u6d4b\u5668\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u6bcf\u4e2a\u68c0\u6d4b\u5230\u7684\u4eba\u4f53\u4e0a\u5e94\u7528\u5168\u8eab\u59ff\u6001\u4f30\u8ba1\uff0c\u5e76\u5229\u7528\u5176\u81ea\u8eab\u7684\u9ad8\u5206\u9884\u6d4b\u5fae\u8c03\u59ff\u6001\u6a21\u578b\u3002\u57284D-OR\u6a21\u62df\u624b\u672f\u6570\u636e\u96c6\u548c\u6211\u4eec\u7684\u771f\u5b9e\u624b\u672f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u53ec\u56de\u7387\u8d85\u8fc797%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5b9e\u65f6\u5168\u8eab\u68c0\u6d4b\u5668\uff0c\u53d6\u5f97\u4e86\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u672c\u65b9\u6cd5\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/CAMMA-public/OR_anonymization\u3002"}}
{"id": "2602.02873", "pdf": "https://arxiv.org/pdf/2602.02873", "abs": "https://arxiv.org/abs/2602.02873", "authors": ["Weihang You", "Qingchan Zhu", "David Liu", "Yi Pan", "Geng Yuan", "Hanqi Jiang"], "title": "ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying", "categories": ["cs.CV"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.", "AI": {"tldr": "ViThinker \u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u67e5\u8be2\u673a\u5236\u52a8\u6001\u751f\u6210\u4efb\u52a1\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u8fc7\u65e9\u5c06\u89c6\u89c9\u4fe1\u606f\u8f6c\u4e3a\u6587\u672c\uff0c\u4e22\u5931\u4e86\u51e0\u4f55\u548c\u7a7a\u95f4\u5e03\u5c40\u7b49\u8fde\u7eed\u4fe1\u606f\uff1b\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u88ab\u52a8\u5904\u7406\u9884\u8ba1\u7b97\u8f93\u5165\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51fa ViThinker \u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u81ea\u4e3b\u751f\u6210\u51b3\u7b56\uff08\u67e5\u8be2\uff09token\uff0c\u6309\u9700\u5408\u6210\u4e0e\u4e13\u5bb6\u5bf9\u9f50\u7684\u89c6\u89c9\u7279\u5f81\uff1b\u8bad\u7ec3\u9636\u6bb5\u5185\u5316\u89c6\u89c9\u4e13\u5bb6\u80fd\u529b\uff0c\u63a8\u7406\u9636\u6bb5\u8fdb\u884c\u751f\u6210\u5f0f\u5fc3\u667a\u6a21\u62df\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u5148\u5c06\u51bb\u7ed3\u4e13\u5bb6\u84b8\u998f\u8fdb\u6a21\u578b\u53c2\u6570\uff0c\u518d\u901a\u8fc7\u7a00\u758f\u6027\u60e9\u7f5a\u5b66\u4e60\u4efb\u52a1\u9a71\u52a8\u7684\u67e5\u8be2\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViThinker \u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u5176\u5728\u611f\u77e5\u5b9a\u4f4d\u548c\u63a8\u7406\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u88ab\u52a8\u65b9\u6cd5\u3002", "conclusion": "\u4e3b\u52a8\u67e5\u8be2\u751f\u6210\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u80fd\u529b\uff0cViThinker \u4e3a\u5b9e\u73b0\u7c7b\u4eba\u4e3b\u52a8\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "summary_cn": "\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5374\u9762\u4e34\u6311\u6218\uff0c\u539f\u56e0\u5728\u4e8e\u8fc7\u65e9\u5730\u5c06\u89c6\u89c9\u4fe1\u606f\u8f6c\u6362\u4e3a\u6587\u672c\uff0c\u4ece\u800c\u4e22\u5931\u4e86\u51e0\u4f55\u7ed3\u6784\u548c\u7a7a\u95f4\u5e03\u5c40\u7b49\u8fde\u7eed\u4fe1\u606f\u3002\u5c3d\u7ba1\u8fd1\u671f\u65b9\u6cd5\u901a\u8fc7\u9759\u6001\u679a\u4e3e\u6216\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u9009\u62e9\u673a\u5236\u6765\u589e\u5f3a CoT\uff0c\u4f46\u5b83\u4eec\u4ecd\u5c5e\u4e8e\u88ab\u52a8\u65b9\u5f0f\uff0c\u5373\u4ec5\u5904\u7406\u9884\u5148\u8ba1\u7b97\u597d\u7684\u8f93\u5165\uff0c\u800c\u975e\u4e3b\u52a8\u63a2\u5bfb\u4efb\u52a1\u76f8\u5173\u7ec6\u8282\u3002\u53d7\u4eba\u7c7b\u4e3b\u52a8\u611f\u77e5\u673a\u5236\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ViThinker \u6846\u67b6\uff0c\u4f7f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u751f\u6210\u51b3\u7b56\uff08\u67e5\u8be2\uff09token\uff0c\u6309\u9700\u89e6\u53d1\u4e0e\u4e13\u5bb6\u5bf9\u9f50\u7684\u89c6\u89c9\u7279\u5f81\u5408\u6210\u3002ViThinker \u5728\u8bad\u7ec3\u9636\u6bb5\u5185\u5316\u89c6\u89c9\u4e13\u5bb6\u80fd\u529b\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u65e0\u9700\u5916\u90e8\u5de5\u5177\u8c03\u7528\u5373\u53ef\u6267\u884c\u751f\u6210\u5f0f\u5fc3\u667a\u6a21\u62df\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1a\u9996\u5148\u5c06\u51bb\u7ed3\u7684\u4e13\u5bb6\u6a21\u578b\u84b8\u998f\u5230\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u7136\u540e\u901a\u8fc7\u7a00\u758f\u6027\u60e9\u7f5a\u5b66\u4e60\u4efb\u52a1\u9a71\u52a8\u7684\u67e5\u8be2\u673a\u5236\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e00\u6b65\u63a8\u7406\u53d1\u73b0\u6700\u5c0f\u4e14\u5145\u5206\u7684\u611f\u77e5\u4fe1\u606f\u3002\u5728\u591a\u4e2a\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cViThinker \u59cb\u7ec8\u53d6\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u4e3b\u52a8\u67e5\u8be2\u751f\u6210\u5728\u611f\u77e5\u5b9a\u4f4d\u548c\u63a8\u7406\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u88ab\u52a8\u65b9\u6cd5\u3002"}}
{"id": "2602.02894", "pdf": "https://arxiv.org/pdf/2602.02894", "abs": "https://arxiv.org/abs/2602.02894", "authors": ["Daivik Patel", "Shrenik Patel"], "title": "DoubleTake: Contrastive Reasoning for Faithful Decision-Making in Medical Imaging", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate decision making in medical imaging requires reasoning over subtle visual differences between confusable conditions, yet most existing approaches rely on nearest neighbor retrieval that returns redundant evidence and reinforces a single hypothesis. We introduce a contrastive, document-aware reference selection framework that constructs compact evidence sets optimized for discrimination rather than similarity by explicitly balancing visual relevance, embedding diversity, and source-level provenance using ROCO embeddings and metadata. While ROCO provides large-scale image-caption pairs, it does not specify how references should be selected for contrastive reasoning, and naive retrieval frequently yields near-duplicate figures from the same document. To address this gap, we release a reproducible reference selection protocol and curated reference bank that enable a systematic study of contrastive retrieval in medical image reasoning. Building on these contrastive evidence sets, we propose Counterfactual-Contrastive Inference, a confidence-aware reasoning framework that performs structured pairwise visual comparisons and aggregates evidence using margin-based decision rules with faithful abstention. On the MediConfusion benchmark, our approach achieves state-of-the-art performance, improving set-level accuracy by nearly 15% relative to prior methods while reducing confusion and improving individual accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u533b\u5b66\u56fe\u50cf\u63a8\u7406\u7684\u5bf9\u6bd4\u5f0f\u3001\u6587\u6863\u611f\u77e5\u7684\u53c2\u8003\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8bc1\u636e\u96c6\u7684\u591a\u6837\u6027\u4e0e\u76f8\u5173\u6027\u63d0\u5347\u5224\u522b\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u53cd\u4e8b\u5b9e-\u5bf9\u6bd4\u63a8\u7406\u65b9\u6cd5\uff0c\u5728MediConfusion\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5f71\u50cf\u51b3\u7b56\u65b9\u6cd5\u591a\u4f9d\u8d56\u8fd1\u90bb\u68c0\u7d22\uff0c\u8fd4\u56de\u5197\u4f59\u8bc1\u636e\u5e76\u5f3a\u5316\u5355\u4e00\u5047\u8bbe\uff0c\u96be\u4ee5\u6709\u6548\u533a\u5206\u89c6\u89c9\u4e0a\u76f8\u4f3c\u4f46\u8bca\u65ad\u4e0d\u540c\u7684\u75c5\u75c7\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1ROCO\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5927\u91cf\u56fe\u6587\u5bf9\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5bf9\u6bd4\u63a8\u7406\u7684\u53c2\u8003\u9009\u62e9\u673a\u5236\uff0c\u5e38\u5bfc\u81f4\u4ece\u540c\u4e00\u6587\u6863\u4e2d\u68c0\u7d22\u51fa\u8fd1\u4f3c\u91cd\u590d\u56fe\u50cf\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5bf9\u6bd4\u5f0f\u3001\u6587\u6863\u611f\u77e5\u7684\u53c2\u8003\u9009\u62e9\u6846\u67b6\uff0c\u5229\u7528ROCO\u5d4c\u5165\u548c\u5143\u6570\u636e\u663e\u5f0f\u5e73\u8861\u89c6\u89c9\u76f8\u5173\u6027\u3001\u5d4c\u5165\u591a\u6837\u6027\u548c\u6765\u6e90\u51fa\u5904\uff0c\u751f\u6210\u7d27\u51d1\u4e14\u5177\u5224\u522b\u529b\u7684\u8bc1\u636e\u96c6\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u201c\u53cd\u4e8b\u5b9e-\u5bf9\u6bd4\u63a8\u7406\u201d\uff08Counterfactual-Contrastive Inference\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6210\u5bf9\u89c6\u89c9\u6bd4\u8f83\u548c\u57fa\u4e8e\u8fb9\u8ddd\u7684\u51b3\u7b56\u89c4\u5219\u8fdb\u884c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u8bc1\u636e\u805a\u5408\uff0c\u5e76\u652f\u6301\u53ef\u9760\u5f03\u6743\u3002", "result": "\u5728MediConfusion\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u8f83\u5148\u524d\u65b9\u6cd5\u5c06\u96c6\u5408\u7ea7\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u5347\u8fd115%\uff0c\u540c\u65f6\u964d\u4f4e\u6df7\u6dc6\u7387\u5e76\u63d0\u9ad8\u4e2a\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6587\u6863\u611f\u77e5\u7684\u5bf9\u6bd4\u53c2\u8003\u9009\u62e9\u673a\u5236\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u5de5\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u4e2d\u5bf9\u6613\u6df7\u6dc6\u75c5\u75c7\u7684\u5224\u522b\u80fd\u529b\uff0c\u4e3a\u53ef\u89e3\u91ca\u3001\u53ef\u9760\u7684\u533b\u5b66\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "summary_cn": "\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u51c6\u786e\u51b3\u7b56\u9700\u8981\u5bf9\u6613\u6df7\u6dc6\u75c5\u75c7\u4e4b\u95f4\u7684\u7ec6\u5fae\u89c6\u89c9\u5dee\u5f02\u8fdb\u884c\u63a8\u7406\uff0c\u7136\u800c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6700\u8fd1\u90bb\u68c0\u7d22\uff0c\u8fd4\u56de\u5197\u4f59\u8bc1\u636e\u5e76\u5f3a\u5316\u5355\u4e00\u5047\u8bbe\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5f0f\u3001\u6587\u6863\u611f\u77e5\u7684\u53c2\u8003\u9009\u62e9\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528ROCO\u5d4c\u5165\u548c\u5143\u6570\u636e\uff0c\u901a\u8fc7\u663e\u5f0f\u5e73\u8861\u89c6\u89c9\u76f8\u5173\u6027\u3001\u5d4c\u5165\u591a\u6837\u6027\u548c\u6765\u6e90\u51fa\u5904\uff0c\u6784\u5efa\u7528\u4e8e\u5224\u522b\u800c\u975e\u76f8\u4f3c\u6027\u4f18\u5316\u7684\u7d27\u51d1\u8bc1\u636e\u96c6\u3002\u5c3d\u7ba1ROCO\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\uff0c\u4f46\u5b83\u5e76\u672a\u6307\u660e\u5982\u4f55\u4e3a\u5bf9\u6bd4\u63a8\u7406\u9009\u62e9\u53c2\u8003\u6837\u672c\uff0c\u800c\u6734\u7d20\u68c0\u7d22\u5e38\u5e38\u4ece\u540c\u4e00\u6587\u6863\u4e2d\u8fd4\u56de\u8fd1\u4e4e\u91cd\u590d\u7684\u56fe\u50cf\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u53c2\u8003\u9009\u62e9\u534f\u8bae\u548c\u7ecf\u8fc7\u6574\u7406\u7684\u53c2\u8003\u5e93\uff0c\u4ee5\u652f\u6301\u5bf9\u533b\u5b66\u56fe\u50cf\u63a8\u7406\u4e2d\u5bf9\u6bd4\u68c0\u7d22\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u57fa\u4e8e\u8fd9\u4e9b\u5bf9\u6bd4\u8bc1\u636e\u96c6\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u201c\u53cd\u4e8b\u5b9e-\u5bf9\u6bd4\u63a8\u7406\u201d\uff08Counterfactual-Contrastive Inference\uff09\u2014\u2014\u4e00\u79cd\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6267\u884c\u7ed3\u6784\u5316\u7684\u6210\u5bf9\u89c6\u89c9\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u8fb9\u8ddd\u7684\u51b3\u7b56\u89c4\u5219\u805a\u5408\u8bc1\u636e\uff0c\u540c\u65f6\u652f\u6301\u53ef\u9760\u7684\u5f03\u6743\u673a\u5236\u3002\u5728MediConfusion\u57fa\u51c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff0c\u76f8\u8f83\u4ee5\u5f80\u65b9\u6cd5\u5c06\u96c6\u5408\u7ea7\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u5347\u4e86\u8fd115%\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6df7\u6dc6\u5e76\u63d0\u9ad8\u4e86\u4e2a\u4f53\u51c6\u786e\u7387\u3002"}}
{"id": "2602.02914", "pdf": "https://arxiv.org/pdf/2602.02914", "abs": "https://arxiv.org/abs/2602.02914", "authors": ["Wenqi Guo", "Shan Du"], "title": "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction", "categories": ["cs.CV"], "comment": null, "summary": "Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.", "AI": {"tldr": "\u73b0\u6709\u57fa\u4e8e\u50cf\u7d20\u91cd\u6784\u6307\u6807\uff08\u5982PSNR\u3001SSIM\uff09\u8bc4\u4f30\u9690\u79c1\u4fdd\u62a4\u4eba\u8138\u8bc6\u522b\uff08PPFR\uff09\u7cfb\u7edf\u7684\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff1b\u4f5c\u8005\u63d0\u51faFaceLinkGen\u653b\u51fb\u65b9\u6cd5\uff0c\u65e0\u9700\u6062\u590d\u539f\u59cb\u50cf\u7d20\u5373\u53ef\u4ece\u53d7\u4fdd\u62a4\u6a21\u677f\u4e2d\u9ad8\u7cbe\u5ea6\u5730\u63d0\u53d6\u8eab\u4efd\u4fe1\u606f\u5e76\u518d\u751f\u4eba\u8138\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u6807\u51c6\u4e0e\u771f\u5b9e\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u7684\u7ed3\u6784\u6027\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dPPFR\u7cfb\u7edf\u7684\u9690\u79c1\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5bf9\u50cf\u7d20\u7ea7\u91cd\u6784\u7684\u62b5\u6297\u80fd\u529b\uff08\u5982PSNR\u548cSSIM\uff09\uff0c\u4f46\u8fd9\u79cd\u4ee5\u91cd\u6784\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89d2\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u7cfb\u7edf\u5728\u8eab\u4efd\u4fe1\u606f\u6cc4\u9732\u65b9\u9762\u7684\u98ce\u9669\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u8bc4\u4f30\u76f2\u70b9\uff0c\u5e76\u8bc1\u660e\u5373\u4f7f\u89c6\u89c9\u4e0a\u6a21\u7cca\u6216\u53d8\u5f62\u7684\u4eba\u8138\u6570\u636e\u4ecd\u53ef\u80fd\u66b4\u9732\u8eab\u4efd\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u540d\u4e3aFaceLinkGen\u7684\u8eab\u4efd\u63d0\u53d6\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u4ece\u53d7\u4fdd\u62a4\u7684\u4eba\u8138\u6a21\u677f\u4e2d\u8fdb\u884c\u8eab\u4efd\u94fe\u63a5/\u5339\u914d\u548c\u4eba\u8138\u518d\u751f\uff0c\u800c\u65e0\u9700\u6062\u590d\u539f\u59cb\u50cf\u7d20\u3002\u5728\u4e09\u79cd\u6700\u65b0\u7684PPFR\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u5728\u63a5\u8fd1\u96f6\u77e5\u8bc6\u7684\u8bbe\u5b9a\u4e0b\u6d4b\u8bd5\u5176\u6709\u6548\u6027\u3002", "result": "FaceLinkGen\u5728\u4e09\u4e2aPPFR\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc798.5%\u7684\u5339\u914d\u51c6\u786e\u7387\u548c96%\u4ee5\u4e0a\u7684\u4eba\u8138\u518d\u751f\u6210\u529f\u7387\uff1b\u5373\u4f7f\u5728\u63a5\u8fd1\u96f6\u77e5\u8bc6\u7684\u8bbe\u5b9a\u4e0b\uff0c\u5339\u914d\u51c6\u786e\u7387\u4ecd\u8d85\u8fc792%\uff0c\u518d\u751f\u6210\u529f\u7387\u8d85\u8fc794%\u3002", "conclusion": "\u5f53\u524d\u5e7f\u6cdb\u4f7f\u7528\u7684\u50cf\u7d20\u5931\u771f\u6307\u6807\uff08\u5982PSNR\u3001SSIM\uff09\u65e0\u6cd5\u6709\u6548\u8861\u91cfPPFR\u7cfb\u7edf\u7684\u771f\u5b9e\u9690\u79c1\u4fdd\u62a4\u6c34\u5e73\uff1b\u89c6\u89c9\u4e0a\u7684\u6a21\u7cca\u5904\u7406\u5e76\u4e0d\u80fd\u9632\u6b62\u8eab\u4efd\u4fe1\u606f\u88ab\u5916\u90e8\u653b\u51fb\u8005\u6216\u4e0d\u53ef\u4fe1\u7684\u670d\u52a1\u63d0\u4f9b\u5546\u63d0\u53d6\uff0c\u8868\u660e\u73b0\u6709\u8bc4\u4f30\u8303\u5f0f\u5b58\u5728\u4e25\u91cd\u4e0d\u8db3\u3002", "summary_cn": "\u57fa\u4e8e\u53d8\u6362\u7684\u9690\u79c1\u4fdd\u62a4\u4eba\u8138\u8bc6\u522b\uff08PPFR\uff09\u65e8\u5728\u9a8c\u8bc1\u8eab\u4efd\u7684\u540c\u65f6\uff0c\u9632\u6b62\u9762\u90e8\u6570\u636e\u88ab\u653b\u51fb\u8005\u548c\u6076\u610f\u670d\u52a1\u63d0\u4f9b\u5546\u83b7\u53d6\u3002\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u5927\u591a\u5c06\u9690\u79c1\u5b9a\u4e49\u4e3a\u5bf9\u50cf\u7d20\u7ea7\u91cd\u6784\u7684\u62b5\u6297\u80fd\u529b\uff0c\u5e76\u901a\u8fc7PSNR\u548cSSIM\u7b49\u6307\u6807\u8fdb\u884c\u8861\u91cf\u3002\u6211\u4eec\u6307\u51fa\uff0c\u8fd9\u79cd\u4ee5\u91cd\u6784\u4e3a\u4e2d\u5fc3\u7684\u89c2\u70b9\u662f\u5931\u8d25\u7684\u3002\u6211\u4eec\u63d0\u51fa\u4e86FaceLinkGen\u2014\u2014\u4e00\u79cd\u8eab\u4efd\u63d0\u53d6\u653b\u51fb\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u76f4\u63a5\u4ece\u53d7\u4fdd\u62a4\u7684\u6a21\u677f\u4e2d\u8fdb\u884c\u8eab\u4efd\u5173\u8054/\u5339\u914d\u548c\u4eba\u8138\u518d\u751f\uff0c\u800c\u65e0\u9700\u6062\u590d\u539f\u59cb\u50cf\u7d20\u3002\u5728\u4e09\u79cd\u6700\u65b0\u7684PPFR\u7cfb\u7edf\u4e0a\uff0cFaceLinkGen\u7684\u5339\u914d\u51c6\u786e\u7387\u8d85\u8fc798.5%\uff0c\u518d\u751f\u6210\u529f\u7387\u8d85\u8fc796%\uff1b\u5373\u4f7f\u5728\u63a5\u8fd1\u96f6\u77e5\u8bc6\u7684\u8bbe\u5b9a\u4e0b\uff0c\u5176\u5339\u914d\u51c6\u786e\u7387\u4ecd\u8d85\u8fc792%\uff0c\u518d\u751f\u6210\u529f\u7387\u8d85\u8fc794%\u3002\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dPPFR\u8bc4\u4f30\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u50cf\u7d20\u5931\u771f\u6307\u6807\u4e0e\u771f\u5b9e\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u5b58\u5728\u7684\u7ed3\u6784\u6027\u5dee\u8ddd\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u89c6\u89c9\u4e0a\u7684\u6a21\u7cca\u5904\u7406\u4ecd\u7136\u4f7f\u8eab\u4efd\u4fe1\u606f\u5e7f\u6cdb\u66b4\u9732\u4e8e\u5916\u90e8\u5165\u4fb5\u8005\u548c\u4e0d\u53ef\u4fe1\u7684\u670d\u52a1\u63d0\u4f9b\u5546\u9762\u524d\u3002"}}
{"id": "2602.02918", "pdf": "https://arxiv.org/pdf/2602.02918", "abs": "https://arxiv.org/abs/2602.02918", "authors": ["Jagan Mohan Reddy Dwarampudi", "Joshua Wong", "Hien Van Nguyen", "Tania Banerjee"], "title": "A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.TO"], "comment": "Accepted to ISBI 2026, 4 pages with 2 figures", "summary": "We introduce Multi-scale Adaptive Recurrent Biomedical Linear-time Encoder (MARBLE), the first \\textit{purely Mamba-based} multi-state multiple instance learning (MIL) framework for whole-slide image (WSI) analysis. MARBLE processes multiple magnification levels in parallel and integrates coarse-to-fine reasoning within a linear-time state-space model, efficiently capturing cross-scale dependencies with minimal parameter overhead. WSI analysis remains challenging due to gigapixel resolutions and hierarchical magnifications, while existing MIL methods typically operate at a single scale and transformer-based approaches suffer from quadratic attention costs. By coupling parallel multi-scale processing with linear-time sequence modeling, MARBLE provides a scalable and modular alternative to attention-based architectures. Experiments on five public datasets show improvements of up to \\textbf{6.9\\%} in AUC, \\textbf{20.3\\%} in accuracy, and \\textbf{2.3\\%} in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.", "AI": {"tldr": "MARBLE is a purely Mamba-based multi-scale MIL framework for whole-slide image analysis that achieves state-of-the-art performance with linear-time complexity.", "motivation": "Whole-slide image (WSI) analysis is challenging due to gigapixel resolution and hierarchical magnifications. Existing MIL methods usually work at a single scale, and transformer-based models suffer from high computational cost due to quadratic attention complexity.", "method": "MARBLE employs a purely Mamba-based architecture to process multiple magnification levels in parallel within a linear-time state-space model, enabling efficient cross-scale dependency modeling with minimal parameter overhead.", "result": "Evaluated on five public datasets, MARBLE improves AUC by up to 6.9%, accuracy by 20.3%, and C-index by 2.3% over existing methods.", "conclusion": "MARBLE offers a scalable, modular, and efficient alternative to attention-based models for multi-scale WSI analysis, demonstrating strong generalization and performance gains.", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u5faa\u73af\u751f\u7269\u533b\u5b66\u7ebf\u6027\u65f6\u95f4\u7f16\u7801\u5668\uff08MARBLE\uff09\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u7eafMamba\u67b6\u6784\u7684\u591a\u72b6\u6001\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u5206\u6790\u3002MARBLE\u5e76\u884c\u5904\u7406\u591a\u4e2a\u653e\u5927\u500d\u7387\uff0c\u5e76\u5728\u7ebf\u6027\u65f6\u95f4\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u878d\u5408\u4ece\u7c97\u5230\u7ec6\u7684\u63a8\u7406\u673a\u5236\uff0c\u4ee5\u6781\u5c11\u7684\u53c2\u6570\u5f00\u9500\u9ad8\u6548\u6355\u6349\u8de8\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\u3002\u7531\u4e8e\u5168\u5207\u7247\u56fe\u50cf\u5177\u6709\u5341\u4ebf\u50cf\u7d20\u7ea7\u5206\u8fa8\u7387\u548c\u5206\u5c42\u653e\u5927\u7ed3\u6784\uff0c\u5176\u5206\u6790\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff1b\u800c\u73b0\u6709MIL\u65b9\u6cd5\u901a\u5e38\u4ec5\u5728\u5355\u4e00\u5c3a\u5ea6\u4e0a\u64cd\u4f5c\uff0c\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u5219\u56e0\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u800c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u901a\u8fc7\u5c06\u5e76\u884c\u591a\u5c3a\u5ea6\u5904\u7406\u4e0e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u76f8\u7ed3\u5408\uff0cMARBLE\u4e3a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6a21\u5757\u5316\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMARBLE\u5728AUC\u4e0a\u6700\u591a\u63d0\u53476.9%\uff0c\u51c6\u786e\u7387\u63d0\u534720.3%\uff0cC-index\u63d0\u53472.3%\uff0c\u8bc1\u660e\u5176\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u591a\u5c3a\u5ea6WSI\u5206\u6790\u6846\u67b6\u3002"}}
