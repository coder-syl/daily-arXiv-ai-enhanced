<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading](https://arxiv.org/abs/2601.20881)
*Matteo Rossi*

Main category: cs.CV

TL;DR: 本文提出了一种名为MA-LipNet的多注意力唇读网络，通过通道、空间和时间三个维度的注意力机制净化视觉特征，显著提升了唇读性能。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法因发音动作细微，常面临特征判别力不足和泛化能力差的问题。

Method: 提出MA-LipNet，依次应用三个注意力模块：通道注意力（CA）模块用于自适应校准通道特征；联合时空注意力（JSTA）模块进行粗粒度的时空过滤；分离时空注意力（SSTA）模块则分别建模时间和空间注意力以实现细粒度优化。

Result: 在CMLR和GRID数据集上的实验表明，MA-LipNet显著降低了字符错误率（CER）和词错误率（WER），优于多种先进方法。

Conclusion: 多维度特征精炼对提升视觉语音识别的鲁棒性至关重要。

Abstract: Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\textit{Joint Spatial-Temporal Attention (JSTA)} and \textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.

Abstract (中文翻译): 唇读技术旨在从无声的唇部运动视频中解码出说话内容，在公共安全等领域具有重要的应用价值。然而，由于发音动作十分细微，现有的唇读方法常常面临特征判别能力有限和泛化能力差的问题。为应对这些挑战，本文深入研究了从时间、空间和通道三个维度对视觉特征进行净化，并提出了一种新颖的方法——多注意力唇读网络（MA-LipNet）。MA-LipNet的核心在于依次应用三个专用的注意力模块：首先，采用通道注意力（CA）模块自适应地重新校准通道级特征，以减轻信息量较少通道的干扰；随后，利用两种不同粒度的时空注意力模块——联合时空注意力（JSTA）和分离时空注意力（SSTA）——来抑制无关像素和视频帧的影响。其中，JSTA模块通过在时空维度上计算统一的权重图进行粗粒度过滤，而SSTA模块则通过分别建模时间与空间注意力实现更细粒度的优化。在CMLR和GRID数据集上进行的大量实验表明，MA-LipNet显著降低了字符错误率（CER）和词错误率（WER），验证了其相较于多种先进方法的有效性和优越性。本研究突显了多维度特征精炼对于实现鲁棒视觉语音识别的重要性。

</details>


### [2] [Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs](https://arxiv.org/abs/2601.20911)
*Haochen Zhang,Animesh Sinha,Felix Juefei-Xu,Haoyu Ma,Kunpeng Li,Zhipeng Fan,Meng Dong,Xiaoliang Dai,Tingbo Hou,Peizhao Zhang,Zecheng He*

Main category: cs.CV

TL;DR: 本文提出了一种针对非马尔可夫多轮对话图像生成的训练与推理框架，通过引入回滚式编辑、基于名称的个性化绑定、历史条件建模和高保真重建技术，显著提升了模型在多轮交互中对长期历史的利用能力与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多轮对话图像生成方法和评测基准采用马尔可夫假设，即仅依赖最近一轮的图像进行生成，忽略了更早的对话历史。然而真实场景中用户常会引用早期状态、撤销修改或提及多轮前引入的实体，因此需要建模非马尔可夫的长期依赖。

Method: 作者提出了（i）非马尔可夫多轮数据构建策略，如回滚式编辑和基于名称的跨轮次个性化；（ii）带token级缓存的历史条件训练与推理框架，防止身份漂移；（iii）高保真图像重建与可编辑个性化技术，包括基于重建的DiT解码器和多阶段微调课程。

Result: 实验表明，显式训练非马尔可夫交互能显著提升多轮一致性与指令遵循能力，同时保持强大的单轮编辑与个性化性能。

Conclusion: 为实现真正连贯的多轮对话图像生成，必须超越马尔可夫假设，建模长期历史依赖。本文所提方法在多个维度上验证了这一方向的有效性。

Abstract: Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.

Abstract (中文翻译): 对话式图像生成要求模型在多轮交互中遵循用户指令，并以交织的文本和图像作为累积的聊天历史进行推理。尽管近期的多模态大语言模型（MLLMs）已具备生成和编辑图像的能力，但大多数现有的多轮基准和训练方法本质上是马尔可夫式的：即下一轮输出主要依赖于最近的图像，从而允许模型通过忽略长期历史的“捷径”方式解决问题。本文正式定义并聚焦于更具挑战性的非马尔可夫设定，在该设定中，用户可能回溯引用早期状态、撤销先前更改，或提及数轮前引入的实体。我们提出了：（i）非马尔可夫多轮数据构建策略，包括强制检索早期视觉状态的回滚式编辑，以及将名称与外观跨轮绑定的基于名称的多轮个性化；（ii）一种历史条件化的训练与推理框架，通过token级缓存防止多轮身份漂移；（iii）支持高保真图像重建与可编辑个性化的改进技术，包括基于重建的DiT解码器和多阶段微调课程。实验表明，显式针对非马尔可夫交互进行训练，能在保持强大单轮编辑与个性化能力的同时，显著提升多轮一致性与指令遵循效果。

</details>


### [3] [Text controllable PET denoising](https://arxiv.org/abs/2601.20990)
*Xuehua Ye,Hongxu Yang,Adam J. Schwarz*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本引导的PET图像去噪方法，利用CLIP和U-Net架构，在单个模型中有效提升不同计数水平下的图像质量。


<details>
  <summary>Details</summary>
Motivation: PET图像常受复杂噪声影响，降低诊断准确性；现有方法难以兼顾多种成像条件（如不同计数水平、采集时间等），因此需要一种更灵活高效的去噪方案。

Method: 结合预训练CLIP模型的文本/图像特征与U-Net结构，构建一个文本引导的去噪网络，用于处理不同计数水平的PET图像。

Result: 所提方法在定性和定量评估中均显著优于现有方法，能有效提升图像质量，并具备缩短采集时间或应对复杂去噪需求的潜力。

Conclusion: 文本引导的去噪框架为PET图像增强提供了一种通用且灵活的新思路，具有临床应用前景。

Abstract: Positron Emission Tomography (PET) imaging is a vital tool in medical diagnostics, offering detailed insights into molecular processes within the human body. However, PET images often suffer from complicated noise, which can obscure critical diagnostic information. The quality of the PET image is impacted by various factors including scanner hardware, image reconstruction, tracer properties, dose/count level, and acquisition time. In this study, we propose a novel text-guided denoising method capable of enhancing PET images across a wide range of count levels within a single model. The model utilized the features from a pretrained CLIP model with a U-Net based denoising model. Experimental results demonstrate that the proposed model leads significant improvements in both qualitative and quantitative assessments. The flexibility of the model shows the potential for helping more complicated denoising demands or reducing the acquisition time.

Abstract (中文翻译): 正电子发射断层扫描（PET）成像是医学诊断中的重要工具，能够提供人体内分子过程的详细信息。然而，PET图像常常受到复杂噪声的干扰，从而掩盖关键的诊断信息。图像质量受到多种因素的影响，包括扫描仪硬件、图像重建算法、示踪剂特性、剂量/计数水平以及采集时间。在本研究中，我们提出了一种新颖的文本引导去噪方法，能够在单一模型中有效提升广泛计数水平下的PET图像质量。该模型结合了预训练CLIP模型的特征与基于U-Net的去噪网络。实验结果表明，所提出的方法在定性和定量评估方面均取得了显著改进。该模型的灵活性显示出其在应对更复杂的去噪需求或缩短采集时间方面的潜力。

</details>


### [4] [Low performing pixel correction in computed tomography with unrolled network and synthetic data training](https://arxiv.org/abs/2601.20995)
*Hongxu Yang,Levente Lippenszky,Edina Timko,Lehel Ferenczi,Gopal Avinash*

Main category: cs.CV

TL;DR: 本文提出一种基于合成数据的双域展开方法，无需真实临床数据即可有效校正CT探测器低性能像素（LPP）引起的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有LPP伪影校正方法依赖昂贵的真实标注数据，且仅在图像域或正弦图域单独处理，忽略了CT几何前向操作中两个域之间的内在关联。

Method: 提出一种基于合成数据的双域展开方法，利用自然图像生成的合成数据建模正弦图域与图像域中LPP的内在关联，从而在无需真实临床数据的情况下训练模型进行伪影校正。

Result: 在模拟1-2%探测器缺陷靠近等中心的实验中，所提方法显著优于当前最先进的方法。

Conclusion: 该方法无需收集真实数据即可有效校正LPP伪影，且适用于不同扫描仪设置，具有良好的软件部署潜力。

Abstract: Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.

Abstract (中文翻译): 计算机断层扫描（CT）探测器中的低性能像素（LPP）会导致重建图像中出现环状和条纹伪影，使其无法用于临床诊断。近年来，已有若干方法被提出用于校正LPP伪影，包括在图像域或正弦图域中采用监督深度学习的方法。然而，这些方法需要专门的数据集进行训练，而此类数据的获取成本高昂。此外，现有方法仅专注于图像空间或正弦图空间的校正，忽略了CT几何前向操作所带来的两个域之间的内在关联。本文提出了一种基于合成数据的双域展开方法来校正LPP伪影。具体而言，通过从自然图像生成的合成数据，利用正弦图域与图像域中LPP的内在关联，使训练后的模型无需任何真实临床数据即可完成伪影校正。在模拟等中心附近1–2%探测器缺陷的实验中，所提方法显著优于当前最先进的方法。结果表明，本方案无需为模型训练收集数据即可有效校正LPP伪影，并可适应不同扫描仪设置，适用于基于软件的应用场景。

</details>


### [5] [AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples](https://arxiv.org/abs/2601.21022)
*Andrea Camilloni,Chiara Micoli,Nita Mulliqi,Erik Everett Palm,Thorgerdur Palsdottir,Kelvin Szolnoky,Xiaoyi Ji,Sol Erika Boman,Andrea Discacciati,Henrik Grönberg,Lars Egevad,Tobias Nordström,Kimmo Kartasalo,Martin Eklund*

Main category: cs.CV

TL;DR: 该研究开发了一种基于AI的模型，利用诊断性前列腺活检切片预测根治性前列腺切除术后生化复发（BCR）风险，在多个外部队列中验证了其泛化能力，并表明AI模型在结合临床变量后可优于传统CAPRA-S评分。


<details>
  <summary>Details</summary>
Motivation: 当前用于预测前列腺癌根治术后生化复发（BCR）的预后工具不够精确，亟需更可靠的方法来识别具有不良预后的高危患者。

Method: 研究基于STHLM3队列（n=676）的诊断性前列腺活检切片，利用基础模型（foundation models）和基于注意力机制的多实例学习（attention-based multiple instance learning）训练AI模型以预测个体患者的BCR风险，并在三个外部根治术队列（LEOPARD、CHIMERA、TCGA-PRAD）中评估其泛化性能；同时将AI预测结果与临床变量整合，并与CAPRA-S评分进行比较。

Result: 该图像驱动的AI模型在三个外部队列中分别达到了0.64、0.70和0.70的5年时间依赖AUC；结合临床变量后显著提升了风险分层能力，并在术后预后判断上优于CAPRA-S评分。

Conclusion: 基于活检切片训练的组织病理AI模型能够跨不同标本类型泛化，支持术前和术后决策，但AI多模态方法相比更简单预测模型的附加价值仍需在后续研究中严格评估。

Abstract: Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.

Abstract (中文翻译): 根治性前列腺切除术（RP）后的生化复发（BCR）是侵袭性前列腺癌伴不良预后的替代标志物，然而目前的预后工具仍不够精确。我们基于STHLM3队列（n = 676）的诊断性前列腺活检切片，利用基础模型和基于注意力机制的多实例学习方法训练了一个AI模型，用于预测患者特异性的BCR风险。该模型在三个外部RP队列（LEOPARD，n = 508；CHIMENA，n = 95；TCGA-PRAD，n = 379）中进行了泛化能力评估。该基于图像的方法在上述三个队列中分别实现了0.64、0.70和0.70的5年时间依赖AUC。将临床变量整合进模型后提供了互补的预后价值，并实现了具有统计学意义的风险分层。与指南推荐的CAPRA-S评分相比，AI模型在术后预后判断方面带来了增量改进。这些结果表明，基于活检训练的组织病理AI模型可在不同标本类型间泛化，有助于术前和术后决策制定，但AI多模态方法相较于更简单预测模型的附加价值仍需在后续研究中加以严格审视。

</details>


### [6] [BadDet+: Robust Backdoor Attacks for Object Detection](https://arxiv.org/abs/2601.21066)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: 本文提出BadDet+，一种基于惩罚机制的后门攻击框架，统一了区域误分类攻击（RMA）和目标消失攻击（ODA），在保持干净样本性能的同时，显著提升了对真实物理场景的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对目标检测的后门攻击方法存在严重缺陷，包括依赖不切实际的假设以及缺乏物理世界验证，导致其实际威胁被高估或低估。因此，亟需一种更贴近现实、具备物理鲁棒性的攻击方法以揭示目标检测模型的真实脆弱性。

Method: 作者提出BadDet+框架，通过引入对数障碍惩罚项（log-barrier penalty）抑制触发样本中真实类别的预测置信度，从而实现统一的RMA与ODA攻击。该方法在特征空间中作用于特定于触发器的子空间，确保攻击有效且不影响正常推理。

Result: 在真实世界基准测试中，BadDet+在合成到物理的迁移效果上优于现有RMA和ODA基线方法，同时保持了良好的干净样本性能。理论分析也验证了其攻击机制的有效性和针对性。

Conclusion: 目标检测模型面临比以往认知更严重的后门攻击风险，现有防御策略可能不足以应对此类攻击，亟需开发专门针对检测任务的防御机制。

Abstract: Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.

Abstract (中文翻译): 后门攻击对深度学习构成严重威胁，但其在目标检测中的影响相较于图像分类仍缺乏深入理解。尽管已有相关攻击方法被提出，但我们发现现有基于检测的攻击方法存在关键缺陷，尤其是依赖不切实际的假设且缺乏物理验证。为弥补这一差距，我们提出了BadDet+——一种基于惩罚机制的统一框架，整合了区域误分类攻击（RMA）和目标消失攻击（ODA）。其核心机制利用对数障碍惩罚项来抑制触发输入中真实类别的预测，从而实现（i）位置与尺度不变性，以及（ii）更强的物理鲁棒性。在真实世界基准测试中，BadDet+在从合成到物理的迁移效果上优于现有的RMA和ODA基线方法，同时保持了良好的干净样本性能。理论分析证实，所提出的惩罚项作用于特定于触发器的特征子空间，能够在不损害标准推理的前提下可靠地引发攻击。这些结果凸显了目标检测系统中存在的重大安全漏洞，并强调了开发专用防御措施的必要性。

</details>


### [7] [Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization](https://arxiv.org/abs/2601.21078)
*Jiaqi Li,Guangming Wang,Shuntian Zheng,Minzhe Ni,Xiaoman Lu,Guanghui Ye,Yu Guan*

Main category: cs.CV

TL;DR: ActionVLM通过动态重加权和残差聚合策略，在时序动作定位中缓解视觉-语言模型的模态偏置，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在时序动作定位任务中过度依赖语言先验，导致模态偏置，削弱了视觉信息的作用，影响定位性能。

Method: 提出ActionVLM框架，包含：(i) 去偏重加权模块，动态评估语言相对于纯视觉预测的增益并调整其权重；(ii) 残差聚合策略，将语言作为视觉主导信号的补充而非主驱动力。

Result: 在THUMOS14数据集上，模型相较当前最优方法提升最高达3.2% mAP。

Conclusion: 通过保留视觉主导地位并自适应引入语言信息，ActionVLM有效缓解模态偏置，提升时序动作定位的准确性和鲁棒性。

Abstract: Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.

Abstract (中文翻译): 时序动作定位（TAL）需要在未剪辑视频中识别动作的边界和类别。尽管视觉-语言模型（VLMs）能够提供丰富的语义信息以补充视觉证据，但现有方法往往过度强调语言先验，牺牲了视觉性能，从而导致显著的模态偏置。我们提出了ActionVLM，一种视觉-语言融合框架，系统性地缓解TAL中的模态偏置。我们的核心思想是在保持视觉为主导信号的同时，仅在有益时自适应地利用语言信息。为此，我们引入了（i）一个去偏重加权模块，用于估计语言优势——即语言相对于纯视觉预测所带来的增量收益，并据此动态调整语言模态的权重；以及（ii）一种残差聚合策略，将语言视为辅助性的细化手段，而非主要驱动因素。这种组合有效缓解了模态偏置，降低了语言先验带来的过度自信，并增强了时序推理能力。在THUMOS14上的实验表明，我们的模型相较于当前最先进的方法，mAP最高提升了3.2%。

</details>


### [8] [Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought](https://arxiv.org/abs/2601.21081)
*Yu Huo,Siyu Zhang,Kun Zeng,Haoyue Liu,Owen Lee,Junlin Chen,Yuquan Lu,Yifu Guo,Yaodong Liang,Xiaoying Tang*

Main category: cs.CV

TL;DR: 本文提出Shape-of-Thought（SoT）框架，通过在推理时无需外部引擎的连贯2D投影实现渐进式形状组装，提升文本到图像生成模型在组合结构约束（如数量、属性绑定和部件关系）下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态文生图模型虽具备高视觉保真度，但在处理组合结构性约束（如生成数量准确性、属性绑定和部件级关系）方面仍显脆弱，亟需更鲁棒且可解释的方法。

Method: 提出SoT视觉思维链框架，训练统一的多模态自回归模型，交替生成文本计划与渲染的中间状态，以隐式学习形状组装逻辑；同时构建SoT-26K数据集（基于CAD部件层次的装配轨迹）和T2S-CompBench评测基准。

Result: 在SoT-26K上微调后，组件数量准确率达88.4%，结构拓扑准确率达84.8%，比纯文本基线高出约20%。

Conclusion: SoT为透明、过程监督的组合式生成建立了新范式，显著提升模型在结构完整性与生成忠实度方面的性能。

Abstract: Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.

Abstract (中文翻译): 用于文本到图像生成的多模态模型虽然实现了较强的视觉保真度，但在组合结构性约束下仍显脆弱，尤其是在生成数量准确性、属性绑定和部件级关系方面。为应对这些挑战，我们提出了“思维形状”（Shape-of-Thought, SoT）框架——一种视觉思维链方法，可在推理时无需外部引擎，通过连贯的二维投影实现渐进式形状组装。SoT训练了一个统一的多模态自回归模型，交替生成文本计划与渲染的中间状态，从而在不显式生成几何表示的情况下捕捉形状组装逻辑。为支持该范式，我们构建了SoT-26K数据集（源自基于部件的CAD层次结构的大规模接地装配轨迹）以及T2S-CompBench评测基准（用于评估结构完整性与轨迹忠实度）。在SoT-26K上微调后，模型在组件数量任务上达到88.4%的准确率，在结构拓扑任务上达到84.8%，比纯文本基线高出约20%。SoT为透明、过程监督的组合式生成建立了一种新范式。代码已开源（https://anonymous.4open.science/r/16FE/），SoT-26K数据集将在论文被接收后发布。

</details>


### [9] [An AI Framework for Microanastomosis Motion Assessment](https://arxiv.org/abs/2601.21120)
*Yan Meng,Eduardo J. Torres-Rodríguez,Marcelle Altshuler,Nishanth Gowda,Arhum Naeem,Recai Yilmaz,Omar Arnaout,Daniel A. Donoho*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化框架，用于客观评估显微吻合术中的器械操作技能，整合了YOLO检测、DeepSORT跟踪、器械尖端定位和专家标注的分类模块，在实验中实现了97%的检测精度和96%的mAP。


<details>
  <summary>Details</summary>
Motivation: 传统显微外科技术评估依赖专家主观判断，存在评分者间差异大、标准不统一、易受认知偏差影响及耗时等问题，亟需一种客观、可靠且可扩展的自动化评估系统。

Method: 提出一个包含四个模块的AI框架：(1) 基于YOLO的器械检测模块；(2) 基于DeepSORT的器械跟踪模块；(3) 利用形状描述符的器械尖端定位模块；(4) 基于专家标注数据训练的监督分类模块，用于评估器械操作熟练度。

Result: 实验结果显示该框架在器械检测上达到97%的精确率，mAP50-95为96%，验证了其有效性。

Conclusion: 所提出的AI框架能有效实现显微吻合术器械操作技能的自动化、客观化评估，具备高精度与可扩展性，有望改善当前依赖主观评价的局限。

Abstract: Proficiency in microanastomosis is a fundamental competency across multiple microsurgical disciplines. These procedures demand exceptional precision and refined technical skills, making effective, standardized assessment methods essential. Traditionally, the evaluation of microsurgical techniques has relied heavily on the subjective judgment of expert raters. They are inherently constrained by limitations such as inter-rater variability, lack of standardized evaluation criteria, susceptibility to cognitive bias, and the time-intensive nature of manual review. These shortcomings underscore the urgent need for an objective, reliable, and automated system capable of assessing microsurgical performance with consistency and scalability. To bridge this gap, we propose a novel AI framework for the automated assessment of microanastomosis instrument handling skills. The system integrates four core components: (1) an instrument detection module based on the You Only Look Once (YOLO) architecture; (2) an instrument tracking module developed from Deep Simple Online and Realtime Tracking (DeepSORT); (3) an instrument tip localization module employing shape descriptors; and (4) a supervised classification module trained on expert-labeled data to evaluate instrument handling proficiency. Experimental results demonstrate the effectiveness of the framework, achieving an instrument detection precision of 97%, with a mean Average Precision (mAP) of 96%, measured by Intersection over Union (IoU) thresholds ranging from 50% to 95% (mAP50-95).

Abstract (中文翻译): 显微吻合术的熟练程度是多个显微外科领域的一项基本能力。此类手术要求极高的精准度和精湛的技术，因此有效的标准化评估方法至关重要。传统上，显微外科技术的评估主要依赖专家评分者的主观判断，但这种方法存在评分者间差异、缺乏统一标准、易受认知偏差影响以及人工评审耗时等固有局限。这些不足凸显了对一种客观、可靠且可自动化的系统进行显微外科操作评估的迫切需求。为填补这一空白，我们提出了一种新颖的人工智能框架，用于自动评估显微吻合术中的器械操作技能。该系统整合了四个核心组件：(1) 基于You Only Look Once（YOLO）架构的器械检测模块；(2) 基于Deep Simple Online and Realtime Tracking（DeepSORT）开发的器械跟踪模块；(3) 采用形状描述符的器械尖端定位模块；以及(4) 基于专家标注数据训练的监督分类模块，用于评估器械操作熟练度。实验结果表明，该框架具有良好的效果，器械检测精确率达到97%，在IoU阈值从50%到95%范围内测得的平均精度（mAP50-95）为96%。

</details>


### [10] [Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2601.21159)
*Jianzheng Wang,Huan Ni*

Main category: cs.CV

TL;DR: 提出了一种无需训练的开放词汇语义分割框架SDCI，通过双分支协同推理、交叉注意力融合和超像素结构优化，显著提升了高分辨率遥感影像的分割精度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像具有密集的地物分布和复杂边界，对几何定位和语义预测提出了更高要求。现有无需训练的开放词汇语义分割方法通常采用单向注入和浅层后处理策略，难以满足这些需求。

Method: 提出SDCI框架：1）在特征编码阶段引入交叉模型注意力融合（CAF）模块，通过互相注入自注意力图实现协同推理；2）设计双向交叉图扩散精炼（BCDR）模块，通过迭代随机游走扩散增强双分支分割得分的可靠性；3）结合低层超像素结构，开发基于凸优化的超像素协同预测（CSCP）机制以进一步优化对象边界。

Result: 在多个遥感语义分割基准上的实验表明，该方法优于现有方法。消融研究进一步证实，在深度学习框架中，利用超像素结构的传统面向对象遥感图像分析方法仍然有效。

Conclusion: 所提出的SDCI框架通过多阶段协同优化策略，有效提升了高分辨率遥感图像开放词汇语义分割的性能，验证了传统超像素方法与现代深度学习结合的潜力。

Abstract: High-resolution remote sensing imagery is characterized by densely distributed land-cover objects and complex boundaries, which places higher demands on both geometric localization and semantic prediction. Existing training-free open-vocabulary semantic segmentation (OVSS) methods typically fuse CLIP and vision foundation models (VFMs) using "one-way injection" and "shallow post-processing" strategies, making it difficult to satisfy these requirements. To address this issue, we propose a spatial-regularization-aware dual-branch collaborative inference framework for training-free OVSS, termed SDCI. First, during feature encoding, SDCI introduces a cross-model attention fusion (CAF) module, which guides collaborative inference by injecting self-attention maps into each other. Second, we propose a bidirectional cross-graph diffusion refinement (BCDR) module that enhances the reliability of dual-branch segmentation scores through iterative random-walk diffusion. Finally, we incorporate low-level superpixel structures and develop a convex-optimization-based superpixel collaborative prediction (CSCP) mechanism to further refine object boundaries. Experiments on multiple remote sensing semantic segmentation benchmarks demonstrate that our method achieves better performance than existing approaches. Moreover, ablation studies further confirm that traditional object-based remote sensing image analysis methods leveraging superpixel structures remain effective within deep learning frameworks. Code: https://github.com/yu-ni1989/SDCI.

Abstract (中文翻译): 高分辨率遥感影像具有密集分布的地物对象和复杂的边界，这对几何定位和语义预测提出了更高的要求。现有的无需训练的开放词汇语义分割（OVSS）方法通常采用“单向注入”和“浅层后处理”策略融合CLIP和视觉基础模型（VFMs），难以满足上述需求。为解决这一问题，我们提出了一种面向无需训练OVSS的空间正则化感知双分支协同推理框架（SDCI）。首先，在特征编码阶段，SDCI引入交叉模型注意力融合（CAF）模块，通过相互注入自注意力图来引导协同推理。其次，我们提出了双向交叉图扩散精炼（BCDR）模块，通过迭代随机游走扩散增强双分支分割得分的可靠性。最后，我们结合低层超像素结构，开发了一种基于凸优化的超像素协同预测（CSCP）机制，以进一步优化对象边界。在多个遥感语义分割基准上的实验表明，我们的方法优于现有方法。此外，消融研究进一步证实，在深度学习框架中，利用超像素结构的传统面向对象遥感图像分析方法仍然有效。代码：https://github.com/yu-ni1989/SDCI。

</details>
