<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GR3EN: Generative Relighting for 3D Environments](https://arxiv.org/abs/2601.16272)
*Xiaoyan Xing,Philipp Henzler,Junhwa Hur,Runze Li,Jonathan T. Barron,Pratul P. Srinivasan,Dor Verbin*

Main category: cs.CV

TL;DR: 本文提出了一种对大型房间尺度3D场景进行可控重光照的新方法，通过将视频到视频的扩散模型蒸馏到3D重建中，避免了传统逆渲染问题，实现了高质量的真实感重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重光照方法通常需解决欠定或病态的逆渲染问题，难以在复杂真实场景中生成高质量结果；而基于扩散模型的方法多限于2D图像/视频或单个物体的3D重光照，缺乏对房间尺度场景的支持。

Method: 将视频到视频的重光照扩散模型的输出蒸馏到3D重建中，从而绕过复杂的逆渲染过程，实现对房间尺度3D场景的可控重光照。

Result: 在合成和真实数据集上的实验表明，该方法能忠实渲染新光照条件下场景的新视角。

Conclusion: 所提方法有效解决了大规模3D场景重光照的挑战，兼具灵活性与高质量，为复杂真实世界场景的光照编辑提供了实用方案。

Abstract: We present a method for relighting 3D reconstructions of large room-scale environments. Existing solutions for 3D scene relighting often require solving under-determined or ill-conditioned inverse rendering problems, and are as such unable to produce high-quality results on complex real-world scenes. Though recent progress in using generative image and video diffusion models for relighting has been promising, these techniques are either limited to 2D image and video relighting or 3D relighting of individual objects. Our approach enables controllable 3D relighting of room-scale scenes by distilling the outputs of a video-to-video relighting diffusion model into a 3D reconstruction. This side-steps the need to solve a difficult inverse rendering problem, and results in a flexible system that can relight 3D reconstructions of complex real-world scenes. We validate our approach on both synthetic and real-world datasets to show that it can faithfully render novel views of scenes under new lighting conditions.

Abstract (中文翻译): 我们提出了一种对大型房间尺度环境的3D重建进行重光照的方法。现有的3D场景重光照解决方案通常需要求解欠定或病态的逆渲染问题，因此难以在复杂的真实世界场景中生成高质量的结果。尽管最近利用生成式图像和视频扩散模型进行重光照的研究取得了可喜进展，但这些技术要么局限于2D图像和视频的重光照，要么仅适用于单个物体的3D重光照。我们的方法通过将视频到视频重光照扩散模型的输出蒸馏到3D重建中，实现了对房间尺度场景的可控3D重光照。该方法规避了求解困难的逆渲染问题，构建出一个灵活的系统，能够对复杂真实场景的3D重建进行重光照。我们在合成和真实世界数据集上验证了该方法，结果表明其能够在新光照条件下逼真地渲染场景的新视角。

</details>


### [2] [Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory](https://arxiv.org/abs/2601.16296)
*Dohun Lee,Chun-Hao Paul Huang,Xuelin Chen,Jong Chul Ye,Duygu Ceylan,Hyeonho Jeong*

Main category: cs.CV

TL;DR: 本文提出Memory-V2V，首次解决多轮视频编辑中的跨一致性问题，通过引入显式记忆机制和可学习的token压缩器，在保持或提升任务性能的同时显著提高一致性并降低30%计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视频到视频扩散模型在单次编辑中表现良好，但在多轮交互式编辑中难以维持跨编辑的一致性。真实世界视频编辑通常是迭代过程，用户需多次调整，因此亟需一种能保持历史编辑信息一致性的方法。

Method: Memory-V2V框架为现有视频编辑模型添加显式记忆模块：利用外部缓存存储先前编辑结果，通过精准检索与动态token化策略将历史信息作为当前编辑的条件；同时在DiT骨干网络中引入可学习的token压缩器，减少冗余条件token以提升效率。

Result: 在视频新视角合成和文本引导长视频编辑等任务上验证表明，Memory-V2V显著提升了跨编辑一致性，计算开销增加极小，并在任务性能上达到或优于当前最先进方法，整体提速30%。

Conclusion: Memory-V2V有效解决了多轮视频编辑中的跨一致性难题，通过结合显式记忆与高效token压缩，在不牺牲性能的前提下实现了更稳定、高效的交互式视频编辑。

Abstract: Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V

Abstract (中文翻译): 近期的基础视频到视频扩散模型在通过修改外观、运动或摄像机运动来编辑用户提供的视频方面取得了令人印象深刻的结果。然而，现实世界的视频编辑通常是一个迭代过程，用户需要在多轮交互中不断优化结果。在这种多轮设定下，当前的视频编辑器难以在连续编辑之间保持跨一致性。在本研究中，我们首次探讨了多轮视频编辑中的跨一致性问题，并提出了Memory-V2V——一个简单而有效的框架，通过显式记忆机制增强现有视频到视频模型。该框架利用一个外部缓存存储先前编辑过的视频，并采用精确检索和动态分词策略，将当前编辑步骤与先前结果相关联。为进一步减少冗余和计算开销，我们在DiT骨干网络中引入了一个可学习的token压缩器，在保留关键视觉线索的同时压缩冗余的条件token，从而实现整体30%的速度提升。我们在视频新视角合成和文本条件下的长视频编辑等具有挑战性的任务上验证了Memory-V2V的有效性。大量实验表明，Memory-V2V生成的视频在跨一致性方面显著优于现有方法，同时计算开销极低，并在特定任务性能上达到甚至超越当前最先进的基线模型。

</details>


### [3] [FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging](https://arxiv.org/abs/2601.16302)
*Abhijeet Parida,Antonia Alomar,Zhifan Jiang,Pooneh Roshanitabrizi,Austin Tapp,Ziyue Xu,Syed Muhammad Anwar,Maria J. Ledesma-Carbayo,Holger R. Roth,Marius George Linguraru*

Main category: cs.CV

TL;DR: FeTTL is a novel federated learning framework that jointly learns a global template and task model to align data distributions across institutions, significantly improving performance on medical imaging tasks under domain shift.


<details>
  <summary>Details</summary>
Motivation: Federated learning in medical imaging suffers from performance degradation due to domain shifts caused by differences in acquisition protocols, scanner types, and patient populations across institutions.

Method: The proposed FeTTL framework harmonizes multi-institutional data by jointly learning a global template and a task-specific model in a federated setting to align client data distributions.

Result: FeTTL significantly outperforms state-of-the-art federated baselines (p < 0.002) on two tasks: retinal fundus optical disc segmentation and histopathological metastasis classification across multiple institutions.

Conclusion: FeTTL provides a principled and extensible approach to mitigate distribution shifts in federated medical imaging, enabling more robust and generalizable model deployment in real-world multi-institutional settings.

Abstract: Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values <0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.

Abstract (中文翻译): 联邦学习能够在保护数据隐私的同时，实现地理分布的医疗中心之间的协作模型训练。然而，数据中的域偏移和异质性常常导致模型性能下降。医学影像应用尤其受到采集协议、扫描仪类型和患者群体差异的影响。为解决这些问题，我们提出了联邦模板与任务学习（FeTTL），这是一种新颖的框架，旨在联邦环境中协调多机构医学影像数据。FeTTL通过联合学习一个全局模板和一个任务模型，以对齐各参与方的数据分布。我们在两个具有挑战性且多样化的多机构医学影像任务上评估了FeTTL：视网膜眼底视盘分割和组织病理学转移灶分类。实验结果表明，FeTTL在视盘分割和转移灶分类任务上显著优于当前最先进的联邦学习基线方法（p值<0.002）。我们的实验进一步强调了联合学习模板与任务的重要性。这些发现表明，FeTTL为缓解联邦学习中的分布偏移问题提供了一种有原则且可扩展的解决方案，有助于在真实世界的多机构环境中实现鲁棒的模型部署。

</details>


### [4] [Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments](https://arxiv.org/abs/2601.16333)
*Aditya K Surikuchi,Raquel Fernández,Sandro Pezzelle*

Main category: cs.CV

TL;DR: 本文研究了基础模型在识别足球视频中重要子事件方面的能力，发现当前多模态模型表现接近随机水平，主要因其过度依赖单一模态且难以有效融合多源信息。


<details>
  <summary>Details</summary>
Motivation: 现实应用中常需从时序多模态事件生成语言描述，而识别关键子事件是实现有效叙述或摘要的前提。然而现有模型在此任务上的能力尚不明确，尤其在复杂场景如足球比赛中。

Method: 作者利用足球比赛集锦中隐含的人类对“重要性”的偏好，构建了一个无需额外标注的新数据集，并基于该数据集评估多个前沿多模态模型区分重要与非重要子事件的能力。

Result: 实验表明，当前最先进的多模态模型在此任务上的表现仅略优于随机猜测；进一步分析显示这些模型倾向于依赖单一主导模态，难以有效整合多模态信息。

Conclusion: 研究强调了设计能处理多模态数据样本级异质性的模块化架构的重要性，以及开发能最大化跨模态协同效应的训练方法的必要性。

Abstract: Foundation models are used for many real-world applications involving language generation from temporally-ordered multimodal events. In this work, we study the ability of models to identify the most important sub-events in a video, which is a fundamental prerequisite for narrating or summarizing multimodal events. Specifically, we focus on football games and evaluate models on their ability to distinguish between important and non-important sub-events in a game. To this end, we construct a new dataset by leveraging human preferences for importance implicit in football game highlight reels, without any additional annotation costs. Using our dataset, which we will publicly release to the community, we compare several state-of-the-art multimodal models and show that they are not far from chance level performance. Analyses of models beyond standard evaluation metrics reveal their tendency to rely on a single dominant modality and their ineffectiveness in synthesizing necessary information from multiple sources. Our findings underline the importance of modular architectures that can handle sample-level heterogeneity in multimodal data and the need for complementary training procedures that can maximize cross-modal synergy.

Abstract (中文翻译): 基础模型被广泛应用于许多涉及从时序多模态事件生成语言的现实场景。本文研究了模型识别视频中最重要子事件的能力，这是对多模态事件进行叙述或摘要的基本前提。具体而言，我们聚焦于足球比赛，评估模型区分比赛中重要与非重要子事件的能力。为此，我们通过利用足球比赛集锦中隐含的人类对“重要性”的偏好，构建了一个无需额外标注成本的新数据集。我们将公开发布该数据集，并用其对多个当前最先进的多模态模型进行比较，结果表明它们的表现仅略高于随机水平。超越标准评估指标的深入分析揭示了这些模型倾向于依赖单一主导模态，且无法有效融合来自多个来源的必要信息。我们的发现强调了采用能够处理多模态数据样本级异质性的模块化架构的重要性，以及开发能够最大化跨模态协同效应的配套训练方法的必要性。

</details>


### [5] [Coarse-to-Fine Non-rigid Multi-modal Image Registration for Historical Panel Paintings based on Crack Structures](https://arxiv.org/abs/2601.16348)
*Aline Sindel,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络和图神经网络的粗到精非刚性多模态图像配准方法，利用历史画作表面的龟裂纹理（craquelure）作为关键特征，有效实现了高精度、高效率的多模态图像对齐。


<details>
  <summary>Details</summary>
Motivation: 历史木板绘画的多模态成像分析（如可见光、红外、紫外、X射线等）需要像素级对齐，但传统手动对齐费时费力且精度有限。现有自动配准方法在面对图像分辨率差异大、尺寸巨大、非刚性形变及模态内容差异等问题时仍具挑战，因此亟需一种高效、鲁棒的自动配准方法。

Method: 作者提出一种单阶段、粗到精的非刚性多模态配准方法：1）利用卷积神经网络联合检测和描述基于龟裂纹理的关键点；2）采用图神经网络进行基于图像块的描述符匹配，并通过局部单应性重投影误差筛选匹配点；3）设计一种新颖的多层级关键点优化策略，实现混合分辨率图像向最高分辨率对齐。

Result: 作者构建了一个包含大量关键点标注的多模态木板绘画数据集和涵盖五种模态、多种分辨率的大规模测试集。消融实验验证了各模块的有效性，所提方法在关键点匹配和密集匹配任务中均优于现有方法，取得了最佳配准效果。

Conclusion: 该方法充分利用历史画作共有的龟裂纹理特征，结合深度学习与几何优化，成功解决了多模态、多分辨率、非刚性历史绘画图像的高精度配准难题，为艺术科技分析提供了高效自动化工具。

Abstract: Art technological investigations of historical panel paintings rely on acquiring multi-modal image data, including visual light photography, infrared reflectography, ultraviolet fluorescence photography, x-radiography, and macro photography. For a comprehensive analysis, the multi-modal images require pixel-wise alignment, which is still often performed manually. Multi-modal image registration can reduce this laborious manual work, is substantially faster, and enables higher precision. Due to varying image resolutions, huge image sizes, non-rigid distortions, and modality-dependent image content, registration is challenging. Therefore, we propose a coarse-to-fine non-rigid multi-modal registration method efficiently relying on sparse keypoints and thin-plate-splines. Historical paintings exhibit a fine crack pattern, called craquelure, on the paint layer, which is captured by all image systems and is well-suited as a feature for registration. In our one-stage non-rigid registration approach, we employ a convolutional neural network for joint keypoint detection and description based on the craquelure and a graph neural network for descriptor matching in a patch-based manner, and filter matches based on homography reprojection errors in local areas. For coarse-to-fine registration, we introduce a novel multi-level keypoint refinement approach to register mixed-resolution images up to the highest resolution. We created a multi-modal dataset of panel paintings with a high number of keypoint annotations, and a large test set comprising five multi-modal domains and varying image resolutions. The ablation study demonstrates the effectiveness of all modules of our refinement method. Our proposed approaches achieve the best registration results compared to competing keypoint and dense matching methods and refinement methods.

Abstract (中文翻译): 对历史木板绘画进行艺术科技研究依赖于获取多模态图像数据，包括可见光摄影、红外反射成像、紫外荧光摄影、X射线成像和宏观摄影。为了进行全面分析，这些多模态图像需要进行像素级对齐，而目前这一过程通常仍需手动完成。多模态图像配准能够减少这种繁重的手动工作，显著加快处理速度，并实现更高的精度。然而，由于图像分辨率各异、尺寸巨大、存在非刚性形变以及模态间图像内容差异，配准任务极具挑战性。因此，我们提出了一种粗到精的非刚性多模态配准方法，该方法高效地依赖于稀疏关键点和薄板样条（thin-plate-splines）。历史画作的颜料层上普遍存在一种称为“龟裂”（craquelure）的精细裂纹图案，该图案可被所有成像系统捕捉到，非常适合作为配准特征。在我们提出的单阶段非刚性配准方法中，我们采用卷积神经网络基于龟裂纹理联合进行关键点检测与描述，并采用图神经网络以图像块方式匹配描述符，同时基于局部区域的单应性重投影误差对匹配点进行筛选。为了实现粗到精的配准，我们引入了一种新颖的多层级关键点优化方法，可将混合分辨率的图像配准至最高分辨率。我们构建了一个包含大量关键点标注的木板绘画多模态数据集，以及一个涵盖五种多模态域和不同图像分辨率的大规模测试集。消融研究证明了我们优化方法中所有模块的有效性。与现有的关键点匹配、密集匹配及优化方法相比，我们提出的方法取得了最佳的配准结果。

</details>


### [6] [Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models](https://arxiv.org/abs/2601.16378)
*Bridget Leonard,Scott O. Murray*

Main category: cs.CV

TL;DR: 本文通过引入“视角标记”（perspective tokens）增强多模态语言模型的空间推理能力，使其能更好地进行他者视角的视觉理解，显著提升在多种基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在语义视觉-语言任务上表现良好，但在需要采用他人视觉视角的空间推理任务中表现不佳，存在自我中心偏见，缺乏有效的异我中心（allocentric）推理能力。

Method: 受人类空间认知启发，提出两种视角标记：(1) 基于身体关键点的具身线索；(2) 支持心理旋转的抽象表示。将这些标记集成到LLaVA-1.5-13B模型中，并在多个基准上进行评估。

Result: 在Isle Bricks V2、COCO和3DSRBench等合成与自然基准上，视角标记显著提升模型准确率，其中基于旋转的标记还能泛化至非人类参考主体。

Conclusion: 多模态语言模型已具备异我中心推理的潜在能力，但缺乏合适的内部结构；将认知合理的空间结构直接嵌入token空间，是一种轻量且模型无关的视角采择机制。

Abstract: Multimodal language models (MLMs) perform well on semantic vision-language tasks but fail at spatial reasoning that requires adopting another agent's visual perspective. These errors reflect a persistent egocentric bias and raise questions about whether current models support allocentric reasoning. Inspired by human spatial cognition, we introduce perspective tokens, specialized embeddings that encode orientation through either (1) embodied body-keypoint cues or (2) abstract representations supporting mental rotation. Integrating these tokens into LLaVA-1.5-13B yields performance on level-2 visual perspective-taking tasks. Across synthetic and naturalistic benchmarks (Isle Bricks V2, COCO, 3DSRBench), perspective tokens improve accuracy, with rotation-based tokens generalizing to non-human reference agents. Representational analyses reveal that fine-tuning enhances latent orientation sensitivity already present in the base model, suggesting that MLMs contain precursors of allocentric reasoning but lack appropriate internal structure. Overall, embedding cognitively grounded spatial structure directly into token space provides a lightweight, model-agnostic mechanism for perspective-taking and more human-like spatial reasoning.

Abstract (中文翻译): 多模态语言模型（MLMs）在语义视觉-语言任务上表现优异，但在需要采用其他智能体视觉视角的空间推理任务中表现不佳。这类错误反映出模型存在持续的自我中心偏见，并引发对当前模型是否支持异我中心（allocentric）推理的质疑。受人类空间认知机制启发，本文提出了“视角标记”（perspective tokens）——一种专门的嵌入表示，可通过（1）具身化的身体关键点线索或（2）支持心理旋转的抽象表示来编码方向信息。将这些标记集成到LLaVA-1.5-13B模型中后，其在二级视觉视角采择任务上的表现显著提升。在合成与自然场景的多个基准（Isle Bricks V2、COCO、3DSRBench）上，视角标记均提高了模型准确率，其中基于旋转的标记还能泛化至非人类参考主体。表征分析表明，微调增强了基础模型中已存在的潜在方向敏感性，说明多模态语言模型已包含异我中心推理的雏形，但缺乏适当的内部结构。总体而言，将认知上合理（cognitively grounded）的空间结构直接嵌入到token空间，提供了一种轻量级、模型无关的视角采择机制，可实现更类人的空间推理能力。

</details>


### [7] [VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection](https://arxiv.org/abs/2601.16381)
*Yuxin Jiang,Yunkang Cao,Yuqi Cheng,Yiheng Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: 本文提出VTFusion，一种面向少样本异常检测（FSAD）的视觉-文本多模态融合框架，通过自适应特征提取器和专用融合模块，显著提升工业场景下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有FSAD方法主要依赖在自然场景上预训练的特征，忽略了工业检测所需的细粒度、领域特定语义；同时，主流的多模态融合策略过于简单，无法解决视觉与文本模态间的语义错位问题，导致模型鲁棒性不足。

Method: VTFusion框架包含两个核心设计：1）为图像和文本模态引入自适应特征提取器，学习任务特定表征以弥合预训练模型与工业数据间的领域差距，并通过生成多样化的合成异常增强特征判别力；2）设计专用的多模态预测融合模块，包括促进跨模态信息交互的融合块和在多模态指导下生成精细像素级异常图的分割网络。

Result: 在2-shot设置下，VTFusion在MVTec AD和VisA数据集上分别达到96.8%和86.2%的图像级AUROC；在论文引入的真实工业汽车塑料部件数据集上，取得了93.5%的AUPRO，验证了其在实际工业场景中的有效性。

Conclusion: VTFusion通过针对性的自适应特征学习和深度多模态融合，有效解决了FSAD中领域差异和模态错位问题，在多个基准和真实工业数据集上均取得了卓越性能，展现了强大的实用价值。

Abstract: Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.

Abstract (中文翻译): 少样本异常检测（FSAD）已成为一种关键范式，旨在利用稀缺的正常样本参考来识别异常。尽管近期方法已将文本语义融入以补充视觉数据，但它们主要依赖于在自然场景上预训练的特征，从而忽略了工业检测所必需的细粒度、领域特定语义。此外，当前主流的融合策略往往采用简单的拼接方式，未能解决视觉与文本模态之间固有的语义错位问题，这会削弱模型对跨模态干扰的鲁棒性。为弥合这些差距，本研究提出了VTFusion，一个专为FSAD设计的视觉-文本多模态融合框架。该框架基于两项核心设计：首先，引入了针对图像和文本模态的自适应特征提取器，以学习任务特定的表征，弥合预训练模型与工业数据之间的领域鸿沟；并通过生成多样化的合成异常来增强特征的判别能力。其次，开发了一个专用的多模态预测融合模块，该模块包含一个促进丰富跨模态信息交换的融合块，以及一个在多模态引导下生成精细化像素级异常图的分割网络。VTFusion显著推进了FSAD的性能，在MVTec AD和VisA数据集的2-shot场景下，图像级AUROC分别达到了96.8%和86.2%。此外，VTFusion在本文引入的一个真实工业汽车塑料部件数据集上实现了93.5%的AUPRO，进一步证明了其在严苛工业场景中的实际应用价值。

</details>


### [8] [ResAgent: Entropy-based Prior Point Discovery and Visual Reasoning for Referring Expression Segmentation](https://arxiv.org/abs/2601.16394)
*Yihao Wang,Jusheng Zhang,Ziyi Tang,Keze Wang,Meng Yang*

Main category: cs.CV

TL;DR: 本文提出\model，一种结合基于熵的点发现（EBD）和基于视觉的推理（VBR）的新框架，用于指代表达分割（RES），在四个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLM）的RES方法存在两个关键问题：一是MLLM生成的粗略边界框导致冗余或非判别性点提示；二是依赖文本坐标推理不可靠，难以区分视觉上相似的干扰物。

Method: \model采用粗到精的流程：首先初始化边界框，然后通过EBD模块在边界框内建模空间不确定性以发现高信息量候选点，再利用VBR模块通过视觉-语义联合对齐验证点的正确性，最后进行掩码解码。EBD将点选择视为信息最大化过程，VBR则摒弃纯文本坐标推理，采用更鲁棒的视觉验证机制。

Result: 在RefCOCO、RefCOCO+、RefCOCOg和ReasonSeg四个基准数据集上的大量实验表明，\model在所有数据集上均达到新的最先进性能。

Conclusion: \model通过引入基于熵的点发现和基于视觉的推理机制，有效提升了指代表达分割的准确性和语义一致性，仅需少量提示即可生成高质量分割掩码。

Abstract: Referring Expression Segmentation (RES) is a core vision-language segmentation task that enables pixel-level understanding of targets via free-form linguistic expressions, supporting critical applications such as human-robot interaction and augmented reality. Despite the progress of Multimodal Large Language Model (MLLM)-based approaches, existing RES methods still suffer from two key limitations: first, the coarse bounding boxes from MLLMs lead to redundant or non-discriminative point prompts; second, the prevalent reliance on textual coordinate reasoning is unreliable, as it fails to distinguish targets from visually similar distractors. To address these issues, we propose \textbf{\model}, a novel RES framework integrating \textbf{E}ntropy-\textbf{B}ased Point \textbf{D}iscovery (\textbf{EBD}) and \textbf{V}ision-\textbf{B}ased \textbf{R}easoning (\textbf{VBR}). Specifically, EBD identifies high-information candidate points by modeling spatial uncertainty within coarse bounding boxes, treating point selection as an information maximization process. VBR verifies point correctness through joint visual-semantic alignment, abandoning text-only coordinate inference for more robust validation. Built on these components, \model implements a coarse-to-fine workflow: bounding box initialization, entropy-guided point discovery, vision-based validation, and mask decoding. Extensive evaluations on four benchmark datasets (RefCOCO, RefCOCO+, RefCOCOg, and ReasonSeg) demonstrate that \model achieves new state-of-the-art performance across all four benchmarks, highlighting its effectiveness in generating accurate and semantically grounded segmentation masks with minimal prompts.

Abstract (中文翻译): 指代表达分割（RES）是一项核心的视觉-语言分割任务，它通过自由形式的语言表达实现对目标的像素级理解，支持人机交互和增强现实等关键应用。尽管基于多模态大语言模型（MLLM）的方法已取得进展，但现有RES方法仍存在两个关键局限：首先，MLLM生成的粗略边界框导致冗余或非判别性的点提示；其次，普遍依赖的文本坐标推理不可靠，无法有效区分视觉上相似的干扰物。为解决这些问题，我们提出了\model，一种新颖的RES框架，整合了基于熵的点发现（EBD）和基于视觉的推理（VBR）。具体而言，EBD通过在粗略边界框内建模空间不确定性来识别高信息量的候选点，将点选择视为一个信息最大化过程；VBR则通过视觉-语义联合对齐验证点的正确性，摒弃仅依赖文本的坐标推理，实现更鲁棒的验证。基于这些组件，\model实现了从粗到细的工作流程：边界框初始化、熵引导的点发现、基于视觉的验证和掩码解码。在四个基准数据集（RefCOCO、RefCOCO+、RefCOCOg和ReasonSeg）上的大量评估表明，\model在所有四个基准上均取得了新的最先进性能，凸显了其在生成准确且语义一致的分割掩码方面的有效性，且仅需极少的提示。

</details>


### [9] [A Cosine Network for Image Super-Resolution](https://arxiv.org/abs/2601.16413)
*Chunwei Tian,Chengyuan Zhang,Bob Zhang,Zhiwu Li,C. L. Philip Chen,David Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种用于图像超分辨率的余弦网络（CSRNet），通过设计奇偶异构模块提取互补结构信息，并结合余弦退火机制优化训练策略，在性能上达到当前先进水平。


<details>
  <summary>Details</summary>
Motivation: 在图像超分辨率任务中，深度卷积神经网络虽能逐层提取结构信息，但如何有效保留并利用这些结构信息仍具挑战。现有方法在结构信息的多样性和训练稳定性方面存在不足。

Method: 提出CSRNet网络：1）设计奇偶异构块以增强架构差异，提取互补的同源结构信息；2）融合线性与非线性结构信息以提升鲁棒性；3）采用余弦退火机制进行带热重启的学习率调整，优化训练过程。

Result: 实验表明，所提出的CSRNet在图像超分辨率任务中性能优于或与当前最先进方法相当。

Conclusion: 通过改进网络架构和训练策略，CSRNet有效提升了图像超分辨率的质量和稳定性，验证了结构信息多样性与优化训练机制的重要性。

Abstract: Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.

Abstract (中文翻译): 深度卷积神经网络能够利用层次化信息逐步提取结构特征以恢复高质量图像。然而，在图像超分辨率中，保持所获得结构信息的有效性至关重要。本文提出了一种用于图像超分辨率的余弦网络（CSRNet），通过改进网络架构和优化训练策略来实现这一目标。为提取互补的同源结构信息，设计了奇偶异构模块，以扩大架构差异并提升图像超分辨率性能。结合线性与非线性结构信息可克服同源信息的局限性，增强所得结构信息的鲁棒性。考虑到梯度下降易陷入局部最小值的问题，采用余弦退火机制，通过热重启和动态调整学习率来优化训练过程。实验结果表明，所提出的CSRNet在图像超分辨率任务中具有与当前最先进方法相媲美的性能。

</details>


### [10] [DCCS-Det: Directional Context and Cross-Scale-Aware Detector for Infrared Small Target](https://arxiv.org/abs/2601.16428)
*Shuying Li,Qiang Ma,San Zhang,Chuang Yang*

Main category: cs.CV

TL;DR: 提出DCCS-Det方法，通过DSE模块和LaSEA模块联合建模局部-全局特征并抑制特征冗余，在红外小目标检测任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有红外小目标检测方法在局部-全局特征联合建模不足或存在特征冗余与语义稀释问题，导致目标-背景区分能力弱、目标表征质量差。

Method: 提出DCCS-Det检测器，包含Dual-stream Saliency Enhancement (DSE) 块和Latent-aware Semantic Extraction and Aggregation (LaSEA) 模块：DSE融合局部感知与方向感知上下文以捕获长程依赖和局部细节；LaSEA通过跨尺度特征提取和随机池化采样策略缓解特征退化，增强判别性特征并抑制噪声。

Result: 在多个数据集上实现领先的检测精度和有竞争力的效率，消融实验验证了DSE和LaSEA对提升复杂场景下目标感知与特征表示的有效性。

Conclusion: DCCS-Det有效解决了红外小目标检测中的局部-全局建模不足与特征冗余问题，显著提升了检测性能。

Abstract: Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. \href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det Official Code is Available Here!}

Abstract (中文翻译): 红外小目标检测（IRSTD）在遥感和监视等应用中至关重要，其目标是在复杂背景下识别出小而低对比度的目标。然而，现有方法常常难以充分联合建模局部-全局特征（损害目标-背景区分能力），或存在特征冗余和语义稀释问题（降低目标表征质量）。为解决这些问题，我们提出了DCCS-Det（面向红外小目标的方向上下文与跨尺度感知检测器），该方法引入了双流显著性增强（DSE）模块和潜在感知语义提取与聚合（LaSEA）模块。DSE模块结合局部感知与方向感知的上下文聚合，以捕捉长程空间依赖性和局部细节。在此基础上，LaSEA模块通过跨尺度特征提取和随机池化采样策略缓解特征退化，增强判别性特征并抑制噪声。大量实验表明，DCCS-Det在多个数据集上均取得了最先进的检测精度，并具有良好的效率。消融研究进一步验证了DSE和LaSEA在复杂场景下提升目标感知与特征表示能力的贡献。\href{https://huggingface.co/InPeerReview/InfraredSmallTargetDetection-IRSTD.DCCS}{DCCS-Det官方代码已公开！}

</details>
