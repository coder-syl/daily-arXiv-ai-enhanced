<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Future Optical Flow Prediction Improves Robot Control & Video Generation](https://arxiv.org/abs/2601.10781)
*Kanchana Ranasinghe,Honglu Zhou,Yu Fang,Luyu Yang,Le Xue,Ran Xu,Caiming Xiong,Silvio Savarese,Michael S Ryoo,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: FOFPred 是一个结合视觉语言模型（VLM）与扩散架构的语言条件光流预测模型，利用大规模网络人类活动数据进行训练，展现出在机器人控制和视频生成等下游任务中的跨领域通用性。


<details>
  <summary>Details</summary>
Motivation: 预测可泛化的空间密集运动表示（如光流）仍具挑战，尤其在从嘈杂的真实世界数据中学习方面研究较少。

Method: 提出 FOFPred 模型，融合统一的视觉语言模型（VLM）与扩散架构，通过语言条件引导，在像素级别实现高保真未来运动预测；使用大规模网络人类活动视频-文本对数据，并结合关键的数据预处理与强图像预训练策略。

Result: 在语言驱动的机器人操作和视频生成任务中，FOFPred 表现出优异的跨域性能，验证了其架构和从多样化网络数据中可扩展学习的有效性。

Conclusion: 统一的 VLM-扩散架构结合可扩展的网络数据训练，能有效提升未来光流预测的泛化能力与应用广度。

Abstract: Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.

Abstract (中文翻译): 未来运动表示（如光流）在控制和生成任务中具有巨大价值。然而，预测可泛化的空间密集运动表示仍是一个关键挑战，且从嘈杂的真实世界数据中学习此类预测方法的研究相对较少。我们提出了 FOFPred，一种新颖的语言条件光流预测模型，其采用统一的视觉语言模型（VLM）与扩散架构。这种独特组合实现了强大的多模态推理能力，并在像素级别上保持高保真度以预测未来运动。我们的模型在大规模网络人类活动数据（一种高度可扩展但非结构化的数据源）上进行训练。为从这些含噪的视频-文本数据中提取有意义的信号，我们采用了关键的数据预处理技术，并结合具有强大图像预训练能力的统一架构。训练完成的模型进一步被拓展用于解决控制与生成领域的两项不同下游任务。在语言驱动设置下的机器人操作和视频生成任务中的评估结果，验证了 FOFPred 的跨领域通用性，也证实了统一的 VLM-扩散架构以及从多样化网络数据中进行可扩展学习对未来光流预测的重要价值。

</details>


### [2] [ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research](https://arxiv.org/abs/2601.10802)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 本文提出了ICONIC-444，一个包含310万张图像、444个类别的大规模工业图像数据集，专为分布外（OOD）检测研究设计，支持从细粒度到粗粒度的多种视觉任务，并提供了22种先进OOD方法的基准结果。


<details>
  <summary>Details</summary>
Motivation: 当前分布外（OOD）检测研究受限于缺乏大规模、高质量且具有明确定义OOD类别（涵盖从近分布到远分布不同难度级别）的数据集，难以同时支持细粒度和粗粒度的计算机视觉任务。

Method: 作者构建了名为ICONIC-444的大型工业图像数据集，包含超过310万张RGB图像和444个类别，利用原型工业分拣机采集，贴近真实应用场景，并定义了四个参考任务用于OOD检测基准测试。

Result: 在ICONIC-444上对22种最先进的后处理OOD检测方法进行了基准测试，提供了基线结果，证明该数据集适用于不同复杂度任务下的严格OOD评估。

Conclusion: ICONIC-444填补了现有OOD检测数据集的空白，为未来研究提供了结构化、多样化且贴近工业实际的大规模基准平台。

Abstract: Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.

Abstract (中文翻译): 当前分布外（OOD）检测的进展受限于缺乏大规模、高质量的数据集，这些数据集需具备明确定义的OOD类别（涵盖从近分布到远分布的不同难度级别），并能同时支持细粒度和粗粒度的计算机视觉任务。为解决这一局限性，我们提出了ICONIC-444（Image Classification and OOD Detection with Numerous Intricate Complexities），这是一个专门用于OOD检测研究的大规模工业图像数据集，包含超过310万张RGB图像，涵盖444个类别。该数据集通过一台原型工业分拣机采集，高度模拟现实世界任务。ICONIC-444通过提供结构化且多样化的数据，补充了现有数据集，适用于在各种任务复杂度下进行严格的OOD评估。我们在ICONIC-444中定义了四个参考任务，用于对OOD检测研究进行基准测试，并为22种最先进的后处理OOD检测方法提供了基线结果。

</details>


### [3] [A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems](https://arxiv.org/abs/2601.10819)
*Yizhou Wang,Sameer Pusegaonkar,Yuxing Wang,Anqi Li,Vishal Kumar,Chetan Sethi,Ganapathy Aiyer,Yun He,Kartikay Thakkar,Swapnil Rathi,Bhushan Rupde,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种针对大规模基础设施环境优化的Sparse4D框架，通过引入世界坐标几何先验和遮挡感知ReID模块提升多相机跟踪性能，并利用NVIDIA COSMOS进行无监督域适应增强模型鲁棒性，在AI City Challenge 2025上取得SOTA结果，同时开发了TensorRT插件实现高效实时部署。


<details>
  <summary>Details</summary>
Motivation: 将自动驾驶中“由内向外”的感知模型迁移到工业基础设施中“由外向内”的静态多相机网络面临相机布局异构与严重遮挡等挑战，亟需专为该场景优化的3D感知与多目标跟踪方法。

Method: 基于Sparse4D框架，引入绝对世界坐标几何先验和遮挡感知的ReID嵌入模块；采用NVIDIA COSMOS生成式数据增强策略进行无标签Sim2Real域适应；并开发TensorRT插件优化多尺度可变形聚合（MSDA）以加速推理。

Result: 在AI City Challenge 2025基准上，仅使用相机的系统达到45.22的HOTA指标；硬件加速版本在现代GPU上实现2.15倍提速，单块Blackwell级GPU可支持64路以上并发视频流。

Conclusion: 所提方法有效解决了静态多相机网络下的3D目标感知与跨视角跟踪难题，兼具高精度与强实时性，适用于大规模工业基础设施的数字化部署。

Abstract: Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.

Abstract (中文翻译): 精确的3D物体感知与多目标多相机（MTMC）跟踪是工业基础设施数字化转型的基础。然而，将“由内向外”的自动驾驶模型迁移到“由外向内”的静态相机网络面临巨大挑战，主要源于异构的相机布设和极端遮挡。本文提出了一种专为大规模基础设施环境优化的Sparse4D框架。该系统利用绝对世界坐标几何先验，并引入一种遮挡感知的ReID嵌入模块，以在分布式传感器网络中保持身份一致性。为在无需人工标注的情况下弥合仿真到现实（Sim2Real）的域差距，我们采用基于NVIDIA COSMOS框架的生成式数据增强策略，生成多样化的环境风格，从而增强模型对表观变化的鲁棒性。在AI City Challenge 2025基准测试中，我们仅使用相机的框架取得了45.22的HOTA指标，达到当前最优水平。此外，为满足实时部署需求，我们开发了一个针对多尺度可变形聚合（MSDA）的优化TensorRT插件。经硬件加速后，该实现在现代GPU架构上实现了2.15倍的速度提升，使得单块Blackwell级GPU能够支持超过64路并发相机视频流。

</details>


### [4] [Can Vision-Language Models Understand Construction Workers? An Exploratory Study](https://arxiv.org/abs/2601.10835)
*Hieu Bui,Nathaniel E. Chodosh,Arash Tavakoli*

Main category: cs.CV

TL;DR: 本文评估了三种主流视觉语言模型（GPT-4o、Florence 2 和 LLaVa-1.5）在建筑工地静态图像中识别人类行为和情绪的能力。结果表明，GPT-4o 表现最佳，而所有模型在区分语义相近类别时仍存在困难，需进一步改进以提升实际应用可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在建筑流程中的广泛应用，其理解并响应人类行为的能力对实现安全高效的人机协作至关重要。由于建筑领域标注数据稀缺，且监测工人行为与情绪对安全和效率意义重大，无需大量领域特定训练的视觉语言模型（VLMs）成为有潜力的解决方案。

Method: 研究使用包含1000张图像的数据集，每张图像标注了10种行为和10种情绪类别，通过标准化推理流程和多种评估指标，对GPT-4o、Florence 2和LLaVa-1.5三种VLM进行性能评估。

Result: GPT-4o在行为识别（F1: 0.756，准确率: 0.799）和情绪识别（F1: 0.712，准确率: 0.773）上均表现最佳；Florence 2次之；LLaVa-1.5整体表现最弱。所有模型在区分语义相近类别（如团队协作 vs. 与主管沟通）方面存在困难。

Conclusion: 通用视觉语言模型可为建筑场景中的人类行为识别提供基础能力，但要实现实际应用的可靠性，仍需结合领域适配、时序建模或多模态感知等改进策略。

Abstract: As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.

Abstract (中文翻译): 随着机器人越来越多地融入建筑工作流程，其理解和响应人类行为的能力对于实现安全高效的合作至关重要。视觉语言模型（VLMs）已成为视觉理解任务中一种有前景的工具，能够在无需大量领域特定训练的情况下识别人类行为。这一特性使其在建筑领域尤为吸引人，因为该领域标注数据稀缺，而监测工人的行为和情绪状态对安全性和生产效率至关重要。本研究评估了三种主流VLM——GPT-4o、Florence 2和LLaVa-1.5——在静态工地图像中检测建筑工人行为和情绪的表现。我们使用了一个包含1000张图像的精选数据集，每张图像均标注了10种行为类别和10种情绪类别，并通过标准化推理流程和多种评估指标对各模型输出进行评估。结果显示，GPT-4o在两项任务中始终得分最高：行为识别的平均F1得分为0.756，准确率为0.799；情绪识别的F1得分为0.712，准确率为0.773。Florence 2表现中等，行为和情绪识别的F1得分分别为0.497和0.414；而LLaVa-1.5整体表现最差，行为和情绪识别的F1得分分别为0.466和0.461。混淆矩阵分析表明，所有模型在区分语义相近的类别（例如“团队协作”与“与主管沟通”）方面均存在困难。尽管结果表明通用VLM可在建筑环境中提供人类行为识别的基础能力，但要实现现实场景中的可靠应用，可能仍需引入领域适配、时序建模或多模态感知等进一步改进。

</details>


### [5] [One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection](https://arxiv.org/abs/2601.10836)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 该论文通过大规模实证研究发现，模型在分布内（ID）准确率与分布外（OOD）检测性能之间并非单调正相关；当训练策略过度提升ID准确率时，OOD性能反而下降，且不同训练策略与OOD检测方法之间存在强依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法与现代高精度训练流程之间的相互作用尚未充分研究，尤其缺乏对ID准确率提升是否真正有助于OOD检测的系统性评估。

Method: 固定使用ResNet-50架构，对56个采用不同训练策略在ImageNet上训练的模型，系统评估21种最先进的后处理OOD检测方法在8个OOD测试集上的表现。

Result: 发现ID准确率与OOD性能呈非单调关系：初期随准确率提升而改善，但当训练策略使准确率超过基线后，OOD性能反而下降；同时，训练策略、检测器选择和OOD性能三者高度相关。

Conclusion: 不存在一种在所有训练策略下都最优的OOD检测方法，需根据具体训练方式选择合适的检测器，且盲目追求高ID准确率可能损害OOD鲁棒性。

Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.

Abstract (中文翻译): 分布外（OOD）检测对于在开放世界环境中部署鲁棒且可靠的机器学习系统至关重要。尽管OOD检测器不断取得进展，但其与旨在最大化分布内（ID）准确率和泛化能力的现代训练流程之间的相互作用仍缺乏深入探索。本文通过一项全面的实证研究探讨了这一关联。研究固定采用广泛使用的ResNet-50架构，对通过多种训练策略获得的56个ImageNet训练模型，系统评估了21种最先进的后处理OOD检测方法在8个OOD测试集上的表现。与“更高ID准确率意味着更好OOD检测性能”的普遍假设相反，我们发现二者之间存在非单调关系：OOD性能起初随准确率提升而改善，但一旦先进训练方案将准确率推高至基线以上，OOD性能反而下降。此外，我们观察到训练策略、检测器选择与最终OOD性能之间存在强烈依赖关系，表明并不存在一种普适最优的检测方法。

</details>


### [6] [Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification](https://arxiv.org/abs/2601.10854)
*Mohammad Rasras,Iuliana Marin,Serban Radu,Irina Mocanu*

Main category: cs.CV

TL;DR: 本文研究了在提高视频帧分辨率的同时减少时间信息对3D ResNet模型（MC3、R3D、R(2+1)D）动作识别性能的影响，并通过引入多种注意力机制（如CBAM、TCN、多头注意力等）进行改进，在UCF101数据集上最高达到88.98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 探索在提升视频帧分辨率但减少时间维度信息的情况下，如何通过引入注意力机制来弥补缺失的时间特征，从而维持或提升人类动作识别的性能。

Method: 在原始MC3、R3D和R(2+1)D模型基础上加入Dropout层，并为每种模型设计十种变体，分别集成CBAM、TCN、多头注意力和通道注意力等模块，以评估各类注意力机制对受限时间模型性能的影响。

Result: 在UCF101数据集上，加入多头注意力机制的改进版R(2+1)D模型取得了88.98%的准确率；不同变体在整体性能相近的情况下，类别级准确率表现各异。

Conclusion: 缺失的时间特征对高分辨率模型的动作识别性能具有显著影响，而不同注意力机制虽能提升整体表现，但在细粒度类别上的效果存在差异。

Abstract: Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.

Abstract (中文翻译): 由于人类动作识别在计算机视觉中具有广泛的应用，已成为重要的研究热点。基于3D ResNet的CNN模型（尤其是MC3、R3D和R(2+1)D）采用不同的卷积滤波器来提取时空特征。本文研究了在提高视频帧分辨率的同时减少时间维度所捕获信息的影响。为此，我们首先构建了与原始三种模型结构相似但在最终分类器前加入Dropout层的基准模型；随后，针对每种模型分别开发了十种新变体，这些变体在其架构中集成了特定的注意力模块，包括卷积块注意力模块（CBAM）、时间卷积网络（TCN），以及多头注意力和通道注意力机制。目的是观察这些模块对受限时间模型性能的影响程度。所有模型在UCF101数据集上的测试结果表明，将多头注意力机制加入改进后的R(2+1)D模型可达到88.98%的准确率。本文得出结论：在新构建的高分辨率模型中，缺失的时间特征对性能具有显著影响。尽管各类变体在整体性能提升方面相似，但在类别级别的准确率上表现出不同的行为。

</details>


### [7] [Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation](https://arxiv.org/abs/2601.10880)
*Chongcong Jiang,Tianxingjian Ding,Chuhan Song,Jiachen Tu,Ziyang Yan,Yihua Shao,Zhenyi Wang,Yuzhang Shang,Tianyu Han,Yu Tian*

Main category: cs.CV

TL;DR: 本文提出Medical SAM3，通过对SAM3在大规模异构医学影像数据上进行全参数微调，显著提升了其在医学图像分割中的性能，尤其在语义模糊、形态复杂和长程3D上下文等挑战性场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: 原始SAM3模型在医学图像分割中受限于严重的领域偏移、缺乏有效的空间提示以及难以处理复杂的解剖结构和体数据，导致性能大幅下降。

Method: 对SAM3模型在涵盖10种医学成像模态的33个2D/3D医学数据集上进行全参数微调，利用配对的分割掩码和文本提示，使其获得领域特定表示并保留提示驱动的灵活性。

Result: 在多种器官、成像模态和维度上的实验表明，Medical SAM3相比原始SAM3有显著且一致的性能提升，尤其在语义模糊、形态复杂和长程3D上下文等挑战性场景中。

Conclusion: Medical SAM3是一个通用的、文本引导的医学图像分割基础模型，证明了在严重领域偏移下，全面的模型适配对于实现鲁棒提示驱动分割的重要性。

Abstract: Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.

Abstract (中文翻译): 诸如SAM3之类的可提示分割基础模型通过交互式和基于概念的提示展现了强大的泛化能力。然而，它们在医学图像分割中的直接适用性仍受到严重领域偏移、缺乏特权空间提示以及需要推理复杂解剖结构和体数据的限制。本文提出了Medical SAM3——一个用于通用提示驱动医学图像分割的基础模型，该模型通过对SAM3在大规模异构2D和3D医学影像数据集（包含配对的分割掩码和文本提示）上进行全参数微调而获得。通过对原始SAM3的系统分析，我们发现其在医学数据上的性能显著下降，其表面上的竞争力很大程度上依赖于如真实边界框等强几何先验。这些发现促使我们超越仅靠提示工程，进行完整的模型适配。通过在涵盖10种医学成像模态的33个数据集上微调SAM3的模型参数，Medical SAM3获得了鲁棒的领域特定表示，同时保留了提示驱动的灵活性。在多种器官、成像模态和维度上的广泛实验表明，其性能始终显著优于基线，尤其在语义模糊、形态复杂和长程3D上下文等挑战性场景中。我们的结果确立了Medical SAM3作为医学影像领域通用文本引导分割基础模型的地位，并强调了在严重领域偏移下，全面模型适配对实现鲁棒提示驱动分割的重要性。代码和模型将发布于https://github.com/AIM-Research-Lab/Medical-SAM3。

</details>


### [8] [FrankenMotion: Part-level Human Motion Generation and Composition](https://arxiv.org/abs/2601.10909)
*Chuqiao Li,Xianghui Xie,Yong Cao,Andreas Geiger,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 本文提出FrankenMotion，一种基于扩散模型的细粒度人体运动生成方法，通过构建带有原子级、时间感知的身体部位文本标注的新数据集，实现对各身体部位在时间和空间上的独立控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的人体运动生成方法大多依赖序列级或动作级描述，缺乏细粒度的身体部位运动标注，导致对个体身体部位的可控性不足。

Method: 利用大语言模型（LLM）构建一个高质量的人体运动数据集，包含原子级、时间感知的身体部位文本标注；在此基础上，提出FrankenMotion框架，采用扩散模型，为每个身体部位提供独立的时间结构化文本提示以生成运动。

Result: 实验表明，FrankenMotion在新设定下优于所有适配并重新训练的基线模型，并能组合生成训练中未见过的动作。

Conclusion: 本文首次实现了具有原子级、时间感知的身体部位运动标注，并提出了支持空间（身体部位）与时间（原子动作）双重控制的运动生成模型，显著提升了文本驱动运动生成的细粒度可控性。

Abstract: Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

Abstract (中文翻译): 近年来，基于文本提示的人体运动生成取得了显著进展。然而，由于缺乏细粒度的部位级运动标注，现有方法主要依赖序列级或动作级描述，限制了对各个身体部位的可控性。在本研究中，我们利用大语言模型（LLM）的推理能力，构建了一个高质量的人体运动数据集，其中包含原子级、时间感知的部位级文本标注。与以往仅提供固定时间段同步部位字幕或仅依赖全局序列标签的数据集不同，我们的数据集在精细的时间分辨率下捕捉了异步且语义不同的部位运动。基于该数据集，我们提出了一种基于扩散模型的部位感知运动生成框架——FrankenMotion，其中每个身体部位均由其自身的时间结构化文本提示进行引导。据我们所知，这是首个提供原子级、时间感知部位级运动标注的工作，并实现了同时具备空间（身体部位）和时间（原子动作）控制能力的运动生成模型。实验表明，FrankenMotion在我们的设定下优于所有经过适配和重新训练的先前基线模型，并能够组合生成训练期间未见过的动作。我们的代码和数据集将在论文发表后公开。

</details>


### [9] [Classification of Chest XRay Diseases through image processing and analysis techniques](https://arxiv.org/abs/2601.10913)
*Santiago Martínez Novoa,María Catalina Ibáñez,Lina Gómez Mesa,Jeremias Kramer*

Main category: cs.CV

TL;DR: 本文综述了用于胸部X光多分类任务的多种方法（包括DenseNet121），通过实验比较其性能，分析所提方法的不足，并开源了一个Web应用及代码。


<details>
  <summary>Details</summary>
Motivation: 胸部X光图像是诊断胸腔疾病最常用的放射学检查手段之一，因此需要有效且可靠的多分类方法来辅助诊断。

Method: 综述并实验比较了多种胸部X光图像多分类方法（如DenseNet121），同时开发并部署了一个开源的Web应用。

Result: 对不同方法进行了性能测试，识别出当前方法的局限性。

Conclusion: 所提出的方法虽有一定效果，但仍存在不足，未来可通过改进模型结构或数据处理策略进一步提升性能。

Abstract: Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML

Abstract (中文翻译): 多分类胸部X光图像是一种用于诊断胸腔疾病最常见的放射学检查方式。在本研究中，我们简要概述了用于解决该任务的若干方法，包括DenseNet121。此外，我们还部署了一个开源的基于Web的应用程序。在研究中，我们对不同方法进行了测试，以比较它们的性能表现，并深入分析了所提出方法的弱点，提出了未来改进的方向。我们的代码已公开于：https://github.com/AML4206-MINE20242/Proyecto_AML

</details>


### [10] [Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images](https://arxiv.org/abs/2601.10917)
*Pouya Afshin,David Helminiak,Tianling Niu,Julie M. Jorns,Tina Yen,Bing Yu,Dong Hye Ye*

Main category: cs.CV

TL;DR: 本文提出一种结合自监督学习与潜在扩散模型的方法，用于生成高质量的深紫外荧光扫描显微图像合成数据，以提升乳腺保乳手术中切缘评估的深度学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 深紫外荧光扫描显微（DUV-FSM）可用于乳腺保乳手术中的快速高分辨率成像，但缺乏标注数据限制了深度学习模型的训练。

Method: 利用自监督学习（SSL）引导的潜在扩散模型（LDM），通过微调后的DINO教师模型嵌入信息注入细胞结构语义细节，生成合成图像；将真实与合成图像结合用于微调Vision Transformer（ViT），并通过图像块预测聚合实现全切片（WSI）级别分类。

Result: 在5折交叉验证下，该方法达到96.47%准确率，FID分数降至45.72，显著优于类别条件基线模型。

Conclusion: 所提方法有效缓解了DUV-FSM图像标注数据稀缺问题，显著提升了切缘评估模型的性能，具有临床应用潜力。

Abstract: Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.

Abstract (中文翻译): 保乳手术（BCS）需要精确的术中切缘评估以保留健康组织。深紫外荧光扫描显微镜（DUV-FSM）为此提供了快速、高分辨率的表面成像能力；然而，标注的DUV数据稀缺阻碍了鲁棒深度学习模型的训练。为解决此问题，我们提出一种由自监督学习（SSL）引导的潜在扩散模型（LDM），用于生成高质量的合成训练图像块。通过使用微调后的DINO教师模型嵌入来引导LDM，我们将丰富的细胞结构语义细节注入合成数据中。我们将真实与合成图像块结合，用于微调Vision Transformer（ViT），并利用图像块预测聚合实现全切片（WSI）级别的分类。基于5折交叉验证的实验表明，该方法达到了96.47%的准确率，并将FID分数降至45.72，显著优于类别条件基线方法。

</details>
