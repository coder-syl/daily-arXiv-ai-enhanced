{"id": "2602.12361", "pdf": "https://arxiv.org/pdf/2602.12361", "abs": "https://arxiv.org/abs/2602.12361", "authors": ["Constantino \u00c1lvarez Casado", "Mohammad Rahman", "Sasan Sharifipour", "Nhi Nguyen", "Manuel Lage Ca\u00f1ellas", "Xiaoting Wu", "Miguel Bordallo L\u00f3pez"], "title": "Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring", "categories": ["cs.CV"], "comment": "7 pages, 6 figures, 3 tables, 22 references, 1 equation, conference", "summary": "Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \\pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \\pm 1.1$ bpm, while HR estimation yields $13.8 \\pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u9762\u90e8\u70ed\u6210\u50cf\u89c6\u9891\uff0c\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u6d41\u7a0b\u65e0\u63a5\u89e6\u5730\u4f30\u8ba1\u76ae\u80a4\u7535\u6d3b\u52a8\uff08EDA\uff09\u3001\u5fc3\u7387\uff08HR\uff09\u548c\u547c\u5438\u7387\uff08BR\uff09\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u51c6\u4e0e\u8bbe\u8ba1\u6307\u5bfc\u3002", "motivation": "\u53ef\u89c1\u5149\u65b9\u6cd5\u867d\u80fd\u4f30\u8ba1HR\u548cBR\uff0c\u4f46\u65e0\u6cd5\u83b7\u53d6\u4f5c\u4e3a\u4ea4\u611f\u795e\u7ecf\u6fc0\u6d3b\u6807\u5fd7\u7684EDA\u3002\u70ed\u7ea2\u5916\u6210\u50cf\u53ef\u6355\u6349\u7531\u81ea\u4e3b\u795e\u7ecf\u8c03\u8282\u5f15\u8d77\u7684\u76ae\u80a4\u6e29\u5ea6\u53d8\u5316\uff0c\u6709\u671b\u5b9e\u73b0\u5bf9\u8fd9\u4e09\u79cd\u751f\u7406\u4fe1\u53f7\u7684\u65e0\u63a5\u89e6\u540c\u6b65\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4fe1\u53f7\u5904\u7406\u6d41\u7a0b\uff1a\u8ddf\u8e2a\u9762\u90e8\u89e3\u5256\u533a\u57df\u3001\u8fdb\u884c\u7a7a\u95f4\u805a\u5408\uff0c\u5e76\u5206\u79bb\u7f13\u6162\u7684\u6c57\u817a\u6d3b\u52a8\u8d8b\u52bf\u4e0e\u8f83\u5feb\u7684\u5fc3\u80ba\u6210\u5206\u3002\u5bf9HR\u91c7\u7528\u591a\u533a\u57df\u6b63\u4ea4\u77e9\u9635\u56fe\u50cf\u53d8\u6362\uff08OMIT\uff09\u5206\u89e3\uff1b\u5bf9BR\u5219\u5e73\u5747\u9f3b\u90e8\u4e0e\u8138\u988a\u4fe1\u53f7\u540e\u8fdb\u884c\u9891\u8c31\u5cf0\u503c\u68c0\u6d4b\uff1b\u5bf9EDA\u8bc4\u4f30\u4e86288\u79cd\u914d\u7f6e\uff0c\u5305\u62ec\u4e0d\u540c\u533a\u57df\u548c\u6ee4\u6ce2\u7b56\u7565\u3002", "result": "\u5728SIM1\u6570\u636e\u96c631\u4e2a\u4f1a\u8bdd\u4e2d\uff0c\u6700\u4f73\u56fa\u5b9aEDA\u914d\u7f6e\uff08\u9f3b\u533a+\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff09\u4e0e\u624b\u638cEDA\u7684\u5e73\u5747\u7edd\u5bf9\u76f8\u5173\u7cfb\u6570\u4e3a0.40\u00b10.23\uff08\u4e2a\u522b\u8fbe0.89\uff09\uff1bBR\u4f30\u8ba1MAE\u4e3a3.1\u00b11.1 bpm\uff1bHR\u4f30\u8ba1MAE\u4e3a13.8\u00b17.5 bpm\uff08\u53d7\u9650\u4e8e7.5 Hz\u4f4e\u5e27\u7387\uff09\u3002\u8fd8\u89c2\u5bdf\u5230\u4fe1\u53f7\u6781\u6027\u4ea4\u66ff\u3001\u77ed\u70ed\u529b\u5b66\u5ef6\u8fdf\u53ca\u6761\u4ef6\u4e0e\u4eba\u53e3\u7edf\u8ba1\u5b66\u5bf9\u63d0\u53d6\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u57fa\u4e8e\u70ed\u6210\u50cf\u7684\u65e0\u63a5\u89e6\u751f\u7406\u4fe1\u53f7\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u7ebf\u548c\u7cfb\u7edf\u8bbe\u8ba1\u6307\u5bfc\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9a7e\u9a76\u5458\u76d1\u6d4b\u7b49\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002", "summary_cn": "\u70ed\u7ea2\u5916\u6210\u50cf\u80fd\u591f\u6355\u6349\u7531\u81ea\u4e3b\u795e\u7ecf\u8c03\u8282\u9a71\u52a8\u7684\u76ae\u80a4\u6e29\u5ea6\u53d8\u5316\uff0c\u6709\u671b\u5b9e\u73b0\u5bf9\u76ae\u80a4\u7535\u6d3b\u52a8\uff08EDA\uff09\u3001\u5fc3\u7387\uff08HR\uff09\u548c\u547c\u5438\u7387\uff08BR\uff09\u7684\u65e0\u63a5\u89e6\u4f30\u8ba1\u3002\u867d\u7136\u53ef\u89c1\u5149\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406HR\u548cBR\uff0c\u4f46\u65e0\u6cd5\u83b7\u53d6\u4f5c\u4e3a\u4ea4\u611f\u795e\u7ecf\u6fc0\u6d3b\u6807\u51c6\u6307\u6807\u7684EDA\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4fe1\u53f7\u5904\u7406\u6d41\u7a0b\uff0c\u4ece\u9762\u90e8\u70ed\u6210\u50cf\u89c6\u9891\u4e2d\u63d0\u53d6\u8fd9\u4e09\u79cd\u751f\u7406\u4fe1\u53f7\uff1a\u8be5\u6d41\u7a0b\u8ddf\u8e2a\u89e3\u5256\u533a\u57df\u3001\u8fdb\u884c\u7a7a\u95f4\u805a\u5408\uff0c\u5e76\u5c06\u7f13\u6162\u7684\u6c57\u817a\u6d3b\u52a8\u8d8b\u52bf\u4e0e\u8f83\u5feb\u7684\u5fc3\u80ba\u6210\u5206\u5206\u79bb\u3002\u5bf9\u4e8eHR\uff0c\u6211\u4eec\u5728\u591a\u4e2a\u9762\u90e8\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u5e94\u7528\u6b63\u4ea4\u77e9\u9635\u56fe\u50cf\u53d8\u6362\uff08OMIT\uff09\u5206\u89e3\uff1b\u5bf9\u4e8eBR\uff0c\u5219\u5728\u9891\u8c31\u5cf0\u503c\u68c0\u6d4b\u524d\u5bf9\u9f3b\u90e8\u548c\u8138\u988a\u4fe1\u53f7\u53d6\u5e73\u5747\u3002\u6211\u4eec\u5728\u516c\u5f00\u7684SIMULATOR STUDY 1\uff08SIM1\uff09\u9a7e\u9a76\u5458\u76d1\u6d4b\u6570\u636e\u96c6\u768431\u4e2a\u4f1a\u8bdd\u4e2d\uff0c\u8bc4\u4f30\u4e86288\u79cdEDA\u914d\u7f6e\u4ee5\u53caHR/BR\u6d41\u7a0b\u3002\u6700\u4f73\u56fa\u5b9aEDA\u914d\u7f6e\uff08\u9f3b\u533a+\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff09\u4e0e\u624b\u638cEDA\u7684\u5e73\u5747\u7edd\u5bf9\u76f8\u5173\u7cfb\u6570\u8fbe\u52300.40\u00b10.23\uff0c\u4e2a\u522b\u4f1a\u8bdd\u9ad8\u8fbe0.89\uff1bBR\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a3.1\u00b11.1 bpm\uff1bHR\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a13.8\u00b17.5 bpm\uff0c\u53d7\u9650\u4e8e\u8f83\u4f4e\u7684\u76f8\u673a\u5e27\u7387\uff087.5 Hz\uff09\u3002\u6211\u4eec\u8fd8\u62a5\u544a\u4e86\u4e0d\u540c\u4f1a\u8bdd\u4e2d\u4fe1\u53f7\u6781\u6027\u7684\u4ea4\u66ff\u73b0\u8c61\u3001\u826f\u597d\u8ffd\u8e2a\u4fe1\u53f7\u7684\u77ed\u70ed\u529b\u5b66\u5ef6\u8fdf\uff0c\u4ee5\u53ca\u63d0\u53d6\u8d28\u91cf\u53d7\u5b9e\u9a8c\u6761\u4ef6\u548c\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u70ed\u6210\u50cf\u65e0\u63a5\u89e6\u751f\u7406\u4fe1\u53f7\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u51c6\u548c\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2602.12370", "pdf": "https://arxiv.org/pdf/2602.12370", "abs": "https://arxiv.org/abs/2602.12370", "authors": ["Zekun Li", "Sizhe An", "Chengcheng Tang", "Chuan Guo", "Ivan Shugurov", "Linguang Zhang", "Amy Zhao", "Srinath Sridhar", "Lingling Tao", "Abhay Mittal"], "title": "LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens", "categories": ["cs.CV"], "comment": "Project page: https://kunkun0w0.github.io/project/LLaMo/", "summary": "Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.", "AI": {"tldr": "LLaMo is a unified motion-language model that extends pretrained LLMs with a Mixture-of-Transformers architecture, enabling high-quality bidirectional generation (text-to-motion and motion-to-text) while preserving linguistic capabilities and supporting real-time streaming motion generation.", "motivation": "Existing unified motion-language models suffer from catastrophic forgetting of language skills due to limited motion-text data and introduce jitter artifacts by using discrete motion representations. There is a need for a framework that preserves LLM capabilities while enabling effective, continuous, and scalable motion-language integration.", "method": "The authors propose LLaMo, which uses a modality-specific Mixture-of-Transformers (MoT) to extend pretrained LLMs without altering their original parameters. Human motion is encoded into a causal continuous latent space, and a lightweight flow-matching head is added to the decoder-only backbone to maintain next-token prediction, enabling real-time streaming motion generation.", "result": "LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning, excelling particularly in zero-shot motion generation, while operating in real-time (>30 FPS).", "conclusion": "LLaMo represents a significant step toward a general unified motion-language large model by effectively combining continuous motion representation with pretrained LLMs, preserving language understanding, and enabling scalable multimodal adaptation.", "summary_cn": "\u8fd1\u671f\u5927\u6a21\u578b\u7684\u8fdb\u5c55\u5728\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u4e0e\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002\u7136\u800c\uff0c\u7edf\u4e00\u8fd0\u52a8-\u8bed\u8a00\u751f\u6210\u4e0e\u7406\u89e3\u7684\u6a21\u578b\u5f00\u53d1\u4ecd\u9c9c\u6709\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u914d\u5bf9\u7684\u8fd0\u52a8-\u6587\u672c\u6570\u636e\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4f46\u7531\u4e8e\u53ef\u7528\u6587\u672c-\u8fd0\u52a8\u5bf9\u6570\u636e\u89c4\u6a21\u6709\u9650\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bed\u8a00\u80fd\u529b\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002\u6b64\u5916\uff0c\u4ee5\u5f80\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u91cf\u5316\u5c06\u8fd0\u52a8\u8f6c\u6362\u4e3a\u79bb\u6563\u8868\u793a\u4ee5\u4e0e\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff0c\u8fd9\u79cd\u79bb\u6563\u5316\u5206\u8bcd\u4f1a\u5f15\u5165\u660e\u663e\u7684\u6296\u52a8\u4f2a\u5f71\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LLaMo\u2014\u2014\u4e00\u79cd\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7684\u6df7\u5408\u53d8\u6362\u5668\uff08Mixture-of-Transformers, MoT\uff09\u67b6\u6784\u6269\u5c55\u9884\u8bad\u7ec3LLM\u7684\u7edf\u4e00\u6846\u67b6\u3002\u8be5\u8bbe\u8ba1\u5728\u672c\u8d28\u4e0a\u4fdd\u7559\u4e86\u57fa\u7840\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u652f\u6301\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u9002\u914d\u3002\u6211\u4eec\u5c06\u4eba\u4f53\u8fd0\u52a8\u7f16\u7801\u5230\u4e00\u4e2a\u56e0\u679c\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u6d41\u5339\u914d\u5934\uff08flow-matching head\uff09\u5728\u4ec5\u89e3\u7801\u5668\u4e3b\u5e72\u4e2d\u7ef4\u6301\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\u8303\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5f0f\u8fd0\u52a8\u751f\u6210\uff08>30 FPS\uff09\u3002\u501f\u52a9\u9884\u8bad\u7ec3LLM\u7684\u5168\u9762\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548c\u5927\u89c4\u6a21\u8fd0\u52a8-\u6587\u672c\u9884\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8868\u660eLLaMo\u5728\u901a\u7528\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u548c\u8fd0\u52a8\u5230\u6587\u672c\u63cf\u8ff0\uff0c\u5c24\u5176\u5728\u96f6\u6837\u672c\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u6807\u5fd7\u7740\u5411\u901a\u7528\u7edf\u4e00\u8fd0\u52a8-\u8bed\u8a00\u5927\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2602.12381", "pdf": "https://arxiv.org/pdf/2602.12381", "abs": "https://arxiv.org/abs/2602.12381", "authors": ["Marco Willi", "Melanie Mathys", "Michael Graber"], "title": "Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues", "categories": ["cs.CV"], "comment": "11 figures; 23 pages", "summary": "Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eCLIP\u7684\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\uff08SID\uff09\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4e3b\u8981\u4f9d\u8d56\u9ad8\u5c42\u6b21\u6444\u5f71\u5c5e\u6027\u800c\u975e\u751f\u6210\u5668\u7279\u6709\u4f2a\u5f71\uff0c\u5728\u9ad8\u8d28\u91cf\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5f3a\u8c03\u9700\u6301\u7eed\u66f4\u65b0\u6a21\u578b\u5e76\u6269\u5927\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u80fd\u4ea7\u51fa\u8fd1\u4e4e\u903c\u771f\u7684\u56fe\u50cf\uff0c\u7167\u7247\u53ef\u4fe1\u5ea6\u53d7\u5230\u6311\u6218\uff0c\u4fc3\u4f7f\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\uff08SID\uff09\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u7136\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u65b0\u578b\u751f\u6210\u6a21\u578b\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u4e0d\u6e05\u695aCLIP\u7b49\u6a21\u578b\u662f\u4f9d\u8d56\u89c6\u89c9\u4f2a\u5f71\u8fd8\u662f\u8bed\u4e49\u504f\u5dee\u8fdb\u884c\u68c0\u6d4b\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aSynthCLIC\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u7167\u7247\u53ca\u5176\u7531\u6700\u65b0\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u914d\u5bf9\u5408\u6210\u56fe\u50cf\uff0c\u4ee5\u51cf\u5c11\u8bed\u4e49\u504f\u5dee\u3002\u901a\u8fc7\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u7ebf\u6027\u5206\u7c7b\u5934\uff08\u5177\u6709\u53bb\u76f8\u5173\u6fc0\u6d3b\uff09\u548c\u57fa\u4e8e\u6587\u672c\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u5206\u6790CLIP\u7279\u5f81\u4e2d\u7528\u4e8e\u68c0\u6d4b\u7684\u5173\u952e\u7ebf\u7d22\u3002", "result": "\u57fa\u4e8eCLIP\u7684\u7ebf\u6027\u68c0\u6d4b\u5668\u5728GAN\u57fa\u51c6\u4e0a\u8fbe\u52300.96 mAP\uff0c\u4f46\u5728SynthCLIC\u6570\u636e\u96c6\u4e0a\u964d\u81f30.92\uff1b\u8de8\u751f\u6210\u5668\u5bb6\u65cf\u7684\u6cdb\u5316\u6027\u80fd\u6700\u4f4e\u964d\u81f30.37 mAP\u3002\u5206\u6790\u8868\u660e\uff0c\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u5982\u6781\u7b80\u98ce\u683c\u3001\u955c\u5934\u5149\u6655\u3001\u666f\u6df1\u5206\u5c42\u7b49\u9ad8\u5c42\u6444\u5f71\u5c5e\u6027\uff0c\u800c\u975e\u660e\u663e\u7684\u751f\u6210\u5668\u4f2a\u5f71\u3002", "conclusion": "CLIP\u4e3a\u57fa\u7840\u7684SID\u65b9\u6cd5\u6574\u4f53\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e0d\u540c\u751f\u6210\u67b6\u6784\u95f4\u6cdb\u5316\u4e0d\u5747\uff0c\u9700\u6301\u7eed\u66f4\u65b0\u6a21\u578b\u5e76\u6269\u5927\u8bad\u7ec3\u8986\u76d6\u8303\u56f4\u3002\u540c\u65f6\uff0c\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86CLIP\u4f5c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u3001\u9c81\u68d2SID\u7cfb\u7edf\u7684\u57fa\u7840\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "summary_cn": "\u8fd1\u671f\u751f\u6210\u6a21\u578b\u80fd\u591f\u751f\u6210\u63a5\u8fd1\u771f\u5b9e\u7167\u7247\u7684\u56fe\u50cf\uff0c\u8fd9\u5bf9\u7167\u7247\u7684\u53ef\u4fe1\u5ea6\u6784\u6210\u4e86\u6311\u6218\uff0c\u56e0\u6b64\u5408\u6210\u56fe\u50cf\u68c0\u6d4b\uff08SID\uff09\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u9886\u57df\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u6307\u51fa\u5408\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u7167\u7247\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4f46SID\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002CLIP\u4f5c\u4e3a\u4e00\u79cd\u57fa\u7840\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u56fe\u6587\u5d4c\u5165\uff0c\u5728SID\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0cCLIP\u7279\u5f81\u4e2d\u6240\u5305\u542b\u7684\u76f8\u5173\u5224\u522b\u7ebf\u7d22\u5c1a\u4e0d\u660e\u786e\uff1a\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u57fa\u4e8eCLIP\u7684\u68c0\u6d4b\u5668\u662f\u4ec5\u68c0\u6d4b\u660e\u663e\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u8fd8\u662f\u5229\u7528\u4e86\u7ec6\u5fae\u7684\u8bed\u4e49\u504f\u5dee\u2014\u2014\u8fd9\u4e24\u79cd\u60c5\u51b5\u90fd\u4f1a\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u6216\u9762\u5bf9\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u65f6\u5931\u6548\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SynthCLIC\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u771f\u5b9e\u7167\u7247\u53ca\u5176\u5bf9\u5e94\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\uff08\u6765\u81ea\u6700\u65b0\u6269\u6563\u6a21\u578b\uff09\u7ec4\u6210\u7684\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u65e8\u5728\u51cf\u5c11SID\u4e2d\u7684\u8bed\u4e49\u504f\u5dee\u3002\u901a\u8fc7\u91c7\u7528\u5177\u6709\u53bb\u76f8\u5173\u6fc0\u6d3b\u7684\u53ef\u89e3\u91ca\u7ebf\u6027\u5206\u7c7b\u5934\u548c\u57fa\u4e8e\u6587\u672c\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u6211\u4eec\u5206\u6790\u4e86\u57fa\u4e8eCLIP\u7684\u68c0\u6d4b\u5668\u6240\u5b66\u4e60\u7684\u5185\u5bb9\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eCLIP\u7684\u7ebf\u6027\u68c0\u6d4b\u5668\u5728\u57fa\u4e8eGAN\u7684\u57fa\u51c6\u4e0a\u8fbe\u52300.96 mAP\uff0c\u4f46\u5728\u6211\u4eec\u7684\u9ad8\u8d28\u91cf\u6269\u6563\u6570\u636e\u96c6SynthCLIC\u4e0a\u4ec5\u4e3a0.92\uff1b\u8de8\u751f\u6210\u5668\u5bb6\u65cf\u7684\u6cdb\u5316\u6027\u80fd\u751a\u81f3\u4f4e\u81f30.37 mAP\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u8fd9\u4e9b\u68c0\u6d4b\u5668\u4e3b\u8981\u4f9d\u8d56\u4e8e\u9ad8\u5c42\u6444\u5f71\u5c5e\u6027\uff08\u4f8b\u5982\u6781\u7b80\u98ce\u683c\u3001\u955c\u5934\u5149\u6655\u6216\u666f\u6df1\u5c42\u6b21\uff09\uff0c\u800c\u975e\u660e\u663e\u7684\u751f\u6210\u5668\u7279\u6709\u4f2a\u5f71\u3002\u603b\u4f53\u800c\u8a00\uff0c\u57fa\u4e8eCLIP\u7684\u68c0\u6d4b\u5668\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e0d\u540c\u751f\u6210\u67b6\u6784\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u5747\u8861\u3002\u8fd9\u7a81\u663e\u4e86\u6301\u7eed\u66f4\u65b0\u6a21\u578b\u548c\u6269\u5927\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u7684\u5fc5\u8981\u6027\uff0c\u540c\u65f6\u4e5f\u5f3a\u5316\u4e86CLIP\u65b9\u6cd5\u4f5c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u3001\u9c81\u68d2SID\u7cfb\u7edf\u575a\u5b9e\u57fa\u7840\u7684\u5730\u4f4d\u3002"}}
{"id": "2602.12393", "pdf": "https://arxiv.org/pdf/2602.12393", "abs": "https://arxiv.org/abs/2602.12393", "authors": ["Ali Subhan", "Ashir Raza"], "title": "Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "16 pages, 8 figures. Reproducibility study of DragDiffusion (CVPR 2024). Submitted to TMLR Reproducibility Challenge. Code available on GitHub", "summary": "DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.", "AI": {"tldr": "\u672c\u6587\u5bf9DragDiffusion\u65b9\u6cd5\u8fdb\u884c\u4e86\u53ef\u590d\u73b0\u6027\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u5176\u6838\u5fc3\u4e3b\u5f20\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u6027\u80fd\u5bf9\u5c11\u6570\u8d85\u53c2\u6570\uff08\u5982\u4f18\u5316\u7684\u65f6\u95f4\u6b65\u548c\u8fd0\u52a8\u76d1\u7763\u7279\u5f81\u5c42\u7ea7\uff09\u8f83\u4e3a\u654f\u611f\u3002", "motivation": "\u9a8c\u8bc1DragDiffusion\u8fd9\u4e00\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u70b9\u62d6\u62fd\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u7684\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u63a2\u7a76\u5176\u5173\u952e\u7ec4\u4ef6\u548c\u8d85\u53c2\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4f5c\u8005\u53d1\u5e03\u7684\u4ee3\u7801\u548cDragBench\u57fa\u51c6\uff0c\u590d\u73b0\u4e86\u5173\u4e8e\u6269\u6563\u65f6\u95f4\u6b65\u9009\u62e9\u3001LoRA\u5fae\u8c03\u3001\u63a9\u7801\u6b63\u5219\u5316\u5f3a\u5ea6\u548cUNet\u7279\u5f81\u76d1\u7763\u7684\u4e3b\u8981\u6d88\u878d\u5b9e\u9a8c\uff0c\u5e76\u989d\u5916\u8bc4\u4f30\u4e86\u4e00\u79cd\u591a\u65f6\u95f4\u6b65\u6f5c\u5728\u53d8\u91cf\u4f18\u5316\u7684\u53d8\u4f53\u3002", "result": "\u590d\u73b0\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u539f\u8bba\u6587\u62a5\u544a\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8d8b\u52bf\u9ad8\u5ea6\u4e00\u81f4\u3002\u7814\u7a76\u53d1\u73b0\u6027\u80fd\u5bf9\u4f18\u5316\u65f6\u95f4\u6b65\u548c\u8fd0\u52a8\u76d1\u7763\u7279\u5f81\u5c42\u7ea7\u7b49\u5c11\u6570\u8d85\u53c2\u6570\u654f\u611f\uff0c\u800c\u5176\u4ed6\u7ec4\u4ef6\u5219\u6709\u66f4\u5bbd\u6cdb\u7684\u6709\u6548\u8303\u56f4\u3002\u591a\u65f6\u95f4\u6b65\u4f18\u5316\u53d8\u4f53\u672a\u80fd\u63d0\u5347\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u53cd\u800c\u5927\u5e45\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u7814\u7a76\u603b\u4f53\u4e0a\u652f\u6301DragDiffusion\u7684\u6838\u5fc3\u4e3b\u5f20\uff0c\u5e76\u660e\u786e\u4e86\u5176\u53ef\u9760\u590d\u73b0\u6240\u9700\u7684\u5177\u4f53\u6761\u4ef6\u3002", "summary_cn": "DragDiffusion\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u70b9\u62d6\u62fd\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u76f4\u63a5\u62d6\u52a8\u9009\u5b9a\u7684\u70b9\u6765\u64cd\u7eb5\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u58f0\u79f0\uff0c\u901a\u8fc7\u5728\u4e2d\u95f4\u65f6\u95f4\u6b65\u4f18\u5316\u5355\u4e2a\u6269\u6563\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u7ed3\u5408\u8eab\u4efd\u4fdd\u6301\u5fae\u8c03\u548c\u7a7a\u95f4\u6b63\u5219\u5316\uff0c\u53ef\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\u3002\u672c\u5de5\u4f5c\u5229\u7528\u4f5c\u8005\u53d1\u5e03\u7684\u5b9e\u73b0\u4ee3\u7801\u548cDragBench\u57fa\u51c6\uff0c\u5bf9DragDiffusion\u8fdb\u884c\u4e86\u53ef\u590d\u73b0\u6027\u7814\u7a76\u3002\u6211\u4eec\u590d\u73b0\u4e86\u5173\u4e8e\u6269\u6563\u65f6\u95f4\u6b65\u9009\u62e9\u3001\u57fa\u4e8eLoRA\u7684\u5fae\u8c03\u3001\u63a9\u7801\u6b63\u5219\u5316\u5f3a\u5ea6\u548cUNet\u7279\u5f81\u76d1\u7763\u7684\u4e3b\u8981\u6d88\u878d\u7814\u7a76\uff0c\u89c2\u5bdf\u5230\u7684\u7ed3\u679c\u4e0e\u539f\u4f5c\u62a5\u544a\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u8d8b\u52bf\u975e\u5e38\u543b\u5408\u3002\u540c\u65f6\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6027\u80fd\u5bf9\u5c11\u91cf\u8d85\u53c2\u6570\u5047\u8bbe\uff08\u7279\u522b\u662f\u4f18\u5316\u7684\u65f6\u95f4\u6b65\u548c\u7528\u4e8e\u8fd0\u52a8\u76d1\u7763\u7684\u7279\u5f81\u5c42\u7ea7\uff09\u8f83\u4e3a\u654f\u611f\uff0c\u800c\u5176\u4ed6\u7ec4\u4ef6\u5219\u5177\u6709\u66f4\u5bbd\u6cdb\u7684\u6709\u6548\u64cd\u4f5c\u8303\u56f4\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc4\u4f30\u4e86\u4e00\u79cd\u591a\u65f6\u95f4\u6b65\u6f5c\u5728\u53d8\u91cf\u4f18\u5316\u7684\u53d8\u4f53\uff0c\u53d1\u73b0\u5b83\u5e76\u672a\u63d0\u9ad8\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u53cd\u800c\u663e\u8457\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u652f\u6301DragDiffusion\u7684\u6838\u5fc3\u4e3b\u5f20\uff0c\u540c\u65f6\u9610\u660e\u4e86\u5176\u53ef\u9760\u590d\u73b0\u7684\u6761\u4ef6\u3002\u4ee3\u7801\u53ef\u5728https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge\u83b7\u53d6\u3002"}}
{"id": "2602.12395", "pdf": "https://arxiv.org/pdf/2602.12395", "abs": "https://arxiv.org/abs/2602.12395", "authors": ["Xirui Li", "Ming Li", "Tianyi Zhou"], "title": "What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u201c\u5f17\u5170\u80af\u65af\u5766\u5f0f\u201d\u5206\u6790\u6846\u67b6\u63ed\u793a\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u5e76\u975e\u5168\u9762\u63d0\u5347\u89c6\u89c9\u611f\u77e5\uff0c\u800c\u662f\u7cfb\u7edf\u6027\u5730\u4f18\u5316Transformer\u4e2d\u540e\u5c42\u7684\u8ba1\u7b97\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9\u5230\u63a8\u7406\u7684\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96be\u4ee5\u5398\u6e05\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u76f8\u8f83\u4e8e\u76d1\u7763\u5fae\u8c03\uff08\u4f5c\u4e3a\u51b7\u542f\u52a8\u521d\u59cb\u5316\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5177\u4f53\u63d0\u5347\u4e86\u54ea\u4e9b\u80fd\u529b\uff0c\u56e0\u4e3a\u7aef\u5230\u7aef\u57fa\u51c6\u6d4b\u8bd5\u7684\u63d0\u5347\u6df7\u6742\u4e86\u591a\u79cd\u56e0\u7d20\uff0c\u65e0\u6cd5\u5f52\u56e0\u4e8e\u7279\u5b9a\u6280\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u201c\u5f17\u5170\u80af\u65af\u5766\u5f0f\u201d\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ec\uff1a(i) \u901a\u8fc7\u56e0\u679c\u63a2\u9488\u8fdb\u884c\u529f\u80fd\u5b9a\u4f4d\uff1b(ii) \u901a\u8fc7\u53c2\u6570\u6bd4\u8f83\u523b\u753b\u66f4\u65b0\u7279\u5f81\uff1b(iii) \u901a\u8fc7\u6a21\u578b\u5408\u5e76\u8fdb\u884c\u53ef\u8fc1\u79fb\u6027\u6d4b\u8bd5\u3002", "result": "RL\u4e3b\u8981\u5728\u63a8\u7406\u9636\u6bb5\u5f15\u8d77\u6a21\u578b\u4e2d\u540e\u5c42\u7684\u4e00\u81f4\u6027\u53d8\u5316\uff1b\u8fd9\u4e9b\u4e2d\u540e\u5c42\u7684\u6539\u8fdb\u65e2\u53ef\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u8fc1\u79fb\uff0c\u4e5f\u53ef\u901a\u8fc7\u51bb\u7ed3\u9a8c\u8bc1\u5176\u5bf9RL\u6027\u80fd\u63d0\u5347\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "RL\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u8d21\u732e\u662f\u7cfb\u7edf\u6027\u5730\u4f18\u5316Transformer\u4e2d\u540e\u5c42\u7684\u8ba1\u7b97\uff0c\u4ee5\u6539\u5584\u89c6\u89c9\u5230\u63a8\u7406\u7684\u5bf9\u9f50\u548c\u63a8\u7406\u6027\u80fd\uff0c\u800c\u975e\u5747\u5300\u63d0\u5347\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u8fd9\u51f8\u663e\u4e86\u4ec5\u4f9d\u8d56\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u6539\u8fdb\u7684\u5c40\u9650\u6027\u3002", "summary_cn": "\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5df2\u6210\u4e3a\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u6807\u51c6\u540e\u8bad\u7ec3\u9636\u6bb5\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u4e0e\u4f5c\u4e3a\u51b7\u542f\u52a8\u521d\u59cb\u5316\uff08IN\uff09\u7684\u76d1\u7763\u5fae\u8c03\u76f8\u6bd4\uff0cRL\u7a76\u7adf\u63d0\u5347\u4e86\u54ea\u4e9b\u80fd\u529b\u3002\u7aef\u5230\u7aef\u57fa\u51c6\u6d4b\u8bd5\u7684\u63d0\u5347\u6df7\u6742\u4e86\u591a\u79cd\u56e0\u7d20\uff0c\u96be\u4ee5\u5c06\u6539\u8fdb\u5f52\u56e0\u4e8e\u7279\u5b9a\u6280\u80fd\u3002\u4e3a\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5f17\u5170\u80af\u65af\u5766\u5f0f\u201d\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ec\uff1a(i) \u901a\u8fc7\u56e0\u679c\u63a2\u9488\u8fdb\u884c\u529f\u80fd\u5b9a\u4f4d\uff1b(ii) \u901a\u8fc7\u53c2\u6570\u6bd4\u8f83\u523b\u753b\u66f4\u65b0\u7279\u5f81\uff1b\u4ee5\u53ca (iii) \u901a\u8fc7\u6a21\u578b\u5408\u5e76\u8fdb\u884c\u53ef\u8fc1\u79fb\u6027\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0cRL\u4e3b\u8981\u5728\u63a8\u7406\u9636\u6bb5\u5f15\u8d77\u6a21\u578b\u4e2d\u540e\u5c42\u7684\u4e00\u81f4\u6027\u53d8\u5316\uff0c\u4e14\u8fd9\u4e9b\u4e2d\u540e\u5c42\u7684\u6539\u8fdb\u65e2\u53ef\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u8fc1\u79fb\uff0c\u4e5f\u53ef\u901a\u8fc7\u51bb\u7ed3\u9a8c\u8bc1\u5176\u5bf9RL\u6027\u80fd\u63d0\u5347\u7684\u5fc5\u8981\u6027\u3002\u603b\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cRL\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u8d21\u732e\u5e76\u975e\u5bf9\u89c6\u89c9\u611f\u77e5\u7684\u5747\u5300\u589e\u5f3a\uff0c\u800c\u662f\u5bf9Transformer\u4e2d\u540e\u5c42\u8ba1\u7b97\u7684\u7cfb\u7edf\u6027\u4f18\u5316\uff0c\u4ece\u800c\u6539\u5584\u4e86\u89c6\u89c9\u5230\u63a8\u7406\u7684\u5bf9\u9f50\u548c\u63a8\u7406\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u4ec5\u4f9d\u8d56\u57fa\u51c6\u6d4b\u8bd5\u6765\u7406\u89e3\u591a\u6a21\u6001\u63a8\u7406\u6539\u8fdb\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.12401", "pdf": "https://arxiv.org/pdf/2602.12401", "abs": "https://arxiv.org/abs/2602.12401", "authors": ["Zihan Ye", "Shreyank N Gowda", "Kaile Du", "Weijian Luo", "Ling Shao"], "title": "ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning", "categories": ["cs.CV"], "comment": "Under review", "summary": "Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faZeroDiff++\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6269\u6563\u589e\u5f3a\u3001\u76d1\u7763\u5bf9\u6bd4\u8868\u793a\u3001\u591a\u89c6\u89d2\u5224\u522b\u5668\u4ee5\u53ca\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u751f\u6210\u7b56\u7565\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u73b0\u6709\u751f\u6210\u5f0fZSL\u65b9\u6cd5\u4e2d\u56e0\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u7684\u865a\u5047\u89c6\u89c9-\u8bed\u4e49\u5173\u8054\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3\u6837\u672c\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5bb9\u6613\u5b66\u4e60\u5230\u865a\u5047\u7684\u89c6\u89c9-\u8bed\u4e49\u5173\u8054\uff0c\u4e14\u5176\u751f\u6210\u5668\u65e0\u6cd5\u81ea\u9002\u5e94\u5730\u751f\u6210\u4e0e\u771f\u5b9e\u6d4b\u8bd5\u6837\u672c\u76f8\u5173\u7684\u7279\u5f81\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faZeroDiff++\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u6269\u6563\u589e\u5f3a\u3001\u76d1\u7763\u5bf9\u6bd4\u8868\u793a\u548c\u591a\u89c6\u89d2Wasserstein\u4e92\u5b66\u4e60\u5224\u522b\u5668\uff1b\u751f\u6210\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u6269\u6563\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08DiffTTA\uff09\u548c\u6d4b\u8bd5\u65f6\u751f\u6210\uff08DiffGen\uff09\u7b56\u7565\uff0c\u4ee5\u8fde\u63a5\u771f\u5b9e\u4e0e\u751f\u6210\u6570\u636e\u5e76\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2aZSL\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cZeroDiff++\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u673a\u5236\uff0cZeroDiff++\u6709\u6548\u589e\u5f3a\u4e86\u89c6\u89c9-\u8bed\u4e49\u5173\u8054\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u751f\u6210\u5f0f\u96f6\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u96f6\u6837\u672c\u5b66\u4e60\uff08ZSL\uff09\u4f7f\u5206\u7c7b\u5668\u80fd\u591f\u8bc6\u522b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\uff0c\u901a\u5e38\u901a\u8fc7\u751f\u6210\u5f0f\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\uff1a\uff081\uff09\u4ece\u5df2\u89c1\u7c7b\u522b\u4e2d\u5b66\u4e60\u89c6\u89c9\u4e0e\u8bed\u4e49\u4e4b\u95f4\u7684\u5173\u8054\uff1b\uff082\uff09\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u5408\u6210\u672a\u89c1\u7c7b\u522b\u7684\u7279\u5f81\u4ee5\u8bad\u7ec3\u5206\u7c7b\u5668\u3002\u672c\u6587\u6307\u51fa\uff0c\u73b0\u6709\u751f\u6210\u5f0fZSL\u65b9\u6cd5\u5728\u5df2\u89c1\u7c7b\u522b\u6837\u672c\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u4f1a\u52a0\u5267\u865a\u5047\u7684\u89c6\u89c9-\u8bed\u4e49\u5173\u8054\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u6307\u6807\u6765\u91cf\u5316\u5df2\u89c1\u548c\u672a\u89c1\u7c7b\u522b\u4e2d\u7684\u8fd9\u79cd\u865a\u5047\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6307\u51fa\u4e00\u4e2a\u66f4\u5173\u952e\u7684\u74f6\u9888\uff1a\u73b0\u6709\u751f\u6210\u5668\u91c7\u7528\u975e\u81ea\u9002\u5e94\u7684\u5168\u566a\u58f0\u751f\u6210\u65b9\u5f0f\uff0c\u6240\u751f\u6210\u7684\u7279\u5f81\u4e0e\u771f\u5b9e\u6d4b\u8bd5\u6837\u672c\u8131\u8282\uff0c\u4e5f\u5bfc\u81f4\u4e86\u865a\u5047\u5173\u8054\u3002\u4e3a\u589e\u5f3a\u5df2\u89c1\u548c\u672a\u89c1\u7c7b\u522b\u7684\u89c6\u89c9-\u8bed\u4e49\u5173\u8054\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ZeroDiff++\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\u3002\u5728\u8bad\u7ec3\u9636\u6bb5\uff0cZeroDiff++\u91c7\u7528\uff08i\uff09\u6269\u6563\u589e\u5f3a\u4ee5\u751f\u6210\u591a\u6837\u5316\u7684\u52a0\u566a\u6837\u672c\uff0c\uff08ii\uff09\u76d1\u7763\u5bf9\u6bd4\uff08SC\uff09\u8868\u793a\u4ee5\u83b7\u53d6\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u4fe1\u606f\uff0c\u4ee5\u53ca\uff08iii\uff09\u7ed3\u5408Wasserstein\u4e92\u5b66\u4e60\u7684\u591a\u89c6\u89d2\u5224\u522b\u5668\u6765\u8bc4\u4f30\u751f\u6210\u7279\u5f81\u3002\u5728\u751f\u6210\u9636\u6bb5\uff0c\u6211\u4eec\u5f15\u5165\uff08iv\uff09\u57fa\u4e8e\u6269\u6563\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08DiffTTA\uff09\uff0c\u5229\u7528\u4f2a\u6807\u7b7e\u91cd\u5efa\u6765\u8c03\u6574\u751f\u6210\u5668\uff0c\u4ee5\u53ca\uff08v\uff09\u57fa\u4e8e\u6269\u6563\u7684\u6d4b\u8bd5\u65f6\u751f\u6210\uff08DiffGen\uff09\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6269\u6563\u53bb\u566a\u8def\u5f84\u751f\u6210\u90e8\u5206\u5408\u6210\u7279\u5f81\uff0c\u4ece\u800c\u8fde\u63a5\u771f\u5b9e\u4e0e\u751f\u6210\u6570\u636e\uff0c\u5e76\u8fdb\u4e00\u6b65\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u5728\u4e09\u4e2aZSL\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cZeroDiff++\u4e0d\u4ec5\u663e\u8457\u4f18\u4e8e\u73b0\u6709ZSL\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2602.12403", "pdf": "https://arxiv.org/pdf/2602.12403", "abs": "https://arxiv.org/abs/2602.12403", "authors": ["Ali Nasiri-Sarvi", "Anh Tien Nguyen", "Hassan Rivaz", "Dimitris Samaras", "Mahdi S. Hosseini"], "title": "MonoLoss: A Training Objective for Interpretable Monosemantic Representations", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b21\u904d\u5386\u7b97\u6cd5\u6765\u8ba1\u7b97MonoScore\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86Monosemanticity Loss\uff08MonoLoss\uff09\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u4e2d\u76f4\u63a5\u63d0\u5347\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u5b66\u5230\u7684\u7279\u5f81\u7684\u5355\u8bed\u4e49\u6027\uff0c\u663e\u8457\u52a0\u901f\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u7eaf\u5ea6\u3002", "motivation": "\u6807\u51c6\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u8bad\u7ec3\u76ee\u6807\u5bf9\u5206\u89e3\u591a\u4e49\u795e\u7ecf\u8868\u5f81\uff08polysemantic representations\uff09\u4e3a\u5355\u4e49\u7279\u5f81\uff08monosemantic features\uff09\u7684\u6fc0\u52b1\u8f83\u5f31\uff0c\u4e14\u73b0\u6709\u5355\u4e49\u6027\u5ea6\u91cf\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5728\u8bad\u7ec3\u4e2d\u9ad8\u6548\u4f7f\u7528\u3002", "method": "\u4f5c\u8005\u6539\u8fdb\u4e86\u8fd1\u671f\u63d0\u51fa\u7684MonoScore\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5355\u6b21\u904d\u5386\u7b97\u6cd5\uff0c\u4f7f\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u4e8c\u6b21\u65b9\u964d\u81f3\u7ebf\u6027\uff1b\u5e76\u4ee5\u6b64\u4e3a\u57fa\u7840\u63d0\u51fa\u4e86Monosemanticity Loss\uff08MonoLoss\uff09\uff0c\u4f5c\u4e3a\u53ef\u63d2\u62d4\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u76f4\u63a5\u9f13\u52b1\u8bed\u4e49\u4e00\u81f4\u7684\u6fc0\u6d3b\u3002\u8be5\u635f\u5931\u51fd\u6570\u88ab\u7528\u4e8e\u591a\u79cdSAE\u67b6\u6784\uff08\u5982BatchTopK\u3001TopK\u3001JumpReLU\uff09\u53ca\u4e0d\u540c\u89c6\u89c9\u6a21\u578b\uff08CLIP\u3001SigLIP2\u3001ViT\uff09\u7684\u7279\u5f81\u4e0a\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f5c\u4e3a\u8f85\u52a9\u6b63\u5219\u9879\u7528\u4e8eResNet-50\u548cCLIP-ViT-B/32\u7684\u5fae\u8c03\u3002", "result": "\u5728OpenImagesV7\u4e0a\uff0c\u65b0\u7b97\u6cd5\u5728\u8bc4\u4f30\u9636\u6bb5\u5b9e\u73b0\u6700\u9ad81200\u500d\u3001\u8bad\u7ec3\u9636\u6bb5159\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u6bcf\u8f6e\u4ec5\u589e\u52a0\u7ea64%\u5f00\u9500\u3002MonoLoss\u5728\u5927\u591a\u6570\u6f5c\u5728\u53d8\u91cf\u4e0a\u63d0\u5347\u4e86MonoScore\uff0c\u5e76\u5728\u6240\u6709\u7f16\u7801\u5668\u4e0eSAE\u7ec4\u5408\u4e2d\u4e00\u81f4\u63d0\u9ad8\u4e86\u7c7b\u7eaf\u5ea6\uff08class purity\uff09\uff0c\u6700\u5927\u63d0\u5347\u4ece0.152\u52300.723\u3002\u5728ImageNet-1K\u5fae\u8c03\u4e2d\u5e26\u6765\u6700\u9ad80.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4ea7\u751f\u66f4\u6e05\u6670\u7684\u5355\u4e49\u6fc0\u6d3b\u6a21\u5f0f\u3002", "conclusion": "MonoLoss\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u80fd\u663e\u8457\u63d0\u5347SAE\u6240\u5b66\u7279\u5f81\u7684\u5355\u4e49\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u517c\u5bb9\u591a\u79cd\u6a21\u578b\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u8868\u793a\u5b66\u4e60\u4e0e\u6a21\u578b\u5fae\u8c03\u573a\u666f\u3002", "summary_cn": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u80fd\u591f\u5c06\u591a\u4e49\u795e\u7ecf\u8868\u5f81\uff08\u5373\u5355\u4e2a\u795e\u7ecf\u5143\u54cd\u5e94\u591a\u4e2a\u65e0\u5173\u6982\u5ff5\uff09\u5206\u89e3\u4e3a\u6355\u6349\u5355\u4e00\u3001\u53ef\u89e3\u91ca\u6982\u5ff5\u7684\u5355\u4e49\u7279\u5f81\u3002\u7136\u800c\uff0c\u6807\u51c6\u8bad\u7ec3\u76ee\u6807\u5bf9\u6b64\u7c7b\u5206\u89e3\u7684\u6fc0\u52b1\u4f5c\u7528\u8f83\u5f31\uff0c\u4e14\u73b0\u6709\u7684\u5355\u4e49\u6027\u5ea6\u91cf\u65b9\u6cd5\u9700\u8981\u5bf9\u6570\u636e\u96c6\u4e2d\u6240\u6709\u6837\u672c\u8fdb\u884c\u4e24\u4e24\u6bd4\u8f83\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u8bc4\u4f30\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u6587\u7814\u7a76\u4e86\u8fd1\u671f\u63d0\u51fa\u7684MonoScore\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u79cd\u5355\u6b21\u904d\u5386\u7b97\u6cd5\uff0c\u53ef\u5728\u4fdd\u6301\u7ed3\u679c\u5b8c\u5168\u4e00\u81f4\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u8ba1\u7b97\u6210\u672c\u4ece\u968f\u56fe\u50cf\u6570\u91cf\u5448\u4e8c\u6b21\u65b9\u589e\u957f\u964d\u4f4e\u4e3a\u7ebf\u6027\u589e\u957f\u3002\u5728OpenImagesV7\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u9636\u6bb5\u6700\u591a\u5b9e\u73b01200\u500d\u7684\u65f6\u949f\u65f6\u95f4\u52a0\u901f\uff0c\u8bad\u7ec3\u9636\u6bb5\u52a0\u901f\u8fbe159\u500d\uff0c\u4e14\u6bcf\u8f6e\u8bad\u7ec3\u4ec5\u589e\u52a0\u7ea64%\u7684\u989d\u5916\u5f00\u9500\u3002\u8fd9\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u5c06MonoScore\u7528\u4f5c\u8bad\u7ec3\u4fe1\u53f7\uff1a\u6211\u4eec\u63d0\u51fa\u4e86\u5355\u4e49\u6027\u635f\u5931\uff08Monosemanticity Loss, MonoLoss\uff09\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u76ee\u6807\u51fd\u6570\uff0c\u76f4\u63a5\u5956\u52b1\u8bed\u4e49\u4e00\u81f4\u7684\u6fc0\u6d3b\uff0c\u4ee5\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u5355\u4e49\u8868\u5f81\u3002\u5728CLIP\u3001SigLIP2\u548c\u9884\u8bad\u7ec3ViT\u7279\u5f81\u4e0a\u8bad\u7ec3\u7684\u5404\u7c7bSAE\uff08\u5305\u62ecBatchTopK\u3001TopK\u548cJumpReLU\uff09\u4e2d\uff0cMonoLoss\u63d0\u5347\u4e86\u5927\u591a\u6570\u6f5c\u5728\u53d8\u91cf\u7684MonoScore\u3002\u6b64\u5916\uff0c\u5728\u6240\u6709\u7f16\u7801\u5668\u4e0eSAE\u7ec4\u5408\u4e2d\uff0cMonoLoss\u59cb\u7ec8\u63d0\u9ad8\u4e86\u7c7b\u7eaf\u5ea6\uff08\u5373\u67d0\u4e2a\u6f5c\u5728\u53d8\u91cf\u6fc0\u6d3b\u7684\u56fe\u50cf\u4e2d\u5c5e\u4e8e\u5176\u4e3b\u5bfc\u7c7b\u522b\u7684\u6bd4\u4f8b\uff09\uff0c\u6700\u5927\u63d0\u5347\u5c06\u57fa\u7ebf\u7eaf\u5ea6\u4ece0.152\u63d0\u9ad8\u81f30.723\u3002\u5728ResNet-50\u548cCLIP-ViT-B/32\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u5c06MonoLoss\u4f5c\u4e3a\u8f85\u52a9\u6b63\u5219\u9879\u4f7f\u7528\uff0c\u53ef\u5728ImageNet-1K\u4e0a\u5e26\u6765\u6700\u9ad80.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4ea7\u751f\u6e05\u6670\u7684\u5355\u4e49\u6fc0\u6d3b\u6a21\u5f0f\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u53d1\u5e03\u4e8e https://github.com/AtlasAnalyticsLab/MonoLoss\u3002"}}
{"id": "2602.12441", "pdf": "https://arxiv.org/pdf/2602.12441", "abs": "https://arxiv.org/abs/2602.12441", "authors": ["Lihe Liu", "Xiaoxi Pan", "Yinyin Yuan", "Lulu Shang"], "title": "Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction", "categories": ["cs.CV"], "comment": null, "summary": "Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPathoSpatial\uff0c\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u878d\u5408\u914d\u51c6\u7684\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\uff08ST\uff09\u6570\u636e\uff0c\u4ee5\u5b66\u4e60\u5177\u6709\u7a7a\u95f4\u4fe1\u606f\u7684\u9884\u540e\u8868\u5f81\uff0c\u5728\u591a\u4e2a\u751f\u5b58\u7ec8\u70b9\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u652f\u6301\u539f\u578b\u89e3\u91ca\u548c\u5206\u5b50\u98ce\u9669\u5206\u89e3\u3002", "motivation": "\u968f\u7740\u914d\u5bf9\u7684WSI-ST\u961f\u5217\u6269\u5c55\u5230\u4eba\u7fa4\u89c4\u6a21\uff0c\u5229\u7528\u5176\u4e92\u8865\u7684\u7a7a\u95f4\u4fe1\u53f7\u8fdb\u884c\u9884\u540e\u5efa\u6a21\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u8be5\u8303\u5f0f\u7684\u6709\u6548\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\u3002", "method": "PathoSpatial\u91c7\u7528\u591a\u5c42\u7ea7\u4e13\u5bb6\u67b6\u6784\uff0c\u7ed3\u5408\u4efb\u52a1\u5f15\u5bfc\u7684\u539f\u578b\u5b66\u4e60\uff0c\u81ea\u9002\u5e94\u5730\u534f\u8c03\u6a21\u6001\u5185\u65e0\u76d1\u7763\u53d1\u73b0\u4e0e\u8de8\u6a21\u6001\u6709\u76d1\u7763\u805a\u5408\uff0c\u6574\u5408\u5171\u914d\u51c6\u7684WSI\u548cST\u6570\u636e\u3002", "result": "\u5728\u4e09\u9634\u6027\u4e73\u817a\u764c\u961f\u5217\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPathoSpatial\u5728\u4e94\u4e2a\u751f\u5b58\u7ec8\u70b9\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u52b2\u4e14\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6216\u5ab2\u7f8e\u5f53\u524d\u9886\u5148\u7684\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u4e8b\u540e\u539f\u578b\u89e3\u91ca\u548c\u5206\u5b50\u98ce\u9669\u5206\u89e3\u3002", "conclusion": "PathoSpatial\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u7ec4\u5b66\u4e0e\u75c5\u7406\u5b66\u7684\u878d\u5408\u3002", "summary_cn": "\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u901a\u8fc7\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5b9e\u73b0\u5f31\u76d1\u7763\u9884\u540e\u5efa\u6a21\u3002\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u4fdd\u7559\u539f\u4f4d\u57fa\u56e0\u8868\u8fbe\uff0c\u63d0\u4f9b\u8865\u5145\u5f62\u6001\u5b66\u7684\u7a7a\u95f4\u5206\u5b50\u80cc\u666f\u3002\u968f\u7740\u914d\u5bf9\u7684WSI-ST\u961f\u5217\u6269\u5c55\u81f3\u4eba\u7fa4\u89c4\u6a21\uff0c\u5229\u7528\u5176\u4e92\u8865\u7684\u7a7a\u95f4\u4fe1\u53f7\u8fdb\u884c\u9884\u540e\u5206\u6790\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff1b\u7136\u800c\uff0c\u9488\u5bf9\u8fd9\u4e00\u8303\u5f0f\u4ecd\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PathoSpatial\u2014\u2014\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u5171\u914d\u51c6\u7684WSI\u4e0eST\u6570\u636e\uff0c\u4ee5\u5b66\u4e60\u5177\u6709\u7a7a\u95f4\u4fe1\u606f\u7684\u9884\u540e\u8868\u5f81\u3002PathoSpatial\u5728\u591a\u5c42\u7ea7\u4e13\u5bb6\u67b6\u6784\u4e2d\u91c7\u7528\u4efb\u52a1\u5f15\u5bfc\u7684\u539f\u578b\u5b66\u4e60\uff0c\u81ea\u9002\u5e94\u5730\u534f\u8c03\u6a21\u6001\u5185\u7684\u65e0\u76d1\u7763\u53d1\u73b0\u4e0e\u8de8\u6a21\u6001\u7684\u6709\u76d1\u7763\u805a\u5408\u3002\u8be5\u8bbe\u8ba1\u5728\u4fdd\u6301\u5224\u522b\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u5305\u542b\u914d\u5bf9ST\u4e0eWSI\u7684\u4e09\u9634\u6027\u4e73\u817a\u764c\u961f\u5217\u4e0a\u8bc4\u4f30\u4e86PathoSpatial\u3002\u7ed3\u679c\u8868\u660e\uff0cPathoSpatial\u5728\u4e94\u4e2a\u751f\u5b58\u7ec8\u70b9\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u52b2\u4e14\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6216\u5ab2\u7f8e\u5f53\u524d\u9886\u5148\u7684\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u3002PathoSpatial\u5929\u7136\u652f\u6301\u4e8b\u540e\u539f\u578b\u89e3\u91ca\u548c\u5206\u5b50\u98ce\u9669\u5206\u89e3\uff0c\u63d0\u4f9b\u5b9a\u91cf\u4e14\u5177\u6709\u751f\u7269\u5b66\u4f9d\u636e\u7684\u89e3\u91ca\uff0c\u7a81\u51fa\u4e86\u6f5c\u5728\u7684\u9884\u540e\u56e0\u5b50\u3002\u6211\u4eec\u5c06PathoSpatial\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u5b66\u4e60\u5728\u7a7a\u95f4\u7ec4\u5b66-\u75c5\u7406\u5b66\u878d\u5408\u4e2d\u7684\u6982\u5ff5\u9a8c\u8bc1\u3002"}}
{"id": "2602.12461", "pdf": "https://arxiv.org/pdf/2602.12461", "abs": "https://arxiv.org/abs/2602.12461", "authors": ["Jiacheng Zhang", "Jinhao Li", "Hanxun Huang", "Sarah M. Erfani", "Benjamin I. P. Rubinstein", "Feng Liu"], "title": "Semantic-aware Adversarial Fine-tuning for CLIP", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u5bf9\u6297\u5fae\u8c03\u65b9\u6cd5\uff08SAFT\uff09\uff0c\u901a\u8fc7\u4f7f\u7528\u8bed\u4e49\u96c6\u6210\u653b\u51fb\u751f\u6210\u66f4\u6709\u6548\u7684\u5bf9\u6297\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5229\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u751f\u6210\u5bf9\u6297\u6837\u672c\u4ee5\u589e\u5f3aCLIP\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5355\u4e00\u56fe\u50cf\u4e0e\u624b\u5de5\u6a21\u677f\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0d\u8db3\u4ee5\u51c6\u786e\u8861\u91cf\u56fe\u6587\u5bf9\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u5728\u66f4\u4f18\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u4e0b\u5931\u6548\uff0c\u4ece\u800c\u9650\u5236\u4e86\u9c81\u68d2\u6027\u63d0\u5347\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u8bed\u4e49\u96c6\u6210\u653b\u51fb\uff1a\u9996\u5148\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684\u6587\u672c\u63cf\u8ff0\u4ee5\u6355\u6349\u6838\u5fc3\u8bed\u4e49\uff0c\u518d\u5bf9\u5176\u8fdb\u884c\u53bb\u5e7b\u89c9\u7cbe\u70bc\uff1b\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u63cf\u8ff0\u96c6\u5408\u7684\u5e73\u5747\u76f8\u4f3c\u5ea6\u6700\u5c0f\u5316\u6765\u751f\u6210\u8bed\u4e49\u611f\u77e5\u7684\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u4ee5\u6b64\u5bf9CLIP\u56fe\u50cf\u7f16\u7801\u5668\u8fdb\u884c\u5bf9\u6297\u5fae\u8c03\uff08SAFT\uff09\u3002", "result": "\u572816\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSAFT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u66f4\u4e30\u5bcc\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u6587\u672c\u63cf\u8ff0\u6765\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u80fd\u6709\u6548\u63d0\u5347CLIP\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u611f\u77e5\u5bf9\u6297\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "summary_cn": "\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5bf9CLIP\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u7801\u5668\u8fdb\u884c\u5bf9\u6297\u5fae\u8c03\uff08\u4f7f\u7528\u5bf9\u6297\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff09\uff0c\u53ef\u4ee5\u589e\u5f3a\u5176\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u5bf9\u6297\u6837\u672c\u901a\u8fc7\u6700\u5c0f\u5316\u56fe\u50cf\u4e0e\u624b\u5de5\u8bbe\u8ba1\u6a21\u677f\uff08\u4f8b\u5982\u201c\u4e00\u5f20{\u6807\u7b7e}\u7684\u7167\u7247\u201d\uff09\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u751f\u6210\u3002\u7136\u800c\uff0c\u5df2\u6709\u7814\u7a76\u6307\u51fa\uff0c\u5355\u5f20\u56fe\u50cf\u4e0e\u5355\u4e00\u624b\u5de5\u6a21\u677f\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0d\u8db3\u4ee5\u51c6\u786e\u8861\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u76f8\u4f3c\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u672c\u6587\u53d1\u73b0\uff1a\u5f53\u5c06\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u66ff\u6362\u4e3a\u8bed\u4e49\u66f4\u4e30\u5bcc\u7684\u66ff\u4ee3\u65b9\u6cd5\u65f6\uff0c\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u6b3a\u9a97CLIP\u6a21\u578b\uff0c\u5bfc\u81f4\u4f7f\u7528\u6b64\u7c7b\u5bf9\u6297\u6837\u672c\u5fae\u8c03\u540e\u7684\u56fe\u50cf\u7f16\u7801\u5668\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u6211\u4eec\u9996\u5148\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u96c6\u6210\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u539f\u59cb\u56fe\u50cf\u4e0e\u4e00\u7ec4\u7cbe\u70bc\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7684\u5e73\u5747\u76f8\u4f3c\u5ea6\uff0c\u751f\u6210\u8bed\u4e49\u611f\u77e5\u7684\u5bf9\u6297\u6837\u672c\u3002\u8fd9\u4e9b\u6587\u672c\u63cf\u8ff0\u9996\u5148\u7531\u57fa\u7840\u6a21\u578b\u751f\u6210\uff0c\u4ee5\u6355\u6349\u8d85\u8d8a\u624b\u5de5\u6a21\u677f\u7684\u6838\u5fc3\u8bed\u4e49\u7279\u5f81\uff0c\u968f\u540e\u7ecf\u8fc7\u7cbe\u70bc\u4ee5\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u4e49\u611f\u77e5\u5bf9\u6297\u5fae\u8c03\uff08SAFT\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e49\u611f\u77e5\u5bf9\u6297\u6837\u672c\u5bf9CLIP\u7684\u56fe\u50cf\u7f16\u7801\u5668\u8fdb\u884c\u5fae\u8c03\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSAFT\u572816\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u63d0\u5347\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/tmlr-group/SAFT\u3002"}}
{"id": "2602.12484", "pdf": "https://arxiv.org/pdf/2602.12484", "abs": "https://arxiv.org/abs/2602.12484", "authors": ["Md. Ehsanul Haque", "Md. Saymon Hosen Polash", "Rakib Hasan Ovi", "Aminul Kader Bulbul", "Md Kamrul Siam", "Tamim Hasan Saykat"], "title": "A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted and Presented at 28th International Conference on Computer and Information Technology (ICCIT)", "summary": "Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316DenseNet121\u7684\u8461\u8404\u53f6\u75c5\u5bb3\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u9884\u5904\u7406\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0899.27%\uff09\u7684\u540c\u65f6\u5b9e\u73b0\u8f83\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eYOLO\u7b49\u6846\u67b6\u7684\u81ea\u52a8\u5316\u8461\u8404\u75c5\u5bb3\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u8461\u8404\u53f6\u75c5\u5bb3\u8bc6\u522b\u65b9\u6cd5\u4ee5\u652f\u6301\u8461\u8404\u56ed\u7684\u53ef\u6301\u7eed\u7ba1\u7406\u3002", "method": "\u91c7\u7528\u4f18\u5316\u540e\u7684DenseNet121\u6a21\u578b\uff0c\u7ed3\u5408\u9488\u5bf9\u8461\u8404\u53f6\u56fe\u50cf\u7684\u9886\u57df\u7279\u5b9a\u9884\u5904\u7406\uff0c\u5229\u7528\u5bc6\u96c6\u8fde\u63a5\u63d0\u53d6\u75c5\u5bb3\u76f8\u5173\u7279\u5f81\uff08\u5982\u53f6\u8109\u3001\u8fb9\u7f18\u548c\u75c5\u6591\uff09\u3002\u4f7f\u7528Grad-CAM\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5e94\u5bf9\u5c0f\u6837\u672c\u548c\u4e0d\u5e73\u8861\u6570\u636e\u95ee\u9898\u3002", "result": "\u6240\u63d0\u6a21\u578b\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u523099.27%\u51c6\u786e\u7387\u300199.28% F1\u5206\u6570\u300199.71%\u7279\u5f02\u6027\u53ca98.86% Kappa\u7cfb\u6570\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a9\u79d2\uff1b\u4ea4\u53c9\u9a8c\u8bc1\u5e73\u5747\u51c6\u786e\u7387\u4e3a99.12%\uff0c\u4f18\u4e8eResNet18\u3001VGG16\u3001AlexNet\u548cSqueezeNet\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u9ad8\u6548\u67b6\u6784\u3001\u9886\u57df\u9884\u5904\u7406\u548c\u53ef\u89e3\u91ca\u8f93\u51fa\uff0c\u5728\u4fdd\u8bc1\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u8461\u8404\u53f6\u75c5\u5bb3\u7684\u5b9e\u65f6\u68c0\u6d4b\u3002", "summary_cn": "\u8461\u8404\u662f\u5168\u7403\u6700\u5177\u7ecf\u6d4e\u548c\u6587\u5316\u4ef7\u503c\u7684\u6c34\u679c\u4e4b\u4e00\uff0c\u6b27\u6d32\u548c\u4e9a\u6d32\u5927\u91cf\u751f\u4ea7\u9c9c\u98df\u8461\u8404\u548c\u8461\u8404\u9152\u3002\u7ec6\u83cc\u6027\u8150\u70c2\u3001\u971c\u9709\u75c5\u548c\u767d\u7c89\u75c5\u7b49\u8461\u8404\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u5176\u4ea7\u91cf\u4e0e\u54c1\u8d28\uff0c\u56e0\u6b64\uff0c\u8461\u8404\u56ed\u7684\u53ef\u6301\u7eed\u7ba1\u7406\u9700\u8981\u5bf9\u8fd9\u4e9b\u75c5\u5bb3\u8fdb\u884c\u65e9\u671f\u4e14\u7cbe\u51c6\u7684\u8bc6\u522b\u3002\u76ee\u524d\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u57fa\u4e8eYOLO\u6846\u67b6\u7684\u65b9\u6cd5\uff09\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316DenseNet121\u7684\u8461\u8404\u53f6\u75c5\u5bb3\u5206\u7c7b\u65b9\u6cd5\u3002\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u9884\u5904\u7406\u548c\u5bc6\u96c6\u8fde\u63a5\u7ed3\u6784\uff0c\u6709\u6548\u63ed\u793a\u4e86\u4e0e\u75c5\u5bb3\u76f8\u5173\u7684\u7279\u5f81\uff0c\u5982\u53f6\u8109\u3001\u8fb9\u7f18\u548c\u75c5\u6591\u3002\u4e0eResNet18\u3001VGG16\u3001AlexNet\u548cSqueezeNet\u7b49\u57fa\u51c6CNN\u6a21\u578b\u7684\u5e7f\u6cdb\u6bd4\u8f83\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u6027\u80fd\u66f4\u4f18\uff1a\u51c6\u786e\u7387\u8fbe99.27%\uff0cF1\u5206\u6570\u4e3a99.28%\uff0c\u7279\u5f02\u6027\u4e3a99.71%\uff0cKappa\u7cfb\u6570\u4e3a98.86%\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a9\u79d2\u3002\u4ea4\u53c9\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\u51fa99.12%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u8868\u660e\u6a21\u578b\u5728\u6240\u6709\u7c7b\u522b\u4e0a\u5747\u5177\u5907\u5f3a\u5065\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u91c7\u7528Grad-CAM\u6280\u672f\u7a81\u51fa\u663e\u793a\u75c5\u5bb3\u76f8\u5173\u533a\u57df\uff0c\u786e\u4fdd\u6a21\u578b\u5173\u6ce8\u751f\u7406\u4e0a\u76f8\u5173\u7684\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u5347\u900f\u660e\u5ea6\u4e0e\u53ef\u4fe1\u5ea6\u3002\u6a21\u578b\u4f18\u5316\u964d\u4f4e\u4e86\u5b9e\u65f6\u90e8\u7f72\u7684\u5904\u7406\u9700\u6c42\uff0c\u800c\u8fc1\u79fb\u5b66\u4e60\u5219\u4fdd\u969c\u4e86\u5728\u5c0f\u89c4\u6a21\u548c\u4e0d\u5e73\u8861\u6837\u672c\u4e0a\u7684\u7a33\u5b9a\u6027\u3002\u7efc\u4e0a\uff0c\u8be5\u6846\u67b6\u51ed\u501f\u9ad8\u6548\u7684\u67b6\u6784\u3001\u9886\u57df\u7279\u5b9a\u7684\u9884\u5904\u7406\u548c\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u7cbe\u5ea6\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u5ec9\u7684\u8461\u8404\u53f6\u75c5\u5bb3\u68c0\u6d4b\u3002"}}
