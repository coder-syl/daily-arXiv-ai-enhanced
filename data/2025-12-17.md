<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline](https://arxiv.org/abs/2512.13731)
*Weikang Bai,Yongkun Du,Yuchen Su,Yazhen Xie,Zhineng Chen*

Main category: cs.CV

TL;DR: 本文提出CMER-Bench基准、大规模复杂数学表达式数据集MER-17M和CMER-3M，以及新表示方法Structured Mathematical Language和专用模型CMERNet，在复杂数学表达式识别任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前数学表达式识别（MER）在处理包含大量符号和多行结构的复杂表达式时性能显著下降，主要原因是现有公开训练数据集中简单样本占主导，缺乏对复杂表达式的覆盖。

Method: 作者构建了按难度分级的CMER-Bench基准，并提出了两个强调复杂表达式的大规模数据集MER-17M和CMER-3M；同时设计了一种新的表达式分词器和名为Structured Mathematical Language的结构化表示方法，以显式建模表达式的层次与空间布局；在此基础上，开发了基于编码器-解码器架构的专用模型CMERNet，并在CMER-3M上进行训练。

Result: 实验表明，仅含1.25亿参数的CMERNet在CMER-Bench上显著优于现有的MER模型和通用多模态大语言模型（MLLMs），特别是在复杂表达式识别任务上。

Conclusion: 针对复杂数学表达式识别的挑战，本文通过构建高质量基准与数据集、提出结构化表示方法及专用模型，有效提升了MER系统在复杂场景下的准确性和鲁棒性。

Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.

Abstract (中文翻译): 数学表达式识别（MER）在识别简单表达式方面已取得显著进展，但对包含大量符号和多行结构的复杂数学表达式的鲁棒识别仍是一项重大挑战。本文首先提出了CMER-Bench，这是一个精心构建的基准，将表达式划分为易、中、难三个难度等级。利用CMER-Bench，我们对现有的MER模型和通用多模态大语言模型（MLLMs）进行了全面评估。结果表明，当前方法在易、中等难度表达式上表现良好，但在处理复杂表达式时性能显著下降，主要原因是现有公开训练数据集主要由简单样本构成。为此，我们提出了MER-17M和CMER-3M两个大规模数据集，重点强调复杂数学表达式的识别，为开发高精度、鲁棒的复杂MER模型提供丰富多样的样本支持。此外，为应对复杂表达式复杂空间布局带来的挑战，我们引入了一种新颖的表达式分词器和一种称为“结构化数学语言”（Structured Mathematical Language）的新表示方法，该方法能显式建模超越LaTeX格式的表达式层次结构与空间关系。基于上述成果，我们提出了一种名为CMERNet的专用模型，该模型采用编码器-解码器架构，并在CMER-3M上进行训练。实验结果表明，仅含1.25亿参数的CMERNet在CMER-Bench上显著优于现有的MER模型和MLLMs。

</details>


### [2] [Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage](https://arxiv.org/abs/2512.13739)
*Yajie Yang,Yuqing Zhao,Xiaochao Xi,Yinan Zhu*

Main category: cs.CV

TL;DR: 本文探讨了在新闻特稿中可控AIGC图像生成的路径，通过两个实验揭示现有工具在语义对齐、文化特异性和视觉真实性方面的不足，并提出一种结合高精度分割、语义对齐与风格调控的人机协同模块化流程，以提升内容准确性与可信度。


<details>
  <summary>Details</summary>
Motivation: AIGC辅助图像生成在新闻业引发争议，主要因其“黑箱”特性难以满足新闻对内容准确性与语义一致性的双重需求，并带来伦理、社会技术及信任方面的挑战。

Method: 开展两项实验：(1) 通过标准化提示测试三大平台在三类场景下的跨平台适应性；(2) 构建人机协同的模块化流程，整合SAM/GroundingDINO（高精度分割）、BrushNet（语义对齐）和Style-LoRA/Prompt-to-Prompt（风格调控），并通过CLIP语义评分、NSFW/OCR/YOLO过滤及可验证内容凭证保障编辑忠实度。

Result: 实验1发现训练语料偏差和平台级过滤导致语义对齐、文化特异性和视觉真实性的显著差异；实验2验证了所提模块化流程能有效保留语义表征并实现可追溯部署。

Conclusion: 提出适用于新闻特稿的人-AI协作机制，并建议从角色身份稳定性（CIS）、文化表达准确性（CEA）和公众用户适宜性（U-PA）三个维度评估AIGC图像生成质量。

Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).

Abstract (中文翻译): 人工智能生成内容（AIGC）辅助图像制作在新闻业中引发争议，同时受到媒体机构的关注。关键问题包括虚假信息、真实性、语义保真度和可解释性。大多数AIGC工具是不透明的“黑箱”，难以满足内容准确性与语义一致性的双重需求，从而引发伦理、社会技术及信任困境。本文探索了在新闻特稿中实现可控图像生成的路径，并基于中国媒体机构的项目开展了两项实验：（1）实验一通过在三种场景中使用标准化提示测试跨平台适应性，揭示了由训练语料偏差和平台级过滤所导致的语义对齐、文化特异性和视觉真实性方面的差异；（2）实验二构建了一种人在回路的模块化流程，结合高精度分割（SAM、GroundingDINO）、语义对齐（BrushNet）和风格调控（Style-LoRA、Prompt-to-Prompt），并通过基于CLIP的语义评分、NSFW/OCR/YOLO过滤以及可验证的内容凭证确保编辑忠实度，实现可追溯的语义表征保留。据此，本文提出了一种面向新闻特稿的AIGC辅助图像生成人机协作机制，并建议从角色身份稳定性（CIS）、文化表达准确性（CEA）和用户-公众适宜性（U-PA）三个维度进行评估。

</details>


### [3] [DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](https://arxiv.org/abs/2512.13742)
*Md. Najib Hasan,Imran Ahmad,Sourav Basak Shuvo,Md. Mahadi Hasan Ankon,Sunanda Das,Nazmul Siddique,Hui Wang*

Main category: cs.CV

TL;DR: 本文提出一个结合深度学习图像分类器（MobileCoAtNet）与大语言模型（LLMs）的框架，用于生成结构化的临床推理解释。尽管强分类性能提升了LLM解释质量，但所有测试的32个LLM在专家验证的医学推理基准上均未达到人类稳定性水平，提示当前LLM尚不可靠用于高风险医疗决策。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类器缺乏可解释性，而大语言模型虽能生成临床文本，但在视觉推理方面表现不佳且解释不稳定，导致模型输出与临床医生期望的推理之间存在差距。

Method: 设计一种新型混合模型MobileCoAtNet用于内窥镜图像分类，其输出驱动多个大语言模型生成临床推理；构建两个由专家验证的涵盖病因、症状、治疗、生活方式和随访的推理基准，评估32个LLM的表现。

Result: MobileCoAtNet在八类胃部疾病分类中达到高准确率；强分类结果提升了LLM生成解释的质量，但所有LLM在推理稳定性上均未达人类水平，且对提示变化敏感。

Conclusion: 结合深度学习与大语言模型可生成有用的临床叙述，但当前LLM在高风险医疗决策中仍不可靠；所提框架有助于揭示其局限性，并为构建更安全的推理系统提供路径。

Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.

Abstract (中文翻译): 医学图像分类器在检测胃肠道疾病方面表现良好，但无法解释其决策过程。大型语言模型虽能生成临床文本，却在视觉推理方面存在困难，且常产生不稳定或错误的解释。这导致模型所“看到”的内容与临床医生所期望的推理类型之间存在差距。我们提出一个将图像分类与结构化临床推理相结合的框架。为此，我们设计了一种用于内窥镜图像的新混合模型MobileCoAtNet，在八类胃部相关疾病分类中实现了高准确率。该模型的输出随后用于驱动多个大语言模型进行推理。为评估这些推理质量，我们构建了两个经专家验证的基准数据集，涵盖病因、症状、治疗、生活方式及随访护理等方面。我们对32个大语言模型在这些金标准上的表现进行了评估。结果表明，强大的分类能力可提升其解释质量，但没有任何模型达到人类级别的稳定性。即使表现最好的大语言模型，在提示词变化时也会改变其推理。本研究表明，将深度学习与大语言模型结合可生成有用的临床叙述，但当前的大语言模型在高风险医疗决策中仍不可靠。该框架更清晰地揭示了它们的局限性，并为构建更安全的推理系统提供了可行路径。本研究使用的完整源代码和数据集已在https://github.com/souravbasakshuvo/DL3M公开。

</details>


### [4] [Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making](https://arxiv.org/abs/2512.13747)
*Siyuan Dai,Lunxiao Li,Kun Zhao,Eardi Lila,Paul K. Crane,Heng Huang,Dongkuan Xu,Haoteng Tang,Liang Zhan*

Main category: cs.CV

TL;DR: 当前多模态大语言模型（MLLMs）在生物医学视觉-语言任务中表现不佳，甚至不如纯文本推理；作者通过阿尔茨海默病分类和胸部X光诊断两个数据集验证该问题，并提出三种改进策略，揭示了MLLMs缺乏具身视觉理解。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在通用视觉-语言任务中展现出强大的零样本能力，但在生物医学领域，即便是最先进的MLLMs在基础的医疗决策任务上仍表现不佳。作者旨在探究这一局限性及其原因。

Method: 作者使用两个具有挑战性的数据集进行实证研究：(1) 三阶段阿尔茨海默病（AD）分类（正常、轻度认知障碍、痴呆），其类别间视觉差异细微；(2) MIMIC-CXR胸部X光片分类，包含14种非互斥疾病。研究比较了纯文本、纯视觉及多模态输入的表现，并尝试三种缓解策略：(1) 使用带推理注释的示例进行上下文学习；(2) 先对图像生成描述再进行纯文本推理；(3) 对视觉编码器进行少量样本微调并辅以分类监督。

Result: 实验表明，在医疗决策任务中，纯文本推理始终优于纯视觉或多模态输入，且多模态输入常常比仅用文本效果更差。所提出的三种策略在一定程度上缓解了该问题。

Conclusion: 当前MLLMs在医疗场景中缺乏扎实的视觉理解能力，未来需加强视觉与语义的对齐，以提升多模态医疗决策性能。

Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.

Abstract (中文翻译): 随着大语言模型（LLMs）的快速发展，先进的多模态大语言模型（MLLMs）在视觉-语言任务中展现了令人印象深刻的零样本能力。然而，在生物医学领域，即便是最先进的MLLMs在基本的医疗决策（MDM）任务上也表现不佳。我们利用两个具有挑战性的数据集对此局限性展开研究：(1) 三阶段阿尔茨海默病（AD）分类（正常、轻度认知障碍、痴呆），其中各类别之间的视觉差异非常细微；(2) MIMIC-CXR胸部X光片分类，包含14种非互斥的病理状况。我们的实证研究表明，纯文本推理始终优于纯视觉或视觉-文本联合输入，而多模态输入的表现往往比仅使用文本更差。为缓解这一问题，我们探索了三种策略：(1) 利用带有推理注释的示例进行上下文学习；(2) 先对图像生成文字描述，再进行纯文本推理；(3) 在分类监督下对视觉编码器进行少样本微调。这些发现揭示了当前MLLMs缺乏具身的视觉理解能力，并指出了提升医疗领域多模态决策能力的有前景方向。

</details>


### [5] [STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning](https://arxiv.org/abs/2512.13752)
*Jie Qin,Jiancheng Huang,Limeng Qiao,Lin Ma*

Main category: cs.CV

TL;DR: STAR是一种堆叠自回归方案，通过分阶段处理理解、生成和编辑任务，在不干扰已有能力的前提下提升多模态大语言模型的生成性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在统一理解和生成目标方面面临优化冲突与性能权衡的挑战，亟需一种既能增强生成能力又不损害理解能力的方法。

Method: 提出STAR方法：将多模态学习分解为理解、生成和编辑多个阶段；冻结基础自回归（AR）模型参数，并逐层堆叠同构AR模块以避免任务间干扰；引入高容量VQ提升图像表征粒度，并采用隐式推理机制改善复杂条件下的生成质量。

Result: STAR在GenEval（0.91）、DPG-Bench（87.44）和ImgEdit（4.34）上达到SOTA性能。

Conclusion: STAR有效实现了统一多模态学习，在保持理解能力的同时显著提升了生成性能。

Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.

Abstract (中文翻译): 多模态大语言模型（MLLMs）在推动通用人工智能的发展中发挥着关键作用。然而，由于优化冲突和性能权衡，实现多模态理解与生成的统一目标仍然具有挑战性。为了在保留现有理解能力的同时有效提升生成性能，我们提出了STAR：一种用于任务渐进式统一多模态学习的堆叠自回归方案。该方法将多模态学习分解为多个阶段：理解、生成和编辑。通过冻结基础自回归（AR）模型的参数并逐步堆叠同构的AR模块，该方法在扩展模型能力的同时避免了跨任务干扰。同时，我们引入了一个高容量的矢量量化（VQ）模块以增强图像表征的粒度，并采用隐式推理机制来提升复杂条件下的生成质量。实验表明，STAR在GenEval（0.91）、DPG-Bench（87.44）和ImgEdit（4.34）上均取得了当前最优的性能，验证了其在统一多模态学习中的有效性。

</details>


### [6] [Time-aware UNet and super-resolution deep residual networks for spatial downscaling](https://arxiv.org/abs/2512.13753)
*Mika Sipilä,Sabrina Maggio,Sandra De Iaco,Klaus Nordhausen,Monica Palma,Sara Taskinen*

Main category: cs.CV

TL;DR: 本文提出在SRDRN和UNet中加入轻量级时间模块（使用正弦或RBF编码）以提升对流层臭氧卫星数据的空间降尺度性能，在意大利案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 卫星大气污染物数据空间分辨率较低，限制了其在局地环境分析与决策中的应用，因此需要有效的空间降尺度方法。

Method: 在SRDRN和UNet两种深度学习架构中引入轻量级时间模块，利用正弦或径向基函数（RBF）对观测时间进行编码，并将其与网络中的空间特征融合。

Result: 在意大利臭氧降尺度案例中，加入时间模块的方法在仅略微增加计算复杂度的情况下，显著提升了降尺度性能和收敛速度。

Conclusion: 引入时间感知模块能有效增强深度学习模型在臭氧卫星数据空间降尺度任务中的表现，具有良好的实用价值。

Abstract: Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.

Abstract (中文翻译): 大气污染物的卫星数据通常仅具有较粗的空间分辨率，限制了其在局地尺度环境分析和决策中的应用。空间降尺度方法旨在将粗分辨率的卫星数据转化为高分辨率场。本研究考虑了两种广泛使用的深度学习架构——超分辨率深度残差网络（SRDRN）和基于编码器-解码器结构的UNet，用于对流层臭氧的空间降尺度。两种方法均扩展了一个轻量级的时间模块，该模块使用正弦或径向基函数（RBF）对观测时间进行编码，并将时间特征与网络中的空间表示进行融合。所提出的时间感知扩展方法在意大利臭氧降尺度的案例研究中与其原始基线模型进行了对比评估。结果表明，尽管仅略微增加了计算复杂度，但时间模块显著提升了降尺度性能和收敛速度。

</details>


### [7] [Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries](https://arxiv.org/abs/2512.13796)
*Victor Rong,Jan Held,Victor Chu,Daniel Rebain,Marc Van Droogenbroeck,Kiriakos N. Kutulakos,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 本文提出一种将几何与外观解耦的新表示方法，使用surfels表示几何，并结合全局神经场与每个图元的颜色来表示外观，在显著减少图元数量和内存占用的同时，达到与3D Gaussian Splatting相当的感知质量，并实现更快的渲染速度。


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting虽然在新视角合成中效果出色，但即使场景几何结构简单，仍需数百万个图元来建模高纹理场景，导致表示冗余、内存占用高且计算开销大。

Method: 该方法超越了基于点的渲染，将几何与外观解耦：使用surfels表示几何，外观则由一个全局神经场与每个图元的颜色共同建模；神经场为每个像素固定数量的图元提供纹理，从而控制额外计算开销。

Result: 在室外场景中，该方法使用9.7倍更少的图元和5.5倍更少的内存；在室内场景中，图元减少31倍、内存减少3.7倍，同时保持与3D Gaussian Splatting相当的感知质量。此外，渲染速度是现有纹理图元方法的两倍，且视觉质量更优。

Conclusion: 所提出的几何-外观解耦表示方法在大幅压缩图元数量和内存消耗的同时，实现了高质量、高效率的新视角合成，优于现有基于图元的渲染方法。

Abstract: Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\times$ fewer primitives and $5.5\times$ less memory on outdoor scenes and using $31\times$ fewer primitives and $3.7\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.

Abstract (中文翻译): 尽管高斯泼溅（Gaussian splatting）在新视角合成方面取得了令人印象深刻的结果，但它需要数百万个图元才能对高度纹理化的场景进行建模，即使场景的几何结构很简单也是如此。我们提出了一种超越基于点渲染的表示方法，将几何与外观解耦，以实现更紧凑的表示。我们使用surfels表示几何，并结合全局神经场与每个图元的颜色来表示外观。神经场为每个像素固定数量的图元提供纹理，从而确保额外计算开销较低。我们的表示方法在室外场景中使用了9.7倍更少的图元和5.5倍更少的内存，在室内场景中使用了31倍更少的图元和3.7倍更少的内存，同时达到了与3D高斯泼溅相当的感知质量。此外，我们的方法渲染速度是现有纹理化图元方法的两倍，并在视觉质量上有所提升。

</details>


### [8] [VajraV1 -- The most accurate Real Time Object Detector of the YOLO family](https://arxiv.org/abs/2512.13834)
*Naman Balbir Singh Makkar*

Main category: cs.CV

TL;DR: VajraV1 是一种新型实时目标检测架构，在保持与现有 YOLO 模型相当推理速度的同时，在 COCO 数据集上实现了当前最优的精度。


<details>
  <summary>Details</summary>
Motivation: 近年来 YOLO 系列模型（如 YOLOv10 至 YOLOv13）在实时目标检测领域取得显著进展，但仍有提升精度与效率平衡的空间。本文旨在通过整合并优化已有 YOLO 架构中的有效设计，进一步推动实时检测性能边界。

Method: 提出 VajraV1 模型架构，融合先前 YOLO 模型中的高效设计选择，构建一系列不同规模（Nano、Small、Medium、Large、Xlarge）的检测器。

Result: 在 COCO 验证集上，VajraV1 各版本均超越对应规模的 YOLOv12 和 YOLOv13：VajraV1-Nano 达到 44.3% mAP，VajraV1-Small 为 50.4% mAP，VajraV1-Medium 为 52.7% mAP，VajraV1-Large 为 53.7% mAP，VajraV1-Xlarge 达到 56.2% mAP，且推理延迟具有竞争力。

Conclusion: VajraV1 架构通过有效整合现有 YOLO 模型的设计优势，在实时目标检测任务中同时实现了最先进的准确率和有竞争力的推理速度，成为当前性能最强的实时检测器。

Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.

Abstract (中文翻译): 近年来，实时目标检测领域取得了显著进展，2024至2025年间相继发布了YOLOv10、YOLO11、YOLOv12和YOLOv13。本技术报告提出了VajraV1模型架构，该架构在现有YOLO系列检测器的基础上进行了架构增强。VajraV1融合了先前YOLO模型中的有效设计选择，在保持与现有模型相当的推理速度的同时，在实时目标检测器中实现了最先进的精度。在COCO验证集上，VajraV1-Nano达到了44.3%的mAP，比YOLOv12-N高出3.7%，比YOLOv13-N高出2.7%，且其延迟与YOLOv12-N和YOLOv11-N相当。VajraV1-Small达到了50.4%的mAP，超过YOLOv12-S和YOLOv13-S达2.4%。VajraV1-Medium达到了52.7%的mAP，优于YOLOv12-M达0.2%。VajraV1-Large达到了53.7%的mAP，超过YOLOv13-L达0.3%。VajraV1-Xlarge达到了56.2%的mAP，超越了所有现有的实时目标检测器。

</details>


### [9] [MoLingo: Motion-Language Alignment for Text-to-Motion Generation](https://arxiv.org/abs/2512.13840)
*Yannan He,Garvita Tiwari,Xiaohan Zhang,Pankaj Bora,Tolga Birdal,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: MoLingo 是一种基于连续潜在空间去噪的文本到动作生成模型，通过语义对齐的潜在空间和多令牌交叉注意力文本条件机制，在生成逼真且与文本高度一致的人体动作方面达到新 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法在潜在空间扩散策略和文本条件注入方式上仍有改进空间，作者旨在探索如何使连续动作潜在空间上的扩散更有效，并提升生成动作与文本描述的一致性。

Method: 提出一种语义对齐的动作编码器，利用帧级文本标签训练，使语义相近的动作潜在表示更接近；同时比较单令牌与多令牌交叉注意力文本条件机制，采用后者以增强动作真实感与文本-动作对齐。

Result: 在标准指标和用户研究中均取得当前最优性能，显著优于现有方法。

Conclusion: 通过语义对齐潜在空间、自回归生成和交叉注意力文本条件机制，MoLingo 在人体动作生成任务中实现了新的技术突破，并将开源代码与模型以促进后续研究。

Abstract: We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.

Abstract (中文翻译): 我们提出了 MoLingo，一种文本到动作（T2M）模型，通过在连续潜在空间中进行去噪来生成逼真、自然的人体动作。近期工作在潜在空间中进行扩散，要么一次性处理整个潜在表示，要么在多个潜在表示上自回归地进行。本文研究了如何使连续动作潜在空间上的扩散效果最佳。我们聚焦于两个问题：（1）如何构建语义对齐的潜在空间，使扩散更有效；（2）如何最好地注入文本条件，使生成的动作紧密遵循文本描述。我们提出了一种语义对齐的动作编码器，利用帧级文本标签进行训练，使得具有相似文本语义的潜在表示彼此靠近，从而使潜在空间更有利于扩散过程。我们还比较了单令牌条件机制与多令牌交叉注意力机制，发现交叉注意力能带来更好的动作真实感和文本-动作对齐效果。结合语义对齐的潜在表示、自回归生成和交叉注意力文本条件机制，我们的模型在标准评估指标和用户研究中均达到了人体动作生成的新前沿水平。我们将发布代码和模型以支持进一步研究和下游应用。

</details>


### [10] [Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging](https://arxiv.org/abs/2512.13855)
*Ujjwal Mishra,Vinita Shukla,Praful Hambarde,Amit Shukla*

Main category: cs.CV

TL;DR: 提出了一种名为“Telescopic Adapters”的新型参数高效微调方法，通过在CLIPSeg模型中按层深度动态增加适配器容量，在仅使用613k可训练参数（比全微调少244倍）的情况下，在五个医学图像分割任务上取得优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统微调视觉语言分割模型（VLSM）用于医学图像计算开销大；现有参数高效微调（PEFT）方法对所有Transformer层使用统一的适配器维度，导致参数分配次优、适应效率低。

Method: 提出Telescopic Adapters框架，在CLIPSeg的视觉和文本编码器中引入轻量级瓶颈模块，并根据层深度和语义相关性动态缩放适配器维度，实现从浅层到深层逐步增加适配器容量。

Result: 在包括息肉分割、皮肤病变检测和乳腺超声成像在内的五个医学数据集上，仅用613k可训练参数（比端到端微调少244倍）就取得了优于现有方法的性能；消融实验验证了深层需要更多适配容量。

Conclusion: Telescopic Adapters为医学VLSM的高效微调提供了新范式，可在资源受限的临床环境中部署，同时保持有竞争力的分割精度。

Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.

Abstract (中文翻译): 将视觉语言分割模型（VLSM）适配到医学影像领域时，采用传统的微调方法会带来巨大的计算开销。现有的参数高效微调（PEFT）方法在所有Transformer层中使用统一的适配器维度，导致参数分配次优并降低适配效率。我们提出了“望远镜式适配器”（Telescopic Adapters），这是一种新颖的PEFT框架，通过深度感知缩放策略，从浅层到深层逐步增加适配器容量。我们的方法在CLIPSeg的视觉和文本编码器中集成轻量级瓶颈模块，适配器维度根据层深度和语义相关性动态调整。仅使用61.3万个可训练参数（比端到端微调少244倍），Telescopic Adapters在涵盖息肉分割、皮肤病变检测和乳腺超声成像的五个多样化医学数据集上实现了卓越性能。全面的消融研究证明，深层网络比浅层需要显著更多的适配容量，从而验证了我们的望远镜式缩放假设。该方法为医学VLSM的高效微调建立了新范式，使其能够在资源受限的临床环境中部署，同时保持具有竞争力的分割精度。

</details>
