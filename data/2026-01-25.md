<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: 本文提出三种方法以提升涵洞与污水管道缺陷的自动分割效果，尤其在标注数据有限的情况下：（1）有效的预处理策略；（2）新架构FORTRESS；（3）基于注意力机制的少样本语义分割方法。


<details>
  <summary>Details</summary>
Motivation: 涵洞和污水管道是排水系统的关键组成部分，其缺陷检测对公共安全和环境至关重要。然而，该领域数据采集与标注成本高、依赖专业知识，难以获得大规模标注数据集，因此亟需在数据稀缺条件下提升自动化缺陷分割性能。

Method: 1. 采用传统数据增强与动态标签注入等预处理策略；  
2. 提出新架构FORTRESS，融合深度可分离卷积、自适应Kolmogorov-Arnold网络（KAN）和多尺度注意力机制；  
3. 探索少样本语义分割，使用带注意力机制的双向原型网络进行训练。

Result: 1. 预处理策略显著提升了IoU和F1分数；  
2. FORTRESS在涵洞管道缺陷数据集上达到SOTA性能，同时大幅减少参数量与计算开销；  
3. 少样本方法在有限标注数据下仍取得令人满意的评估指标表现。

Conclusion: 在标注数据稀缺的实际场景中，通过改进训练数据或调整模型架构，均可有效提升涵洞与污水管道缺陷的自动分割性能。所提三种方法为该领域提供了实用且高效的解决方案。

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

Abstract (中文翻译): 涵洞和污水管道是排水系统的关键组成部分，其失效可能对公共安全和环境造成严重风险。本论文探索了提升涵洞与污水管道缺陷自动分割性能的方法。该领域的数据收集与标注过程繁琐且需要专业知识，因此难以获得用于结构缺陷检测的大规模数据集。我们所提出的方法在标注数据有限的条件下进行了测试，以验证其在现实场景中的适用性。总体而言，本论文提出了三种方法，显著增强了缺陷分割能力并应对数据稀缺问题，具体可通过增强训练数据或调整模型架构来实现。首先，我们评估了包括传统数据增强和动态标签注入在内的预处理策略，这些技术显著提升了分割性能，提高了交并比（IoU）和F1分数。其次，我们提出了名为FORTRESS的新架构，该架构结合了深度可分离卷积、自适应Kolmogorov-Arnold网络（KAN）以及多尺度注意力机制，在涵洞污水管道缺陷数据集上实现了最先进的性能，同时显著减少了可训练参数数量和计算成本。最后，我们研究了少样本语义分割在缺陷检测中的适用性。少样本学习旨在仅使用有限数据训练模型，通过采用带有注意力机制的双向原型网络，模型获得了更丰富的特征表示，并在各项评估指标上取得了令人满意的结果。

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文系统评估了多模态大语言模型（MLLMs）在异质人脸识别（HFR）任务中的表现，发现其在跨光谱条件下显著落后于传统人脸识别系统。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs在视觉-语言任务中的成功，研究者开始关注其在生物识别（特别是异质人脸识别）中的应用潜力。然而，尚缺乏对其在跨模态人脸识别场景中性能的系统性评估。

Method: 作者在多种跨模态场景（如VIS-NIR、VIS-SWIR、VIS-THERMAL）下，对多个开源MLLMs进行了基准测试，采用生物识别协议和指标（如Acquire Rate、EER、TAR）进行评估。

Result: 实验结果显示，尽管MLLMs近期取得了进展，但在具有挑战性的跨光谱条件下，其性能与传统人脸识别系统相比仍存在显著差距。

Conclusion: 当前的MLLMs在异质人脸识别任务中仍存在明显局限，若考虑将其部署于人脸识别系统，必须进行严格的生物识别评估。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

Abstract (中文翻译): 多模态大语言模型（MLLMs）最近在广泛的视觉-语言任务中展现出强大的性能，引发了人们对其在生物识别应用中潜力的关注。本文对最先进的MLLMs在异质人脸识别（HFR）任务中进行了系统性评估，其中注册图像和查询图像来自不同的传感模态，包括可见光（VIS）、近红外（NIR）、短波红外（SWIR）和热成像相机。我们在多种跨模态场景（包括VIS-NIR、VIS-SWIR和VIS-THERMAL人脸识别）下对多个开源MLLMs进行了基准测试。MLLMs的识别性能采用生物识别协议并基于不同指标（包括获取率Acquire Rate、等错误率EER和真接受率TAR）进行评估。我们的结果表明，尽管MLLMs近期取得了进展，但在具有挑战性的跨光谱条件下，其性能与传统人脸识别系统之间仍存在显著差距。我们的研究结果突显了当前MLLMs在HFR任务中的局限性，同时也强调了在考虑将其部署于人脸识别系统时进行严格生物识别评估的重要性。

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE 是一种无需额外数据的错误感知课程学习框架，通过动态调整样本难度提升医学视觉-语言模型的视觉定位准确性和报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在生成放射学报告时存在视觉定位不准和事实一致性差的问题，导致文本描述与影像证据错位，影响报告可靠性。

Method: CURE 在短语定位、基于定位的报告生成和解剖结构引导的报告生成任务上对多模态指令模型进行微调，利用公开数据集，并根据模型表现动态调整采样策略，优先训练更难样本以增强空间与文本对齐。

Result: CURE 将定位准确率提升 +0.37 IoU，报告质量提高 +0.188 CXRFEScore，并减少 18.6% 的幻觉现象。

Conclusion: CURE 是一种数据高效的框架，显著提升了医学报告的定位准确性与事实可靠性。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

Abstract (中文翻译): 医学视觉-语言模型能够自动生​​成放射学报告，但在准确的视觉定位和事实一致性方面仍面临挑战。现有模型常常将文本发现与视觉证据错误对齐，导致预测结果不可靠或定位薄弱。我们提出了 CURE，这是一种错误感知的课程学习框架，无需任何额外数据即可提升定位能力和报告质量。CURE 利用公开数据集，在短语定位、基于定位的报告生成以及解剖结构引导的报告生成任务上对多模态指令模型进行微调。该方法根据模型性能动态调整样本采样策略，重点训练更具挑战性的样本，从而改善空间与文本的对齐效果。实验表明，CURE 将定位准确率（IoU）提升了 +0.37，报告质量（CXRFEScore）提高了 +0.188，并减少了 18.6% 的幻觉现象。CURE 是一种数据高效的框架，有效增强了定位准确性和报告可靠性。代码发布于 https://github.com/PabloMessina/CURE，模型权重发布于 https://huggingface.co/pamessina/medgemma-4b-it-cure。

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出DuFal，一种双频域感知学习框架，通过结合全局与局部高频增强的傅里叶神经算子，在稀疏视角锥束CT重建中显著提升高频解剖细节的恢复能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的锥束CT重建因采样不足难以恢复高频解剖细节，而传统CNN方法偏向学习低频信息，无法有效重建精细结构。

Method: 提出DuFal框架，包含双路径架构：全局高频增强傅里叶神经算子和局部高频增强傅里叶神经算子；引入谱-通道因子化降低参数量，并设计跨注意力频率融合模块整合空间与频域特征，最终通过特征解码器和强度场解码流程重建CT体积。

Result: 在LUNA16和ToothFairy数据集上的实验表明，DuFal在极稀疏视角下显著优于现有最先进方法，尤其在保留高频解剖特征方面表现突出。

Conclusion: DuFal通过联合建模空间域与频域信息，有效解决了稀疏视角CT重建中高频细节丢失的问题，为医学影像重建提供了新思路。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

Abstract (中文翻译): 稀疏视角锥束计算机断层扫描（Cone-Beam Computed Tomography, CBCT）重建由于X射线投影数量有限，导致对对应于高频成分的精细解剖结构采样不足，这在医学成像中仍是一个具有挑战性的问题。传统的基于卷积神经网络（CNN）的方法通常难以恢复这些精细结构，因为它们倾向于学习低频信息。为解决这一问题，本文提出了DuFal（Dual-Frequency-Aware Learning），一种新颖的框架，通过双路径架构融合频域与空间域处理。其核心创新在于所提出的高-局部分解傅里叶神经算子（High-Local Factorized Fourier Neural Operator），该算子包含两个互补分支：一个全局高频增强傅里叶神经算子用于捕捉全局频域模式，另一个局部高频增强傅里叶神经算子则对空间分块进行处理，以保留可能在全局频域分析中丢失的空间局部性。为提高效率，我们设计了谱-通道因子化方案以减少傅里叶神经算子的参数量，并设计了跨注意力频率融合模块，有效整合空间与频域特征。融合后的特征通过特征解码器生成投影表示，再经由强度场解码流程重建最终的CT体积。在LUNA16和ToothFairy数据集上的实验结果表明，DuFal在保留高频解剖特征方面显著优于现有最先进的方法，尤其是在极稀疏视角设置下。

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 本文提出了一种偏差引导的提示学习框架，用于少样本正常图像下的异常检测（FNSAD），通过结合视觉-语言模型的语义能力和基于统计偏差的评分机制，在MVTecAD和VISA数据集上实现了优于PromptAD等基线方法的像素级检测性能。


<details>
  <summary>Details</summary>
Motivation: 少样本正常图像异常检测（FNSAD）任务极具挑战性，因为训练样本极少且异常类型多样。现有基于CLIP等视觉-语言模型的方法在正常与异常提示之间的判别能力较弱，且缺乏有效的逐块（patch-level）异常评分机制。

Method: 作者提出一个偏差引导的提示学习框架：1）使用可学习的上下文向量替代固定提示前缀，并在正常与异常提示间共享；2）引入异常特定的后缀标记以实现类别感知对齐；3）设计一种结合Top-K多实例学习（MIL）的偏差损失函数，将图像块特征建模为正态分布的高斯偏差，从而赋予显著偏离的图像块更高异常分数。

Result: 在MVTecAD和VISA基准上的实验表明，该方法在像素级异常检测性能上优于PromptAD及其他基线方法。消融实验验证了可学习提示、基于偏差的评分机制以及Top-K MIL策略的有效性。

Conclusion: 所提出的偏差引导提示学习框架有效提升了FNSAD任务中异常区域的定位精度与可解释性，为少样本异常检测提供了一种兼具语义理解与统计可靠性的新思路。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

Abstract (中文翻译): 少样本正常图像异常检测（FNSAD）旨在仅使用少量正常训练样本检测图像中的异常区域，由于监督信息有限且潜在缺陷种类繁多，该任务极具挑战性。近期方法利用CLIP等视觉-语言模型，通过基于提示的学习来对齐图像与文本特征。然而，现有方法通常在正常与异常提示之间判别能力较弱，且缺乏针对图像块级别异常的合理评分机制。本文提出一种偏差引导的提示学习框架，将视觉-语言模型的语义能力与基于偏差的统计可靠性相结合。具体而言，我们用可学习的上下文向量替代固定的提示前缀，并在正常与异常提示之间共享；同时，引入异常特定的后缀标记以实现类别感知对齐。为增强可分性，我们引入一种结合Top-K多实例学习（MIL）的偏差损失函数，将图像块特征建模为相对于正常分布的高斯偏差。这使得网络能够为具有统计显著偏差的图像块分配更高的异常分数，从而提升异常定位能力与可解释性。在MVTecAD和VISA基准上的实验表明，该方法在像素级检测性能上优于PromptAD及其他基线方法。消融研究进一步验证了可学习提示、基于偏差的评分机制以及Top-K MIL策略的有效性。

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 提出一种结合事件相机数据与物理传感器模型的NeRF框架，从单次曝光模糊LDR图像及其对应事件中实现高质量HDR去模糊新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用事件数据进行新视角合成时忽略了相机输出与真实世界辐射之间的传感器物理不匹配问题，导致HDR重建和去模糊效果不佳。

Method: 构建一个统一的、基于传感器物理的NeRF框架：1）用NeRF直接表示HDR域中的真实3D场景辐射；2）引入像素级RGB映射场将渲染值与输入LDR图像对齐；3）设计事件映射场连接物理场景动态与事件传感器输出；4）联合优化两个映射场与NeRF网络，利用事件中的时空动态信息提升HDR 3D表示。

Result: 在自建和公开数据集上的实验表明，该方法在仅使用单次曝光模糊LDR图像和对应事件的情况下，实现了当前最优的去模糊HDR新视角合成效果。

Conclusion: 通过显式建模传感器物理特性并联合利用事件数据的时空信息，所提方法有效解决了模糊LDR图像下HDR新视角合成的挑战，显著优于现有方法。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

Abstract (中文翻译): 从野外常见的低动态范围（LDR）模糊图像中进行新视角合成，在极端光照条件下难以恢复高动态范围（HDR）和清晰的3D表示。尽管现有方法利用事件数据来解决这一问题，但它们忽略了相机输出与物理世界辐射之间的传感器物理不匹配，导致HDR重建和去模糊结果不理想。为解决此问题，我们提出了一种统一的、基于传感器物理的NeRF框架，用于从单次曝光模糊LDR图像及其对应的事件中实现清晰HDR新视角合成。我们采用NeRF直接表示HDR域中3D场景的真实辐射，并像物理世界中那样对击中传感器像素的原始HDR场景光线进行建模。引入一个像素级RGB映射场，将上述渲染的像素值与输入图像中传感器记录的LDR像素值对齐。同时设计了一种新颖的事件映射场，以连接物理场景动态与实际事件传感器输出。这两个映射场与NeRF网络联合优化，利用事件中的时空动态信息增强清晰HDR 3D表示的学习。在自建和公开数据集上的实验表明，我们的方法仅使用单次曝光模糊LDR图像和对应事件即可实现最先进的去模糊HDR新视角合成效果。

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 用Vision Transformer替代U-Net编码器可更有效减少胸片分类器中的性别/年龄属性泄露，同时保持诊断性能。


<details>
  <summary>Details</summary>
Motivation: 胸片AI分类器常因依赖性别和年龄等捷径特征而对少数群体系统性误诊；现有基于卷积编码器的属性中和方法无法在临床可用强度下完全消除属性泄露。

Method: 在属性中和框架中，将U-Net卷积编码器替换为数据高效的DeiT-S Vision Transformer骨干网络，在ChestX-ray14数据集上训练，并在11个编辑强度下生成图像，分别由独立AI评估属性泄露、由ConvNet评估疾病预测性能。

Result: 在中等编辑强度（alpha=0.5）下，ViT中和器将性别识别AUC降至约0.80，比原U-Net方法低约10个百分点（且训练轮次减半）；15种疾病宏观ROC AUC与原始图像相差不超过5个百分点，最差子群体AUC仍接近0.70。

Conclusion: 基于全局自注意力的视觉模型能更有效地抑制属性泄露而不牺牲临床效用，为构建更公平的胸片AI提供可行路径。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

Abstract (中文翻译): 胸片分类器中的偏见通常源于对性别和年龄相关捷径特征的依赖，导致对少数群体的系统性漏诊。以往基于像素空间的属性中和方法依赖卷积编码器，虽能减轻但无法在临床可用的编辑强度下完全消除此类属性泄露。本研究评估了在属性中和框架中，用Vision Transformer骨干网络替代U-Net卷积编码器是否能在保留诊断准确性的同时进一步减少人口统计学属性泄露。研究在ChestX-ray14数据集上训练了一个数据高效的Image Transformer Small（DeiT-S）中和器，并在十一个编辑强度级别下生成编辑图像，分别由独立AI评估属性泄露程度，以及由卷积神经网络（ConvNet）评估疾病预测性能。在中等编辑强度（alpha = 0.5）下，Vision Transformer（ViT）中和器将患者性别识别的曲线下面积（AUC）降至约0.80，比原始框架的卷积U-Net编码器低约10个百分点，尽管其训练轮次仅为后者的一半。同时，15种疾病发现的宏观受试者工作特征曲线下面积（ROC AUC）与未编辑基线相比波动在五个百分点以内，最差子群体的AUC仍保持在0.70左右。这些结果表明，基于全局自注意力机制的视觉模型可在不牺牲临床效用的前提下进一步抑制属性泄露，为实现更公平的胸片AI提供了切实可行的路径。

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: LASAGNA 是一个统一框架，能同时生成带有逼真背景和高质量透明前景（含阴影、反射等效果）的图像及其分层表示，支持多种条件输入，实现高可控性和一致性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在编辑特定元素时难以保持可控性和一致性；分层表示方法虽具灵活性，但常无法生成具有合理合成关系的图层，且缺乏真实视觉效果（如阴影、反射）。

Method: 提出 LASAGNA 框架，联合生成图像及其组成图层（逼真背景 + 带真实视觉效果的透明前景），并利用新构建的数据集 LASAGNA-48K（包含干净背景与带物理真实效果的 RGBA 前景）进行训练；同时引入首个分层编辑基准 LASAGNABENCH。

Result: LASAGNA 能在多种条件输入（文本提示、前景、背景、位置掩码）下高效学习正确图像合成，生成高度一致且连贯的多图层结果，支持多样化的后期编辑应用，并准确保留身份特征与视觉效果。

Conclusion: LASAGNA 有效解决了图像分层生成中的可控性、一致性和真实感问题，其数据集与基准将公开发布以促进社区研究。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

Abstract (中文翻译): 近期的图像生成模型虽取得显著进展，但在用户尝试编辑现有图像中的特定元素时，往往难以产生可控且一致的结果。分层表示能够实现灵活的、用户驱动的内容创作，但现有方法通常无法生成具有连贯合成关系的图层，且其对象图层通常缺乏阴影和反射等逼真的视觉效果。为克服这些局限，我们提出了 LASAGNA——一种新颖的统一框架，可同时生成图像及其组成图层：一个逼真的背景和一个具有引人注目的视觉效果的高质量透明前景。与以往工作不同，LASAGNA 能够从多种条件输入（包括文本提示、前景、背景和位置掩码）中高效学习正确的图像合成方式，从而为现实应用提供更强的可控性。为此，我们构建了 LASAGNA-48K 数据集，其中包含干净的背景和带有物理真实视觉效果的 RGBA 前景。我们还提出了 LASAGNABENCH——首个面向图层编辑的基准。实验表明，LASAGNA 能够在多个图像图层上同时生成高度一致且连贯的结果，支持多样化的后期编辑应用，并能准确保留对象身份和视觉效果。LASAGNA-48K 和 LASAGNABENCH 将公开发布，以促进社区的开放研究。项目页面为 https://rayjryang.github.io/LASAGNA-Page/。

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出一种利用手背皮肤形变信息的双流差分编码器方法，仅使用裁剪后的手背图像即可在手指严重遮挡情况下显著提升手部姿态估计精度，并支持新型交互方式。


<details>
  <summary>Details</summary>
Motivation: 第一人称视角（egocentric）下的手部姿态估计常因手指频繁自遮挡而面临挑战，现有方法依赖完整手部几何结构和大型模型，在遮挡场景下性能受限。

Method: 提出一种双流差分编码器（dual-stream delta encoder），通过对比动态手部与放松基准姿态的密集视觉特征，从手背皮肤形变中学习姿态信息。

Result: 在手指遮挡率≥50%的场景下，仅使用裁剪的手背图像，该方法相比当前最优技术将平均关节角度误差（MPJAE）降低了18%。

Conclusion: 该方法不仅提升了遮挡条件下下游任务（如食指捏合与点击估计）的可靠性，还支持无需可见动作的等距力检测等新交互范式，同时减小了模型规模。

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

Abstract (中文翻译): XR设备的普及使得以自我为中心的手部姿态估计成为一项关键任务，但该视角下手指频繁自遮挡带来了固有挑战。为解决此问题，我们提出一种新方法，利用近期密集视觉特征提取器进展所揭示的手背皮肤形变中的丰富信息。我们引入了一种双流差分编码器，通过对比动态手部与基准放松姿态的特征来学习姿态。评估结果表明，仅使用裁剪的手背图像，我们的方法在自遮挡场景（手指遮挡率≥50%）下，相比依赖完整手部几何结构和大型模型主干的最先进方法，将平均关节角度误差（MPJAE）降低了18%。因此，我们的方法不仅提高了遮挡场景下如食指捏合与点击估计等下游任务的可靠性，还实现了新型交互范式，例如在无可见动作的情况下检测等距力以实现表面“点击”，同时最小化模型尺寸。

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 本文提出VIOLA框架，通过结合少量专家标注与大量无标签视频数据，实现多模态大语言模型在新视频领域的高效上下文学习。


<details>
  <summary>Details</summary>
Motivation: 将多模态大语言模型（MLLMs）泛化到新的视频领域对现实部署至关重要，但由于标注数据稀缺，尤其在工业或手术等专业场景中难以获得大量专家标注，现有基于大量标注样本的上下文学习方法不切实际。

Method: 提出VIOLA框架：1）采用密度-不确定性加权采样策略，在严格标注预算下选择兼具多样性、代表性和信息量的样本；2）构建混合池并引入置信度感知检索与提示机制，利用无标签数据同时避免噪声传播，使模型能区分真实标签与伪标签。

Result: 在9个多样化基准和4种MLLM上的实验表明，该方法在低资源场景下显著优于多种基线，仅需极少标注即可实现稳健的领域适应。

Conclusion: VIOLA通过最小标注与无标签数据的有效协同，为多模态大语言模型在标注稀缺的新视频领域中的高效上下文学习提供了可行方案。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

Abstract (中文翻译): 将多模态大语言模型（MLLMs）泛化到新颖的视频领域对于现实世界的部署至关重要，但由于标注数据稀缺，这一任务仍具挑战性。尽管上下文学习（ICL）提供了一种无需训练的适应路径，但标准方法依赖于大规模标注样本池，在工业或手术等专业环境中往往不切实际，因为这些场景需要专家进行标注。为弥合这一差距，我们提出了VIOLA（Video In-cOntext Learning with minimal Annotation），一种标签高效的框架，将最少的专家监督与大量无标签数据协同利用。首先，为在严格的标注预算下最大化效率，我们提出了密度-不确定性加权采样方法。与可能选择视觉异常值的标准多样性或不确定性策略不同，我们的方法利用密度估计来识别同时具备多样性、代表性和信息量的样本。其次，为在不传播噪声的前提下利用剩余的无标签数据，我们构建了一个混合池，并引入了置信度感知检索与置信度感知提示机制。这些机制显式建模标签可靠性，根据相似性与置信度的综合得分检索示例，并使MLLM能够自适应地区分已验证的真实标签与含噪的伪标签。在四个MLLM和九个多样化基准上的大量实验表明，该框架在低资源设置下显著优于多种基线方法，仅需极少标注即可实现稳健的适应。

</details>
