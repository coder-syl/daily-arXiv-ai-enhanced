<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段对齐框架，通过构建分层细粒度评价标准并引入复杂偏好优化（CPO）方法，显著提升了扩散模型在绘画生成任务中与人类专家知识的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型的后训练对齐依赖于简化的信号（如标量奖励或二元偏好），难以捕捉人类专家所具备的层次化、细粒度的复杂判断。

Method: 首先与领域专家共同构建一个树状结构的分层细粒度图像质量评价标准；然后采用两阶段对齐框架：第一阶段通过监督微调将领域知识注入辅助扩散模型，第二阶段提出复杂偏好优化（CPO）方法，扩展DPO以同时最大化正向属性概率并最小化负向属性概率。

Result: 在绘画生成任务上的大量实验表明，所提出的CPO方法显著提升了生成质量和与专家知识的对齐效果。

Conclusion: 该研究为扩散模型与细粒度、非二元人类偏好之间的对齐提供了有效途径，开辟了利用复杂评价标准进行模型对齐的新方向。

Abstract: Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

Abstract (中文翻译): 扩散模型的后训练对齐依赖于简化的信号，例如标量奖励或二元偏好，这限制了其与复杂人类专业知识（具有层次性和细粒度）的对齐能力。为解决此问题，我们首先与领域专家共同构建了一个层次化、细粒度的评估标准，将图像质量分解为多个正向和负向属性，并以树状结构组织。在此基础上，我们提出了一种两阶段对齐框架：第一阶段通过监督微调（Supervised Fine-Tuning）将领域知识注入一个辅助扩散模型；第二阶段提出“复杂偏好优化”（Complex Preference Optimization, CPO），将DPO方法扩展至适用于非二元、层次化的评价标准。具体而言，我们将对齐问题重新形式化，以在辅助扩散模型的指导下，同时最大化正向属性的概率并最小化负向属性的概率。我们在绘画生成领域实例化了该方法，并基于所构建的标准，使用带有细粒度属性标注的绘画数据集进行了CPO训练。大量实验表明，CPO显著提升了生成质量以及与专家知识的对齐程度，为细粒度标准下的模型对齐开辟了新途径。

</details>


### [2] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

TL;DR: 本文提出一种基于RGB空间中五进制像素强度组合的新文本嵌入图像方法，每个RGB像素可编码一个完整字符，避免了传统LSB/MSB、变换域或深度学习方法中的多像素操作或高计算开销，实验表明图像失真极小且嵌入效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有文本隐写技术（如LSB、MSB、PVD、变换域、量化、边缘区域方法及深度学习等）普遍存在依赖多像素强度翻转、引入噪声、编码解码确定性强或计算复杂度高等问题，亟需一种高效、低失真且单像素即可嵌入完整字符的新方法。

Method: 在RGB颜色空间中，对每个R、G、B通道采用五种受控的像素强度变化，形成5×5×5=125种不同的像素强度组合，并将这些组合映射到字母、数字、空格和常用特殊字符；通过单个RGB像素即可完成一个完整文本符号的嵌入。

Result: 通过MSE、MAE、SNR、PSNR、SSIM、直方图比较和热力图分析等多种指标评估，嵌入后图像与原始图像相比无显著失真；同时，该方法在嵌入效率上优于传统LSB/MSB及基于变换或学习的方法。

Conclusion: 所提五进制像素强度组合方法能高效、低失真地将文本嵌入图像，单像素即可表示完整字符，克服了现有方法在计算开销、图像质量和嵌入效率方面的局限。

Abstract: This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

Abstract (中文翻译): 本文提出了一种利用RGB空间中五进制像素强度组合将文本数据嵌入图像的新技术。现有方法主要依赖最低有效位（LSB）和最高有效位（MSB）操作、像素值差异（PVD）、RGB通道的空间扰动、变换域方法、量化方法、基于边缘和区域的方法，以及近期兴起的深度学习和生成式AI技术在图像空间域中隐藏文本信息。其中大多数方法依赖于跨多个像素的像素强度翻转（如LSB及其组合方法）或变换系数，常导致噪声形式的失真；且多数现有方法的编码与解码过程是确定性的，在深度学习和生成式AI等高级模型中计算开销较大。所提出的方法在RGB空间中采用五进制像素强度组合，即在R、G、B每个通道中使用五种受控的不同像素强度变化，从而形成最多125种不同的像素强度组合。这些组合被映射到文本符号，能够表示大小写字母、数字、空格及常用特殊字符。通过MSE、MAE、SNR、PSNR、SSIM、直方图比较和热力图分析等多种指标对原始图像和嵌入图像进行评估，结果表明图像未出现显著失真。此外，该方法通过单个RGB像素即可编码一个完整的文本符号，相较于通常需要多个像素或多步处理的LSB和MSB方法，以及计算开销更高的变换域和学习型方法，实现了更高的嵌入效率。

</details>


### [3] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文提出通过后训练实现文本与图像的完全统一生成，使模型在单次推理中自主从文本推理过渡到视觉合成，从而提升跨模态耦合与T2I性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态生成架构通常依赖显式的模态切换，在生成推理文本后再手动切换至图像生成，这种分步、顺序的推理过程限制了跨模态耦合，无法实现自动化的多模态生成。

Method: 采用离线、奖励加权的后训练方法，利用完全自生成的合成数据，并探索不同后训练数据策略，包括针对性数据集、通用图文语料和基准对齐数据。

Result: 在四个多样化的文本到图像（T2I）基准上实现了多模态图像生成性能的提升，表明同时对文本和图像模态进行奖励加权以及精心设计后训练数据的有效性。

Conclusion: 完全统一的文本-图像生成可通过后训练实现，且有针对性的后训练数据配合双模态奖励加权能显著提升T2I生成效果。

Abstract: Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

Abstract (中文翻译): 最近，能够联合生成文本和图像的统一多模态生成架构已成为文本到图像（T2I）合成的一个有前景的方向。然而，许多现有系统依赖显式的模态切换机制，即先生成推理文本，再手动切换到图像生成阶段。这种分离的、顺序式的推理过程限制了跨模态之间的耦合，并阻碍了自动化的多模态生成。本研究探索通过后训练实现完全统一的文本-图像生成，使模型能够在单次推理过程中自主地从文本推理过渡到视觉合成。我们考察了联合文本-图像生成对T2I性能的影响，以及在后训练过程中各模态的相对重要性。此外，我们还探索了不同的后训练数据策略，结果表明，针对特定局限性设计的定向数据集相比广泛的图像-标题语料库或与基准对齐的数据能取得更优的效果。通过采用离线、奖励加权的后训练方法，并结合完全自生成的合成数据，我们的方法在四个多样化的T2I基准上均提升了多模态图像生成性能，验证了对两个模态同时进行奖励加权以及战略性设计后训练数据的有效性。

</details>


### [4] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

TL;DR: ReHyAt 是一种混合注意力机制，结合了 softmax 注意力的高质量与线性注意力的高效性，实现了常量内存使用和线性复杂度，显著降低训练成本并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于 Transformer 的视频扩散模型因采用二次复杂度的注意力机制，在处理长序列时存在可扩展性瓶颈。

Method: 提出 ReHyAt（Recurrent Hybrid Attention）机制，融合 softmax 注意力与线性注意力，支持分块递归重构和常量内存占用，并通过轻量蒸馏与微调流程从现有 softmax 模型高效迁移知识。

Result: 在 VBench 和 VBench-2.0 基准及人类偏好研究中，ReHyAt 在将注意力复杂度从二次降至线性的同时，达到当前最优视频生成质量，训练成本降至约 160 GPU 小时。

Conclusion: ReHyAt 实现了高质量、低复杂度的视频生成，为未来双向 softmax 模型提供了一种高效可扩展的部署方案，适用于长时长和端侧视频生成任务。

Abstract: Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

Abstract (中文翻译): 近期视频扩散模型的研究转向基于 Transformer 的架构，在视频生成方面取得了最先进的成果，但其注意力机制具有二次复杂度，严重限制了对更长序列的可扩展性。我们提出了 ReHyAt（Recurrent Hybrid Attention，递归混合注意力）机制，该机制结合了 softmax 注意力的高保真度与线性注意力的高效性，支持分块递归重构并实现恒定内存占用。与仅使用线性注意力的同期工作 SANA Video 不同，ReHyAt 的混合设计使其能够高效地从现有的基于 softmax 的模型中进行知识蒸馏，将训练成本降低两个数量级至约 160 GPU 小时，同时保持具有竞争力的生成质量。我们提出的轻量级蒸馏与微调流程为未来最先进的双向 softmax 模型提供了一种通用方案。在 VBench 和 VBench-2.0 基准测试以及人类偏好研究中的实验表明，ReHyAt 在将注意力计算复杂度从二次降低至线性的同时，实现了最先进的视频生成质量，从而为长时长视频生成和端侧部署提供了实用的可扩展性。项目页面见 https://qualcomm-ai-research.github.io/rehyat。

</details>


### [5] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

TL;DR: 本文提出了一种基于残差向量量化的新型渐进式3D高斯泼溅压缩编解码器，利用多分辨率哈希网格引导的自回归熵模型，显著提升了压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅模型在中大型场景中存储需求高，限制了其在云和流媒体服务中的部署；而当前基于标量量化的渐进压缩方法未能充分利用高维特征向量间的相关性，影响率失真性能。

Method: 提出一种新的渐进式编解码器，采用残差向量量化（Residual Vector Quantization）替代传统标量量化，并设计了一个由多分辨率哈希网格引导的自回归熵模型，用于准确预测每个传输索引的条件概率。

Result: 该方法能高效压缩粗略层与细化层，在保持高质量视图合成的同时显著降低比特率。

Conclusion: 通过引入残差向量量化和自回归熵模型，本文有效提升了3D高斯泼溅模型的压缩效率，为实际部署提供了更优解决方案。

Abstract: Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

Abstract (中文翻译): 近期3D高斯泼溅技术的发展实现了实时、高保真的新视角合成。然而，这些模型在中大型场景中具有较高的存储需求，阻碍了其在云端和流媒体服务中的部署。目前最先进的渐进压缩方法依赖于渐进掩码和标量量化技术，利用空间上下文模型来降低高斯属性的比特率。尽管有效，但标量量化可能无法最优地捕捉高维特征向量之间的相关性，从而可能限制率失真性能。为此，我们提出了一种新颖的3D高斯泼溅渐进式编解码器，用更强大的残差向量量化方法替代传统技术以压缩图元特征。我们的核心贡献是一种由多分辨率哈希网格引导的自回归熵模型，能够精确预测每个连续传输索引的条件概率，从而高效压缩粗略层和细化层。

</details>


### [6] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

TL;DR: 在五个孟加拉国图像数据集上的实验表明，使用微调的迁移学习（尤其是ResNet-18）显著优于从头训练的自定义CNN和特征提取方法，在部分数据集上准确率提升高达76%，甚至达到100%；尽管自定义CNN参数更少、训练更快，但迁移学习在复杂或小样本任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 为图像分类任务选择合适的深度学习方法（自定义CNN vs. 预训练模型+迁移学习）缺乏针对特定地区（如孟加拉国）多样数据集的系统性比较研究，本文旨在填补这一空白，为实践者提供选型依据。

Method: 对自建CNN与两种主流预训练模型（ResNet-18、VGG-16）进行对比，分别采用特征提取和微调两种迁移学习策略，在五个来自孟加拉国的不同图像分类数据集上评估性能。

Result: 微调的迁移学习在所有数据集上均显著优于其他方法，准确率提升3%-76%；ResNet-18微调在Road Damage BD数据集上达到100%准确率；自定义CNN参数量（3.4M）远小于预训练模型（11M-134M），且在简单任务上训练更快。

Conclusion: 对于计算资源有限或数据集较简单的场景，可考虑轻量级自定义CNN；但在复杂任务或训练数据有限时，采用微调的预训练模型（如ResNet-18）能获得最佳性能，是更优的选择。

Abstract: This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

Abstract (中文翻译): 本研究对自建卷积神经网络（CNN）与流行的预训练架构（ResNet-18和VGG-16）进行了全面的对比分析，采用了特征提取和迁移学习两种方法。我们在五个来自孟加拉国的不同图像分类数据集上评估了这些模型：Footpath Vision、Auto Rickshaw Detection、Mango Image Classification、Paddy Variety Recognition和Road Damage Detection。实验结果表明，在不同数据集上，经过微调的迁移学习方法始终优于从零开始构建的自定义CNN和特征提取方法，准确率提升幅度在3%至76%之间。值得注意的是，采用微调的ResNet-18在Road Damage BD数据集上达到了完美的100%准确率。虽然自定义CNN在模型规模（340万参数，而预训练模型为1100万至1.34亿参数）和简单任务的训练效率方面具有优势，但结合迁移学习的预训练模型在复杂分类任务（尤其是训练数据有限的情况下）提供了更优越的性能。本研究为从业者根据数据集特性、计算资源和性能需求选择合适的深度学习方法提供了实用见解。

</details>


### [7] [PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache](https://arxiv.org/abs/2601.04359)
*Kunyang Li,Mubarak Shah,Yuzhang Shang*

Main category: cs.CV

TL;DR: 本文提出PackCache，一种无需训练的KV缓存管理方法，通过条件锚定、跨帧衰减建模和空间保留位置编码，显著加速统一自回归视频生成，尤其在长序列末尾帧上提速达2.6–3.7倍。


<details>
  <summary>Details</summary>
Motivation: 统一自回归模型在多模态任务中依赖KV缓存机制，但其缓存大小随生成长度线性增长，成为推理效率和生成长度的主要瓶颈，尤其在视频生成中更为突出。

Method: 作者分析KV缓存中不同token的时空特性，提出PackCache方法，包含三个机制：保留语义参考的条件锚定、根据时间距离分配缓存预算的跨帧衰减建模，以及在缓存压缩时维持3D结构一致性的空间保留位置嵌入。

Result: PackCache在48帧视频生成中实现1.7–2.2倍的整体加速，在最耗时的最后四帧上，分别在A40和H200上达到2.6倍和3.7倍加速。

Conclusion: PackCache有效缓解了统一自回归视频生成中的KV缓存瓶颈，显著提升长序列生成效率，具有推动更长视频生成的潜力。

Abstract: A unified autoregressive model is a Transformer-based framework that addresses diverse multimodal tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.

Abstract (中文翻译): 统一自回归模型是一种基于Transformer的框架，将多样化的多模态任务（如文本、图像、视频）视为共享词元空间下的单一序列建模问题。这类模型依赖KV缓存机制将注意力计算复杂度从O(T²)降至O(T)；然而，KV缓存大小随生成词元数量线性增长，迅速成为限制推理效率和生成长度的主要瓶颈。统一自回归视频生成也继承了这一限制。我们的分析表明，KV缓存中的词元表现出明显的时空特性：(i) 文本和条件图像词元作为持久的语义锚点，始终获得高注意力权重；(ii) 对先前帧的注意力随时间距离自然衰减。基于这些观察，我们提出了PackCache——一种无需训练的KV缓存管理方法，通过三种协同机制动态压缩KV缓存：保留语义参考的条件锚定、依据时间距离分配缓存预算的跨帧衰减建模，以及在缓存移除过程中维持连贯三维结构的空间保留位置嵌入。在效率方面，PackCache在48帧长序列上实现了1.7–2.2倍的端到端生成加速，展现出其在支持更长序列视频生成方面的强大潜力。值得注意的是，在受KV缓存持续膨胀影响最大、因而计算成本最高的最后四帧部分，PackCache在A40和H200上分别实现了2.6倍和3.7倍的加速。

</details>


### [8] [Combining facial videos and biosignals for stress estimation during driving](https://arxiv.org/abs/2601.04376)
*Paraskevi Valergaki,Vassilis C. Nicodemou,Iason Oikonomidis,Antonis Argyros,Anastasios Roussos*

Main category: cs.CV

TL;DR: 本文利用EMOCA提取的3D面部表情与姿态系数，在分心驾驶场景下分析压力反应，发现多数系数具有与生理指标相当的一致性；进一步提出基于Transformer的时序建模框架，并通过跨模态注意力融合EMOCA与生理信号，在压力识别任务中取得最优性能（AUROC 92%，准确率86.7%）。


<details>
  <summary>Details</summary>
Motivation: 由于压力具有主观性且人们可自主控制面部表情，从面部视频中可靠地识别压力极具挑战。现有方法多依赖面部动作单元（Facial Action Units），而解耦的3D面部几何特征在压力识别中的作用尚未充分探索。

Method: 作者使用EMOCA模型提取3D面部表情和姿态系数，在分心驾驶情境下对比基线阶段与压力阶段的数据；采用配对假设检验识别出具有显著相位特异性响应的系数，并构建基于Transformer的时序建模框架，评估单模态、早期融合和跨模态注意力三种融合策略。

Result: 56个系数中有41个在压力阶段表现出与生理指标相当的一致性；跨模态注意力融合EMOCA与生理信号效果最佳（AUROC 92%，准确率86.7%），EMOCA与注视信息融合也表现优异（AUROC 91.8%）。

Conclusion: 3D面部几何特征（特别是EMOCA系数）能有效反映压力状态；结合时序建模与跨模态注意力机制可显著提升压力识别性能，为非接触式压力检测提供了新思路。

Abstract: Reliable stress recognition from facial videos is challenging due to stress's subjective nature and voluntary facial control. While most methods rely on Facial Action Units, the role of disentangled 3D facial geometry remains underexplored. We address this by analyzing stress during distracted driving using EMOCA-derived 3D expression and pose coefficients. Paired hypothesis tests between baseline and stressor phases reveal that 41 of 56 coefficients show consistent, phase-specific stress responses comparable to physiological markers. Building on this, we propose a Transformer-based temporal modeling framework and assess unimodal, early-fusion, and cross-modal attention strategies. Cross-Modal Attention fusion of EMOCA and physiological signals achieves best performance (AUROC 92\%, Accuracy 86.7\%), with EMOCA-gaze fusion also competitive (AUROC 91.8\%). This highlights the effectiveness of temporal modeling and cross-modal attention for stress recognition.

Abstract (中文翻译): 由于压力具有主观性且人们能够自主控制面部表情，从面部视频中可靠地识别压力极具挑战性。尽管大多数方法依赖于面部动作单元（Facial Action Units），但解耦的3D面部几何特征在压力识别中的作用仍鲜有研究。本文通过使用EMOCA提取的3D表情与姿态系数，在分心驾驶情境下分析压力反应。基线阶段与压力阶段之间的配对假设检验表明，56个系数中有41个呈现出与生理指标相当的一致性且具有阶段特异性的压力响应。在此基础上，作者提出了一种基于Transformer的时序建模框架，并评估了单模态、早期融合以及跨模态注意力等不同融合策略。实验结果显示，EMOCA与生理信号的跨模态注意力融合取得了最佳性能（AUROC达92%，准确率为86.7%），而EMOCA与注视信息的融合也表现优异（AUROC为91.8%）。该研究突显了时序建模与跨模态注意力机制在压力识别中的有效性。

</details>


### [9] [Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection](https://arxiv.org/abs/2601.04381)
*Maxim Clouser,Kia Khezeli,John Kalantari*

Main category: cs.CV

TL;DR: 本文研究了如何通过少量配对样本，将主要在RGB图像上预训练的流匹配基础模型（FLUX.1 Kontext）适配为跨光谱翻译器，以生成红外（IR）和合成孔径雷达（SAR）图像，并用于提升下游目标检测性能。通过在每个域仅使用100张配对图像微调低秩适配（LoRA）模块，该方法能生成像素对齐的IR/SAR图像，从而复用现有边界框训练纯目标模态下的检测器。实验表明，仅用50张保留图像计算的LPIPS指标可有效预测下游检测性能（mAP），且利用外部RGB数据集生成的合成IR/SAR图像显著提升了KAIST IR行人检测和M4-SAR基础设施检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型主要在RGB数据上训练，而许多安全关键应用（如自动驾驶、遥感）依赖非可见光模态（如红外、SAR）。如何利用现有强大的RGB基础模型，以极少量样本快速适配到这些非可见光模态，是实现高效跨模态感知的关键挑战。

Method: 基于预训练的流匹配基础模型FLUX.1 Kontext，插入低秩适配（LoRA）模块，并在KAIST（RGB→IR）和M4-SAR（RGB→SAR）两个数据集上，分别仅使用100张配对图像进行微调。利用生成的像素对齐合成图像，在目标模态下训练YOLOv11n和DETR等检测器。通过网格搜索LoRA超参数，并以LPIPS作为下游性能的代理指标。

Result: 1. LPIPS在仅50张保留图像上的表现能强相关地预测下游检测mAP；2. 利用LLVIP、FLIR ADAS等外部RGB数据生成的合成IR图像，提升了KAIST IR行人检测性能；3. 合成SAR图像与少量真实SAR结合，显著提升了M4-SAR上的基础设施检测效果。

Conclusion: 少量样本（few-shot）的LoRA适配方法，能够有效将流匹配基础模型转化为跨光谱翻译器，为非可见光模态提供类似基础模型的支持，是一种有前景的技术路径。

Abstract: Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.

Abstract (中文翻译): 视觉领域的基础模型主要在RGB数据上进行训练，而许多安全关键型应用则依赖于红外（IR）和合成孔径雷达（SAR）等非可见光模态。我们研究了是否可以将一个主要在RGB图像上预训练的流匹配基础模型，仅通过少量共测样本重新用于跨光谱翻译，并探究由此生成的合成数据能否提升下游检测任务的性能。我们从FLUX.1 Kontext模型出发，插入低秩适配（LoRA）模块，并在两个场景中分别仅使用每域100张配对图像进行微调：在KAIST数据集上实现RGB到IR的转换，在M4-SAR数据集上实现RGB到SAR的转换。经过适配的模型能够将RGB图像翻译为像素对齐的IR/SAR图像，从而让我们能够复用现有的边界框，并纯粹在目标模态下训练目标检测模型。在一系列LoRA超参数的网格搜索中，我们发现仅基于50张保留配对图像计算的LPIPS指标是下游性能的强有力代理指标：更低的LPIPS值始终预示着YOLOv11n在IR和SAR上以及DETR在KAIST IR测试数据上更高的mAP。利用基于最佳LPIPS选择的LoRA适配器，从外部RGB数据集（LLVIP、FLIR ADAS）生成的合成IR图像提升了KAIST IR行人检测性能；而合成SAR图像在与有限的真实SAR数据结合时，显著提升了M4-SAR上的基础设施检测效果。我们的结果表明，对流匹配基础模型进行少量样本的LoRA适配，是为非可见光模态提供基础模型级支持的一条有前景的路径。

</details>


### [10] [Performance Analysis of Image Classification on Bangladeshi Datasets](https://arxiv.org/abs/2601.04397)
*Mohammed Sami Khan,Fabiha Muniat,Rowzatul Zannat*

Main category: cs.CV

TL;DR: 预训练CNN在准确率和收敛速度上优于自定义CNN，但自定义模型参数更少、计算更高效。


<details>
  <summary>Details</summary>
Motivation: 在图像分类任务中，选择从头设计自定义CNN还是使用预训练架构是一个重要的实际问题，本文旨在比较两者优劣。

Method: 作者设计了一个自定义CNN并从头训练，同时在相同实验条件下使用迁移学习对VGG-16、ResNet-50和MobileNet等主流架构进行训练，并用准确率、精确率、召回率和F1分数等指标评估性能。

Result: 预训练模型在分类准确率和收敛速度上始终优于自定义CNN，尤其在数据有限时；但自定义CNN以更少参数和更低计算复杂度实现了具有竞争力的性能。

Conclusion: 研究揭示了模型复杂度、性能与计算效率之间的权衡，为图像分类任务中CNN架构的选择提供了实用指导。

Abstract: Convolutional Neural Networks (CNNs) have demonstrated remarkable success in image classification tasks; however, the choice between designing a custom CNN from scratch and employing established pre-trained architectures remains an important practical consideration. In this work, we present a comparative analysis of a custom-designed CNN and several widely used deep learning architectures, including VGG-16, ResNet-50, and MobileNet, for an image classification task. The custom CNN is developed and trained from scratch, while the popular architectures are employed using transfer learning under identical experimental settings. All models are evaluated using standard performance metrics such as accuracy, precision, recall, and F1-score. Experimental results show that pre-trained CNN architectures consistently outperform the custom CNN in terms of classification accuracy and convergence speed, particularly when training data is limited. However, the custom CNN demonstrates competitive performance with significantly fewer parameters and reduced computational complexity. This study highlights the trade-offs between model complexity, performance, and computational efficiency, and provides practical insights into selecting appropriate CNN architectures for image classification problems.

Abstract (中文翻译): 卷积神经网络（CNN）在图像分类任务中展现了卓越的成功；然而，在从头设计自定义CNN与采用成熟的预训练架构之间做出选择仍是一个重要的实际考量。本文对一种自定义设计的CNN与若干广泛使用的深度学习架构（包括VGG-16、ResNet-50和MobileNet）在图像分类任务中的表现进行了对比分析。自定义CNN从零开始开发和训练，而主流架构则在相同的实验设置下采用迁移学习方式进行应用。所有模型均使用准确率、精确率、召回率和F1分数等标准性能指标进行评估。实验结果表明，预训练CNN架构在分类准确率和收敛速度方面始终优于自定义CNN，尤其是在训练数据有限的情况下。然而，自定义CNN在参数数量显著更少且计算复杂度更低的前提下，仍展现出具有竞争力的性能。本研究突显了模型复杂度、性能与计算效率之间的权衡，并为图像分类问题中选择合适的CNN架构提供了实用见解。

</details>
