{"id": "2512.17939", "pdf": "https://arxiv.org/pdf/2512.17939", "abs": "https://arxiv.org/abs/2512.17939", "authors": ["Yuncheng Lu", "Yucen Shi", "Aobo Li", "Zehao Li", "Junying Li", "Bo Wang", "Tony Tae-Hyoung Kim"], "title": "A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes", "categories": ["cs.CV"], "comment": "2 pages, 7 figures, conference paper published in IEEE Asian Solid-State Circuits Conference 2025", "summary": "We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u80fd\u6548\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u7ed3\u5408\u5e27\u57fa\u4e0e\u4e8b\u4ef6\u9a71\u52a8\u7684\u76ee\u6807\u8ddf\u8e2a\uff0c\u572840 nm\u5de5\u827a\u4e0b\u5b9e\u73b0\u6781\u4f4e\u529f\u8017\u548c98.2%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u68c0\u6d4b\u5c0f\u578b\u3001\u9ad8\u901f\u65e0\u4eba\u673a\u65f6\u5b58\u5728\u80fd\u6548\u4f4e\u3001\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u4f4e\u529f\u8017\u4e0e\u9ad8\u53ef\u9760\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u57fa\u4e8e\u6e38\u7a0b\u7f16\u7801\u7684\u4e8c\u503c\u4e8b\u4ef6\u5e27\u91cd\u5efa\u3001\u533a\u57df\u63d0\u8bae\u751f\u6210\uff0c\u5e76\u6839\u636e\u76ee\u6807\u5c3a\u5bf8\u4e0e\u901f\u5ea6\u81ea\u9002\u5e94\u5207\u6362\u5e27\u6a21\u5f0f\u4e0e\u4e8b\u4ef6\u6a21\u5f0f\uff1b\u5f15\u5165\u5feb\u901f\u76ee\u6807\u8ddf\u8e2a\u5355\u5143\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u548c\u8f68\u8ff9\u5206\u7c7b\u63d0\u5347\u9ad8\u901f\u76ee\u6807\u8ddf\u8e2a\u9c81\u68d2\u6027\uff1b\u795e\u7ecf\u5904\u7406\u5355\u5143\u91c7\u7528\u5b9a\u5236\u6307\u4ee4\u96c6\u4e0e\u96f6\u8df3\u8fc7MAC\u67b6\u6784\uff0c\u652f\u6301\u7070\u5ea6\u5757\u4e0e\u8f68\u8ff9\u63a8\u7406\u3002", "result": "\u82af\u7247\u9762\u79ef2 mm\u00b2\uff0c\u5de5\u4f5c\u7535\u538b0.8 V\uff0c\u6bcf\u50cf\u7d20\u6bcf\u5e27\u80fd\u801796 pJ\uff0c\u6bcf\u4e8b\u4ef6\u80fd\u801761 pJ\uff0c\u572850\u2013400\u7c73\u8ddd\u79bb\u548c5\u201380\u50cf\u7d20/\u79d2\u901f\u5ea6\u8303\u56f4\u5185\uff0c\u5728\u516c\u5f00UAV\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.2%\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u795e\u7ecf\u8ba1\u7b97\u5197\u4f59\u51cf\u5c11\u8d8597%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u7aef\u5230\u7aef\u80fd\u6548\u65b9\u9762\u8fbe\u5230\u5f53\u524d\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9886\u5148\u6c34\u5e73\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u4e0e\u8d85\u4f4e\u529f\u8017\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u80fd\u6548\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u878d\u5408\u4e86\u57fa\u4e8e\u5e27\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5c0f\u578b\u9ad8\u901f\u65e0\u4eba\u673a\u7684\u53ef\u9760\u68c0\u6d4b\u3002\u7cfb\u7edf\u5229\u7528\u6e38\u7a0b\u7f16\u7801\u91cd\u5efa\u4e8c\u503c\u4e8b\u4ef6\u5e27\uff0c\u751f\u6210\u533a\u57df\u63d0\u8bae\uff0c\u5e76\u6839\u636e\u76ee\u6807\u7684\u5c3a\u5bf8\u548c\u901f\u5ea6\u81ea\u9002\u5e94\u5730\u5728\u5e27\u6a21\u5f0f\u4e0e\u4e8b\u4ef6\u6a21\u5f0f\u4e4b\u95f4\u5207\u6362\u3002\u5feb\u901f\u76ee\u6807\u8ddf\u8e2a\u5355\u5143\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u548c\u57fa\u4e8e\u8f68\u8ff9\u7684\u5206\u7c7b\uff0c\u63d0\u9ad8\u4e86\u5bf9\u9ad8\u901f\u76ee\u6807\u7684\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002\u795e\u7ecf\u5904\u7406\u5355\u5143\u91c7\u7528\u5b9a\u5236\u6307\u4ee4\u96c6\u548c\u96f6\u8df3\u8fc7\u4e58\u7d2f\u52a0\uff08MAC\uff09\u67b6\u6784\uff0c\u540c\u65f6\u652f\u6301\u7070\u5ea6\u56fe\u50cf\u5757\u548c\u8f68\u8ff9\u63a8\u7406\uff0c\u5c06\u5197\u4f59\u7684\u795e\u7ecf\u8ba1\u7b97\u51cf\u5c11\u4e8697%\u4ee5\u4e0a\u3002\u8be5\u82af\u7247\u91c7\u752840 nm CMOS\u5de5\u827a\u5b9e\u73b0\uff0c\u9762\u79ef\u4e3a2 mm\u00b2\uff0c\u57280.8 V\u7535\u538b\u4e0b\u6bcf\u50cf\u7d20\u6bcf\u5e27\u80fd\u8017\u4e3a96 pJ\uff0c\u6bcf\u4e2a\u4e8b\u4ef6\u80fd\u8017\u4e3a61 pJ\uff0c\u572850\u81f3400\u7c73\u63a2\u6d4b\u8ddd\u79bb\u548c5\u81f380\u50cf\u7d20/\u79d2\u76ee\u6807\u901f\u5ea6\u8303\u56f4\u5185\uff0c\u5728\u516c\u5f00\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8698.2%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u7aef\u5230\u7aef\u80fd\u6548\u65b9\u9762\u8fbe\u5230\u4e86\u5f53\u524d\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u9886\u5148\u6c34\u5e73\u3002"}}
{"id": "2512.17943", "pdf": "https://arxiv.org/pdf/2512.17943", "abs": "https://arxiv.org/abs/2512.17943", "authors": ["Karthik Prabhakar"], "title": "NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures, 2 tables, code available at https://github.com/knkarthik01/nystagmus-photosensitivity-ai", "summary": "Nystagmus patients with photosensitivity face significant daily challenges due to involuntary eye movements exacerbated by environmental brightness conditions. Current assistive solutions are limited to symptomatic treatments without predictive personalization. This paper proposes NystagmusNet, an AI-driven system that predicts high-risk visual environments and recommends real-time visual adaptations. Using a dual-branch convolutional neural network trained on synthetic and augmented datasets, the system estimates a photosensitivity risk score based on environmental brightness and eye movement variance. The model achieves 75% validation accuracy on synthetic data. Explainability techniques including SHAP and GradCAM are integrated to highlight environmental risk zones, improving clinical trust and model interpretability. The system includes a rule-based recommendation engine for adaptive filter suggestions. Future directions include deployment via smart glasses and reinforcement learning for personalized recommendations.", "AI": {"tldr": "NystagmusNet \u662f\u4e00\u4e2a\u57fa\u4e8e AI \u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u9884\u6d4b\u773c\u9707\u60a3\u8005\u5728\u7279\u5b9a\u5149\u7167\u73af\u5883\u4e0b\u7684\u5149\u654f\u611f\u98ce\u9669\uff0c\u5e76\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u8c03\u8282\u5efa\u8bae\u3002", "motivation": "\u773c\u9707\u60a3\u8005\u56e0\u73af\u5883\u4eae\u5ea6\u5f15\u53d1\u7684\u4e0d\u81ea\u4e3b\u773c\u7403\u8fd0\u52a8\u800c\u9762\u4e34\u65e5\u5e38\u751f\u6d3b\u56f0\u96be\uff0c\u73b0\u6709\u8f85\u52a9\u624b\u6bb5\u7f3a\u4e4f\u4e2a\u6027\u5316\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u5408\u6210\u4e0e\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u6839\u636e\u73af\u5883\u4eae\u5ea6\u548c\u773c\u7403\u8fd0\u52a8\u53d8\u5f02\u4f30\u7b97\u5149\u654f\u611f\u98ce\u9669\u8bc4\u5206\uff0c\u5e76\u96c6\u6210 SHAP \u4e0e GradCAM \u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff1b\u540c\u65f6\u914d\u5907\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u8350\u5f15\u64ce\u4ee5\u63d0\u4f9b\u81ea\u9002\u5e94\u6ee4\u955c\u5efa\u8bae\u3002", "result": "\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fbe\u5230 75% \u7684\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u901a\u8fc7\u53ef\u89c6\u5316\u6280\u672f\u6807\u793a\u73af\u5883\u4e2d\u7684\u9ad8\u98ce\u9669\u533a\u57df\u3002", "conclusion": "NystagmusNet \u4e3a\u773c\u9707\u60a3\u8005\u7684\u5149\u654f\u611f\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u4e2a\u6027\u5316\u7684\u9884\u6d4b\u4e0e\u5e72\u9884\u6846\u67b6\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u667a\u80fd\u773c\u955c\u90e8\u7f72\u5e76\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63a8\u8350\u3002", "summary_cn": "\u60a3\u6709\u5149\u654f\u611f\u7684\u773c\u9707\u60a3\u8005\u7531\u4e8e\u73af\u5883\u4eae\u5ea6\u6761\u4ef6\u52a0\u5267\u4e86\u4e0d\u81ea\u4e3b\u7684\u773c\u7403\u8fd0\u52a8\uff0c\u65e5\u5e38\u751f\u6d3b\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u76ee\u524d\u7684\u8f85\u52a9\u89e3\u51b3\u65b9\u6848\u4ec5\u9650\u4e8e\u5bf9\u75c7\u6cbb\u7597\uff0c\u7f3a\u4e4f\u9884\u6d4b\u6027\u548c\u4e2a\u6027\u5316\u3002\u672c\u6587\u63d0\u51fa\u4e86 NystagmusNet\u2014\u2014\u4e00\u79cd\u7531\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u9884\u6d4b\u9ad8\u98ce\u9669\u89c6\u89c9\u73af\u5883\u5e76\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u9002\u5e94\u5efa\u8bae\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5408\u6210\u4e0e\u589e\u5f3a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6839\u636e\u73af\u5883\u4eae\u5ea6\u548c\u773c\u7403\u8fd0\u52a8\u53d8\u5f02\u4f30\u7b97\u5149\u654f\u611f\u98ce\u9669\u8bc4\u5206\u3002\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fbe\u5230\u4e86 75% \u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u3002\u7cfb\u7edf\u96c6\u6210\u4e86\u5305\u62ec SHAP \u548c GradCAM \u5728\u5185\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u4ee5\u7a81\u51fa\u663e\u793a\u73af\u5883\u4e2d\u7684\u98ce\u9669\u533a\u57df\uff0c\u4ece\u800c\u63d0\u5347\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u8fd8\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u8350\u5f15\u64ce\uff0c\u7528\u4e8e\u63d0\u4f9b\u81ea\u9002\u5e94\u6ee4\u955c\u5efa\u8bae\u3002\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u901a\u8fc7\u667a\u80fd\u773c\u955c\u90e8\u7f72\u8be5\u7cfb\u7edf\uff0c\u4ee5\u53ca\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e2a\u6027\u5316\u63a8\u8350\u3002"}}
{"id": "2512.17951", "pdf": "https://arxiv.org/pdf/2512.17951", "abs": "https://arxiv.org/abs/2512.17951", "authors": ["Kaijie Chen", "Zhiyang Xu", "Ying Shen", "Zihao Lin", "Yuguang Yao", "Lifu Huang"], "title": "SuperFlow: Training Flow Matching Models with RL on the Fly", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Recent progress in flow-based generative models and reinforcement learning (RL) has improved text-image alignment and visual quality. However, current RL training for flow models still has two main problems: (i) GRPO-style fixed per-prompt group sizes ignore variation in sampling importance across prompts, which leads to inefficient sampling and slower training; and (ii) trajectory-level advantages are reused as per-step estimates, which biases credit assignment along the flow. We propose SuperFlow, an RL training framework for flow-based models that adjusts group sizes with variance-aware sampling and computes step-level advantages in a way that is consistent with continuous-time flow dynamics. Empirically, SuperFlow reaches promising performance while using only 5.4% to 56.3% of the original training steps and reduces training time by 5.2% to 16.7% without any architectural changes. On standard text-to-image (T2I) tasks, including text rendering, compositional image generation, and human preference alignment, SuperFlow improves over SD3.5-M by 4.6% to 47.2%, and over Flow-GRPO by 1.7% to 16.0%.", "AI": {"tldr": "SuperFlow \u662f\u4e00\u79cd\u7528\u4e8e\u6d41\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u91c7\u6837\u52a8\u6001\u8c03\u6574\u6bcf\u63d0\u793a\u7ec4\u5927\u5c0f\uff0c\u5e76\u8ba1\u7b97\u4e0e\u8fde\u7eed\u65f6\u95f4\u6d41\u52a8\u529b\u5b66\u4e00\u81f4\u7684\u9010\u6b65\u4f18\u52bf\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6d41\u6a21\u578b\u8bad\u7ec3\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u4e00\u662f\u56fa\u5b9a\u6bcf\u63d0\u793a\u7ec4\u5927\u5c0f\u5ffd\u7565\u4e86\u4e0d\u540c\u63d0\u793a\u5728\u91c7\u6837\u91cd\u8981\u6027\u4e0a\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\uff1b\u4e8c\u662f\u5c06\u8f68\u8ff9\u7ea7\u4f18\u52bf\u91cd\u590d\u7528\u4f5c\u9010\u6b65\u4f30\u8ba1\uff0c\u9020\u6210\u4fe1\u7528\u5206\u914d\u504f\u5dee\u3002", "method": "\u63d0\u51fa SuperFlow \u6846\u67b6\uff0c\u91c7\u7528\u65b9\u5dee\u611f\u77e5\u91c7\u6837\u52a8\u6001\u8c03\u6574\u7ec4\u5927\u5c0f\uff0c\u5e76\u8bbe\u8ba1\u7b26\u5408\u8fde\u7eed\u65f6\u95f4\u6d41\u52a8\u529b\u5b66\u7684\u9010\u6b65\u4f18\u52bf\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "SuperFlow \u5728\u4ec5\u4f7f\u7528\u539f\u8bad\u7ec3\u6b65\u6570 5.4%\u201356.3% \u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u66f4\u4f18\u6027\u80fd\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11 5.2%\u201316.7%\uff1b\u5728\u591a\u9879\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e0a\uff0c\u76f8\u8f83 SD3.5-M \u63d0\u5347 4.6%\u201347.2%\uff0c\u76f8\u8f83 Flow-GRPO \u63d0\u5347 1.7%\u201316.0%\u3002", "conclusion": "SuperFlow \u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6d41\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u91c7\u6837\u6548\u7387\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u67b6\u6784\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u6548\u679c\u3002", "summary_cn": "\u8fd1\u671f\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8fdb\u5c55\u6539\u5584\u4e86\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u89c6\u89c9\u8d28\u91cf\u3002\u7136\u800c\uff0c\u5f53\u524d\u9488\u5bf9\u6d41\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ecd\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(i) GRPO \u98ce\u683c\u7684\u56fa\u5b9a\u6bcf\u63d0\u793a\u7ec4\u5927\u5c0f\u5ffd\u7565\u4e86\u4e0d\u540c\u63d0\u793a\u5728\u91c7\u6837\u91cd\u8981\u6027\u4e0a\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u548c\u8bad\u7ec3\u901f\u5ea6\u53d8\u6162\uff1b(ii) \u5c06\u8f68\u8ff9\u7ea7\u4f18\u52bf\u91cd\u590d\u7528\u4f5c\u9010\u6b65\u4f30\u8ba1\uff0c\u5bfc\u81f4\u6cbf\u6d41\u8def\u5f84\u7684\u4fe1\u7528\u5206\u914d\u51fa\u73b0\u504f\u5dee\u3002\u6211\u4eec\u63d0\u51fa\u4e86 SuperFlow\u2014\u2014\u4e00\u79cd\u7528\u4e8e\u6d41\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u91c7\u6837\u52a8\u6001\u8c03\u6574\u7ec4\u5927\u5c0f\uff0c\u5e76\u4ee5\u4e0e\u8fde\u7eed\u65f6\u95f4\u6d41\u52a8\u529b\u5b66\u4e00\u81f4\u7684\u65b9\u5f0f\u8ba1\u7b97\u9010\u6b65\u4f18\u52bf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSuperFlow \u5728\u4ec5\u4f7f\u7528\u539f\u59cb\u8bad\u7ec3\u6b65\u6570 5.4% \u81f3 56.3% \u7684\u60c5\u51b5\u4e0b\u5373\u80fd\u8fbe\u5230\u4f18\u5f02\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u8fdb\u884c\u4efb\u4f55\u67b6\u6784\u4fee\u6539\u7684\u524d\u63d0\u4e0b\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11 5.2% \u81f3 16.7%\u3002\u5728\u5305\u62ec\u6587\u672c\u6e32\u67d3\u3001\u7ec4\u5408\u56fe\u50cf\u751f\u6210\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u5728\u5185\u7684\u6807\u51c6\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u4efb\u52a1\u4e0a\uff0cSuperFlow \u76f8\u8f83 SD3.5-M \u63d0\u5347\u4e86 4.6% \u81f3 47.2%\uff0c\u76f8\u8f83 Flow-GRPO \u63d0\u5347\u4e86 1.7% \u81f3 16.0%\u3002"}}
{"id": "2512.17953", "pdf": "https://arxiv.org/pdf/2512.17953", "abs": "https://arxiv.org/abs/2512.17953", "authors": ["Ellie Zhou", "Jihoon Chung", "Olga Russakovsky"], "title": "Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to NeurIPS 2025 Workshops: SPACE in Vision, Language, and Embodied AI; and What Makes a Good Video: Next Practices in Video Generation and Evaluation", "summary": "Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u79cd\u89c6\u9891\u7406\u89e3\u6a21\u578b\u4e2d\u7684\u80cc\u666f\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u5206\u7c7b\u6a21\u578b\u3001\u56fe\u6587\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5747\u503e\u5411\u4e8e\u4f9d\u8d56\u80cc\u666f\u800c\u975e\u4eba\u4f53\u52a8\u4f5c\u8fdb\u884c\u5224\u65ad\uff1b\u4f5c\u8005\u63d0\u51fa\u5bf9\u5206\u7c7b\u6a21\u578b\u4f7f\u7528\u4eba\u4f53\u5206\u5272\u8f93\u5165\u53ef\u964d\u4f4e3.78%\u7684\u80cc\u666f\u504f\u89c1\uff0c\u800c\u901a\u8fc7\u63d0\u793a\u8bcd\u8c03\u4f18\u53ef\u4f7f\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u4f53\u805a\u7126\u63a8\u7406\u63d0\u53479.85%\u3002", "motivation": "\u73b0\u6709\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5e38\u4f9d\u8d56\u80cc\u666f\u7ebf\u7d22\u800c\u975e\u4eba\u4f53\u8fd0\u52a8\u4e0e\u59ff\u6001\u8fdb\u884c\u9884\u6d4b\uff0c\u5bfc\u81f4\u80cc\u666f\u504f\u89c1\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u5bf9\u4eba\u7c7b\u52a8\u4f5c\u672c\u8eab\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5bf9\u5206\u7c7b\u6a21\u578b\u3001\u56fe\u6587\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLM\uff09\u8fdb\u884c\u80cc\u666f\u504f\u89c1\u7684\u7cfb\u7edf\u5206\u6790\uff1b\u9488\u5bf9\u5206\u7c7b\u6a21\u578b\u91c7\u7528\u4eba\u4f53\u5206\u5272\u8f93\u5165\u4ee5\u7f13\u89e3\u504f\u89c1\uff1b\u5bf9VLLM\u63a2\u7d22\u624b\u52a8\u4e0e\u81ea\u52a8\u63d0\u793a\u8bcd\u8c03\u4f18\u7b56\u7565\u3002", "result": "\u6240\u6709\u88ab\u6d4b\u6a21\u578b\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u80cc\u666f\u504f\u89c1\uff1b\u4f7f\u7528\u5206\u5272\u540e\u7684\u4eba\u4f53\u8f93\u5165\u53ef\u5c06\u5206\u7c7b\u6a21\u578b\u7684\u80cc\u666f\u504f\u89c1\u964d\u4f4e3.78%\uff1b\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u8bcd\u53ef\u4f7fVLLM\u7684\u4eba\u4f53\u805a\u7126\u63a8\u7406\u80fd\u529b\u63d0\u53479.85%\u3002", "conclusion": "\u80cc\u666f\u504f\u89c1\u5e7f\u6cdb\u5b58\u5728\u4e8e\u5f53\u524d\u4e3b\u6d41\u89c6\u9891\u7406\u89e3\u6a21\u578b\u4e2d\uff0c\u4f46\u53ef\u901a\u8fc7\u8f93\u5165\u5904\u7406\u548c\u63d0\u793a\u5de5\u7a0b\u6709\u6548\u7f13\u89e3\uff0c\u4ece\u800c\u5f15\u5bfc\u6a21\u578b\u66f4\u5173\u6ce8\u4eba\u4f53\u52a8\u4f5c\u672c\u8eab\u3002", "summary_cn": "\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5e38\u5e38\u4f9d\u8d56\u80cc\u666f\u7ebf\u7d22\u800c\u975e\u4eba\u4f53\u8fd0\u52a8\u548c\u59ff\u6001\u6765\u8fdb\u884c\u9884\u6d4b\uff0c\u8fd9\u79cd\u884c\u4e3a\u88ab\u79f0\u4e3a\u80cc\u666f\u504f\u89c1\u3002\u6211\u4eec\u5bf9\u5206\u7c7b\u6a21\u578b\u3001\u5bf9\u6bd4\u5f0f\u56fe\u6587\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u53ca\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLM\uff09\u4e2d\u7684\u80cc\u666f\u504f\u89c1\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u53d1\u73b0\u5b83\u4eec\u90fd\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u4f9d\u8d56\u80cc\u666f\u8fdb\u884c\u63a8\u7406\u7684\u503e\u5411\u3002\u63a5\u7740\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9488\u5bf9\u5206\u7c7b\u6a21\u578b\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u8bc1\u660e\u5f15\u5165\u5206\u5272\u540e\u7684\u4eba\u4f53\u8f93\u5165\u80fd\u6709\u6548\u5c06\u80cc\u666f\u504f\u89c1\u964d\u4f4e3.78%\u3002\u6700\u540e\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u9488\u5bf9VLLM\u7684\u624b\u52a8\u4e0e\u81ea\u52a8\u63d0\u793a\u8bcd\u8c03\u4f18\u65b9\u6cd5\uff0c\u8868\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u8bcd\u53ef\u5c06\u6a21\u578b\u9884\u6d4b\u5f15\u5bfc\u81f3\u66f4\u5173\u6ce8\u4eba\u4f53\u672c\u8eab\u7684\u63a8\u7406\uff0c\u63d0\u5347\u5e45\u5ea6\u8fbe9.85%\u3002"}}
{"id": "2512.17954", "pdf": "https://arxiv.org/pdf/2512.17954", "abs": "https://arxiv.org/abs/2512.17954", "authors": ["Bin Wang", "Fadi Dornaika"], "title": "SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa SCS-SupCon \u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e sigmoid \u7684\u6210\u5bf9\u5bf9\u6bd4\u635f\u5931\u548c\u98ce\u683c-\u5185\u5bb9\u89e3\u8026\u7ea6\u675f\uff0c\u6709\u6548\u7f13\u89e3\u8d1f\u6837\u672c\u7a00\u91ca\u95ee\u9898\u5e76\u5b9e\u73b0\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\uff0c\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97 SOTA \u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e InfoNCE \u635f\u5931\u7684\u65b9\u6cd5\uff09\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d7\u9650\u4e8e\u7c7b\u95f4\u5dee\u5f02\u5fae\u5f31\u3001\u7c7b\u5185\u53d8\u5316\u5927\uff0c\u5b58\u5728\u8d1f\u6837\u672c\u7a00\u91ca\u548c\u7f3a\u4e4f\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5224\u522b\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon)\uff1a1\uff09\u8bbe\u8ba1\u57fa\u4e8e sigmoid \u7684\u6210\u5bf9\u5bf9\u6bd4\u635f\u5931\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6e29\u5ea6\u4e0e\u504f\u7f6e\u53c2\u6570\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\uff0c\u5f3a\u8c03\u96be\u8d1f\u6837\u672c\u5e76\u7f13\u89e3\u8d1f\u6837\u672c\u7a00\u91ca\uff1b2\uff09\u52a0\u5165\u663e\u5f0f\u7684\u98ce\u683c\u8ddd\u79bb\u7ea6\u675f\uff0c\u89e3\u8026\u98ce\u683c\u4e0e\u5185\u5bb9\u8868\u5f81\u3002", "result": "\u5728\u5305\u62ec CUB200-2011 \u548c Stanford Dogs \u5728\u5185\u7684\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSCS-SupCon \u5728 CNN \u4e0e Transformer \u4e3b\u5e72\u7f51\u7edc\u4e0b\u5747\u8fbe\u5230 SOTA\u3002\u5728 CIFAR-100\uff08ResNet-50\uff09\u4e0a\uff0c\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e0b top-1 \u51c6\u786e\u7387\u6bd4 SupCon \u9ad8\u7ea6 3.9%\uff0c\u6bd4 CS-SupCon \u9ad8\u7ea6 1.7%\uff1b\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e CS-SupCon 0.4\u20133.0 \u4e2a\u767e\u5206\u70b9\u3002Friedman \u68c0\u9a8c\u4e0e Nemenyi \u4e8b\u540e\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SCS-SupCon \u901a\u8fc7\u6539\u8fdb\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u548c\u5f15\u5165\u98ce\u683c-\u5185\u5bb9\u89e3\u8026\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "summary_cn": "\u56fe\u50cf\u5206\u7c7b\u53d7\u5230\u5fae\u5f31\u7684\u7c7b\u95f4\u5dee\u5f02\u548c\u663e\u8457\u7684\u7c7b\u5185\u53d8\u5316\u7684\u963b\u788d\uff0c\u8fd9\u9650\u5236\u4e86\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u57fa\u4e8e InfoNCE \u635f\u5931\u7684\u76d1\u7763\u5bf9\u6bd4\u65b9\u6cd5\u5b58\u5728\u8d1f\u6837\u672c\u7a00\u91ca\u95ee\u9898\uff0c\u5e76\u4e14\u7f3a\u4e4f\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u5224\u522b\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e Sigmoid \u7684\u5171\u6027\u4e0e\u98ce\u683c\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08SCS-SupCon\uff09\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e Sigmoid \u7684\u6210\u5bf9\u5bf9\u6bd4\u635f\u5931\uff0c\u5176\u4e2d\u5305\u542b\u53ef\u5b66\u4e60\u7684\u6e29\u5ea6\u548c\u504f\u7f6e\u53c2\u6570\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\u3002\u8be5\u8bbe\u8ba1\u5f3a\u8c03\u96be\u8d1f\u6837\u672c\uff0c\u7f13\u89e3\u8d1f\u6837\u672c\u7a00\u91ca\uff0c\u5e76\u66f4\u6709\u6548\u5730\u5229\u7528\u76d1\u7763\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u663e\u5f0f\u7684\u98ce\u683c\u8ddd\u79bb\u7ea6\u675f\u8fdb\u4e00\u6b65\u89e3\u8026\u4e86\u98ce\u683c\u4e0e\u5185\u5bb9\u8868\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u5b66\u4e60\u3002\u5728\u5305\u62ec CUB200-2011 \u548c Stanford Dogs \u5728\u5185\u7684\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cSCS-SupCon \u5728 CNN \u548c Transformer \u4e3b\u5e72\u7f51\u7edc\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728\u4f7f\u7528 ResNet-50 \u7684 CIFAR-100 \u6570\u636e\u96c6\u4e0a\uff0c\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e0b SCS-SupCon \u7684 top-1 \u51c6\u786e\u7387\u6bd4 SupCon \u63d0\u9ad8\u4e86\u7ea6 3.9 \u4e2a\u767e\u5206\u70b9\uff0c\u6bd4 CS-SupCon \u63d0\u9ad8\u4e86\u7ea6 1.7 \u4e2a\u767e\u5206\u70b9\u3002\u5728\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0c\u5176\u6027\u80fd\u4f18\u4e8e CS-SupCon 0.4 \u81f3 3.0 \u4e2a\u767e\u5206\u70b9\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u548c\u7edf\u8ba1\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6240\u63d0\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0cFriedman \u68c0\u9a8c\u548c Nemenyi \u4e8b\u540e\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6240\u89c2\u5bdf\u5230\u7684\u6027\u80fd\u63d0\u5347\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.17955", "pdf": "https://arxiv.org/pdf/2512.17955", "abs": "https://arxiv.org/abs/2512.17955", "authors": ["Yuxiao Li"], "title": "A Modular Framework for Single-View 3D Reconstruction of Indoor Environments", "categories": ["cs.CV"], "comment": "Master's thesis", "summary": "We propose a modular framework for single-view indoor scene 3D reconstruction, where several core modules are powered by diffusion techniques. Traditional approaches for this task often struggle with the complex instance shapes and occlusions inherent in indoor environments. They frequently overshoot by attempting to predict 3D shapes directly from incomplete 2D images, which results in limited reconstruction quality. We aim to overcome this limitation by splitting the process into two steps: first, we employ diffusion-based techniques to predict the complete views of the room background and occluded indoor instances, then transform them into 3D. Our modular framework makes contributions to this field through the following components: an amodal completion module for restoring the full view of occluded instances, an inpainting model specifically trained to predict room layouts, a hybrid depth estimation technique that balances overall geometric accuracy with fine detail expressiveness, and a view-space alignment method that exploits both 2D and 3D cues to ensure precise placement of instances within the scene. This approach effectively reconstructs both foreground instances and the room background from a single image. Extensive experiments on the 3D-Front dataset demonstrate that our method outperforms current state-of-the-art (SOTA) approaches in terms of both visual quality and reconstruction accuracy. The framework holds promising potential for applications in interior design, real estate, and augmented reality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6a21\u5757\u5316\u5355\u89c6\u89d2\u5ba4\u5185\u573a\u666f3D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6b65\u5b8c\u6210\u906e\u6321\u7269\u4f53\u8865\u5168\u3001\u623f\u95f4\u5e03\u5c40\u4fee\u590d\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u5b9e\u4f8b\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u89c6\u89c9\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5355\u89c6\u89d2\u5ba4\u51853D\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7269\u4f53\u5f62\u72b6\u4e0e\u906e\u6321\u95ee\u9898\uff0c\u76f4\u63a5\u4ece\u4e0d\u5b8c\u65742D\u56fe\u50cf\u9884\u6d4b3D\u5f62\u72b6\u5e38\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u3002", "method": "\u5c06\u91cd\u5efa\u8fc7\u7a0b\u5206\u4e3a\u4e24\u6b65\uff1a\u9996\u5148\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6280\u672f\u9884\u6d4b\u623f\u95f4\u80cc\u666f\u548c\u88ab\u906e\u6321\u7269\u4f53\u7684\u5b8c\u6574\u89c6\u56fe\uff0c\u518d\u5c06\u5176\u8f6c\u6362\u4e3a3D\uff1b\u5177\u4f53\u5305\u62ec\u906e\u6321\u5b9e\u4f8b\u7684\u975e\u6a21\u6001\u8865\u5168\u6a21\u5757\u3001\u4e13\u7528\u4e8e\u623f\u95f4\u5e03\u5c40\u9884\u6d4b\u7684\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u3001\u517c\u987e\u51e0\u4f55\u7cbe\u5ea6\u4e0e\u7ec6\u8282\u8868\u8fbe\u7684\u6df7\u5408\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u53ca\u878d\u54082D/3D\u7ebf\u7d22\u7684\u89c6\u56fe\u7a7a\u95f4\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u57283D-Front\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u5757\u5316\u6846\u67b6\u80fd\u6709\u6548\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u524d\u666f\u7269\u4f53\u548c\u623f\u95f4\u80cc\u666f\uff0c\u5728\u5ba4\u5185\u8bbe\u8ba1\u3001\u623f\u5730\u4ea7\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u9886\u57df\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5355\u89c6\u89d2\u5ba4\u5185\u573a\u666f3D\u91cd\u5efa\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5176\u4e2d\u591a\u4e2a\u6838\u5fc3\u6a21\u5757\u7531\u6269\u6563\u6280\u672f\u9a71\u52a8\u3002\u4f20\u7edf\u7684\u6b64\u7c7b\u4efb\u52a1\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u5e94\u5bf9\u5ba4\u5185\u73af\u5883\u4e2d\u56fa\u6709\u7684\u590d\u6742\u5b9e\u4f8b\u5f62\u72b6\u548c\u906e\u6321\u95ee\u9898\uff0c\u5f80\u5f80\u8bd5\u56fe\u76f4\u63a5\u4ece\u4e0d\u5b8c\u6574\u76842D\u56fe\u50cf\u9884\u6d4b3D\u5f62\u72b6\uff0c\u4ece\u800c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u53d7\u9650\u3002\u4e3a\u514b\u670d\u8fd9\u4e00\u5c40\u9650\uff0c\u6211\u4eec\u5c06\u91cd\u5efa\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u6b65\u9aa4\uff1a\u9996\u5148\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6280\u672f\u9884\u6d4b\u623f\u95f4\u80cc\u666f\u548c\u88ab\u906e\u6321\u5ba4\u5185\u7269\u4f53\u7684\u5b8c\u6574\u89c6\u56fe\uff0c\u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a3D\u5f62\u5f0f\u3002\u6211\u4eec\u7684\u6a21\u5757\u5316\u6846\u67b6\u5305\u542b\u4ee5\u4e0b\u5173\u952e\u7ec4\u4ef6\uff1a\u7528\u4e8e\u6062\u590d\u88ab\u906e\u6321\u7269\u4f53\u5b8c\u6574\u89c6\u56fe\u7684\u975e\u6a21\u6001\u8865\u5168\u6a21\u5757\u3001\u4e13\u95e8\u8bad\u7ec3\u7528\u4e8e\u9884\u6d4b\u623f\u95f4\u5e03\u5c40\u7684\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u3001\u5728\u6574\u4f53\u51e0\u4f55\u7cbe\u5ea6\u4e0e\u7ec6\u8282\u8868\u73b0\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u6df7\u5408\u6df1\u5ea6\u4f30\u8ba1\u6280\u672f\uff0c\u4ee5\u53ca\u5229\u75282D\u548c3D\u7ebf\u7d22\u786e\u4fdd\u573a\u666f\u4e2d\u7269\u4f53\u7cbe\u786e\u653e\u7f6e\u7684\u89c6\u56fe\u7a7a\u95f4\u5bf9\u9f50\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u91cd\u5efa\u524d\u666f\u7269\u4f53\u548c\u623f\u95f4\u80cc\u666f\u3002\u57283D-Front\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u91cd\u5efa\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5728\u5ba4\u5185\u8bbe\u8ba1\u3001\u623f\u5730\u4ea7\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u9886\u57df\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.17987", "pdf": "https://arxiv.org/pdf/2512.17987", "abs": "https://arxiv.org/abs/2512.17987", "authors": ["Omar Faruq Shikdar", "Fahad Ahammed", "B. M. Shahria Alam", "Golam Kibria", "Tawhidur Rahman", "Nishat Tasnim Niloy"], "title": "Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 6 figures, International Conference on Computing and Communication Networks (ICCCNet-2025)", "summary": "Tea is among the most widely consumed drinks globally. Tea production is a key industry for many countries. One of the main challenges in tea harvesting is tea leaf diseases. If the spread of tea leaf diseases is not stopped in time, it can lead to massive economic losses for farmers. Therefore, it is crucial to identify tea leaf diseases as soon as possible. Manually identifying tea leaf disease is an ineffective and time-consuming method, without any guarantee of success. Automating this process will improve both the efficiency and the success rate of identifying tea leaf diseases. The purpose of this study is to create an automated system that can classify different kinds of tea leaf diseases, allowing farmers to take action to minimize the damage. A novel dataset was developed specifically for this study. The dataset contains 5278 images across seven classes. The dataset was pre-processed prior to training the model. We deployed three pretrained models: DenseNet, Inception, and EfficientNet. EfficientNet was used only in the ensemble model. We utilized two different attention modules to improve model performance. The ensemble model achieved the highest accuracy of 85.68%. Explainable AI was introduced for better model interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc6\u522b\u4e03\u7c7b\u8336\u6811\u53f6\u75c5\u5bb3\u3002\u901a\u8fc7\u6784\u5efa\u5305\u542b5278\u5f20\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408DenseNet\u3001Inception\u548cEfficientNet\u6a21\u578b\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6240\u63d0\u51fa\u7684\u96c6\u6210\u6a21\u578b\u8fbe\u5230\u4e8685.68%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5f15\u5165\u53ef\u89e3\u91caAI\u4ee5\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u8336\u662f\u5168\u7403\u6d88\u8d39\u6700\u5e7f\u6cdb\u7684\u996e\u54c1\u4e4b\u4e00\uff0c\u8336\u4ea7\u4e1a\u5bf9\u8bb8\u591a\u56fd\u5bb6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8336\u53f6\u75c5\u5bb3\u82e5\u672a\u80fd\u53ca\u65f6\u63a7\u5236\uff0c\u5c06\u5bfc\u81f4\u519c\u6c11\u906d\u53d7\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\u3002\u4f20\u7edf\u4eba\u5de5\u8bc6\u522b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u81ea\u52a8\u5316\u7684\u75c5\u5bb3\u8bc6\u522b\u65b9\u6848\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5278\u5f20\u56fe\u50cf\u3001\u6db5\u76d6\u4e03\u7c7b\u75c5\u5bb3\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u9884\u5904\u7406\u3002\u91c7\u7528\u4e09\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08DenseNet\u3001Inception \u548c EfficientNet\uff09\uff0c\u5176\u4e2d EfficientNet \u4ec5\u7528\u4e8e\u96c6\u6210\u6a21\u578b\uff1b\u540c\u65f6\u5f15\u5165\u4e24\u79cd\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u53ef\u89e3\u91caAI\u6280\u672f\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u96c6\u6210\u6a21\u578b\u5728\u8336\u6811\u53f6\u75c5\u5bb3\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8685.68%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u8336\u6811\u53f6\u75c5\u5bb3\u8bc6\u522b\u7cfb\u7edf\uff0c\u5177\u5907\u826f\u597d\u7684\u51c6\u786e\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u519c\u6c11\u53ca\u65f6\u91c7\u53d6\u63aa\u65bd\u51cf\u5c11\u635f\u5931\u3002", "summary_cn": "\u8336\u662f\u5168\u7403\u6d88\u8d39\u6700\u5e7f\u6cdb\u7684\u996e\u54c1\u4e4b\u4e00\uff0c\u8336\u53f6\u751f\u4ea7\u662f\u8bb8\u591a\u56fd\u5bb6\u7684\u91cd\u8981\u4ea7\u4e1a\u3002\u8336\u53f6\u91c7\u6536\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u662f\u8336\u6811\u53f6\u75c5\u5bb3\u3002\u5982\u679c\u4e0d\u80fd\u53ca\u65f6\u904f\u5236\u75c5\u5bb3\u7684\u4f20\u64ad\uff0c\u5c06\u7ed9\u519c\u6c11\u5e26\u6765\u5de8\u5927\u7684\u7ecf\u6d4e\u635f\u5931\u3002\u56e0\u6b64\uff0c\u5c3d\u65e9\u8bc6\u522b\u8336\u6811\u53f6\u75c5\u5bb3\u81f3\u5173\u91cd\u8981\u3002\u4eba\u5de5\u8bc6\u522b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3001\u8017\u65f6\u4e14\u65e0\u6cd5\u4fdd\u8bc1\u6548\u679c\uff0c\u800c\u81ea\u52a8\u5316\u8bc6\u522b\u5219\u80fd\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u6210\u529f\u7387\u3002\u672c\u7814\u7a76\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u81ea\u52a8\u5206\u7c7b\u591a\u79cd\u8336\u6811\u53f6\u75c5\u5bb3\u7684\u7cfb\u7edf\uff0c\u5e2e\u52a9\u519c\u6c11\u53ca\u65f6\u91c7\u53d6\u63aa\u65bd\u4ee5\u51cf\u5c11\u635f\u5931\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u4e13\u95e8\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5278\u5f20\u56fe\u50cf\u3001\u6db5\u76d6\u4e03\u7c7b\u75c5\u5bb3\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6a21\u578b\u8bad\u7ec3\u524d\u5bf9\u6570\u636e\u8fdb\u884c\u4e86\u9884\u5904\u7406\u3002\u7814\u7a76\u90e8\u7f72\u4e86\u4e09\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff1aDenseNet\u3001Inception \u548c EfficientNet\uff08\u540e\u8005\u4ec5\u7528\u4e8e\u96c6\u6210\u6a21\u578b\uff09\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u6700\u7ec8\uff0c\u96c6\u6210\u6a21\u578b\u53d6\u5f97\u4e8685.68%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08Explainable AI\uff09\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.18003", "pdf": "https://arxiv.org/pdf/2512.18003", "abs": "https://arxiv.org/abs/2512.18003", "authors": ["Soumava Paul", "Prakhar Kaushik", "Ankit Vaidya", "Anand Bhattad", "Alan Yuille"], "title": "Name That Part: 3D Part Segmentation and Naming", "categories": ["cs.CV"], "comment": "Project page at https://name-that-part.github.io", "summary": "We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faALIGN-Parts\u65b9\u6cd5\uff0c\u5c06\u8bed\u4e493D\u90e8\u4ef6\u5206\u5272\u5efa\u6a21\u4e3a\u96c6\u5408\u5bf9\u9f50\u4efb\u52a1\uff0c\u901a\u8fc7\u878d\u5408\u51e0\u4f55\u3001\u5916\u89c2\u548c\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e00\u6b21\u6027\uff08one-shot\uff09\u76843D\u90e8\u4ef6\u5206\u5272\u4e0e\u547d\u540d\uff0c\u5e76\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u8de8\u6570\u636e\u96c6\u90e8\u4ef6\u672c\u4f53\u3002", "motivation": "\u73b0\u67093D\u90e8\u4ef6\u5206\u5272\u6570\u636e\u96c6\u7684\u6807\u6ce8\u5b9a\u4e49\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u8bad\u7ec3\uff1b\u4ee5\u5f80\u65b9\u6cd5\u8981\u4e48\u4ea7\u751f\u65e0\u6807\u7b7e\u5206\u89e3\uff0c\u8981\u4e48\u4ec5\u68c0\u7d22\u5355\u4e2a\u90e8\u4ef6\u800c\u7f3a\u4e4f\u5b8c\u6574\u5f62\u72b6\u6807\u6ce8\u3002", "method": "\u63d0\u51faALIGN-Parts\u65b9\u6cd5\uff0c\u5c06\u90e8\u4ef6\u547d\u540d\u89c6\u4e3a\u96c6\u5408\u5bf9\u9f50\u4efb\u52a1\uff1a\u5c06\u5f62\u72b6\u5206\u89e3\u4e3a\u9690\u5f0f\u7684\u201cpartlets\u201d\uff0c\u5e76\u901a\u8fc7\u4e8c\u5206\u56fe\u5339\u914d\u5c06\u5176\u4e0e\u90e8\u4ef6\u63cf\u8ff0\u5bf9\u9f50\uff1b\u878d\u54083D\u90e8\u4ef6\u573a\u7684\u51e0\u4f55\u7ebf\u7d22\u3001\u591a\u89c6\u89d2\u89c6\u89c9\u7279\u5f81\u7684\u5916\u89c2\u4fe1\u606f\u4ee5\u53ca\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u529f\u80fd\u6027\u8bed\u4e49\u63cf\u8ff0\uff1b\u91c7\u7528\u6587\u672c\u5bf9\u9f50\u635f\u5931\u4f7fpartlets\u4e0e\u6587\u672c\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u5339\u914d\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5b9e\u73b0\u4e00\u6b21\u60273D\u90e8\u4ef6\u5206\u5272\u4e0e\u547d\u540d\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u5305\u62ec\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u6807\u6ce8\u5f15\u64ce\uff1b\u901a\u8fc7\u96f6\u6837\u672c\u5339\u914d\u4efb\u610f\u63cf\u8ff0\u53ca\u7f6e\u4fe1\u5ea6\u6821\u51c6\u9884\u6d4b\uff0c\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\uff0c\u6784\u5efa\u4e86\u6db5\u76d6PartNet\u30013DCoMPaT++\u548cFind3D\u7684\u7edf\u4e00\u672c\u4f53\uff08\u542b1,794\u4e2a\u552f\u4e003D\u90e8\u4ef6\uff09\uff1b\u5e76\u53d1\u5e03\u4e86\u65b0\u6570\u636e\u96c6Tex-Parts\uff1b\u8fd8\u63d0\u51fa\u4e86\u4e24\u9879\u9002\u7528\u4e8e\u547d\u540d3D\u90e8\u4ef6\u5206\u5272\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "ALIGN-Parts\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65b0\u9896\u7684\u4e00\u6b21\u60273D\u90e8\u4ef6\u5206\u5272\u4e0e\u547d\u540d\u6846\u67b6\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u3001\u96f6\u6837\u672c\u5339\u914d\u548c\u8de8\u6570\u636e\u96c6\u5bf9\u9f50\uff0c\u4e3a3D\u8bed\u4e49\u7406\u89e3\u4e0e\u6807\u6ce8\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u6211\u4eec\u7814\u7a76\u8bed\u4e493D\u90e8\u4ef6\u5206\u5272\u95ee\u9898\uff0c\u5373\u628a\u7269\u4f53\u5206\u89e3\u4e3a\u5177\u6709\u6709\u610f\u4e49\u540d\u79f0\u7684\u90e8\u4ef6\u3002\u5c3d\u7ba1\u5df2\u6709\u5e26\u90e8\u4ef6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u4f46\u5176\u5b9a\u4e49\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u8bad\u7ec3\u3002\u4ee5\u5f80\u65b9\u6cd5\u8981\u4e48\u751f\u6210\u65e0\u6807\u7b7e\u7684\u5206\u89e3\u7ed3\u679c\uff0c\u8981\u4e48\u4ec5\u68c0\u7d22\u5355\u4e2a\u90e8\u4ef6\u800c\u7f3a\u4e4f\u5b8c\u6574\u5f62\u72b6\u7684\u6807\u6ce8\u3002\u6211\u4eec\u63d0\u51faALIGN-Parts\u65b9\u6cd5\uff0c\u5c06\u90e8\u4ef6\u547d\u540d\u5efa\u6a21\u4e3a\u76f4\u63a5\u7684\u96c6\u5408\u5bf9\u9f50\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5c06\u5f62\u72b6\u5206\u89e3\u4e3a\u201cpartlets\u201d\u2014\u2014\u4e00\u79cd\u9690\u5f0f\u76843D\u90e8\u4ef6\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u4e8c\u5206\u56fe\u5339\u914d\u5c06\u5176\u4e0e\u90e8\u4ef6\u63cf\u8ff0\u5bf9\u9f50\u3002\u6211\u4eec\u878d\u5408\u4e86\u6765\u81ea3D\u90e8\u4ef6\u573a\u7684\u51e0\u4f55\u7ebf\u7d22\u3001\u591a\u89c6\u89d2\u89c6\u89c9\u7279\u5f81\u7684\u5916\u89c2\u4fe1\u606f\uff0c\u4ee5\u53ca\u7531\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u529f\u80fd\u6027\u8bed\u4e49\u63cf\u8ff0\u3002\u6587\u672c\u5bf9\u9f50\u635f\u5931\u786e\u4fddpartlets\u4e0e\u6587\u672c\u5171\u4eab\u540c\u4e00\u5d4c\u5165\u7a7a\u95f4\uff0c\u4ece\u800c\u5728\u6570\u636e\u5145\u8db3\u7684\u524d\u63d0\u4e0b\u652f\u6301\u7406\u8bba\u4e0a\u5f00\u653e\u8bcd\u6c47\u7684\u5339\u914d\u8bbe\u7f6e\u3002\u6211\u4eec\u63d0\u51fa\u7684\u9ad8\u6548\u4e14\u65b0\u9896\u7684\u4e00\u6b21\u60273D\u90e8\u4ef6\u5206\u5272\u4e0e\u547d\u540d\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5305\u62ec\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u6807\u6ce8\u5f15\u64ce\u3002\u7531\u4e8e\u6a21\u578b\u652f\u6301\u5bf9\u4efb\u610f\u63cf\u8ff0\u7684\u96f6\u6837\u672c\u5339\u914d\uff0c\u4ee5\u53ca\u5bf9\u5df2\u77e5\u7c7b\u522b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u9884\u6d4b\uff0c\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u672c\u4f53\uff0c\u5bf9\u9f50\u4e86PartNet\u30013DCoMPaT++\u548cFind3D\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u5171\u5305\u542b1,794\u4e2a\u552f\u4e00\u76843D\u90e8\u4ef6\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u65b0\u6784\u5efa\u7684Tex-Parts\u6570\u636e\u96c6\u4e2d\u7684\u793a\u4f8b\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u9879\u9002\u7528\u4e8e\u547d\u540d3D\u90e8\u4ef6\u5206\u5272\u4efb\u52a1\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2512.18004", "pdf": "https://arxiv.org/pdf/2512.18004", "abs": "https://arxiv.org/abs/2512.18004", "authors": ["Shubham Kumar Nigam", "Parjanya Aditya Shukla", "Noel Shallum", "Arnab Bhattacharya"], "title": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted in AILaw @ AAAI 2026 Conference", "summary": "Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4f20\u7edfOCR+\u673a\u5668\u7ffb\u8bd1\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e0e\u7aef\u5230\u7aef\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9a6c\u62c9\u5730\u8bed\u624b\u5199\u6cd5\u5f8b\u6587\u6863\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u65e8\u5728\u4e3a\u8d44\u6e90\u7a00\u7f3a\u73af\u5883\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u90e8\u7f72\u7684\u6cd5\u5f8b\u6587\u672c\u6570\u5b57\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5370\u5ea6\u5730\u65b9\u6cd5\u9662\u548c\u9ad8\u7b49\u6cd5\u9662\u4e2d\u5b58\u5728\u5927\u91cf\u9a6c\u62c9\u5730\u8bed\u624b\u5199\u6cd5\u5f8b\u6587\u4ef6\uff08\u5982FIR\u3001\u8d77\u8bc9\u4e66\u548c\u8bc1\u4eba\u9648\u8ff0\uff09\uff0c\u4e9f\u9700\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u7ffb\u8bd1\u7cfb\u7edf\u8fdb\u884c\u6570\u5b57\u5316\uff0c\u4ee5\u63d0\u5347\u975e\u6bcd\u8bed\u4eba\u58eb\u548c\u6cd5\u5f8b\u4ece\u4e1a\u8005\u5bf9\u6cd5\u5f8b\u4fe1\u606f\u7684\u83b7\u53d6\u80fd\u529b\u3002", "method": "\u5bf9\u6bd4\u4f20\u7edfOCR-MT\u4e24\u9636\u6bb5\u6d41\u7a0b\u4e0e\u7aef\u5230\u7aef\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLM\uff09\u5728\u624b\u5199\u6587\u672c\u56fe\u50cf\u76f4\u63a5\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u81ea\u5efa\u7684\u9a6c\u62c9\u5730\u8bed\u624b\u5199\u6cd5\u5f8b\u6587\u6863\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e24\u79cd\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u6027\u80fd\u5dee\u5f02\u7684\u53ef\u64cd\u4f5c\u6027\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u9c81\u68d2\u3001\u53ef\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7aef\u5230\u7aef\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5177\u6709\u66ff\u4ee3\u4f20\u7edfOCR+MT\u6d41\u7a0b\u7684\u6f5c\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u624b\u5199\u6cd5\u5f8b\u6587\u6863\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "summary_cn": "\u624b\u5199\u6587\u672c\u8bc6\u522b\uff08HTR\uff09\u548c\u673a\u5668\u7ffb\u8bd1\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u5bf9\u4e8e\u9a6c\u62c9\u5730\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u800c\u8a00\uff0c\u8fd9\u7c7b\u8bed\u8a00\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u5b57\u5316\u8bed\u6599\u5e93\uff0c\u4e14\u624b\u5199\u98ce\u683c\u9ad8\u5ea6\u591a\u6837\u5316\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u9996\u5148\u901a\u8fc7OCR\u7cfb\u7edf\u4ece\u624b\u5199\u56fe\u50cf\u4e2d\u63d0\u53d6\u6587\u672c\uff0c\u518d\u5229\u7528\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u5c06\u5176\u7ffb\u8bd1\u4e3a\u76ee\u6807\u8bed\u8a00\u3002\u672c\u6587\u63a2\u7d22\u5e76\u6bd4\u8f83\u4e86\u4f20\u7edfOCR-MT\u6d41\u6c34\u7ebf\u4e0e\u8bd5\u56fe\u5c06\u8fd9\u4e24\u4e2a\u9636\u6bb5\u7edf\u4e00\u8d77\u6765\u3001\u76f4\u63a5\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u7ffb\u8bd1\u624b\u5199\u6587\u672c\u56fe\u50cf\u7684\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08Vision Large Language Models\uff09\u7684\u6027\u80fd\u3002\u6211\u4eec\u7684\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u8feb\u5207\u9700\u8981\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4ee5\u5b9e\u73b0\u5370\u5ea6\u5730\u65b9\u6cd5\u9662\u548c\u9ad8\u7b49\u6cd5\u9662\u4e2d\u8bf8\u5982FIR\uff08\u7b2c\u4e00\u4fe1\u606f\u62a5\u544a\uff09\u3001\u8d77\u8bc9\u4e66\u548c\u8bc1\u4eba\u9648\u8ff0\u7b49\u6cd5\u5f8b\u8bb0\u5f55\u7684\u6570\u5b57\u5316\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u7cbe\u5fc3\u6574\u7406\u7684\u9a6c\u62c9\u5730\u8bed\u624b\u5199\u6cd5\u5f8b\u6587\u6863\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e0a\u8ff0\u4e24\u79cd\u65b9\u6cd5\uff0c\u76ee\u6807\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u6cd5\u5f8b\u6587\u6863\u5904\u7406\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u9c81\u68d2\u3001\u53ef\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u63d0\u5347\u975e\u6bcd\u8bed\u4f7f\u7528\u8005\u548c\u6cd5\u5f8b\u4e13\u4e1a\u4eba\u58eb\u83b7\u53d6\u6cd5\u5f8b\u4fe1\u606f\u7684\u80fd\u529b\u3002"}}
{"id": "2512.18038", "pdf": "https://arxiv.org/pdf/2512.18038", "abs": "https://arxiv.org/abs/2512.18038", "authors": ["Fakrul Islam Tushar", "Ehsan Samei", "Cynthia Rudin", "Joseph Y. Lo"], "title": "NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging", "categories": ["cs.CV", "cs.LG"], "comment": "3 tables, 7 figures, 12 Supplement tables, 9 Supplement figures", "summary": "Objective: Although medical imaging datasets are increasingly available, abnormal and annotation-intensive findings critical to lung cancer screening, particularly small pulmonary nodules, remain underrepresented and inconsistently curated. Methods: We introduce NodMAISI, an anatomically constrained, nodule-oriented CT synthesis and augmentation framework trained on a unified multi-source cohort (7,042 patients, 8,841 CTs, 14,444 nodules). The framework integrates: (i) a standardized curation and annotation pipeline linking each CT with organ masks and nodule-level annotations, (ii) a ControlNet-conditioned rectified-flow generator built on MAISI-v2's foundational blocks to enforce anatomy- and lesion-consistent synthesis, and (iii) lesion-aware augmentation that perturbs nodule masks (controlled shrinkage) while preserving surrounding anatomy to generate paired CT variants. Results: Across six public test datasets, NodMAISI improved distributional fidelity relative to MAISI-v2 (real-to-synthetic FID range 1.18 to 2.99 vs 1.69 to 5.21). In lesion detectability analysis using a MONAI nodule detector, NodMAISI substantially increased average sensitivity and more closely matched clinical scans (IMD-CT: 0.69 vs 0.39; DLCS24: 0.63 vs 0.20), with the largest gains for sub-centimeter nodules where MAISI-v2 frequently failed to reproduce the conditioned lesion. In downstream nodule-level malignancy classification trained on LUNA25 and externally evaluated on LUNA16, LNDbv4, and DLCS24, NodMAISI augmentation improved AUC by 0.07 to 0.11 at <=20% clinical data and by 0.12 to 0.21 at 10%, consistently narrowing the performance gap under data scarcity.", "AI": {"tldr": "NodMAISI \u662f\u4e00\u4e2a\u9762\u5411\u80ba\u7ed3\u8282\u7684 CT \u5408\u6210\u4e0e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u5256\u7ea6\u675f\u548c\u75c5\u7076\u611f\u77e5\u589e\u5f3a\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u3001\u5c0f\u7ed3\u8282\u53ef\u68c0\u6d4b\u6027\u53ca\u4e0b\u6e38\u6076\u6027\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e34\u5e8a\u6570\u636e\u7a00\u7f3a\u65f6\u6548\u679c\u7a81\u51fa\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u6570\u636e\u867d\u65e5\u76ca\u4e30\u5bcc\uff0c\u4f46\u5bf9\u80ba\u764c\u7b5b\u67e5\u81f3\u5173\u91cd\u8981\u7684\u5f02\u5e38\u53d1\u73b0\uff08\u5c24\u5176\u662f\u5c0f\u80ba\u7ed3\u8282\uff09\u4ecd\u5b58\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u548c\u6807\u6ce8\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa NodMAISI \u6846\u67b6\uff0c\u6574\u5408\uff1a(i) \u6807\u51c6\u5316\u6807\u6ce8\u6d41\u7a0b\uff08\u542b\u5668\u5b98\u63a9\u819c\u4e0e\u7ed3\u8282\u7ea7\u6807\u6ce8\uff09\uff0c(ii) \u57fa\u4e8e MAISI-v2 \u5e76\u5f15\u5165 ControlNet \u6761\u4ef6\u7684 rectified-flow \u751f\u6210\u5668\u4ee5\u4fdd\u8bc1\u89e3\u5256\u4e0e\u75c5\u7076\u4e00\u81f4\u6027\uff0c(iii) \u75c5\u7076\u611f\u77e5\u589e\u5f3a\u7b56\u7565\uff08\u53ef\u63a7\u7f29\u5c0f\u7ed3\u8282\u63a9\u819c\u540c\u65f6\u4fdd\u7559\u5468\u56f4\u89e3\u5256\u7ed3\u6784\uff09\u751f\u6210\u914d\u5bf9 CT \u53d8\u4f53\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5f00\u6d4b\u8bd5\u96c6\u4e0a\uff0cNodMAISI \u76f8\u8f83 MAISI-v2 \u663e\u8457\u63d0\u5347\u5206\u5e03\u4fdd\u771f\u5ea6\uff08FID \u66f4\u4f4e\uff09\uff1b\u4f7f\u7528 MONAI \u7ed3\u8282\u68c0\u6d4b\u5668\u8bc4\u4f30\u65f6\uff0c\u5176\u5408\u6210\u56fe\u50cf\u7684\u5e73\u5747\u654f\u611f\u6027\u66f4\u9ad8\uff0c\u5c24\u5176\u5bf9\u4e9a\u5398\u7c73\u7ed3\u8282\uff1b\u5728 LUNA25 \u4e0a\u8bad\u7ec3\u5e76\u5728\u591a\u4e2a\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u7684\u6076\u6027\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cNodMAISI \u589e\u5f3a\u5728\u4ec5\u7528 10\u201320% \u4e34\u5e8a\u6570\u636e\u65f6 AUC \u63d0\u5347 0.07\u20130.21\uff0c\u6709\u6548\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "conclusion": "NodMAISI \u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u89e3\u5256\u4e00\u81f4\u4e14\u75c5\u7076\u771f\u5b9e\u7684\u80ba\u90e8 CT \u56fe\u50cf\uff0c\u5728\u63d0\u5347\u5c0f\u7ed3\u8282\u68c0\u6d4b\u4e0e\u5206\u7c7b\u6027\u80fd\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u4e34\u5e8a\u573a\u666f\u3002", "summary_cn": "\u76ee\u7684\uff1a\u5c3d\u7ba1\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5bf9\u80ba\u764c\u7b5b\u67e5\u81f3\u5173\u91cd\u8981\u7684\u5f02\u5e38\u53d1\u73b0\uff08\u5c24\u5176\u662f\u5c0f\u578b\u80ba\u7ed3\u8282\uff09\u4ecd\u7136\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u4e14\u6807\u6ce8\u4e0d\u4e00\u81f4\u3002\u65b9\u6cd5\uff1a\u6211\u4eec\u63d0\u51fa\u4e86 NodMAISI\u2014\u2014\u4e00\u79cd\u53d7\u89e3\u5256\u7ed3\u6784\u7ea6\u675f\u3001\u9762\u5411\u7ed3\u8282\u7684 CT \u5408\u6210\u4e0e\u589e\u5f3a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6e90\u961f\u5217\uff087,042 \u540d\u60a3\u8005\u30018,841 \u6b21 CT \u626b\u63cf\u300114,444 \u4e2a\u7ed3\u8282\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\uff1a(i) \u4e00\u5957\u6807\u51c6\u5316\u7684\u6570\u636e\u6574\u7406\u4e0e\u6807\u6ce8\u6d41\u7a0b\uff0c\u5c06\u6bcf\u4f8b CT \u4e0e\u5668\u5b98\u63a9\u819c\u53ca\u7ed3\u8282\u7ea7\u6807\u6ce8\u76f8\u5173\u8054\uff1b(ii) \u4e00\u4e2a\u57fa\u4e8e MAISI-v2 \u57fa\u7840\u6a21\u5757\u5e76\u91c7\u7528 ControlNet \u6761\u4ef6\u63a7\u5236\u7684 rectified-flow \u751f\u6210\u5668\uff0c\u4ee5\u786e\u4fdd\u5408\u6210\u56fe\u50cf\u5728\u89e3\u5256\u7ed3\u6784\u548c\u75c5\u7076\u7279\u5f81\u4e0a\u7684\u4e00\u81f4\u6027\uff1b(iii) \u4e00\u79cd\u75c5\u7076\u611f\u77e5\u589e\u5f3a\u7b56\u7565\uff0c\u5728\u4fdd\u7559\u5468\u56f4\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u5bf9\u7ed3\u8282\u63a9\u819c\u8fdb\u884c\u6270\u52a8\uff08\u53ef\u63a7\u6536\u7f29\uff09\uff0c\u4ece\u800c\u751f\u6210\u914d\u5bf9\u7684 CT \u53d8\u4f53\u3002\u7ed3\u679c\uff1a\u5728\u516d\u4e2a\u516c\u5f00\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0cNodMAISI \u76f8\u8f83\u4e8e MAISI-v2 \u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u4fdd\u771f\u5ea6\uff08\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u95f4\u7684 FID \u8303\u56f4\u4e3a 1.18 \u81f3 2.99\uff0c\u800c MAISI-v2 \u4e3a 1.69 \u81f3 5.21\uff09\u3002\u5728\u4f7f\u7528 MONAI \u7ed3\u8282\u68c0\u6d4b\u5668\u8fdb\u884c\u7684\u75c5\u7076\u53ef\u68c0\u6d4b\u6027\u5206\u6790\u4e2d\uff0cNodMAISI \u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u654f\u611f\u6027\uff0c\u5e76\u66f4\u63a5\u8fd1\u4e34\u5e8a\u626b\u63cf\u7ed3\u679c\uff08IMD-CT\uff1a0.69 \u5bf9\u6bd4 0.39\uff1bDLCS24\uff1a0.63 \u5bf9\u6bd4 0.20\uff09\uff0c\u5176\u4e2d\u5bf9\u4e9a\u5398\u7c73\u7ed3\u8282\u7684\u63d0\u5347\u6700\u4e3a\u663e\u8457\uff0c\u56e0\u4e3a MAISI-v2 \u7ecf\u5e38\u65e0\u6cd5\u590d\u73b0\u6240\u8bbe\u5b9a\u7684\u75c5\u7076\u6761\u4ef6\u3002\u5728\u4e0b\u6e38\u7ed3\u8282\u7ea7\u522b\u6076\u6027\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4ee5 LUNA25 \u8bad\u7ec3\u5e76\u5728 LUNA16\u3001LNDbv4 \u548c DLCS24 \u4e0a\u8fdb\u884c\u5916\u90e8\u8bc4\u4f30\u65f6\uff0cNodMAISI \u589e\u5f3a\u5728\u4ec5\u4f7f\u7528 \u226420% \u4e34\u5e8a\u6570\u636e\u65f6\u4f7f AUC \u63d0\u9ad8 0.07 \u81f3 0.11\uff0c\u5728\u4ec5\u4f7f\u7528 10% \u6570\u636e\u65f6\u63d0\u9ad8 0.12 \u81f3 0.21\uff0c\u59cb\u7ec8\u6709\u6548\u7f29\u5c0f\u4e86\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
