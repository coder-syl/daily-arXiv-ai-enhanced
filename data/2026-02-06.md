<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 30]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文首次对3D高斯泼溅（3DGS）知识产权保护领域进行了系统性综述，提出了一个自下而上的分析框架，并指出了未来六大研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯泼溅在实时3D场景合成中的广泛应用及其商业价值提升，其显式的参数化结构引发了知识产权保护的新需求，但当前研究分散，缺乏统一视角。

Method: 提出一个系统性综述框架，从底层高斯扰动机制、被动与主动保护范式、以及生成式AI时代下的鲁棒性威胁三个层面进行分析。

Result: 揭示了当前技术基础和鲁棒性表征方面的不足，并识别出深入研究的机会。

Conclusion: 为3DGS资产的可靠可信知识产权保护提供了清晰的研究路线图，涵盖鲁棒性、效率和保护范式等六个方向。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

Abstract (中文翻译): 3D高斯泼溅（3DGS）已成为实现实时3D场景合成的主流表示方法，广泛应用于虚拟与增强现实、机器人技术和3D内容创作等领域。其日益增长的商业价值和显式的参数化结构引发了新兴的知识产权（IP）保护问题，促使学界对3DGS知识产权保护展开大量研究。然而，当前的研究仍较为零散，缺乏对底层机制、保护范式及鲁棒性挑战的统一认识。为填补这一空白，我们首次对3DGS知识产权保护进行了系统性综述，并提出一个自下而上的分析框架，该框架涵盖：（i）基于高斯的底层扰动机制，（ii）被动与主动保护范式，以及（iii）在新兴生成式AI时代下面临的鲁棒性威胁。该综述揭示了当前在技术基础和鲁棒性表征方面的不足，并指出了进一步深入研究的机会。最后，我们提出了涵盖鲁棒性、效率和保护范式等六个方面的研究方向，为实现可靠且可信的3DGS资产知识产权保护提供了发展路线图。

</details>


### [2] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: 提出TruKAN，一种基于截断幂函数的新KAN架构，在保持表达能力的同时提升准确率、训练速度和可解释性，并在视觉任务中优于现有KAN变体。


<details>
  <summary>Details</summary>
Motivation: 现有Kolmogorov-Arnold Network（KAN）在计算效率与理论原则之间存在权衡，作者希望设计一种兼顾逼近能力与透明性的新架构。

Method: 将KAN中的B样条基替换为源自k阶样条理论的截断幂函数族；每层结合截断幂项与多项式项，并支持共享或独立节点；将其集成到EfficientNet-V2框架中，采用混合优化策略和层归一化进行训练。

Result: TruKAN在多个视觉基准数据集上相比MLP、KAN和SineKAN等模型，在准确率、训练时间和内存占用方面均表现更优，尤其在小型和深层架构中展现出更强的综合性能。

Conclusion: TruKAN通过简化基函数和节点配置，在保持KAN表达能力的同时显著提升了效率与可解释性，拓展了KAN在复杂视觉任务中的适用性。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

Abstract (中文翻译): 为解决计算效率与Kolmogorov-Arnold网络（KAN）原则遵循之间的权衡问题，我们提出了TruKAN——一种基于KAN结构和可学习激活函数的新架构。TruKAN用源自k阶样条理论的一族截断幂函数替代了KAN中的B样条基，这一改动在保持KAN表达能力的同时提升了准确率和训练速度。每个TruKAN层结合了一个截断幂项与一个多项式项，并采用共享或独立的节点（knots）。由于其简化的基函数和节点配置，TruKAN相比其他KAN变体具有更强的可解释性。通过优先选择可解释的基函数，TruKAN旨在平衡逼近效果与透明性。我们开发了TruKAN模型并将其集成到基于EfficientNet-V2的先进框架中，并在计算机视觉基准数据集上进行评估。为确保公平比较，我们构建了多种模型：基于MLP、KAN、SineKAN和TruKAN的EfficientNet框架，并在小型和深层架构下评估它们的训练时间和准确率。训练阶段采用混合优化策略以提高收敛稳定性。此外，我们还研究了所有模型的层归一化技术，并评估了TruKAN中共享节点与独立节点的影响。总体而言，TruKAN在复杂视觉任务中在准确率、计算效率和内存使用方面均优于其他KAN模型，展现出超越以往KAN研究有限设定的优势。

</details>


### [3] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 提出了一种名为DiGAN的新方法，结合扩散模型与注意力卷积网络，用于从有限且不规则的纵向神经影像数据中增强时间上下文并提升早期阿尔茨海默病检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在早期阿尔茨海默病诊断中受限于对大规模纵向数据的依赖，且难以处理临床数据中固有的时间连续性和模态不规则性问题。

Method: 提出Diffusion-Guided Attention Network（DiGAN），将潜在扩散模型与注意力引导的卷积网络相结合：扩散模型从有限数据合成逼真的纵向神经影像轨迹，注意力卷积层则捕捉区分认知正常、轻度认知障碍和主观认知下降个体的结构-时间模式。

Result: 在合成数据集和ADNI数据集上的实验表明，DiGAN优于当前最先进的基线方法，在早期阿尔茨海默病检测方面展现出优越性能。

Conclusion: DiGAN通过融合扩散建模与注意力机制，有效应对了真实临床环境中数据稀疏与不规则的问题，为早期阿尔茨海默病的自动诊断提供了新思路。

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

Abstract (中文翻译): 由于前驱期阶段脑结构变化细微且时间进程不规则，阿尔茨海默病（AD）的早期诊断仍是一项重大挑战。现有的深度学习方法通常需要大量纵向数据集，并且往往无法有效建模真实临床数据中固有的时间连续性和模态不规则性。为解决这些局限性，我们提出了扩散引导注意力网络（DiGAN），该方法将潜在扩散建模与注意力引导的卷积网络相结合。扩散模型能够从有限的训练数据中合成逼真的纵向神经影像轨迹，从而丰富时间上下文信息并提高对访视时间间隔不均的鲁棒性。随后，注意力卷积层可捕捉具有判别性的结构-时间模式，以区分认知正常个体、轻度认知障碍患者和主观认知下降个体。在合成数据集和ADNI数据集上的实验表明，DiGAN优于现有的最先进基线方法，展现出其在早期AD检测中的潜力。

</details>


### [4] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: PriorProbe 是一种基于 MCMC 与人类结合的新方法，能有效提取个体细粒度认知先验，并用于提升神经网络对模糊刺激的个性化预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提取个体认知先验时存在无法唯一识别或引入系统性偏差的问题，限制了神经网络的个性化能力。

Method: 提出 PriorProbe 方法，基于“与人结合的马尔可夫链蒙特卡洛”（MCMC with People），从个体参与者中恢复其特定的认知先验，并将其整合到先进神经网络中。

Result: 在面部表情识别任务中，整合 PriorProbe 提取的先验显著提升了模型对模糊刺激的个体分类预测性能，优于仅用神经网络或其他先验来源，且不损害对真实标签的推理能力。

Conclusion: PriorProbe 为个性化深度神经网络提供了一个通用且可解释的框架。

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

Abstract (中文翻译): 引入个体层面的认知先验是实现神经网络个性化的重要途径，但准确获取此类先验仍具挑战性：现有方法要么无法唯一识别这些先验，要么会引入系统性偏差。本文提出 PriorProbe，这是一种基于“与人结合的马尔可夫链蒙特卡洛”（Markov Chain Monte Carlo with People）的新颖先验提取方法，能够恢复细粒度的、个体特定的先验。我们以面部表情识别任务为例，对个体参与者应用 PriorProbe，并测试将所恢复的先验整合到当前最先进的神经网络中是否能提升其对模糊刺激下个体分类判断的预测能力。结果表明，由 PriorProbe 提取的先验带来了显著的性能提升，不仅优于单独使用神经网络，也优于其他先验来源，同时保留了网络对真实标签的推理能力。综上所述，本研究证明 PriorProbe 为个性化深度神经网络提供了一个通用且可解释的框架。

</details>


### [5] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 该研究提出了一种可解释的计算机视觉框架，用于在增材制造部件的三维断层图像中检测孔隙并评估其临界性。结果表明，孔隙到表面的归一化距离是预测临界性的主导因素，远超其他几何特征。


<details>
  <summary>Details</summary>
Motivation: 增材制造部件中的内部孔隙是影响结构性能的关键缺陷，现有自动检测方法缺乏可解释性，使工程师难以理解临界性预测的物理依据。

Method: 将灰度切片重建为三维数据集，通过基于强度的阈值分割和连通成分分析识别出500个独立孔隙；利用尺寸、长宽比、范围和距边界距离等几何描述符对孔隙进行表征；基于百分位数的欧氏距离构建孔隙交互网络（含24,950条连接）；使用机器学习模型预测孔隙临界性，并通过SHAP分析量化各特征贡献。

Result: 归一化表面距离对模型预测的贡献比其他所有描述符高出一个数量级以上；孔隙尺寸影响微弱，其他几何参数几乎无影响；表面邻近性与临界性呈强负相关，揭示了边界驱动的失效机制。

Conclusion: 该可解释框架实现了透明的缺陷评估，为增材制造的工艺优化和质量控制提供了可操作的见解。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

Abstract (中文翻译): 内部孔隙仍是增材制造部件中一种关键缺陷模式，会损害结构性能并限制其工业应用。尽管已有自动化缺陷检测方法，但它们缺乏可解释性，使工程师无法理解临界性预测的物理基础。本研究提出了一种可解释的计算机视觉框架，用于在三维断层扫描体积中进行孔隙检测和临界性评估。首先将连续的灰度切片重建为体积数据集，并通过基于强度的阈值分割结合连通成分分析识别出500个独立孔隙。每个孔隙均采用几何描述符进行表征，包括尺寸、长宽比、范围以及相对于试样边界的几何位置。接着，利用基于百分位数的欧氏距离准则构建孔隙交互网络，共得到24,950条孔隙间连接。随后，机器学习模型基于提取的特征预测孔隙临界性评分，并通过SHAP分析量化各特征的贡献。结果表明，归一化表面距离在模型预测中占主导地位，其重要性比所有其他描述符高出一个数量级以上；孔隙尺寸的影响微乎其微，而其他几何参数的影响则可忽略不计。表面邻近性与临界性之间存在强烈的负相关关系，揭示了由边界驱动的失效机制。该可解释框架实现了透明的缺陷评估，并为增材制造中的工艺优化和质量控制提供了可操作的见解。

</details>


### [6] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出了4DPC²hat，首个面向动态点云理解的多模态大语言模型（MLLM），并构建了大规模跨模态数据集4DPC²hat-200K，包含44K动态物体序列、700K点云帧和200K问答对。通过引入Mamba增强的时序推理模块和失败感知的自举学习策略，显著提升了动作理解和时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态点云对象，对动态点云序列的理解研究不足，主要受限于缺乏大规模跨模态数据集以及时空上下文中建模运动的困难。

Method: 1）构建大规模跨模态数据集4DPC²hat-200K，采用两阶段流程：拓扑一致的4D点云构建和两级标注；2）提出基于Mamba增强的时序推理MLLM，以捕捉点云序列中的长程依赖和动态模式；3）设计失败感知的自举学习策略，迭代识别模型缺陷并生成针对性问答监督信号，持续增强推理能力。

Result: 大量实验表明，4DPC²hat在动作理解和时序推理方面显著优于现有模型，为4D动态点云理解奠定了坚实基础。

Conclusion: 本文通过构建首个面向动态点云的大规模数据集和专用MLLM架构，有效推动了4D动态点云理解的发展，尤其在复杂时空推理任务上取得显著进展。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

Abstract (中文翻译): 点云为三维物体提供了一种紧凑且富有表现力的表示方式，并已被近期整合到多模态大语言模型（MLLMs）中。然而，现有方法主要聚焦于静态物体，而对动态点云序列的理解仍基本未被探索。这一局限性主要源于缺乏大规模跨模态数据集，以及在时空上下文中建模运动的困难。为弥合这一差距，我们提出了4DPC²hat——首个专为动态点云理解定制的MLLM。为此，我们通过一个精心设计的两阶段流程（包括拓扑一致的4D点云构建和两级标注）构建了大规模跨模态数据集4DPC²hat-200K。该数据集包含超过44K个动态物体序列、700K个点云帧和200K个精心整理的问题-答案（QA）对，支持关于计数、时序关系、动作、空间关系和外观等方面的查询。在框架核心部分，我们引入了一个Mamba增强的时序推理MLLM，以捕捉点云序列中的长程依赖关系和动态模式。此外，我们提出了一种失败感知的自举学习策略，该策略能迭代识别模型缺陷并生成针对性的QA监督信号，从而持续强化相应的推理能力。大量实验表明，与现有模型相比，我们的4DPC²hat在动作理解和时序推理方面显著提升，为4D动态点云理解奠定了坚实基础。

</details>


### [7] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出MQA-RefAVS任务，用于在无真实标注的情况下评估语言引导音视频分割（Ref-AVS）中候选分割掩码的质量，并构建了包含多种错误模式的基准MQ-RAVSBench及基于多模态大语言模型的评估器MQ-Auditor。


<details>
  <summary>Details</summary>
Motivation: 现有Ref-AVS方法虽能生成分割掩码，但缺乏对掩码质量进行丰富且可解释的诊断；尤其在推理阶段无法依赖真实标注来评估掩码质量，因此亟需一种无需真值参考的掩码质量评估机制。

Method: 作者构建了新任务MQA-RefAVS，要求模型基于音视频-语言输入和给定掩码，估计其与未知真值的IoU、识别错误类型并给出质量控制建议；同时构建了涵盖几何与语义错误的基准MQ-RAVSBench，并提出了基于多模态大语言模型（MLLM）的MQ-Auditor，通过显式推理多模态线索和掩码信息进行质量评估。

Result: 实验表明，MQ-Auditor在掩码质量评估上优于多个开源和商用MLLM，并可有效集成到现有Ref-AVS系统中，用于检测分割失败并支持后续分割性能提升。

Conclusion: 本工作首次将掩码质量评估引入Ref-AVS领域，提出的MQA-RefAVS任务、MQ-RAVSBench基准和MQ-Auditor模型为提升Ref-AVS系统的可靠性与可解释性提供了新思路。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

Abstract (中文翻译): 语言引导的音视频分割（Ref-AVS）旨在通过联合推理视频、音频和文本，分割出自然语言所描述的目标物体。除了生成分割掩码外，如何提供丰富且可解释的掩码质量诊断仍鲜有研究。本文提出了Ref-AVS场景下的掩码质量评估任务（MQA-RefAVS），该任务在推理阶段无需依赖真实标注即可评估候选分割掩码的质量。给定音视频-语言输入及每个提供的分割掩码，该任务需估计其与未观测到的真实标注之间的IoU、识别对应的错误类型，并推荐可操作的质量控制决策。为支持该任务，我们构建了MQ-RAVSBench基准，其中包含涵盖几何与语义问题的多样化且具代表性的掩码错误模式。我们进一步提出了MQ-Auditor——一种基于多模态大语言模型（MLLM）的评估器，能够显式地推理多模态线索和掩码信息，从而生成定量与定性的掩码质量评估。大量实验表明，MQ-Auditor优于多个强大的开源和商用MLLM，并可与现有Ref-AVS系统集成，用于检测分割失败并支持下游分割性能的提升。数据与代码将发布于 https://github.com/jasongief/MQA-RefAVS。

</details>


### [8] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: 提出了一种基于高斯核的超快三维光声迭代重建方法（GPAIR），将传统耗时数百秒至数小时的迭代重建加速至亚秒级，显著提升三维光声计算机断层成像（PACT）的临床实用性。


<details>
  <summary>Details</summary>
Motivation: 传统迭代重建（IR）算法虽能有效校正光声计算机断层成像（PACT）中的重建伪影，但其计算耗时极长，尤其在大规模三维成像中需数百秒甚至数小时，严重限制了其实际应用。

Method: 提出GPAIR方法：使用连续各向同性高斯核替代传统空间网格，推导压力波的解析闭式表达，并结合GPU加速的可微分Triton算子实现高效计算。

Result: 在动物实验中，GPAIR对包含840万体素的三维目标实现了亚秒级重建速度，比传统IR方法快几个数量级。

Conclusion: 该超快重建方法使大规模三维光声成像接近实时，极大推动了三维PACT向临床应用的转化。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

Abstract (中文翻译): 尽管迭代重建（IR）算法能够显著校正光声（PA）计算机断层成像（PACT）中的重建伪影，但其重建时间过长，尤其在大规模三维（3D）成像中，IR算法需要数百秒甚至数小时，计算负担严重限制了其实际应用。本研究提出了一种用于三维PACT的超快IR方法，称为基于高斯核的超快三维光声迭代重建（GPAIR），该方法实现了数量级级别的计算加速。GPAIR利用连续各向同性的高斯核替代传统的空间网格，通过推导压力波的解析闭式表达式，并结合强大的GPU加速可微分Triton算子，实现了卓越的超快重建性能。在动物实验中，GPAIR对包含840万体素的三维目标实现了亚秒级的重建速度。这一革命性的超快图像重建技术使大规模三维光声重建接近实时，显著推动了三维PACT向临床应用的发展。

</details>


### [9] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 本文研究了基于Vision Transformer（ViT）的无监督方法能否直接将大量未标注的动物图像聚类到物种级别，并进一步揭示种内变异（如性别、年龄等）。实验表明，DINOv3嵌入结合t-SNE和层次聚类可实现接近完美的物种级聚类（V-measure: 0.958），且无需先验知识的无监督方法也表现优异（0.943），仅需剔除1.14%的异常图像。作者还开源了基准工具包，为生态学家提供方法选择建议。


<details>
  <summary>Details</summary>
Motivation: 手动标注动物图像在生态研究中是一个重大瓶颈，限制了生物多样性监测的规模和效率。因此，亟需自动化方法来减少对人工标注的依赖，特别是在大规模图像数据集上实现物种级别的自动聚类。

Method: 作者构建了一个综合基准框架，评估五种ViT模型（包括DINOv3）、五种降维技术与四种聚类算法（两种有监督、两种无监督）在60个物种（30种哺乳动物和30种鸟类）上的表现。每种物种使用200张经验证的图像进行测试。此外，研究还探讨了聚类结果是否能揭示种内生态学上有意义的变异（如性别、年龄、表型差异）。

Result: 使用DINOv3嵌入配合t-SNE和有监督层次聚类方法，在物种级别聚类上达到V-measure 0.958；无监督方法也取得0.943的高分，仅需剔除1.14%的异常图像。该方法对长尾分布具有鲁棒性，且通过有意过聚类可有效提取种内变异信息。

Conclusion: ViT基础模型（尤其是DINOv3）结合合适的降维与聚类策略，能够在无需人工标注的情况下高效实现物种级图像聚类，并揭示种内生态特征。该研究为生态学家提供了实用的开源工具和方法选择指南，有望显著提升生物多样性监测的自动化水平。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

Abstract (中文翻译): 手动标注动物图像仍然是生态学研究中的一个重大瓶颈，限制了生物多样性监测工作的规模和效率。本研究探讨了最先进的视觉Transformer（ViT）基础模型是否能够直接将数千张未标注的动物图像聚类到物种级别。我们提出了一个全面的基准测试框架，评估了五种ViT模型结合五种降维技术和四种聚类算法（两种有监督、两种无监督）在60个物种（30种哺乳动物和30种鸟类）上的表现，每个测试使用每种物种200张经过验证的随机图像子集。我们研究了聚类在哪些情况下能成功实现物种级别区分、在哪些情况下失败，以及物种内部的聚类是否能揭示具有生态学意义的模式，例如性别、年龄或表型变异。我们的结果表明，使用DINOv3嵌入结合t-SNE和有监督层次聚类方法，可实现近乎完美的物种级聚类（V-measure：0.958）。无监督方法在无需任何先验物种知识的情况下也取得了具有竞争力的性能（0.943），仅将1.14%的图像作为异常值剔除以供专家审查。我们进一步证明了该方法对真实场景中常见的物种长尾分布具有鲁棒性，并表明有意进行过聚类可以可靠地提取种内变异信息，包括年龄阶段、性别二态性和毛色差异。我们发布了一个开源基准测试工具包，并为生态学家提供了针对其特定分类群和数据选择合适方法的建议。

</details>


### [10] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 本文提出了NH-Fair，一个统一的公平性基准，用于在标准化条件下评估视觉和大视觉-语言模型的公平性，发现良好调优的ERM基线常优于许多去偏方法，而复合数据增强策略能有效提升公平性而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据训练的机器学习模型常继承并放大对某些社会群体的偏见，但现有偏见缓解方法因数据集、公平性指标、模型类型和超参调优不一致，难以公平比较其有效性。

Method: 构建NH-Fair统一基准，在标准化数据、指标和训练协议下，系统评估ERM（经验风险最小化）调优对效用与公平性的影响，并对比多种去偏方法在视觉模型和大视觉-语言模型（LVLMs）上的表现，涵盖监督和零样本场景。

Result: (1) ERM调优可显著影响模型效用与公平性，提供减少超参搜索空间的实用指南；(2) 多数去偏方法不如良好调优的ERM基线，而复合数据增强方法能稳定提升公平性且不损失性能；(3) LVLMs虽平均准确率更高，但仍存在子群差异，模型缩放带来的收益通常小于架构或训练协议选择的影响。

Conclusion: NH-Fair为公平性评估提供了可复现、考虑调优的标准化流程，强调在追求高准确率的同时必须关注子群公平性，并指出复合数据增强是一种有前景的实用去偏策略。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

Abstract (中文翻译): 在现实世界数据上训练的机器学习模型往往会继承并放大对某些社会群体的偏见，这引发了对其大规模部署的紧迫担忧。尽管已提出众多偏见缓解方法，但由于数据集异构、公平性指标不一致、视觉模型与多模态模型评估相互孤立，以及超参数调优不足等问题，使得这些方法的有效性难以进行公平比较。我们提出了NH-Fair——一个“无害公平性”（fairness without harm）的统一基准，涵盖视觉模型和大视觉-语言模型（LVLMs），在标准化的数据、指标和训练协议下进行评估，覆盖监督学习和零样本场景。我们的主要贡献包括：（1）一项系统的ERM（经验风险最小化）调优研究，识别出对模型效用和群体差异具有显著影响的训练选择，从而为从业者提供基于实证的指导方针，以缩小实现高公平性与高准确率所需的昂贵超参数搜索空间；（2）证据表明，许多去偏方法并不能稳定地超越经过良好调优的ERM基线，而一种复合数据增强方法则能持续带来公平性提升且不牺牲模型效用，展现出作为实用策略的潜力；（3）分析显示，尽管LVLMs实现了更高的平均准确率，但仍表现出子群体间的差异，且模型规模扩展所带来的收益通常小于架构设计或训练协议选择所带来的改进。NH-Fair提供了一个可复现、关注调优过程的评估流程，支持严格且注重避免伤害的公平性评估。

</details>


### [11] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文首次系统综述了3D高斯泼溅（3DGS）的知识产权保护研究，提出了一个自底向上的分析框架，并指出了未来六个研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯泼溅在实时3D场景合成中的广泛应用及其商业价值提升，其显式参数化结构引发了知识产权保护的新需求，但当前研究较为零散，缺乏统一视角。

Method: 提出一个自底向上的分析框架，涵盖：(i) 基于高斯的扰动机制，(ii) 被动与主动保护范式，(iii) 生成式AI时代下的鲁棒性威胁。

Result: 揭示了当前技术基础和鲁棒性表征方面的不足，明确了深入研究的机会。

Conclusion: 为3DGS资产的可靠、可信知识产权保护提供了路线图，并提出了六个跨鲁棒性、效率和保护范式的未来研究方向。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

Abstract (中文翻译): 3D高斯泼溅（3DGS）已成为实现实时3D场景合成的主流表示方法，广泛应用于虚拟现实、增强现实、机器人技术和3D内容创作等领域。其日益增长的商业价值和显式的参数化结构引发了新兴的知识产权（IP）保护问题，促使学界对3DGS知识产权保护的研究迅速增长。然而，当前的研究仍较为零散，缺乏对底层机制、保护范式及鲁棒性挑战的统一认识。为填补这一空白，本文首次对3DGS知识产权保护进行了系统性综述，并提出一个自底向上的分析框架，该框架考察了：(i) 基于高斯的扰动机制，(ii) 被动与主动保护范式，以及(iii) 在新兴生成式AI时代下面临的鲁棒性威胁。通过该框架，揭示了当前在技术基础和鲁棒性表征方面的不足，并指出了深入研究的机遇。最后，本文提出了六个涵盖鲁棒性、效率和保护范式的研究方向，为实现可靠且可信的3DGS资产知识产权保护提供了发展路线图。

</details>


### [12] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: 提出TruKAN，一种基于截断幂函数的KAN新架构，在保持表达能力的同时提升准确率、训练速度和可解释性，并在视觉任务中优于现有KAN变体。


<details>
  <summary>Details</summary>
Motivation: 现有KAN模型在计算效率与遵循Kolmogorov-Arnold表示定理之间存在权衡，且可解释性不足。作者旨在设计一种兼顾逼近能力、效率与透明度的新架构。

Method: 将KAN中的B样条基替换为源自k阶样条理论的截断幂函数族；每层结合截断幂项与多项式项，支持共享或独立节点；构建TruKAN-EfficientNet-V2模型，并与MLP、KAN、SineKAN等变体在相同框架下对比；采用混合优化策略训练，并研究层归一化及节点配置的影响。

Result: TruKAN在计算机视觉基准数据集上优于其他KAN变体，在准确率、训练时间和内存使用方面表现更佳，尤其在小型和深层架构中均具优势。

Conclusion: TruKAN通过简化基函数和节点设计，在保持KAN表达能力的同时显著提升了效率与可解释性，验证了其在复杂视觉任务中的实用性和泛化能力，超越了先前KAN研究的局限场景。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

Abstract (中文翻译): 为解决计算效率与遵循Kolmogorov-Arnold网络（KAN）原则之间的权衡问题，我们提出了TruKAN——一种基于KAN结构和可学习激活函数的新架构。TruKAN用源自k阶样条理论的一族截断幂函数替代了KAN中的B样条基，这一改动在保持KAN表达能力的同时提高了准确率和训练速度。每个TruKAN层结合了截断幂项与多项式项，并采用共享或独立的节点（knots）。由于其简化的基函数和节点配置，TruKAN相比其他KAN变体具有更强的可解释性。通过优先考虑可解释的基函数，TruKAN旨在平衡逼近效果与透明度。我们开发了TruKAN模型并将其集成到先进的EfficientNet-V2框架中，并在计算机视觉基准数据集上进行评估。为确保公平比较，我们构建了基于MLP、KAN、SineKAN和TruKAN的多种EfficientNet框架，并在小型和深层架构中评估其训练时间和准确率。训练阶段采用混合优化策略以提升收敛稳定性。此外，我们还研究了所有模型的层归一化技术，并评估了TruKAN中共享节点与独立节点的影响。总体而言，TruKAN在复杂视觉任务中在准确率、计算效率和内存使用方面均优于其他KAN模型，展现出超越以往KAN研究有限场景的优势。

</details>


### [13] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 提出了一种名为DiGAN的新方法，结合扩散模型与注意力卷积网络，用于从有限且不规则的纵向神经影像数据中增强时间上下文并提升早期阿尔茨海默病检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在早期阿尔茨海默病诊断中受限于对大规模纵向数据的依赖，且难以处理临床数据中固有的时间连续性和模态不规则性问题。

Method: 提出Diffusion-Guided Attention Network（DiGAN），将潜在扩散模型与注意力引导的卷积网络相结合：扩散模型从有限数据合成逼真的纵向神经影像轨迹，注意力卷积层则捕捉区分认知正常、轻度认知障碍和主观认知下降个体的结构-时间模式。

Result: 在合成数据集和ADNI数据集上的实验表明，DiGAN优于当前最先进的基线方法，在早期阿尔茨海默病检测方面展现出优越性能。

Conclusion: DiGAN通过融合扩散建模与注意力机制，有效应对了真实临床环境中数据稀疏与不规则的挑战，为早期阿尔茨海默病的自动诊断提供了新思路。

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

Abstract (中文翻译): 由于阿尔茨海默病（AD）前驱阶段脑结构变化细微且时间进程不规则，其早期诊断仍是一项重大挑战。现有的深度学习方法通常需要大量纵向数据集，并且往往难以建模真实临床数据中固有的时间连续性和模态不规则性。为解决这些问题，我们提出了扩散引导注意力网络（DiGAN），该方法将潜在扩散模型与注意力引导的卷积网络相结合。扩散模型能够从有限的训练数据中合成逼真的纵向神经影像轨迹，从而丰富时间上下文信息并提升对访视时间不均匀分布的鲁棒性。随后，注意力卷积层可捕捉具有判别性的结构-时间模式，以区分认知正常个体与轻度认知障碍及主观认知下降患者。在合成数据集和ADNI数据集上的实验表明，DiGAN优于当前最先进的基线方法，展现出在早期AD检测中的潜力。

</details>


### [14] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: 本文提出PriorProbe方法，通过结合人类个体认知先验来个性化神经网络，在面部表情识别任务中显著提升模型对模糊刺激的个体预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在获取个体认知先验时存在无法唯一识别或引入系统偏差的问题，限制了神经网络个性化的发展。

Method: 提出PriorProbe方法，基于“以人为中心的马尔可夫链蒙特卡洛”（MCMCP）技术，从个体参与者中恢复细粒度、个体特异的认知先验，并将其整合到前沿神经网络中。

Result: 在面部表情识别任务中，整合PriorProbe获得的先验显著提升了模型对个体在模糊刺激下分类行为的预测能力，优于单独使用神经网络或其他先验来源，同时保持对真实标签的推理能力。

Conclusion: PriorProbe为个性化深度神经网络提供了一个通用且可解释的框架。

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

Abstract (中文翻译): 融入个体层面的认知先验是实现神经网络个性化的重要途径，然而准确获取此类先验仍具挑战性：现有方法要么无法唯一识别这些先验，要么会引入系统性偏差。本文提出PriorProbe，这是一种基于“以人为中心的马尔可夫链蒙特卡洛”（Markov Chain Monte Carlo with People）的新颖先验获取方法，能够恢复细粒度、个体特异的认知先验。我们聚焦于面部表情识别任务，将PriorProbe应用于个体参与者，并检验将所恢复的先验与当前最先进的神经网络相结合是否能提升模型对个体在模糊刺激下分类行为的预测能力。结果表明，由PriorProbe导出的先验带来了显著的性能提升，不仅优于单独使用神经网络，也优于其他先验来源，同时保留了网络对真实标签的推理能力。综上所述，本研究证明PriorProbe为个性化深度神经网络提供了一个通用且可解释的框架。

</details>


### [15] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的计算机视觉框架，用于在增材制造部件的三维断层图像中检测孔隙并评估其临界性。结果表明，孔隙距表面的归一化距离是预测临界性的主导因素，远超其他几何特征的影响。


<details>
  <summary>Details</summary>
Motivation: 增材制造部件中的内部孔隙会显著降低结构性能并阻碍工业应用；现有自动缺陷检测方法缺乏可解释性，使工程师难以理解临界性预测的物理依据。

Method: 将灰度切片重建为三维体数据，通过强度阈值和连通成分分析识别出500个独立孔隙；利用几何描述符（尺寸、长宽比、范围、距边界位置）表征每个孔隙；基于百分位欧氏距离构建孔隙交互网络（含24,950条连接）；使用机器学习模型预测孔隙临界性，并通过SHAP分析量化各特征贡献。

Result: 归一化表面距离对模型预测起决定性作用，其重要性比其他所有描述符高出一个数量级以上；孔隙尺寸影响微弱，几何参数几乎无影响；表面邻近性与临界性呈强负相关，揭示了边界驱动的失效机制。

Conclusion: 该可解释框架实现了透明的缺陷评估，为增材制造中的工艺优化和质量控制提供了可操作的见解。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

Abstract (中文翻译): 内部孔隙仍是增材制造部件中的关键缺陷模式，会损害结构性能并限制其工业应用。尽管已有自动化缺陷检测方法，但这些方法缺乏可解释性，使工程师无法理解临界性预测的物理基础。本研究提出了一种可解释的计算机视觉框架，用于在三维断层扫描体积中进行孔隙检测与临界性评估。首先将连续的灰度切片重建为体数据集，并通过基于强度的阈值分割结合连通成分分析识别出500个独立孔隙。每个孔隙均采用几何描述符进行表征，包括尺寸、长宽比、范围以及相对于试样边界的几何位置。随后，基于百分位数的欧氏距离准则构建了孔隙交互网络，共获得24,950条孔隙间连接。利用提取的特征训练机器学习模型以预测孔隙临界性得分，并通过SHAP分析量化各特征的贡献。结果表明，归一化表面距离在模型预测中占据主导地位，其重要性比所有其他描述符高出一个数量级以上；孔隙尺寸的影响极小，而其他几何参数的影响则可忽略不计。表面邻近性与临界性之间存在强烈的负相关关系，揭示了由边界驱动的失效机制。该可解释框架实现了透明的缺陷评估，并为增材制造中的工艺优化与质量控制提供了可操作的洞见。

</details>


### [16] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出了4DPC²hat，首个专为动态点云理解设计的多模态大语言模型（MLLM），并构建了包含44K动态物体序列和200K问答对的大规模跨模态数据集4DPC²hat-200K。通过Mamba增强的时间推理架构与失败感知的自举学习策略，显著提升了动作理解和时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态点云，缺乏对动态点云序列的理解能力，原因在于缺少大规模跨模态数据集以及难以在时空上下文中建模运动信息。

Method: 构建两阶段流程生成大规模数据集4DPC²hat-200K，并提出基于Mamba的时间推理MLLM架构；同时采用失败感知的自举学习策略，迭代识别模型弱点并生成针对性问答监督信号。

Result: 实验表明，4DPC²hat在动作理解和时序推理方面显著优于现有模型，在动态点云理解任务上建立了强大基线。

Conclusion: 该工作填补了动态点云理解领域的空白，为未来4D场景理解提供了新的数据资源、模型架构和训练范式。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

Abstract (中文翻译): 点云为三维物体提供了一种紧凑而富有表现力的表示方式，并已被近期整合进多模态大语言模型（MLLMs）中。然而，现有方法主要聚焦于静态物体，对动态点云序列的理解仍鲜有探索。这一局限性主要源于缺乏大规模跨模态数据集，以及在时空上下文中建模运动的困难。为弥补这一空白，我们提出了4DPC²hat——首个专为动态点云理解定制的MLLM。为此，我们通过一个精细的两阶段流程构建了大规模跨模态数据集4DPC²hat-200K，该流程包括拓扑一致的4D点云构建和两级描述生成。该数据集包含超过44K个动态物体序列、70万帧点云以及20万条精心整理的问题-答案（QA）对，支持关于计数、时序关系、动作、空间关系和外观等方面的查询。在框架核心部分，我们引入了一种Mamba增强的时间推理MLLM，以捕捉点云序列中的长程依赖和动态模式。此外，我们提出了一种失败感知的自举学习策略，可迭代识别模型缺陷并生成有针对性的QA监督信号，持续强化相应的推理能力。大量实验表明，我们的4DPC²hat在动作理解和时序推理方面相比现有模型有显著提升，为4D动态点云理解奠定了坚实基础。

</details>


### [17] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出MQA-RefAVS任务，用于在无真实标注的情况下评估语言引导音视频分割（Ref-AVS）中候选分割掩码的质量，并构建了包含多种错误模式的基准MQ-RAVSBench及基于多模态大语言模型的评估器MQ-Auditor。


<details>
  <summary>Details</summary>
Motivation: 现有Ref-AVS方法主要关注生成分割掩码，但缺乏对掩码质量进行丰富且可解释的诊断。因此，作者提出在推理阶段无需真实标注即可评估掩码质量的新任务。

Method: 作者构建了MQ-RAVSBench基准数据集，涵盖几何与语义层面的多样化掩码错误类型；并提出MQ-Auditor模型，利用多模态大语言模型对音视频、文本和掩码信息进行联合推理，输出定量IoU估计、错误类型识别和质量控制建议。

Result: 实验表明，MQ-Auditor在掩码质量评估任务上优于现有的开源和商业多模态大语言模型，并能有效集成到现有Ref-AVS系统中，用于检测分割失败并促进下游性能提升。

Conclusion: MQA-RefAVS任务为Ref-AVS系统提供了可解释的质量评估机制，MQ-Auditor展示了多模态大语言模型在此类诊断任务中的潜力，有助于推动更鲁棒和可信的多模态分割系统发展。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

Abstract (中文翻译): 语言引导的音视频分割（Ref-AVS）旨在通过联合推理视频、音频和文本，分割出自然语言所描述的目标物体。除了生成分割掩码外，如何提供丰富且可解释的掩码质量诊断仍鲜有研究。本文提出了Ref-AVS场景下的掩码质量评估任务（MQA-RefAVS），该任务在推理阶段无需依赖真实标注，即可评估候选分割掩码的质量。给定音视频-语言输入及每个提供的分割掩码，该任务要求估计其与未知真实标注的IoU、识别对应的错误类型，并给出可操作的质量控制决策。为支持该任务，我们构建了MQ-RAVSBench基准，其中包含涵盖几何与语义问题的多样化且具代表性的掩码错误模式。我们进一步提出了MQ-Auditor，一种基于多模态大语言模型（MLLM）的评估器，能够显式地对多模态线索和掩码信息进行推理，从而生成定量与定性的掩码质量评估。大量实验表明，MQ-Auditor优于强大的开源和商业MLLM，并可与现有Ref-AVS系统集成，以检测分割失败并支持下游分割性能的提升。数据和代码将发布于 https://github.com/jasongief/MQA-RefAVS。

</details>


### [18] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为GPAIR的超快速迭代重建方法，用于三维光声计算机断层成像（PACT），通过使用高斯核和GPU加速，在亚秒级时间内完成包含840万体素的3D图像重建，极大提升了临床实用性。


<details>
  <summary>Details</summary>
Motivation: 传统迭代重建（IR）算法虽能有效校正光声成像中的伪影，但计算耗时极长（数百秒至数小时），尤其在大规模三维成像中，严重限制了其实际应用。

Method: 提出Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction（GPAIR）方法，将传统空间网格替换为连续各向同性高斯核，并推导出声压波的解析闭式表达，结合GPU加速的可微Triton算子实现高效计算。

Result: 在动物实验中，GPAIR对含840万体素的3D目标实现了亚秒级重建速度，比传统IR方法快几个数量级。

Conclusion: 该超快速重建方法使大规模3D光声成像接近实时，显著推动了3D PACT向临床应用的转化。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

Abstract (中文翻译): 尽管迭代重建（IR）算法能够显著校正光声（PA）计算机断层成像（PACT）中的重建伪影，但其重建时间过长，尤其在大规模三维（3D）成像中，IR算法需要数百秒甚至数小时，计算负担严重限制了其实际应用。本文提出了一种用于3D PACT的超快速IR方法，称为基于高斯核的超快速三维光声迭代重建（GPAIR），实现了数量级级别的计算加速。GPAIR利用连续各向同性的高斯核替代传统的空间网格，并推导出声压波的解析闭式表达，同时采用强大的GPU加速可微Triton算子。在动物实验中，GPAIR对包含840万体素的3D目标实现了亚秒级的超快重建速度。这一革命性的超快速图像重建技术使得大规模3D光声重建接近实时，显著推动了3D PACT向临床应用的发展。

</details>


### [19] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 本文评估了基于Vision Transformer（ViT）的无监督/半监督方法在动物图像物种级聚类中的性能，发现DINOv3结合t-SNE和层次聚类可实现接近完美的物种分离，并能进一步揭示种内变异（如年龄、性别等），同时提供了开源工具包供生态学家使用。


<details>
  <summary>Details</summary>
Motivation: 手动标注动物图像是生态学研究中的主要瓶颈，限制了生物多样性监测的规模与效率。因此，亟需自动化方法将大量未标注图像直接聚类到物种级别，甚至揭示种内生态特征。

Method: 作者构建了一个综合基准框架，评估5种ViT模型、5种降维方法和4种聚类算法（2种有监督、2种无监督）在60个物种（30种哺乳动物+30种鸟类）上的表现，每种使用200张经验证的图像。同时测试模型在长尾分布下的鲁棒性及过聚类对种内变异的提取能力。

Result: 使用DINOv3嵌入、t-SNE降维和有监督层次聚类时，物种级聚类V-measure达0.958；无监督方法也达到0.943，仅1.14%图像被识别为异常需人工审核。过聚类能有效揭示年龄、性别和毛色等种内差异。

Conclusion: ViT基础模型（尤其是DINOv3）结合适当聚类流程可高效实现物种级自动聚类，并挖掘生态上有意义的种内变异，显著减少人工标注负担。作者开源了相关工具并为生态学家提供了方法选择建议。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

Abstract (中文翻译): 动物图像的人工标注仍是生态学研究中的重大瓶颈，限制了生物多样性监测工作的规模与效率。本研究探讨了当前最先进的视觉Transformer（ViT）基础模型是否能够直接将数千张未标注的动物图像聚类至物种级别。我们提出了一个全面的基准测试框架，评估了五种ViT模型结合五种降维技术和四种聚类算法（两种有监督、两种无监督）在60个物种（30种哺乳动物和30种鸟类）上的表现，每个测试均使用每物种200张经过验证的图像的随机子集。我们研究了聚类在何时能成功实现物种级区分、在何处失败，以及物种内部的聚类是否能揭示具有生态学意义的模式，例如性别、年龄或表型变异。结果表明，使用DINOv3嵌入结合t-SNE和有监督层次聚类方法可实现近乎完美的物种级聚类（V-measure：0.958）。无监督方法在无需先验物种知识的情况下也取得了具有竞争力的性能（0.943），仅将1.14%的图像作为异常值剔除，需专家复核。我们进一步证明该方法对现实中常见的物种长尾分布具有鲁棒性，并表明有意进行过聚类可可靠地提取种内变异，包括年龄阶段、两性异形和毛色差异。我们发布了一个开源基准测试工具包，并为生态学家针对其特定分类群和数据选择合适方法提供了建议。

</details>


### [20] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 该论文提出了NH-Fair，一个统一的公平性基准，用于在标准数据、指标和训练协议下评估视觉模型和大型视觉-语言模型（LVLMs）的公平性，强调了良好调参的ERM基线的重要性，并发现一种复合数据增强方法在不牺牲效用的前提下有效提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据训练的机器学习模型常继承并放大对某些社会群体的偏见，而现有偏见缓解方法因数据集异构、公平性指标不一致、仅单独评估视觉或多模态模型以及超参数调优不足等问题，难以进行公平有效的比较。

Method: 作者构建了NH-Fair统一基准，涵盖监督和零样本场景，在标准化的数据、指标和训练协议下评估视觉模型和大型视觉-语言模型（LVLMs）。通过系统性的ERM调参研究，识别影响效用和差异的关键训练选择，并评估多种去偏方法的有效性。

Result: 研究发现：(1) 训练选择对效用和差异有显著影响，可据此缩小超参数搜索空间；(2) 许多去偏方法无法稳定超越良好调参的ERM基线，而复合数据增强方法能持续提升公平性且不牺牲效用；(3) LVLMs虽平均准确率更高，但仍存在子群差异，且模型扩展带来的收益通常小于架构或训练协议选择的影响。

Conclusion: NH-Fair提供了一个可复现、考虑调参的评估流程，有助于进行严谨且关注危害的公平性评估。研究强调了基线调优的重要性，并指出复合数据增强是一种有前景的实用策略。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

Abstract (中文翻译): 在真实世界数据上训练的机器学习模型常常会继承并放大针对某些社会群体的偏见，这引发了人们对其大规模部署的紧迫担忧。尽管已提出众多偏见缓解方法，但由于数据集异构、公平性指标不一致、仅孤立地评估视觉模型或多模态模型，以及超参数调优不足等问题，使得比较这些方法的有效性仍然十分困难。我们提出了NH-Fair——一个“无害公平性”的统一基准，该基准在标准化的数据、指标和训练协议下，同时涵盖视觉模型和大型视觉-语言模型（LVLMs），并覆盖监督学习和零样本学习场景。我们的主要贡献包括：（1）一项系统性的经验风险最小化（ERM）调参研究，识别出对模型效用和群体差异具有重大影响的训练选择，从而为从业者提供基于实证的指导方针，以减少在追求高公平性和高准确性时所需的昂贵超参数搜索空间；（2）证据表明，许多去偏方法并不能稳定地超越一个经过良好调参的ERM基线，而一种复合数据增强方法则能持续带来公平性提升且不牺牲模型效用，成为一种有前景的实用策略；（3）分析显示，尽管LVLMs实现了更高的平均准确率，但它们仍然表现出子群体间的差异，且单纯通过模型规模扩展所带来的收益通常小于由架构设计或训练协议选择所带来的收益。NH-Fair提供了一个可复现、且充分考虑调参因素的评估流程，用于进行严谨且关注潜在危害的公平性评估。

</details>


### [21] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR 是一种基于语义解耦的无限制人脸隐私保护框架，通过在扩散模型潜在空间中进行语义引导重组，在保持机器可识别身份一致性的同时生成视觉上匿名但自然的人脸图像，并支持密码授权下的原始图像恢复。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别技术深度融入在线银行、身份验证等网络服务，如何在图像存储与传输过程中有效将身份信息与视觉表征解耦，成为隐私保护的关键挑战。

Method: SIDeR 将人脸图像分解为机器可识别的身份特征向量和视觉可感知的语义外观成分，利用扩散模型潜在空间中的语义引导重组机制生成对抗性匿名人脸；引入动量驱动的无限制扰动优化和语义-视觉平衡因子，合成多样且自然的对抗样本；同时支持通过密码授权恢复原始图像。

Result: 在 CelebA-HQ 和 FFHQ 数据集上的实验表明，SIDeR 在黑盒场景下攻击成功率达 99%，且在基于 PSNR 的恢复质量上比基线方法高出 41.28%。

Conclusion: SIDeR 在保障人脸图像隐私的同时兼顾了身份一致性和视觉自然性，并支持可控恢复，为人脸隐私保护提供了一种高效且实用的新范式。

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

Abstract (中文翻译): 随着人脸识别技术深度融入在线银行、身份验证等联网服务，在图像存储与传输过程中实现身份信息与视觉表征的有效解耦已成为隐私保护的关键挑战。为解决这一问题，我们提出了 SIDeR——一种基于语义解耦驱动的无限制人脸隐私保护框架。SIDeR 将人脸图像分解为机器可识别的身份特征向量和视觉可感知的语义外观成分，通过在扩散模型潜在空间中进行语义引导的重组，生成视觉上匿名但保持机器级身份一致性的对抗性人脸。该框架结合动量驱动的无限制扰动优化策略与语义-视觉平衡因子，可合成多种视觉多样且高度自然的对抗样本。此外，对于授权访问，当提供正确密码时，受保护图像可被还原为其原始形式。在 CelebA-HQ 和 FFHQ 数据集上的大量实验表明，SIDeR 在黑盒场景下攻击成功率达到 99%，并且在基于 PSNR 的恢复质量方面比基线方法高出 41.28%。

</details>


### [22] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: UniTrack 是一种即插即用的图论损失函数，通过端到端可微学习统一优化检测、身份保持和时空一致性，显著提升多目标跟踪性能，无需修改现有架构。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的多目标跟踪方法通常需要重新设计跟踪架构，缺乏通用性；作者旨在提出一种通用、可插拔的训练目标，直接优化跟踪任务的核心指标，同时兼容现有系统。

Method: 提出 UniTrack，一种统一的可微图论损失函数，将检测精度、身份保持和时空一致性整合为单一端到端可训练目标，通过可微图表示学习建模跨帧运动连续性和身份关系。

Result: 在多个模型（如 Trackformer、MOTR、FairMOT、ByteTrack、GTR、MOTE）和基准上验证，UniTrack 一致提升性能，身份切换最多减少 53%，IDF1 最高提升 12%，GTR 在 SportsMOT 上 MOTA 提升 9.7%。

Conclusion: UniTrack 作为一种通用、即插即用的损失函数，能有效提升各类多目标跟踪系统的性能，证明了统一可微优化在 MOT 中的有效性和广泛适用性。

Abstract: We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

Abstract (中文翻译): 我们提出了 UniTrack，一种即插即用的图论损失函数，旨在通过统一的可微学习直接优化多目标跟踪（MOT）特定目标，从而显著提升跟踪性能。与以往需要重新设计跟踪架构的基于图的 MOT 方法不同，UniTrack 提供了一种通用的训练目标，将检测精度、身份保持和时空一致性整合到一个端到端可训练的损失函数中，能够在不修改现有 MOT 系统架构的情况下无缝集成。通过可微图表示学习，UniTrack 使网络能够学习跨帧的运动连续性和身份关系的整体表示。我们在多种跟踪模型和多个具有挑战性的基准上验证了 UniTrack，结果表明其在所有测试的架构和数据集（包括 Trackformer、MOTR、FairMOT、ByteTrack、GTR 和 MOTE）上均带来一致的性能提升。大量实验显示，在具有挑战性的基准上，身份切换最多减少了 53%，IDF1 指标最高提升了 12%，其中 GTR 在 SportsMOT 数据集上实现了 MOTA 指标 9.7% 的峰值性能增益。

</details>


### [23] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种无需修改架构或额外数据的训练框架，通过增强视觉条件性来提升Vision-Language-Action（VLA）模型在机器人操作任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有将大规模预训练视觉-语言模型（VLMs）扩展到动作空间的方法容易导致视觉-动作错位，即动作预测对当前视觉状态依赖较弱，从而影响动作输出的可靠性。作者观察到成功执行轨迹比失败轨迹具有更强的视觉依赖性，因此希望显式增强VLA模型的视觉条件性。

Method: 该方法首先在一个轨迹跟随代理任务上通过偏好优化使动作预测与视觉输入对齐，然后在监督微调阶段通过潜在空间蒸馏将这种增强的对齐能力迁移到指令跟随任务中。

Result: 该方法在不引入架构修改或额外数据收集的情况下，提升了离散OpenVLA模型的视觉条件性和任务性能，并在连续动作空间的OpenVLA-OFT设置下也取得了一致的性能提升。

Conclusion: 显式增强VLA模型对视觉输入的依赖性可有效缓解视觉-动作错位问题，从而提升模型在机器人操作任务中的表现，且该方法具有良好的通用性和可扩展性。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

Abstract (中文翻译): 视觉-语言-动作（VLA）模型在多种机器人操作任务中展现出强大性能。然而，尽管取得了成功，将大规模预训练的视觉-语言模型（VLMs）扩展到动作空间可能导致视觉-动作错位问题，即动作预测对当前视觉状态的依赖较弱，从而导致不可靠的动作输出。本文从视觉条件性的角度研究VLA模型，并通过实验证明，成功的执行轨迹始终比失败的轨迹表现出更强的视觉依赖性。受此启发，我们提出了一种训练框架，显式地增强VLA模型中的视觉条件性。我们的方法首先在一个轨迹跟随代理任务上通过偏好优化实现动作预测与视觉输入的对齐，然后在监督微调过程中通过潜在空间蒸馏将这种增强的对齐能力迁移到指令跟随任务中。在不引入架构修改或额外数据收集的前提下，该方法提升了离散OpenVLA模型的视觉条件性和任务性能，并在扩展到连续动作空间的OpenVLA-OFT设置时也持续带来性能增益。项目网站：https://vista-vla.github.io/。

</details>


### [24] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文综述了基于图像的膳食评估中用于精确估算食物份量的各种策略，重点解决从2D图像估计3D食物尺寸的难题。


<details>
  <summary>Details</summary>
Motivation: 基于图像的膳食评估在个体健康监测、慢性病及肥胖防控中具有重要作用，但其核心挑战在于从二维图像准确估计食物的三维体积。

Method: 文章探讨了多种应对该挑战的方法，包括使用深度图、多视角图像等辅助输入，基于模型的模板匹配方法，以及利用深度学习从单目图像或结合辅助信息进行份量预测。

Result: 系统梳理并比较了现有不同策略在提升食物份量估计准确性方面的有效性。

Conclusion: 综合运用辅助信息和深度学习技术是克服2D图像在食物三维尺寸估计局限性的关键方向。

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

Abstract (中文翻译): 依赖图像进行膳食评估是一种重要策略，能够准确且便捷地监测个体健康状况，从而在慢性疾病和肥胖的预防与管理中发挥关键作用。然而，基于图像的膳食评估面临一个核心难题：如何从二维图像输入中估算食物的三维尺寸。为克服这一关键限制，研究者提出了多种策略，例如使用深度图、多视角输入等辅助信息，或采用基于模型的方法（如模板匹配）。深度学习技术也有助于弥合这一差距，可通过单目图像或结合图像与辅助输入，精确预测图像中的食物份量。本文探讨了用于实现准确份量估算的不同策略。

</details>


### [25] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出了一种名为视觉概念排序（VCR）的方法，用于识别大型多模态模型（LMMs）在医疗任务中依赖的关键视觉概念，并揭示其在不同人群子组间的表现差异。


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，确保机器学习模型的可靠性至关重要，因此需要审计方法来揭示模型的缺陷。现有方法难以解释LMM在处理医学图像时的行为，尤其在不同人口统计学群体中的表现差异尚不明确。

Method: 作者提出“视觉概念排序”（Visual Concept Ranking, VCR）方法，用于识别LMM在处理医学任务（如皮肤病变分类、胸部X光片分析等）时所依赖的重要视觉概念。通过提供示例提示，结合人工干预验证VCR生成的关于视觉特征依赖的假设。

Result: 研究发现，LMM在不同人口统计学子组之间存在意料之外的性能差距；VCR能够有效生成可验证的视觉特征依赖假设，并通过人工干预加以确认。

Conclusion: VCR为理解和审计LMM在医疗任务中的行为提供了有效工具，有助于揭示模型偏见和潜在失效模式，从而提升其在安全关键场景中的可靠性。

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

Abstract (中文翻译): 确保机器学习模型在医疗等安全关键领域的可靠性，需要能够揭示模型缺陷的审计方法。我们提出了一种用于识别大型多模态模型（LMMs）中重要视觉概念的方法，并利用该方法研究这些模型在面对医疗任务提示时所表现出的行为。我们主要聚焦于从临床皮肤科图像中对恶性皮肤病变进行分类的任务，并辅以胸部X光片和自然图像的补充实验。在展示LMM在使用示例提示时在不同人口统计学子群体之间存在意外的性能差距后，我们将所提出的“视觉概念排序”（Visual Concept Ranking, VCR）方法应用于这些模型和提示。VCR能够生成与不同视觉特征依赖相关的假设，随后我们通过人工干预对这些假设进行了验证。

</details>


### [26] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出CLEAR-HPV框架，在无需概念标签的情况下，通过注意力机制在MIL模型中自动发现可解释的形态学概念（如角化、基底样和间质），将高维特征压缩为仅10个可解释概念的向量，同时保持预测性能，并在多个癌症数据集上验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力机制的多实例学习（MIL）方法虽然在HPV相关全切片病理图像的分类任务中表现良好，但缺乏形态学层面的可解释性。为解决这一问题，作者希望构建一个能在不依赖人工标注概念标签的前提下，自动发现并可视化组织形态学概念的可解释框架。

Method: 作者提出CLEAR-HPV框架，该框架利用注意力权重重构MIL的潜在空间，在其中自动发现关键形态学概念（如角化、基底样、间质），生成空间概念图，并用一个紧凑的概念比例向量（concept-fraction vector）来表示整张切片。

Result: CLEAR-HPV成功地将原始高维（如1536维）MIL嵌入压缩为仅包含10个可解释概念的向量，同时保留了原有的预测信息。该方法在TCGA-HNSCC、TCGA-CESC和CPTAC-HNSCC等多个独立数据集上均展现出良好的泛化能力和一致的可解释性。

Conclusion: CLEAR-HPV提供了一个通用且与主干网络无关的框架，有效提升了注意力MIL模型在全切片病理图像分析中的概念级可解释性，为HPV状态预测提供了兼具高性能与透明度的新方法。

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

Abstract (中文翻译): 人乳头瘤病毒（HPV）状态是头颈癌和宫颈癌预后及治疗反应的关键决定因素。尽管基于注意力机制的多实例学习（MIL）在HPV相关的全切片组织病理学图像的切片级预测方面表现出色，但其在形态学上的可解释性有限。为解决这一局限性，我们提出了“用于HPV的概念级可解释注意力引导表征”（CLEAR-HPV）框架，该框架利用注意力机制重构MIL的潜在空间，从而在训练过程中无需概念标签即可实现概念发现。CLEAR-HPV在注意力加权的潜在空间中自动识别出角化、基底样和间质等形态学概念，生成空间概念图，并使用一个紧凑的概念比例向量来表征每张切片。CLEAR-HPV的概念比例向量在保留原始MIL嵌入预测信息的同时，将高维特征空间（例如1536维）缩减至仅10个可解释的概念。CLEAR-HPV在TCGA-HNSCC、TCGA-CESC和CPTAC-HNSCC等多个数据集上均表现出一致的泛化能力，通过一个通用且与主干网络无关的框架，为全切片组织病理学的注意力MIL模型提供了紧凑的概念级可解释性。

</details>


### [27] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文提出ARGaze，一种基于自回归序列建模的在线第一人称视线估计方法，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在线第一人称视线估计缺乏明确的头部或眼部信号，需从稀疏、间接线索推断视觉注意力；现有方法未充分利用视线在目标导向活动中的时间连续性。

Method: 将视线估计重构为序列预测任务，使用Transformer解码器，结合当前视觉特征与固定长度的近期视线目标估计窗口（Gaze Context Window）进行自回归预测，确保因果性和有限资源下的流式推理。

Result: 在多个第一人称视线估计基准的在线评估中取得SOTA结果，消融实验证明有限历史视线信息的自回归建模对鲁棒预测至关重要。

Conclusion: 利用视线的时间连续性并通过自回归方式建模，能显著提升在线第一人称视线估计的性能，适用于AR和辅助技术等实际场景。

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

Abstract (中文翻译): 在线第一人称视线估计旨在仅使用过去和当前帧的第一人称视频来预测佩戴摄像头者正在注视的位置，这一任务对增强现实和辅助技术至关重要。与第三人称视线估计不同，该设定缺乏显式的头部或眼部信号，要求模型从手-物交互和显著场景内容等稀疏且间接的线索中推断当前的视觉注意力。我们观察到，在目标导向活动中，视线表现出强烈的时间连续性：了解一个人最近注视的位置可为预测其下一步注视位置提供有力先验。受视觉-语言模型中视觉条件自回归解码的启发，我们提出了ARGaze方法，将视线估计重新表述为序列预测任务：在每个时间步，Transformer解码器通过结合（i）当前视觉特征和（ii）一个固定长度的近期视线目标估计窗口（Gaze Context Window）来预测当前视线。该设计保证了因果性，并支持有限资源下的流式推理。我们在多种第一人称视线估计基准上实现了在线评估下的最先进性能，大量消融实验验证了使用有限视线历史的自回归建模对鲁棒预测至关重要。我们将开源代码和预训练模型。

</details>


### [28] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: 本文首次系统评估了基于视觉的手部追踪模型在戴手套场景下的表现，发现现有裸手模型因外观差异导致性能显著下降，并提出AirGlove方法，利用已有手套数据提升对新类型手套的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于传感器的手套追踪方法受限于信号质量和校准精度，而基于视觉的方法虽在裸手上表现优异，但在不同外观的手套上效果尚未充分研究，因此需要探索更鲁棒的视觉追踪方案。

Method: 提出AirGlove方法，在零样本和微调两种设置下，利用已有的手套数据学习可泛化的手套表征，以适应新类型手套的手势姿态估计。

Result: 在多种传感手套上的实验表明，AirGlove能有效将手部姿态模型泛化到新手套设计上，显著优于现有方法。

Conclusion: 视觉驱动的手部追踪在戴手套场景中具有潜力，但需解决外观差异问题；AirGlove通过表征迁移有效提升了模型对新手套的适应性。

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

Abstract (中文翻译): 传感手套已成为遥操作和机器人策略学习的重要工具，因其能够提供速度、加速度和触觉反馈等丰富信号。一种常见的追踪戴手套手部的方法是直接使用传感器信号（如角速度、重力方向）来估计三维手部姿态。然而，基于传感器的追踪在实践中往往受限，因为其精度常受传感器信号质量和校准质量的影响。近期基于视觉的方法通过大规模预训练在人类裸手上取得了优异性能，但它们在具有显著不同视觉外观的戴手套手部上的表现仍缺乏系统研究。本文首次对基于视觉的手部追踪模型在戴手套场景下进行了系统评估，涵盖零样本和微调两种设置。分析表明，现有裸手模型由于裸手与手套设计之间存在显著外观差异，在传感手套上性能大幅下降。为此，我们提出了AirGlove方法，利用已有手套数据将所学的手套表征泛化至仅有少量数据的新类型手套。在多种传感手套上的实验表明，AirGlove能有效将手部姿态模型泛化到新手套设计，并显著优于对比方案。

</details>


### [29] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: 本文提出SHaSaM方法，通过子模硬样本挖掘提升模型公平性与性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练中易从标注数据继承社会和人口统计偏见，导致对种族、年龄、性别等敏感属性的不公平预测；现有方法因数据不平衡问题反而加剧了不公平性。

Method: 提出SHaSaM（Submodular Hard Sample Mining）方法，包含两个阶段：SHaSaM-MINE利用子模子集选择策略挖掘难正负样本以缓解数据不平衡；SHaSaM-LEARN基于子模条件互信息设计组合损失函数，在最大化目标类别决策边界的同时最小化敏感属性影响。

Result: 在CelebA和UTKFace数据集上，SHaSaM在更少训练轮次内实现SOTA结果，Equalized Odds公平性指标提升最多2.7点，准确率提高3.5%。

Conclusion: SHaSaM通过统一的子模优化框架有效抑制模型学习敏感属性相关特征，在不牺牲性能的前提下显著提升公平性。

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

Abstract (中文翻译): 深度神经网络在模型训练过程中常常从标注数据中继承社会和人口统计偏见，导致不公平的预测，尤其是在存在种族、年龄、性别等敏感属性的情况下。现有方法容易受到属性组之间固有数据不平衡的影响，无意中强化了对敏感属性的关注，从而加剧了不公平性和性能下降。为克服这些挑战，我们提出了SHaSaM（Submodular Hard Sample Mining），一种新颖的组合优化方法，将面向公平性的表征学习建模为子模硬样本挖掘问题。我们的两阶段方法包括：SHaSaM-MINE，引入子模子集选择策略来挖掘难正样本和难负样本，有效缓解数据不平衡；以及SHaSaM-LEARN，基于子模条件互信息构建一类组合损失函数，在最大化目标类别间决策边界的同时最小化敏感属性的影响。这一统一框架限制模型学习与敏感属性相关的特征，在不牺牲性能的前提下显著提升公平性。在CelebA和UTKFace上的实验表明，SHaSaM取得了当前最优结果，在更少的训练轮次内，模型公平性（Equalized Odds）最多提升2.7点，准确率提高3.5%。

</details>


### [30] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的图像到图像方法，通过合成水下退化效果并学习逆转这些退化，从而有效增强水下摄影图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下摄影存在对比度降低、空间模糊和波长相关的色彩失真等固有挑战，严重影响海洋生物的视觉表现力，摄影师通常需依赖繁重的后期处理来校正这些问题。

Method: 构建一个合成水下退化效果的流程，并利用扩散生成模型学习逆转这些退化；在Keith Ellenbogen提供的高质量小规模水下意识摄影数据集上从头训练约1100万参数的模型。

Result: 该方法在仅使用约2500张图像训练后，能生成512x768分辨率的图像，在感知一致性和泛化能力方面表现优异。

Conclusion: 所提出的基于扩散的图像增强方法能有效应对水下摄影中的典型退化问题，为水下图像复原提供了一种高效且高质量的解决方案。

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

Abstract (中文翻译): 水下摄影存在显著的固有挑战，包括对比度降低、空间模糊以及与波长相关的色彩失真。这些效应会掩盖海洋生物的生动性，尤其对注重生态意识的摄影师而言，往往需要依赖繁重的后期处理流程来校正这些失真。我们开发了一种图像到图像的处理流程，通过引入合成的水下退化流程，并利用基于扩散的生成模型学习逆转这些退化效应。训练与评估基于Keith Ellenbogen提供的小规模高质量生态意识摄影图像数据集。所提出的方法在从头开始训练约2500张图像后，使用约1100万参数的模型即可生成512×768分辨率的图像，在感知一致性和泛化能力方面均表现出色。

</details>
