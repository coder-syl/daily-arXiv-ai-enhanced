<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文首次对3D高斯泼溅（3DGS）知识产权保护领域进行了系统性综述，提出了一个自下而上的分析框架，并指出了未来六大研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯泼溅在实时3D场景合成中的广泛应用及其商业价值提升，其显式的参数化结构引发了知识产权保护的新需求，但当前研究分散，缺乏统一视角。

Method: 提出一个系统性综述框架，从底层高斯扰动机制、被动与主动保护范式、以及生成式AI时代下的鲁棒性威胁三个层面进行分析。

Result: 揭示了当前技术基础和鲁棒性表征方面的不足，并识别出深入研究的机会。

Conclusion: 为3DGS资产的可靠可信知识产权保护提供了清晰的研究路线图，涵盖鲁棒性、效率和保护范式等六个方向。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

Abstract (中文翻译): 3D高斯泼溅（3DGS）已成为实现实时3D场景合成的主流表示方法，广泛应用于虚拟与增强现实、机器人技术和3D内容创作等领域。其日益增长的商业价值和显式的参数化结构引发了新兴的知识产权（IP）保护问题，促使学界对3DGS知识产权保护展开大量研究。然而，当前的研究仍较为零散，缺乏对底层机制、保护范式及鲁棒性挑战的统一认识。为填补这一空白，我们首次对3DGS知识产权保护进行了系统性综述，并提出一个自下而上的分析框架，该框架涵盖：（i）基于高斯的底层扰动机制，（ii）被动与主动保护范式，以及（iii）在新兴生成式AI时代下面临的鲁棒性威胁。该综述揭示了当前在技术基础和鲁棒性表征方面的不足，并指出了进一步深入研究的机会。最后，我们提出了涵盖鲁棒性、效率和保护范式等六个方面的研究方向，为实现可靠且可信的3DGS资产知识产权保护提供了发展路线图。

</details>


### [2] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: 提出TruKAN，一种基于截断幂函数的新KAN架构，在保持表达能力的同时提升准确率、训练速度和可解释性，并在视觉任务中优于现有KAN变体。


<details>
  <summary>Details</summary>
Motivation: 现有Kolmogorov-Arnold Network（KAN）在计算效率与理论原则之间存在权衡，作者希望设计一种兼顾逼近能力与透明性的新架构。

Method: 将KAN中的B样条基替换为源自k阶样条理论的截断幂函数族；每层结合截断幂项与多项式项，并支持共享或独立节点；将其集成到EfficientNet-V2框架中，采用混合优化策略和层归一化进行训练。

Result: TruKAN在多个视觉基准数据集上相比MLP、KAN和SineKAN等模型，在准确率、训练时间和内存占用方面均表现更优，尤其在小型和深层架构中展现出更强的综合性能。

Conclusion: TruKAN通过简化基函数和节点配置，在保持KAN表达能力的同时显著提升了效率与可解释性，拓展了KAN在复杂视觉任务中的适用性。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

Abstract (中文翻译): 为解决计算效率与Kolmogorov-Arnold网络（KAN）原则遵循之间的权衡问题，我们提出了TruKAN——一种基于KAN结构和可学习激活函数的新架构。TruKAN用源自k阶样条理论的一族截断幂函数替代了KAN中的B样条基，这一改动在保持KAN表达能力的同时提升了准确率和训练速度。每个TruKAN层结合了一个截断幂项与一个多项式项，并采用共享或独立的节点（knots）。由于其简化的基函数和节点配置，TruKAN相比其他KAN变体具有更强的可解释性。通过优先选择可解释的基函数，TruKAN旨在平衡逼近效果与透明性。我们开发了TruKAN模型并将其集成到基于EfficientNet-V2的先进框架中，并在计算机视觉基准数据集上进行评估。为确保公平比较，我们构建了多种模型：基于MLP、KAN、SineKAN和TruKAN的EfficientNet框架，并在小型和深层架构下评估它们的训练时间和准确率。训练阶段采用混合优化策略以提高收敛稳定性。此外，我们还研究了所有模型的层归一化技术，并评估了TruKAN中共享节点与独立节点的影响。总体而言，TruKAN在复杂视觉任务中在准确率、计算效率和内存使用方面均优于其他KAN模型，展现出超越以往KAN研究有限设定的优势。

</details>


### [3] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 提出了一种名为DiGAN的新方法，结合扩散模型与注意力卷积网络，用于从有限且不规则的纵向神经影像数据中增强时间上下文并提升早期阿尔茨海默病检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在早期阿尔茨海默病诊断中受限于对大规模纵向数据的依赖，且难以处理临床数据中固有的时间连续性和模态不规则性问题。

Method: 提出Diffusion-Guided Attention Network（DiGAN），将潜在扩散模型与注意力引导的卷积网络相结合：扩散模型从有限数据合成逼真的纵向神经影像轨迹，注意力卷积层则捕捉区分认知正常、轻度认知障碍和主观认知下降个体的结构-时间模式。

Result: 在合成数据集和ADNI数据集上的实验表明，DiGAN优于当前最先进的基线方法，在早期阿尔茨海默病检测方面展现出优越性能。

Conclusion: DiGAN通过融合扩散建模与注意力机制，有效应对了真实临床环境中数据稀疏与不规则的问题，为早期阿尔茨海默病的自动诊断提供了新思路。

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

Abstract (中文翻译): 由于前驱期阶段脑结构变化细微且时间进程不规则，阿尔茨海默病（AD）的早期诊断仍是一项重大挑战。现有的深度学习方法通常需要大量纵向数据集，并且往往无法有效建模真实临床数据中固有的时间连续性和模态不规则性。为解决这些局限性，我们提出了扩散引导注意力网络（DiGAN），该方法将潜在扩散建模与注意力引导的卷积网络相结合。扩散模型能够从有限的训练数据中合成逼真的纵向神经影像轨迹，从而丰富时间上下文信息并提高对访视时间间隔不均的鲁棒性。随后，注意力卷积层可捕捉具有判别性的结构-时间模式，以区分认知正常个体、轻度认知障碍患者和主观认知下降个体。在合成数据集和ADNI数据集上的实验表明，DiGAN优于现有的最先进基线方法，展现出其在早期AD检测中的潜力。

</details>


### [4] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: PriorProbe 是一种基于 MCMC 与人类结合的新方法，能有效提取个体细粒度认知先验，并用于提升神经网络对模糊刺激的个性化预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提取个体认知先验时存在无法唯一识别或引入系统性偏差的问题，限制了神经网络的个性化能力。

Method: 提出 PriorProbe 方法，基于“与人结合的马尔可夫链蒙特卡洛”（MCMC with People），从个体参与者中恢复其特定的认知先验，并将其整合到先进神经网络中。

Result: 在面部表情识别任务中，整合 PriorProbe 提取的先验显著提升了模型对模糊刺激的个体分类预测性能，优于仅用神经网络或其他先验来源，且不损害对真实标签的推理能力。

Conclusion: PriorProbe 为个性化深度神经网络提供了一个通用且可解释的框架。

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

Abstract (中文翻译): 引入个体层面的认知先验是实现神经网络个性化的重要途径，但准确获取此类先验仍具挑战性：现有方法要么无法唯一识别这些先验，要么会引入系统性偏差。本文提出 PriorProbe，这是一种基于“与人结合的马尔可夫链蒙特卡洛”（Markov Chain Monte Carlo with People）的新颖先验提取方法，能够恢复细粒度的、个体特定的先验。我们以面部表情识别任务为例，对个体参与者应用 PriorProbe，并测试将所恢复的先验整合到当前最先进的神经网络中是否能提升其对模糊刺激下个体分类判断的预测能力。结果表明，由 PriorProbe 提取的先验带来了显著的性能提升，不仅优于单独使用神经网络，也优于其他先验来源，同时保留了网络对真实标签的推理能力。综上所述，本研究证明 PriorProbe 为个性化深度神经网络提供了一个通用且可解释的框架。

</details>


### [5] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 该研究提出了一种可解释的计算机视觉框架，用于在增材制造部件的三维断层图像中检测孔隙并评估其临界性。结果表明，孔隙到表面的归一化距离是预测临界性的主导因素，远超其他几何特征。


<details>
  <summary>Details</summary>
Motivation: 增材制造部件中的内部孔隙是影响结构性能的关键缺陷，现有自动检测方法缺乏可解释性，使工程师难以理解临界性预测的物理依据。

Method: 将灰度切片重建为三维数据集，通过基于强度的阈值分割和连通成分分析识别出500个独立孔隙；利用尺寸、长宽比、范围和距边界距离等几何描述符对孔隙进行表征；基于百分位数的欧氏距离构建孔隙交互网络（含24,950条连接）；使用机器学习模型预测孔隙临界性，并通过SHAP分析量化各特征贡献。

Result: 归一化表面距离对模型预测的贡献比其他所有描述符高出一个数量级以上；孔隙尺寸影响微弱，其他几何参数几乎无影响；表面邻近性与临界性呈强负相关，揭示了边界驱动的失效机制。

Conclusion: 该可解释框架实现了透明的缺陷评估，为增材制造的工艺优化和质量控制提供了可操作的见解。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

Abstract (中文翻译): 内部孔隙仍是增材制造部件中一种关键缺陷模式，会损害结构性能并限制其工业应用。尽管已有自动化缺陷检测方法，但它们缺乏可解释性，使工程师无法理解临界性预测的物理基础。本研究提出了一种可解释的计算机视觉框架，用于在三维断层扫描体积中进行孔隙检测和临界性评估。首先将连续的灰度切片重建为体积数据集，并通过基于强度的阈值分割结合连通成分分析识别出500个独立孔隙。每个孔隙均采用几何描述符进行表征，包括尺寸、长宽比、范围以及相对于试样边界的几何位置。接着，利用基于百分位数的欧氏距离准则构建孔隙交互网络，共得到24,950条孔隙间连接。随后，机器学习模型基于提取的特征预测孔隙临界性评分，并通过SHAP分析量化各特征的贡献。结果表明，归一化表面距离在模型预测中占主导地位，其重要性比所有其他描述符高出一个数量级以上；孔隙尺寸的影响微乎其微，而其他几何参数的影响则可忽略不计。表面邻近性与临界性之间存在强烈的负相关关系，揭示了由边界驱动的失效机制。该可解释框架实现了透明的缺陷评估，并为增材制造中的工艺优化和质量控制提供了可操作的见解。

</details>


### [6] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出了4DPC²hat，首个面向动态点云理解的多模态大语言模型（MLLM），并构建了大规模跨模态数据集4DPC²hat-200K，包含44K动态物体序列、700K点云帧和200K问答对。通过引入Mamba增强的时序推理模块和失败感知的自举学习策略，显著提升了动作理解和时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态点云对象，对动态点云序列的理解研究不足，主要受限于缺乏大规模跨模态数据集以及时空上下文中建模运动的困难。

Method: 1）构建大规模跨模态数据集4DPC²hat-200K，采用两阶段流程：拓扑一致的4D点云构建和两级标注；2）提出基于Mamba增强的时序推理MLLM，以捕捉点云序列中的长程依赖和动态模式；3）设计失败感知的自举学习策略，迭代识别模型缺陷并生成针对性问答监督信号，持续增强推理能力。

Result: 大量实验表明，4DPC²hat在动作理解和时序推理方面显著优于现有模型，为4D动态点云理解奠定了坚实基础。

Conclusion: 本文通过构建首个面向动态点云的大规模数据集和专用MLLM架构，有效推动了4D动态点云理解的发展，尤其在复杂时空推理任务上取得显著进展。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

Abstract (中文翻译): 点云为三维物体提供了一种紧凑且富有表现力的表示方式，并已被近期整合到多模态大语言模型（MLLMs）中。然而，现有方法主要聚焦于静态物体，而对动态点云序列的理解仍基本未被探索。这一局限性主要源于缺乏大规模跨模态数据集，以及在时空上下文中建模运动的困难。为弥合这一差距，我们提出了4DPC²hat——首个专为动态点云理解定制的MLLM。为此，我们通过一个精心设计的两阶段流程（包括拓扑一致的4D点云构建和两级标注）构建了大规模跨模态数据集4DPC²hat-200K。该数据集包含超过44K个动态物体序列、700K个点云帧和200K个精心整理的问题-答案（QA）对，支持关于计数、时序关系、动作、空间关系和外观等方面的查询。在框架核心部分，我们引入了一个Mamba增强的时序推理MLLM，以捕捉点云序列中的长程依赖关系和动态模式。此外，我们提出了一种失败感知的自举学习策略，该策略能迭代识别模型缺陷并生成针对性的QA监督信号，从而持续强化相应的推理能力。大量实验表明，与现有模型相比，我们的4DPC²hat在动作理解和时序推理方面显著提升，为4D动态点云理解奠定了坚实基础。

</details>


### [7] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出MQA-RefAVS任务，用于在无真实标注的情况下评估语言引导音视频分割（Ref-AVS）中候选分割掩码的质量，并构建了包含多种错误模式的基准MQ-RAVSBench及基于多模态大语言模型的评估器MQ-Auditor。


<details>
  <summary>Details</summary>
Motivation: 现有Ref-AVS方法虽能生成分割掩码，但缺乏对掩码质量进行丰富且可解释的诊断；尤其在推理阶段无法依赖真实标注来评估掩码质量，因此亟需一种无需真值参考的掩码质量评估机制。

Method: 作者构建了新任务MQA-RefAVS，要求模型基于音视频-语言输入和给定掩码，估计其与未知真值的IoU、识别错误类型并给出质量控制建议；同时构建了涵盖几何与语义错误的基准MQ-RAVSBench，并提出了基于多模态大语言模型（MLLM）的MQ-Auditor，通过显式推理多模态线索和掩码信息进行质量评估。

Result: 实验表明，MQ-Auditor在掩码质量评估上优于多个开源和商用MLLM，并可有效集成到现有Ref-AVS系统中，用于检测分割失败并支持后续分割性能提升。

Conclusion: 本工作首次将掩码质量评估引入Ref-AVS领域，提出的MQA-RefAVS任务、MQ-RAVSBench基准和MQ-Auditor模型为提升Ref-AVS系统的可靠性与可解释性提供了新思路。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

Abstract (中文翻译): 语言引导的音视频分割（Ref-AVS）旨在通过联合推理视频、音频和文本，分割出自然语言所描述的目标物体。除了生成分割掩码外，如何提供丰富且可解释的掩码质量诊断仍鲜有研究。本文提出了Ref-AVS场景下的掩码质量评估任务（MQA-RefAVS），该任务在推理阶段无需依赖真实标注即可评估候选分割掩码的质量。给定音视频-语言输入及每个提供的分割掩码，该任务需估计其与未观测到的真实标注之间的IoU、识别对应的错误类型，并推荐可操作的质量控制决策。为支持该任务，我们构建了MQ-RAVSBench基准，其中包含涵盖几何与语义问题的多样化且具代表性的掩码错误模式。我们进一步提出了MQ-Auditor——一种基于多模态大语言模型（MLLM）的评估器，能够显式地推理多模态线索和掩码信息，从而生成定量与定性的掩码质量评估。大量实验表明，MQ-Auditor优于多个强大的开源和商用MLLM，并可与现有Ref-AVS系统集成，用于检测分割失败并支持下游分割性能的提升。数据与代码将发布于 https://github.com/jasongief/MQA-RefAVS。

</details>


### [8] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: 提出了一种基于高斯核的超快三维光声迭代重建方法（GPAIR），将传统耗时数百秒至数小时的迭代重建加速至亚秒级，显著提升三维光声计算机断层成像（PACT）的临床实用性。


<details>
  <summary>Details</summary>
Motivation: 传统迭代重建（IR）算法虽能有效校正光声计算机断层成像（PACT）中的重建伪影，但其计算耗时极长，尤其在大规模三维成像中需数百秒甚至数小时，严重限制了其实际应用。

Method: 提出GPAIR方法：使用连续各向同性高斯核替代传统空间网格，推导压力波的解析闭式表达，并结合GPU加速的可微分Triton算子实现高效计算。

Result: 在动物实验中，GPAIR对包含840万体素的三维目标实现了亚秒级重建速度，比传统IR方法快几个数量级。

Conclusion: 该超快重建方法使大规模三维光声成像接近实时，极大推动了三维PACT向临床应用的转化。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

Abstract (中文翻译): 尽管迭代重建（IR）算法能够显著校正光声（PA）计算机断层成像（PACT）中的重建伪影，但其重建时间过长，尤其在大规模三维（3D）成像中，IR算法需要数百秒甚至数小时，计算负担严重限制了其实际应用。本研究提出了一种用于三维PACT的超快IR方法，称为基于高斯核的超快三维光声迭代重建（GPAIR），该方法实现了数量级级别的计算加速。GPAIR利用连续各向同性的高斯核替代传统的空间网格，通过推导压力波的解析闭式表达式，并结合强大的GPU加速可微分Triton算子，实现了卓越的超快重建性能。在动物实验中，GPAIR对包含840万体素的三维目标实现了亚秒级的重建速度。这一革命性的超快图像重建技术使大规模三维光声重建接近实时，显著推动了三维PACT向临床应用的发展。

</details>


### [9] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 本文研究了基于Vision Transformer（ViT）的无监督方法能否直接将大量未标注的动物图像聚类到物种级别，并进一步揭示种内变异（如性别、年龄等）。实验表明，DINOv3嵌入结合t-SNE和层次聚类可实现接近完美的物种级聚类（V-measure: 0.958），且无需先验知识的无监督方法也表现优异（0.943），仅需剔除1.14%的异常图像。作者还开源了基准工具包，为生态学家提供方法选择建议。


<details>
  <summary>Details</summary>
Motivation: 手动标注动物图像在生态研究中是一个重大瓶颈，限制了生物多样性监测的规模和效率。因此，亟需自动化方法来减少对人工标注的依赖，特别是在大规模图像数据集上实现物种级别的自动聚类。

Method: 作者构建了一个综合基准框架，评估五种ViT模型（包括DINOv3）、五种降维技术与四种聚类算法（两种有监督、两种无监督）在60个物种（30种哺乳动物和30种鸟类）上的表现。每种物种使用200张经验证的图像进行测试。此外，研究还探讨了聚类结果是否能揭示种内生态学上有意义的变异（如性别、年龄、表型差异）。

Result: 使用DINOv3嵌入配合t-SNE和有监督层次聚类方法，在物种级别聚类上达到V-measure 0.958；无监督方法也取得0.943的高分，仅需剔除1.14%的异常图像。该方法对长尾分布具有鲁棒性，且通过有意过聚类可有效提取种内变异信息。

Conclusion: ViT基础模型（尤其是DINOv3）结合合适的降维与聚类策略，能够在无需人工标注的情况下高效实现物种级图像聚类，并揭示种内生态特征。该研究为生态学家提供了实用的开源工具和方法选择指南，有望显著提升生物多样性监测的自动化水平。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

Abstract (中文翻译): 手动标注动物图像仍然是生态学研究中的一个重大瓶颈，限制了生物多样性监测工作的规模和效率。本研究探讨了最先进的视觉Transformer（ViT）基础模型是否能够直接将数千张未标注的动物图像聚类到物种级别。我们提出了一个全面的基准测试框架，评估了五种ViT模型结合五种降维技术和四种聚类算法（两种有监督、两种无监督）在60个物种（30种哺乳动物和30种鸟类）上的表现，每个测试使用每种物种200张经过验证的随机图像子集。我们研究了聚类在哪些情况下能成功实现物种级别区分、在哪些情况下失败，以及物种内部的聚类是否能揭示具有生态学意义的模式，例如性别、年龄或表型变异。我们的结果表明，使用DINOv3嵌入结合t-SNE和有监督层次聚类方法，可实现近乎完美的物种级聚类（V-measure：0.958）。无监督方法在无需任何先验物种知识的情况下也取得了具有竞争力的性能（0.943），仅将1.14%的图像作为异常值剔除以供专家审查。我们进一步证明了该方法对真实场景中常见的物种长尾分布具有鲁棒性，并表明有意进行过聚类可以可靠地提取种内变异信息，包括年龄阶段、性别二态性和毛色差异。我们发布了一个开源基准测试工具包，并为生态学家提供了针对其特定分类群和数据选择合适方法的建议。

</details>


### [10] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 本文提出了NH-Fair，一个统一的公平性基准，用于在标准化条件下评估视觉和大视觉-语言模型的公平性，发现良好调优的ERM基线常优于许多去偏方法，而复合数据增强策略能有效提升公平性而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据训练的机器学习模型常继承并放大对某些社会群体的偏见，但现有偏见缓解方法因数据集、公平性指标、模型类型和超参调优不一致，难以公平比较其有效性。

Method: 构建NH-Fair统一基准，在标准化数据、指标和训练协议下，系统评估ERM（经验风险最小化）调优对效用与公平性的影响，并对比多种去偏方法在视觉模型和大视觉-语言模型（LVLMs）上的表现，涵盖监督和零样本场景。

Result: (1) ERM调优可显著影响模型效用与公平性，提供减少超参搜索空间的实用指南；(2) 多数去偏方法不如良好调优的ERM基线，而复合数据增强方法能稳定提升公平性且不损失性能；(3) LVLMs虽平均准确率更高，但仍存在子群差异，模型缩放带来的收益通常小于架构或训练协议选择的影响。

Conclusion: NH-Fair为公平性评估提供了可复现、考虑调优的标准化流程，强调在追求高准确率的同时必须关注子群公平性，并指出复合数据增强是一种有前景的实用去偏策略。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

Abstract (中文翻译): 在现实世界数据上训练的机器学习模型往往会继承并放大对某些社会群体的偏见，这引发了对其大规模部署的紧迫担忧。尽管已提出众多偏见缓解方法，但由于数据集异构、公平性指标不一致、视觉模型与多模态模型评估相互孤立，以及超参数调优不足等问题，使得这些方法的有效性难以进行公平比较。我们提出了NH-Fair——一个“无害公平性”（fairness without harm）的统一基准，涵盖视觉模型和大视觉-语言模型（LVLMs），在标准化的数据、指标和训练协议下进行评估，覆盖监督学习和零样本场景。我们的主要贡献包括：（1）一项系统的ERM（经验风险最小化）调优研究，识别出对模型效用和群体差异具有显著影响的训练选择，从而为从业者提供基于实证的指导方针，以缩小实现高公平性与高准确率所需的昂贵超参数搜索空间；（2）证据表明，许多去偏方法并不能稳定地超越经过良好调优的ERM基线，而一种复合数据增强方法则能持续带来公平性提升且不牺牲模型效用，展现出作为实用策略的潜力；（3）分析显示，尽管LVLMs实现了更高的平均准确率，但仍表现出子群体间的差异，且模型规模扩展所带来的收益通常小于架构设计或训练协议选择所带来的改进。NH-Fair提供了一个可复现、关注调优过程的评估流程，支持严格且注重避免伤害的公平性评估。

</details>
