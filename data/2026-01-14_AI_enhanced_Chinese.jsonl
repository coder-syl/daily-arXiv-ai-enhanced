{"id": "2601.07845", "pdf": "https://arxiv.org/pdf/2601.07845", "abs": "https://arxiv.org/abs/2601.07845", "authors": ["Shree Charran R", "Rahul Kumar Dubey"], "title": "Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration", "categories": ["cs.CV"], "comment": null, "summary": "Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18AI\u7684\u5b9e\u65f6\u8def\u4fa7\u611f\u77e5\u8282\u70b9\uff0c\u7528\u4e8e\u591a\u7c7b\u4ea4\u901a\u8fdd\u89c4\u68c0\u6d4b\u4e0e\u5b89\u5168\u4e8b\u4ef6\u5206\u53d1\uff0c\u5177\u5907\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u529f\u8017\u548c\u65e0\u9700\u4eba\u5de5\u6821\u51c6\u7684\u7279\u70b9\uff0c\u5e76\u652f\u6301V2X\u901a\u4fe1\u4ee5\u589e\u5f3a\u534f\u540c\u611f\u77e5\u4e0e\u4e3b\u52a8\u9053\u8def\u5b89\u5168\u7ba1\u7406\u3002", "motivation": "\u5370\u5ea6\u7b49\u65b0\u5174\u7ecf\u6d4e\u4f53\u673a\u52a8\u8f66\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u6267\u6cd5\u8d44\u6e90\u4e25\u91cd\u4e0d\u8db3\uff082023\u5e74\u8d851100\u4e07\u8d77\u8fdd\u89c4\uff0c\u8b66\u529b\u5bc6\u5ea6\u4ec5\u4e3a\u6bcf4000\u8f86\u8f661\u540d\u8b66\u5458\uff09\uff0c\u4f20\u7edf\u4eba\u5de5\u76d1\u63a7\u4e0e\u5f00\u7f5a\u5355\u65b9\u5f0f\u65e0\u6cd5\u5e94\u5bf9\u5982\u6b64\u89c4\u6a21\uff0c\u4e9f\u9700\u81ea\u4e3b\u3001\u534f\u4f5c\u4e14\u8282\u80fd\u7684\u8fb9\u7f18AI\u611f\u77e5\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u7cfb\u7edf\u5728NVIDIA Jetson Nano\u4e0a\u90e8\u7f72\uff0c\u96c6\u6210YOLOv8 Nano\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u591a\u76ee\u6807\u68c0\u6d4b\u3001DeepSORT\u5b9e\u73b0\u65f6\u5e8f\u4e00\u81f4\u7684\u8f66\u8f86\u8ddf\u8e2a\uff0c\u4ee5\u53ca\u7b26\u5408MoRTH AIS 159\u548cISO 7591\u6807\u51c6\u7684\u89c4\u5219\u5f15\u5bfcOCR\u540e\u5904\u7406\u5f15\u64ce\u8bc6\u522b\u52a3\u5316\u6216\u591a\u8bed\u8a00\u8f66\u724c\uff1b\u901a\u8fc7TensorRT FP16\u91cf\u5316\u4f18\u5316\uff0c\u57289.6W\u529f\u8017\u4e0b\u5b9e\u73b028\u201330 FPS\u63a8\u7406\u3002", "result": "\u7cfb\u7edf\u5728\u4e94\u7c7b\u8fdd\u89c4\uff08\u95ef\u7ea2\u706f\u3001\u6591\u9a6c\u7ebf\u8fdd\u89c4\u3001\u9006\u884c\u3001\u975e\u6cd5\u6389\u5934\u3001\u8d85\u901f\uff09\u68c0\u6d4b\u4e2d\u8fbe\u523097.7%\u7684\u8fdd\u89c4\u68c0\u6d4b\u51c6\u786e\u7387\u548c84.9%\u7684OCR\u8bc6\u522b\u7cbe\u5ea6\uff0c\u65e0\u9700\u4eba\u5de5ROI\u6821\u51c6\uff1b\u76f8\u6bd4YOLOv4 Tiny\u3001PP-YOLOE-S\u548cNanoDetPlus\uff0cmAP\u63d0\u534710.7%\uff0c\u5355\u4f4d\u529f\u8017\u7cbe\u5ea6\u63d0\u53471.4\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8def\u4fa7\u8fb9\u7f18AI\u8282\u70b9\u4e0d\u4ec5\u80fd\u9ad8\u6548\u6267\u884c\u591a\u7c7b\u4ea4\u901a\u8fdd\u89c4\u81ea\u52a8\u6267\u6cd5\uff0c\u8fd8\u80fd\u901a\u8fc7V2X\u534f\u8bae\u5411\u8054\u7f51\u8f66\u8f86\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u53d1\u5e03CAM/DENM\u7c7b\u578b\u7684\u5b89\u5168\u4e8b\u4ef6\uff0c\u6709\u6548\u589e\u5f3aIEEE\u667a\u80fd\u8f66\u8f86\u751f\u6001\u4e2d\u7684\u534f\u540c\u611f\u77e5\u4e0e\u4e3b\u52a8\u9053\u8def\u5b89\u5168\u7ba1\u7406\u80fd\u529b\u3002", "summary_cn": "\u5370\u5ea6\u7b49\u65b0\u5174\u7ecf\u6d4e\u4f53\u7684\u5feb\u901f\u673a\u52a8\u5316\u9020\u6210\u4e86\u4e25\u91cd\u7684\u6267\u6cd5\u4e0d\u5bf9\u79f0\u95ee\u9898\uff1a2023\u5e74\u8bb0\u5f55\u7684\u4ea4\u901a\u8fdd\u89c4\u8d85\u8fc71100\u4e07\u8d77\uff0c\u800c\u4eba\u529b\u8b66\u529b\u5bc6\u5ea6\u4ec5\u4e3a\u6bcf4000\u8f86\u8f66\u7ea61\u540d\u8b66\u5458\u3002\u4f20\u7edf\u7684\u76d1\u63a7\u624b\u6bb5\u548c\u4eba\u5de5\u5f00\u7f5a\u5355\u65b9\u5f0f\u65e0\u6cd5\u5e94\u5bf9\u5982\u6b64\u5e9e\u5927\u7684\u89c4\u6a21\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u81ea\u4e3b\u3001\u534f\u4f5c\u4e14\u8282\u80fd\u7684\u8fb9\u7f18AI\u611f\u77e5\u57fa\u7840\u8bbe\u65bd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u8def\u4fa7\u611f\u77e5\u8282\u70b9\uff0c\u7528\u4e8e\u5728\u8f66\u8054\u7f51\u4e0e\u667a\u80fd\u8f66\u8f86\u751f\u6001\u7cfb\u7edf\u4e2d\u8fdb\u884c\u591a\u7c7b\u522b\u4ea4\u901a\u8fdd\u89c4\u5206\u6790\u4e0e\u5b89\u5168\u4e8b\u4ef6\u5206\u53d1\u3002\u8be5\u8282\u70b9\u96c6\u6210\u4e86YOLOv8 Nano\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u591a\u76ee\u6807\u68c0\u6d4b\uff0c\u91c7\u7528DeepSORT\u8fdb\u884c\u65f6\u5e8f\u4e00\u81f4\u7684\u8f66\u8f86\u8ddf\u8e2a\uff0c\u5e76\u914d\u5907\u4e00\u4e2a\u89c4\u5219\u5f15\u5bfc\u7684OCR\u540e\u5904\u7406\u5f15\u64ce\uff0c\u80fd\u591f\u8bc6\u522b\u7b26\u5408MoRTH AIS 159\u548cISO 7591\u89c6\u89c9\u5bf9\u6bd4\u5ea6\u6807\u51c6\u7684\u52a3\u5316\u6216\u591a\u8bed\u8a00\u8f66\u724c\u3002\u7cfb\u7edf\u90e8\u7f72\u4e8e\u914d\u5907128\u6838Maxwell GPU\u7684NVIDIA Jetson Nano\u5e73\u53f0\uff0c\u5e76\u901a\u8fc7TensorRT FP16\u91cf\u5316\u8fdb\u884c\u4f18\u5316\uff0c\u57289.6\u74e6\u529f\u8017\u4e0b\u53ef\u6301\u7eed\u5b9e\u73b028\u81f330\u5e27\u6bcf\u79d2\u7684\u63a8\u7406\u901f\u5ea6\u3002\u5728\u65e0\u9700\u4eba\u5de5\u8bbe\u5b9a\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u5728\u4e94\u7c7b\u8fdd\u89c4\u884c\u4e3a\uff08\u95ef\u7ea2\u706f\u3001\u6591\u9a6c\u7ebf\u8fdd\u89c4\u3001\u9006\u884c\u3001\u975e\u6cd5\u6389\u5934\u548c\u8d85\u901f\uff09\u68c0\u6d4b\u4e2d\u8fbe\u5230\u4e8697.7%\u7684\u8fdd\u89c4\u68c0\u6d4b\u51c6\u786e\u7387\u548c84.9%\u7684OCR\u8bc6\u522b\u7cbe\u5ea6\u3002\u4e0eYOLOv4 Tiny\u3001PP-YOLOE-S\u548cNanoDetPlus\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\u63d0\u5347\u4e8610.7%\uff0c\u5355\u4f4d\u529f\u8017\u4e0b\u7684\u7cbe\u5ea6\u63d0\u9ad8\u4e861.4\u500d\u3002\u6b64\u5916\uff0c\u8be5\u8282\u70b9\u8fd8\u901a\u8fc7V2X\u534f\u8bae\u5411\u8054\u7f51\u8f66\u8f86\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u540e\u7aef\u53d1\u5e03\u6807\u51c6\u5316\u7684CAM\u548cDENM\u7c7b\u578b\u5b89\u5168\u4e8b\u4ef6\uff0c\u8868\u660e\u8def\u4fa7\u8fb9\u7f18AI\u5206\u6790\u53ef\u6709\u6548\u589e\u5f3aIEEE\u667a\u80fd\u8f66\u8f86\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u534f\u540c\u611f\u77e5\u4e0e\u4e3b\u52a8\u9053\u8def\u5b89\u5168\u7ba1\u7406\u80fd\u529b\u3002"}}
{"id": "2601.07855", "pdf": "https://arxiv.org/pdf/2601.07855", "abs": "https://arxiv.org/abs/2601.07855", "authors": ["Subeen Lee", "Siyeong Lee", "Namil Kim", "Jaesik Choi"], "title": "An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ROAD\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LiDAR\u70b9\u4e91\u5206\u7c7b\u6a21\u578b\u5728\u540c\u65f6\u5b58\u5728\u57df\u504f\u79fb\u548c\u6807\u7b7e\u6f14\u53d8\uff08\u7c7b\u522b\u62c6\u5206\u3001\u6269\u5c55\u3001\u63d2\u5165\uff09\u4e0b\u7684\u6301\u7eed\u5b66\u4e60\u4e0e\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5206\u6790\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9c81\u68d23D\u611f\u77e5\u63d0\u4f9b\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d3D\u70b9\u4e91\u611f\u77e5\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u4e0e\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u8fdc\u843d\u540e\u4e8e2D\u89c6\u89c9\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u540c\u65f6\u53d1\u751f\u57df\u504f\u79fb\u548c\u6807\u7b7e\u504f\u79fb\u573a\u666f\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u66f4\u8d34\u8fd1\u73b0\u5b9e\u5e94\u7528\u573a\u666f\u7684\u7efc\u5408\u8bc4\u6d4b\u57fa\u51c6\u3002", "method": "\u63d0\u51faROBust Autonomous driving under Dataset shifts (ROAD) \u57fa\u51c6\uff0c\u6574\u5408Waymo\u3001NuScenes\u3001Argoverse2\u7b49\u5927\u89c4\u6a21LiDAR\u6570\u636e\u96c6\uff0c\u660e\u786e\u5efa\u6a21\u57df\u504f\u79fb\u53ca\u4e09\u7c7b\u6807\u7b7e\u6f14\u53d8\uff08\u7c7b\u522b\u62c6\u5206\u3001\u6269\u5c55\u3001\u63d2\u5165\uff09\uff0c\u5e76\u8bc4\u4f30\u96f6\u6837\u672c\u8fc1\u79fb\u3001\u7ebf\u6027\u63a2\u9488\u548c\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65b9\u6cd5\uff0c\u5206\u6790\u9aa8\u5e72\u7f51\u7edc\u3001\u8bad\u7ec3\u76ee\u6807\u548cCL\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u540c\u65f6\u53d1\u751f\u7684\u57df\u548c\u6807\u7b7e\u504f\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u591a\u4e2a\u5f3a\u57fa\u7ebf\u7ed3\u679c\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002", "conclusion": "ROAD\u57fa\u51c6\u6709\u6548\u63ed\u793a\u4e86\u5f53\u524d3D\u611f\u77e5\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u80fd\u540c\u65f6\u5904\u7406\u57df\u504f\u79fb\u548c\u6807\u7b7e\u6f14\u5316\u7684\u9c81\u68d2\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002", "summary_cn": "\u8981\u4f7f3D\u611f\u77e5\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\uff08\u4ece\u81ea\u52a8\u9a7e\u9a76\u5230\u5177\u8eab\u667a\u80fd\uff09\u4e2d\u5177\u5907\u5b9e\u7528\u6027\uff0c\u6a21\u578b\u5fc5\u987b\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u7269\u4f53\u5b9a\u4e49\u548c\u4f20\u611f\u5668\u57df\u3002\u7136\u800c\uff0c\u76f8\u8f83\u4e8e2D\u89c6\u89c9\uff0c\u9488\u5bf93D\u70b9\u4e91\u611f\u77e5\u7684\u6301\u7eed\u5b66\u4e60\u4e0e\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u4ecd\u663e\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u540c\u65f6\u5b58\u5728\u57df\u504f\u79fb\u548c\u6807\u7b7e\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u6570\u636e\u96c6\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u81ea\u52a8\u9a7e\u9a76\u201d\uff08ROAD\uff09\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411LiDAR\u70b9\u4e91\u76ee\u6807\u5206\u7c7b\u7684\u7efc\u5408\u6027\u8bc4\u4f30\u5957\u4ef6\uff0c\u660e\u786e\u8003\u8651\u4e86\u57df\u504f\u79fb\u4ee5\u53ca\u4e09\u7c7b\u5173\u952e\u7684\u6807\u7b7e\u6f14\u5316\u5f62\u5f0f\uff1a\u7c7b\u522b\u62c6\u5206\u3001\u7c7b\u522b\u6269\u5c55\u548c\u7c7b\u522b\u63d2\u5165\u3002\u6211\u4eec\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08Waymo\u3001NuScenes\u3001Argoverse2\uff09\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u3001\u7ebf\u6027\u63a2\u9488\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u3001\u8bad\u7ec3\u76ee\u6807\u548c\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u7684\u5f71\u54cd\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u771f\u5b9e\u504f\u79fb\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u9c81\u68d23D\u611f\u77e5\u7814\u7a76\u5efa\u7acb\u4e86\u5f3a\u6709\u529b\u7684\u57fa\u7ebf\u3002"}}
{"id": "2601.07941", "pdf": "https://arxiv.org/pdf/2601.07941", "abs": "https://arxiv.org/abs/2601.07941", "authors": ["Yan Wang", "M M Sayeef Abdullah", "Partho Hassan", "Sabit Hassan"], "title": "Moonworks Lunara Aesthetic Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Lunara\u7f8e\u5b66\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u7531Moonworks Lunara\u6a21\u578b\u751f\u6210\uff0c\u6db5\u76d6\u4e2d\u4e1c\u3001\u5317\u6b27\u3001\u4e1c\u4e9a\u548c\u5357\u4e9a\u7b49\u5730\u533a\u7684\u827a\u672f\u98ce\u683c\u4ee5\u53ca\u7d20\u63cf\u3001\u6cb9\u753b\u7b49\u901a\u7528\u7c7b\u522b\uff0c\u5177\u6709\u9ad8\u8d28\u91cf\u7f8e\u5b66\u8bc4\u5206\u3001\u4e30\u5bcc\u98ce\u683c\u591a\u6837\u6027\uff0c\u5e76\u9644\u5e26\u4eba\u5de5\u4f18\u5316\u7684\u63d0\u793a\u8bcd\u4e0e\u7ed3\u6784\u5316\u6807\u6ce8\uff0c\u4ee5Apache 2.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u652f\u6301\u5b66\u672f\u4e0e\u5546\u4e1a\u7528\u9014\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u7f51\u7edc\u6765\u6e90\u6570\u636e\u96c6\u5f80\u5f80\u6ce8\u91cd\u5e7f\u5ea6\u800c\u5ffd\u89c6\u7cbe\u5ea6\uff0c\u7f3a\u4e4f\u5bf9\u7f8e\u5b66\u8d28\u91cf\u3001\u98ce\u683c\u591a\u6837\u6027\u548c\u8bb8\u53ef\u900f\u660e\u5ea6\u7684\u5173\u6ce8\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u9ad8\u7f8e\u5b66\u4ef7\u503c\u548c\u98ce\u683c\u591a\u6837\u6027\u7684\u56fe\u50cf\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528Moonworks Lunara\u6a21\u578b\u751f\u6210\u56fe\u50cf\uff0c\u6db5\u76d6\u591a\u79cd\u5730\u57df\u6027\u4e0e\u901a\u7528\u827a\u672f\u98ce\u683c\uff1b\u6bcf\u5f20\u56fe\u50cf\u5747\u914d\u6709\u7ecf\u4eba\u5de5\u4f18\u5316\u7684\u63d0\u793a\u8bcd\u53ca\u63cf\u8ff0\u663e\u8457\u5bf9\u8c61\u3001\u5c5e\u6027\u3001\u5173\u7cfb\u548c\u98ce\u683c\u7ebf\u7d22\u7684\u7ed3\u6784\u5316\u6807\u6ce8\u3002", "result": "\u6240\u6784\u5efa\u7684Lunara\u7f8e\u5b66\u6570\u636e\u96c6\u5728\u7f8e\u5b66\u8bc4\u5206\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u7f8e\u5b66\u5bfc\u5411\u548c\u901a\u7528\u76ee\u7684\u6570\u636e\u96c6\uff0c\u6210\u4e3a\u9996\u4e2a\u5728\u6b64\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u7684\u6570\u636e\u96c6\u3002", "conclusion": "Lunara\u7f8e\u5b66\u6570\u636e\u96c6\u901a\u8fc7\u5f3a\u8c03\u7f8e\u5b66\u8d28\u91cf\u3001\u98ce\u683c\u591a\u6837\u6027\u548c\u8bb8\u53ef\u900f\u660e\u5ea6\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u81ea\u7531\u4f7f\u7528\u7684\u8d44\u6e90\u3002", "summary_cn": "\u8be5\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u591a\u79cd\u827a\u672f\u98ce\u683c\uff0c\u5305\u62ec\u6765\u81ea\u4e2d\u4e1c\u3001\u5317\u6b27\u3001\u4e1c\u4e9a\u548c\u5357\u4e9a\u7b49\u5730\u533a\u7684\u672c\u571f\u7f8e\u5b66\u98ce\u683c\uff0c\u4ee5\u53ca\u7d20\u63cf\u548c\u6cb9\u753b\u7b49\u901a\u7528\u7c7b\u522b\u3002\u6240\u6709\u56fe\u50cf\u5747\u7531Moonworks Lunara\u6a21\u578b\u751f\u6210\uff0c\u5e76\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u4ee5\u4f53\u73b0\u72ec\u7279\u4e14\u9ad8\u8d28\u91cf\u7684\u7f8e\u5b66\u98ce\u683c\uff0c\u4ece\u800c\u6784\u5efa\u51fa\u9996\u4e2a\u5728\u7f8e\u5b66\u8bc4\u5206\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u7f8e\u5b66\u5bfc\u5411\u6570\u636e\u96c6\u53ca\u901a\u7528\u6570\u636e\u96c6\u7684\u6570\u636e\u96c6\u3002\u6bcf\u5f20\u56fe\u50cf\u5747\u914d\u6709\u7ecf\u4eba\u5de5\u4f18\u5316\u7684\u63d0\u793a\u8bcd\u548c\u7ed3\u6784\u5316\u6807\u6ce8\uff0c\u5171\u540c\u63cf\u8ff0\u5176\u4e2d\u7684\u663e\u8457\u7269\u4f53\u3001\u5c5e\u6027\u3001\u5173\u7cfb\u53ca\u98ce\u683c\u7ebf\u7d22\u3002\u4e0e\u5f3a\u8c03\u5e7f\u5ea6\u800c\u975e\u7cbe\u5ea6\u7684\u5927\u89c4\u6a21\u7f51\u7edc\u6765\u6e90\u6570\u636e\u96c6\u4e0d\u540c\uff0cLunara\u7f8e\u5b66\u6570\u636e\u96c6\u4f18\u5148\u8003\u8651\u7f8e\u5b66\u8d28\u91cf\u3001\u98ce\u683c\u591a\u6837\u6027\u548c\u8bb8\u53ef\u900f\u660e\u5ea6\uff0c\u5e76\u4ee5Apache 2.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u4ee5\u652f\u6301\u5b66\u672f\u7814\u7a76\u53ca\u65e0\u9650\u5236\u7684\u5546\u4e1a\u7528\u9014\u3002"}}
{"id": "2601.07957", "pdf": "https://arxiv.org/pdf/2601.07957", "abs": "https://arxiv.org/abs/2601.07957", "authors": ["Fikadu Weloday", "Jianmei Su"], "title": "LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification on Edge Devices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Maize disease classification plays a vital role in mitigating yield losses and ensuring food security. However, the deployment of traditional disease detection models in resource-constrained environments, such as those using smartphones and drones, faces challenges due to high computational costs. To address these challenges, we propose LWMSCNN-SE, a lightweight convolutional neural network (CNN) that integrates multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-Excitation (SE) attention mechanisms. This novel combination enables the model to achieve 96.63% classification accuracy with only 241,348 parameters and 0.666 GFLOPs, making it suitable for real-time deployment in field applications. Our approach addresses the accuracy--efficiency trade-off by delivering high accuracy while maintaining low computational costs, demonstrating its potential for efficient maize disease diagnosis on edge devices in precision farming systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7CNN\u6a21\u578bLWMSCNN-SE\uff0c\u7528\u4e8e\u7389\u7c73\u75c5\u5bb3\u5206\u7c7b\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u8fbe\u523096.63%\u7684\u51c6\u786e\u7387\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u4f20\u7edf\u75c5\u5bb3\u68c0\u6d4b\u6a21\u578b\u5728\u667a\u80fd\u624b\u673a\u548c\u65e0\u4eba\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u56f0\u96be\uff0c\u56e0\u5176\u8ba1\u7b97\u5f00\u9500\u9ad8\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u548c\u6548\u7387\u9700\u6c42\u3002", "method": "\u63d0\u51faLWMSCNN-SE\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548cSE\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u6a21\u578b\u5728\u4ec5241,348\u4e2a\u53c2\u6570\u548c0.666 GFLOPs\u4e0b\u5b9e\u73b0\u4e8696.63%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u9002\u5408\u7530\u95f4\u5b9e\u65f6\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5728\u7cbe\u51c6\u519c\u4e1a\u7cfb\u7edf\u4e2d\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8bca\u65ad\u7389\u7c73\u75c5\u5bb3\u7684\u6f5c\u529b\u3002", "summary_cn": "\u7389\u7c73\u75c5\u5bb3\u5206\u7c7b\u5728\u51cf\u5c11\u4ea7\u91cf\u635f\u5931\u548c\u4fdd\u969c\u7cae\u98df\u5b89\u5168\u65b9\u9762\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u75c5\u5bb3\u68c0\u6d4b\u6a21\u578b\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5728\u667a\u80fd\u624b\u673a\u548c\u65e0\u4eba\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86LWMSCNN-SE\u2014\u2014\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u8be5\u7f51\u7edc\u878d\u5408\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u4ee5\u53ca\u538b\u7f29\u6fc0\u52b1\uff08Squeeze-and-Excitation, SE\uff09\u6ce8\u610f\u529b\u673a\u5236\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u7ec4\u5408\u4f7f\u6a21\u578b\u5728\u4ec5\u6709241,348\u4e2a\u53c2\u6570\u548c0.666 GFLOPs\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e8696.63%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u975e\u5e38\u9002\u5408\u5728\u7530\u95f4\u5e94\u7528\u4e2d\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u51c6\u786e\u7387\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u7cbe\u51c6\u519c\u4e1a\u7cfb\u7edf\u4e2d\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8bca\u65ad\u7389\u7c73\u75c5\u5bb3\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.07963", "pdf": "https://arxiv.org/pdf/2601.07963", "abs": "https://arxiv.org/abs/2601.07963", "authors": ["Jiahua Dong", "Yu-Xiong Wang"], "title": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "categories": ["cs.CV"], "comment": null, "summary": "The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa3DGS-Drag\uff0c\u4e00\u79cd\u57fa\u4e8e\u70b9\u76843D\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u62d6\u62fd\u64cd\u4f5c\u5b9e\u73b0\u5bf9\u771f\u5b9e3D\u573a\u666f\u7684\u9ad8\u6548\u3001\u76f4\u89c2\u7f16\u8f91\uff0c\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u53d8\u5f62\u5f15\u5bfc\u548c\u6269\u6563\u6a21\u578b\u5185\u5bb9\u5f15\u5bfc\uff0c\u5728\u51e0\u4f55\u76f8\u5173\u7f16\u8f91\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6548\u679c\uff0c\u5355\u5361RTX 4090\u4e0a\u4ec5\u970010-20\u5206\u949f\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u7f16\u8f91\u5728\u5b9e\u73b0\u76f4\u89c2\u62d6\u62fd\u64cd\u4f5c\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u5728\u51e0\u4f55\u76f8\u5173\u5185\u5bb9\u7f16\u8f91\u4e0a\uff0c\u73b0\u6709\u57fa\u4e8e\u5f62\u53d8\u548c\u57fa\u4e8e2D\u7f16\u8f91\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa3DGS-Drag\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u8fdb\u884c\u4e00\u81f4\u51e0\u4f55\u4fee\u6539\u7684\u5f62\u53d8\u5f15\u5bfc\uff0c\u4ee5\u53ca\u7528\u4e8e\u5185\u5bb9\u4fee\u6b63\u4e0e\u89c6\u89c9\u8d28\u91cf\u63d0\u5347\u7684\u6269\u6563\u5f15\u5bfc\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u7f16\u8f91\u7b56\u7565\u652f\u6301\u5927\u5e45\u5ea6\u62d6\u62fd\u64cd\u4f5c\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u8fd0\u52a8\u53d8\u5316\u3001\u5f62\u72b6\u8c03\u6574\u3001\u56fe\u50cf\u4fee\u590d\u548c\u5185\u5bb9\u6269\u5c55\u7b49\u591a\u79cd\u7f16\u8f91\u7c7b\u578b\uff0c\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u51e0\u4f55\u76f8\u51733D\u5185\u5bb9\u7f16\u8f91\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e14\u7f16\u8f91\u6548\u7387\u9ad8\uff08\u5355\u5f20RTX 4090 GPU\u8017\u65f610-20\u5206\u949f\uff09\u3002", "conclusion": "3DGS-Drag\u6709\u6548\u5f25\u5408\u4e86\u73b0\u67093D\u7f16\u8f91\u65b9\u6cd5\u5728\u51e0\u4f55\u5185\u5bb9\u7f16\u8f91\u4e0a\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u76f4\u89c2\u4e14\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e3D\u573a\u666f\u62d6\u62fd\u7f16\u8f91\u3002", "summary_cn": "\u751f\u6210\u6a21\u578b\u7684\u8fdb\u6b65\u9010\u6b65\u91ca\u653e\u4e863D\u5185\u5bb9\u521b\u4f5c\u7684\u53d8\u9769\u6f5c\u529b\u3002\u8fd1\u671f\uff0c\u5177\u6709\u51e0\u4f55\u53d8\u5316\u7684\u76f4\u89c2\u62d6\u62fd\u7f16\u8f91\u57282D\u7f16\u8f91\u4e2d\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u57283D\u573a\u666f\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa3DGS-Drag\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u70b9\u76843D\u7f16\u8f91\u6846\u67b6\uff0c\u53ef\u5bf9\u771f\u5b9e3D\u573a\u666f\u8fdb\u884c\u9ad8\u6548\u3001\u76f4\u89c2\u7684\u62d6\u62fd\u64cd\u4f5c\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5f25\u5408\u4e86\u57fa\u4e8e\u5f62\u53d8\u548c\u57fa\u4e8e2D\u7f16\u8f91\u76843D\u7f16\u8f91\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u89e3\u51b3\u4e86\u5b83\u4eec\u5728\u51e0\u4f55\u76f8\u5173\u5185\u5bb9\u7f16\u8f91\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u5229\u7528\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a\u4e00\u662f\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u8fdb\u884c\u4e00\u81f4\u51e0\u4f55\u4fee\u6539\u7684\u5f62\u53d8\u5f15\u5bfc\uff0c\u4e8c\u662f\u7528\u4e8e\u5185\u5bb9\u4fee\u6b63\u548c\u89c6\u89c9\u8d28\u91cf\u589e\u5f3a\u7684\u6269\u6563\u5f15\u5bfc\u3002\u6b64\u5916\uff0c\u6e10\u8fdb\u5f0f\u7f16\u8f91\u7b56\u7565\u8fdb\u4e00\u6b65\u652f\u6301\u5927\u5e45\u5ea6\u76843D\u62d6\u62fd\u7f16\u8f91\u3002\u672c\u65b9\u6cd5\u652f\u6301\u591a\u79cd\u7f16\u8f91\u7c7b\u578b\uff0c\u5305\u62ec\u8fd0\u52a8\u53d8\u5316\u3001\u5f62\u72b6\u8c03\u6574\u3001\u56fe\u50cf\u4fee\u590d\u548c\u5185\u5bb9\u6269\u5c55\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c3DGS-Drag\u5728\u5404\u79cd\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5728\u51e0\u4f55\u76f8\u5173\u76843D\u5185\u5bb9\u7f16\u8f91\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u7f16\u8f91\u8fc7\u7a0b\u9ad8\u6548\uff0c\u5728\u5355\u5f20RTX 4090 GPU\u4e0a\u4ec5\u970010\u81f320\u5206\u949f\u3002"}}
{"id": "2601.07970", "pdf": "https://arxiv.org/pdf/2601.07970", "abs": "https://arxiv.org/abs/2601.07970", "authors": ["Sunusi Ibrahim Muhammad", "Ismail Ismail Tijjani", "Saadatu Yusuf Jumare", "Fatima Isah Jibrin"], "title": "Sesame Plant Segmentation Dataset: A YOLO Formatted Annotated Dataset", "categories": ["cs.CV"], "comment": "Presented at International Conference on Computing and advance in Information Technology(ICCAIT2025) The dataset is available at kaggle : https://www.kaggle.com/datasets/ismailismailtijjani/sesame-plant-detection-dataset", "summary": "This paper presents the Sesame Plant Segmentation Dataset, an open source annotated image dataset designed to support the development of artificial intelligence models for agricultural applications, with a specific focus on sesame plants. The dataset comprises 206 training images, 43 validation images, and 43 test images in YOLO compatible segmentation format, capturing sesame plants at early growth stages under varying environmental conditions. Data were collected using a high resolution mobile camera from farms in Jirdede, Daura Local Government Area, Katsina State, Nigeria, and annotated using the Segment Anything Model version 2 with farmer supervision. Unlike conventional bounding box datasets, this dataset employs pixel level segmentation to enable more precise detection and analysis of sesame plants in real world farm settings. Model evaluation using the Ultralytics YOLOv8 framework demonstrated strong performance for both detection and segmentation tasks. For bounding box detection, the model achieved a recall of 79 percent, precision of 79 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 58 percent. For segmentation, it achieved a recall of 82 percent, precision of 77 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 52 percent. The dataset represents a novel contribution to sesame focused agricultural vision datasets in Nigeria and supports applications such as plant monitoring, yield estimation, and agricultural research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u829d\u9ebb\u690d\u682a\u5206\u5272\u6570\u636e\u96c6\uff08Sesame Plant Segmentation Dataset\uff09\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u652f\u6301\u519c\u4e1aAI\u6a21\u578b\u5f00\u53d1\u800c\u8bbe\u8ba1\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b292\u5f20\u5728\u5c3c\u65e5\u5229\u4e9a\u519c\u7530\u91c7\u96c6\u7684\u829d\u9ebb\u65e9\u671f\u751f\u957f\u9636\u6bb5\u56fe\u50cf\uff0c\u91c7\u7528YOLO\u517c\u5bb9\u7684\u5206\u5272\u683c\u5f0f\uff0c\u5e76\u5229\u7528SAM v2\u6a21\u578b\u5728\u519c\u6c11\u76d1\u7763\u4e0b\u8fdb\u884c\u6807\u6ce8\u3002YOLOv8\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u5176\u5728\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u586b\u8865\u4e86\u5c3c\u65e5\u5229\u4e9a\u829d\u9ebb\u519c\u4e1a\u89c6\u89c9\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u829d\u9ebb\u4f5c\u7269\u3001\u7279\u522b\u662f\u9002\u7528\u4e8e\u771f\u5b9e\u519c\u7530\u73af\u5883\u7684\u9ad8\u8d28\u91cf\u50cf\u7d20\u7ea7\u6807\u6ce8\u519c\u4e1a\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86AI\u6280\u672f\u5728\u829d\u9ebb\u79cd\u690d\u76d1\u6d4b\u3001\u4ea7\u91cf\u4f30\u7b97\u7b49\u7cbe\u51c6\u519c\u4e1a\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u829d\u9ebb\u65e9\u671f\u751f\u957f\u9636\u6bb5\u7684\u5f00\u6e90\u5206\u5272\u6570\u636e\u96c6\u6765\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5728\u5c3c\u65e5\u5229\u4e9a\u5361\u9f50\u7eb3\u5dde\u7684\u519c\u7530\u4e2d\uff0c\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u624b\u673a\u76f8\u673a\u91c7\u96c6\u4e86292\u5f20\u829d\u9ebb\u690d\u682a\u56fe\u50cf\u3002\u8fd9\u4e9b\u56fe\u50cf\u88ab\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u5e76\u91c7\u7528Segment Anything Model (SAM) v2\u5728\u519c\u6c11\u7684\u4e13\u4e1a\u76d1\u7763\u4e0b\u8fdb\u884c\u50cf\u7d20\u7ea7\u5206\u5272\u6807\u6ce8\uff0c\u6700\u7ec8\u5f62\u6210YOLO\u517c\u5bb9\u7684\u683c\u5f0f\u3002\u968f\u540e\uff0c\u4f7f\u7528Ultralytics YOLOv8\u6846\u67b6\u5bf9\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5728YOLOv8\u6846\u67b6\u4e0b\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u53ec\u56de\u7387\u548c\u7cbe\u786e\u7387\u5747\u4e3a79%\uff0cmAP@0.50\u4e3a84%\uff0cmAP@0.50:0.95\u4e3a58%\u3002\u5728\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u53ec\u56de\u7387\u4e3a82%\uff0c\u7cbe\u786e\u7387\u4e3a77%\uff0cmAP@0.50\u4e3a84%\uff0cmAP@0.50:0.95\u4e3a52%\u3002", "conclusion": "\u8be5\u829d\u9ebb\u690d\u682a\u5206\u5272\u6570\u636e\u96c6\u662f\u5c3c\u65e5\u5229\u4e9a\u9996\u4e2a\u4e13\u6ce8\u4e8e\u829d\u9ebb\u7684\u519c\u4e1a\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u5176\u50cf\u7d20\u7ea7\u6807\u6ce8\u7279\u6027\u4e3a\u829d\u9ebb\u690d\u682a\u7684\u7cbe\u786e\u8bc6\u522b\u4e0e\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\uff0c\u5bf9\u63a8\u52a8\u829d\u9ebb\u76f8\u5173\u7684\u690d\u7269\u76d1\u6d4b\u3001\u4ea7\u91cf\u9884\u4f30\u53ca\u519c\u4e1a\u7814\u7a76\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "summary_cn": "\u672c\u6587\u4ecb\u7ecd\u4e86\u829d\u9ebb\u690d\u682a\u5206\u5272\u6570\u636e\u96c6\uff08Sesame Plant Segmentation Dataset\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5e26\u6ce8\u91ca\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u519c\u4e1a\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u805a\u7126\u4e8e\u829d\u9ebb\u690d\u682a\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b206\u5f20\u8bad\u7ec3\u56fe\u50cf\u300143\u5f20\u9a8c\u8bc1\u56fe\u50cf\u548c43\u5f20\u6d4b\u8bd5\u56fe\u50cf\uff0c\u91c7\u7528YOLO\u517c\u5bb9\u7684\u5206\u5272\u683c\u5f0f\uff0c\u6355\u6349\u4e86\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u5904\u4e8e\u65e9\u671f\u751f\u957f\u9636\u6bb5\u7684\u829d\u9ebb\u690d\u682a\u3002\u6570\u636e\u91c7\u96c6\u81ea\u5c3c\u65e5\u5229\u4e9a\u5361\u9f50\u7eb3\u5dde\u9053\u62c9\u5730\u65b9\u653f\u5e9c\u533a\u7684\u5409\u5c14\u4ee3\u5fb7\u519c\u573a\uff0c\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u624b\u673a\u76f8\u673a\u62cd\u6444\uff0c\u5e76\u5728\u519c\u6c11\u76d1\u7763\u4e0b\u5229\u7528Segment Anything Model\uff08SAM\uff09\u7b2c\u4e8c\u7248\u8fdb\u884c\u6807\u6ce8\u3002\u4e0e\u4f20\u7edf\u7684\u8fb9\u754c\u6846\u6570\u636e\u96c6\u4e0d\u540c\uff0c\u8be5\u6570\u636e\u96c6\u91c7\u7528\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u771f\u5b9e\u519c\u7530\u73af\u5883\u4e2d\u829d\u9ebb\u690d\u682a\u66f4\u7cbe\u786e\u7684\u68c0\u6d4b\u4e0e\u5206\u6790\u3002\u4f7f\u7528Ultralytics YOLOv8\u6846\u67b6\u8fdb\u884c\u7684\u6a21\u578b\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6570\u636e\u96c6\u5728\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff1a\u5728\u8fb9\u754c\u6846\u68c0\u6d4b\u65b9\u9762\uff0c\u6a21\u578b\u8fbe\u5230\u4e8679%\u7684\u53ec\u56de\u7387\u300179%\u7684\u7cbe\u786e\u7387\u300184%\u7684mAP@0.50\u4ee5\u53ca58%\u7684mAP@0.50:0.95\uff1b\u5728\u5206\u5272\u65b9\u9762\uff0c\u8fbe\u5230\u4e8682%\u7684\u53ec\u56de\u7387\u300177%\u7684\u7cbe\u786e\u7387\u300184%\u7684mAP@0.50\u4ee5\u53ca52%\u7684mAP@0.50:0.95\u3002\u8be5\u6570\u636e\u96c6\u662f\u5c3c\u65e5\u5229\u4e9a\u9996\u4e2a\u4e13\u6ce8\u4e8e\u829d\u9ebb\u7684\u519c\u4e1a\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u4e3a\u690d\u7269\u76d1\u6d4b\u3001\u4ea7\u91cf\u4f30\u7b97\u548c\u519c\u4e1a\u7814\u7a76\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u652f\u6301\u3002"}}
{"id": "2601.07975", "pdf": "https://arxiv.org/pdf/2601.07975", "abs": "https://arxiv.org/abs/2601.07975", "authors": ["Fei Li", "Lang Qiao", "Jiahao Fan", "Yijia Xu", "Shawn M. Kaeppler", "Zhou Zhang"], "title": "An Efficient Additive Kolmogorov-Arnold Transformer for Point-Level Maize Localization in Unmanned Aerial Vehicle Imagery", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution UAV photogrammetry has become a key technology for precision agriculture, enabling centimeter-level crop monitoring and point-level plant localization. However, point-level maize localization in UAV imagery remains challenging due to (1) extremely small object-to-pixel ratios, typically less than 0.1%, (2) prohibitive computational costs of quadratic attention on ultra-high-resolution images larger than 3000 x 4000 pixels, and (3) agricultural scene-specific complexities such as sparse object distribution and environmental variability that are poorly handled by general-purpose vision models.\n  To address these challenges, we propose the Additive Kolmogorov-Arnold Transformer (AKT), which replaces conventional multilayer perceptrons with Pade Kolmogorov-Arnold Network (PKAN) modules to enhance functional expressivity for small-object feature extraction, and introduces PKAN Additive Attention (PAA) to model multiscale spatial dependencies with reduced computational complexity. In addition, we present the Point-based Maize Localization (PML) dataset, consisting of 1,928 high-resolution UAV images with approximately 501,000 point annotations collected under real field conditions.\n  Extensive experiments show that AKT achieves an average F1-score of 62.8%, outperforming state-of-the-art methods by 4.2%, while reducing FLOPs by 12.6% and improving inference throughput by 20.7%. For downstream tasks, AKT attains a mean absolute error of 7.1 in stand counting and a root mean square error of 1.95-1.97 cm in interplant spacing estimation. These results demonstrate that integrating Kolmogorov-Arnold representation theory with efficient attention mechanisms offers an effective framework for high-resolution agricultural remote sensing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdditive Kolmogorov-Arnold Transformer\uff08AKT\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u5f71\u50cf\u4e2d\u7389\u7c73\u690d\u682a\u70b9\u7ea7\u5b9a\u4f4d\u7684\u96be\u9898\u3002\u901a\u8fc7\u5f15\u5165\u57fa\u4e8ePade Kolmogorov-Arnold\u7f51\u7edc\uff08PKAN\uff09\u7684\u6a21\u5757\u548cPKAN Additive Attention\u673a\u5236\uff0c\u5728\u63d0\u5347\u5c0f\u76ee\u6807\u7279\u5f81\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cAKT\u5728F1-score\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd54.2%\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\u3001\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u6444\u5f71\u6d4b\u91cf\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u70b9\u7ea7\u7389\u7c73\u5b9a\u4f4d\u4ecd\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\uff081\uff09\u76ee\u6807\u50cf\u7d20\u5360\u6bd4\u6781\u5c0f\uff08\u901a\u5e38\u4f4e\u4e8e0.1%\uff09\uff1b\uff082\uff09\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff08>3000\u00d74000\u50cf\u7d20\uff09\u4e0a\u4f7f\u7528\u4f20\u7edf\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff1b\uff083\uff09\u519c\u4e1a\u573a\u666f\u7279\u6709\u7684\u7a00\u758f\u5206\u5e03\u4e0e\u73af\u5883\u53d8\u5316\u96be\u4ee5\u88ab\u901a\u7528\u89c6\u89c9\u6a21\u578b\u6709\u6548\u5904\u7406\u3002", "method": "\u4f5c\u8005\u63d0\u51faAdditive Kolmogorov-Arnold Transformer\uff08AKT\uff09\uff1a\u7528Pade Kolmogorov-Arnold Network\uff08PKAN\uff09\u6a21\u5757\u66ff\u4ee3\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\u4ee5\u589e\u5f3a\u5c0f\u76ee\u6807\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1PKAN Additive Attention\uff08PAA\uff09\u673a\u5236\u4ee5\u4f4e\u8ba1\u7b97\u4ee3\u4ef7\u5efa\u6a21\u591a\u5c3a\u5ea6\u7a7a\u95f4\u4f9d\u8d56\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b1,928\u5f20\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\u548c\u7ea650.1\u4e07\u70b9\u6807\u6ce8\u7684Point-based Maize Localization\uff08PML\uff09\u6570\u636e\u96c6\u3002", "result": "AKT\u5728PML\u6570\u636e\u96c6\u4e0a\u8fbe\u523062.8%\u7684\u5e73\u5747F1-score\uff0c\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u9ad84.2%\uff1bFLOPs\u964d\u4f4e12.6%\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u534720.7%\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u682a\u6570\u8ba1\u6570\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a7.1\uff0c\u682a\u95f4\u8ddd\u4f30\u8ba1\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e3a1.95\u20131.97 cm\u3002", "conclusion": "\u5c06Kolmogorov-Arnold\u8868\u793a\u7406\u8bba\u4e0e\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u519c\u4e1a\u9065\u611f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u6846\u67b6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5c0f\u76ee\u6807\u5bc6\u96c6\u5b9a\u4f4d\u4efb\u52a1\u3002", "summary_cn": "\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u6444\u5f71\u6d4b\u91cf\u5df2\u6210\u4e3a\u7cbe\u51c6\u519c\u4e1a\u7684\u5173\u952e\u6280\u672f\uff0c\u53ef\u5b9e\u73b0\u5398\u7c73\u7ea7\u4f5c\u7269\u76d1\u6d4b\u548c\u70b9\u7ea7\u690d\u682a\u5b9a\u4f4d\u3002\u7136\u800c\uff0c\u7531\u4e8e\uff081\uff09\u76ee\u6807\u50cf\u7d20\u5360\u6bd4\u6781\u5c0f\uff08\u901a\u5e38\u4f4e\u4e8e0.1%\uff09\uff0c\uff082\uff09\u5728\u8d85\u8fc73000\u00d74000\u50cf\u7d20\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u4f7f\u7528\u4f20\u7edf\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\u5e26\u6765\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4ee5\u53ca\uff083\uff09\u519c\u4e1a\u573a\u666f\u4e2d\u7a00\u758f\u76ee\u6807\u5206\u5e03\u548c\u73af\u5883\u53d8\u5316\u7b49\u590d\u6742\u56e0\u7d20\u96be\u4ee5\u88ab\u901a\u7528\u89c6\u89c9\u6a21\u578b\u6709\u6548\u5904\u7406\uff0c\u70b9\u7ea7\u7389\u7c73\u5b9a\u4f4d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faAdditive Kolmogorov-Arnold Transformer\uff08AKT\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528Pade Kolmogorov-Arnold\u7f51\u7edc\uff08PKAN\uff09\u6a21\u5757\u66ff\u4ee3\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\uff0c\u4ee5\u589e\u5f3a\u5bf9\u5c0f\u76ee\u6807\u7684\u7279\u5f81\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5f15\u5165PKAN Additive Attention\uff08PAA\uff09\u673a\u5236\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5efa\u6a21\u591a\u5c3a\u5ea6\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u6784\u5efa\u4e86Point-based Maize Localization\uff08PML\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b1,928\u5f20\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\u53ca\u7ea650.1\u4e07\u4e2a\u5728\u771f\u5b9e\u7530\u95f4\u6761\u4ef6\u4e0b\u91c7\u96c6\u7684\u70b9\u6807\u6ce8\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAKT\u6a21\u578b\u5e73\u5747F1-score\u8fbe\u523062.8%\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9ad8\u51fa4.2%\uff0c\u540c\u65f6FLOPs\u964d\u4f4e12.6%\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u534720.7%\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cAKT\u5728\u682a\u6570\u8ba1\u6570\u4e0a\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a7.1\uff0c\u5728\u682a\u95f4\u8ddd\u4f30\u8ba1\u4e0a\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e3a1.95\u20131.97\u5398\u7c73\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5c06Kolmogorov-Arnold\u8868\u793a\u7406\u8bba\u4e0e\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u519c\u4e1a\u9065\u611f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.07982", "pdf": "https://arxiv.org/pdf/2601.07982", "abs": "https://arxiv.org/abs/2601.07982", "authors": ["Howard C. Gifford"], "title": "Likelihood ratio for a binary Bayesian classifier under a noise-exclusion model", "categories": ["cs.CV", "math.ST", "stat.CO"], "comment": "18 pages, 4 figures", "summary": "We develop a new statistical ideal observer model that performs holistic visual search (or gist) processing in part by placing thresholds on minimum extractable image features. In this model, the ideal observer reduces the number of free parameters thereby shrinking down the system. The applications of this novel framework is in medical image perception (for optimizing imaging systems and algorithms), computer vision, benchmarking performance and enabling feature selection/evaluations. Other applications are in target detection and recognition in defense/security as well as evaluating sensors and detectors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u8ba1\u7406\u60f3\u89c2\u5bdf\u8005\u6a21\u578b\uff0c\u901a\u8fc7\u8bbe\u7f6e\u6700\u5c0f\u53ef\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\u7684\u9608\u503c\u6765\u8fdb\u884c\u6574\u4f53\u89c6\u89c9\u641c\u7d22\uff08\u6216\u6982\u89c8\uff09\u5904\u7406\uff0c\u4ece\u800c\u51cf\u5c11\u81ea\u7531\u53c2\u6570\u6570\u91cf\u5e76\u7b80\u5316\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u641c\u7d22\u6a21\u578b\u901a\u5e38\u53c2\u6570\u7e41\u591a\u3001\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5728\u533b\u5b66\u6210\u50cf\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u5b89\u9632\u7b49\u9886\u57df\u9ad8\u6548\u5e94\u7528\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u6d01\u3001\u66f4\u5177\u5b9e\u7528\u6027\u7684\u7406\u60f3\u89c2\u5bdf\u8005\u6a21\u578b\u3002", "method": "\u6784\u5efa\u4e00\u79cd\u65b0\u578b\u7edf\u8ba1\u7406\u60f3\u89c2\u5bdf\u8005\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5bf9\u6700\u5c0f\u53ef\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\u8bbe\u5b9a\u9608\u503c\uff0c\u5b9e\u73b0\u6574\u4f53\u89c6\u89c9\uff08gist\uff09\u5904\u7406\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u6a21\u578b\u81ea\u7531\u53c2\u6570\u6570\u91cf\u3002", "result": "\u8be5\u6a21\u578b\u6210\u529f\u7b80\u5316\u4e86\u7cfb\u7edf\u7ed3\u6784\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u4e2a\u9886\u57df\uff0c\u5305\u62ec\u533b\u5b66\u56fe\u50cf\u611f\u77e5\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3001\u7279\u5f81\u9009\u62e9\u4e0e\u8bc4\u4f30\uff0c\u4ee5\u53ca\u56fd\u9632/\u5b89\u5168\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u8bc6\u522b\u548c\u4f20\u611f\u5668\u8bc4\u4f30\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7406\u60f3\u89c2\u5bdf\u8005\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u6210\u50cf\u7cfb\u7edf\u3001\u7b97\u6cd5\u8bc4\u4f30\u53ca\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u7279\u5f81\u5206\u6790\u3002", "summary_cn": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u8ba1\u7406\u60f3\u89c2\u5bdf\u8005\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u8bbe\u5b9a\u6700\u5c0f\u53ef\u63d0\u53d6\u56fe\u50cf\u7279\u5f81\u7684\u9608\u503c\uff0c\u90e8\u5206\u5730\u5b9e\u73b0\u6574\u4f53\u89c6\u89c9\u641c\u7d22\uff08\u6216\u79f0\u201c\u6982\u89c8\u201d\uff09\u5904\u7406\u3002\u5728\u6b64\u6a21\u578b\u4e2d\uff0c\u7406\u60f3\u89c2\u5bdf\u8005\u51cf\u5c11\u4e86\u81ea\u7531\u53c2\u6570\u7684\u6570\u91cf\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u6574\u4e2a\u7cfb\u7edf\u3002\u8be5\u65b0\u6846\u67b6\u7684\u5e94\u7528\u5305\u62ec\u533b\u5b66\u56fe\u50cf\u611f\u77e5\uff08\u7528\u4e8e\u4f18\u5316\u6210\u50cf\u7cfb\u7edf\u548c\u7b97\u6cd5\uff09\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u7279\u5f81\u9009\u62e9\u4e0e\u8bc4\u4f30\u3002\u5176\u4ed6\u5e94\u7528\u8fd8\u5305\u62ec\u56fd\u9632\u4e0e\u5b89\u5168\u9886\u57df\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b\uff0c\u4ee5\u53ca\u4f20\u611f\u5668\u548c\u63a2\u6d4b\u5668\u7684\u8bc4\u4f30\u3002"}}
{"id": "2601.07998", "pdf": "https://arxiv.org/pdf/2601.07998", "abs": "https://arxiv.org/abs/2601.07998", "authors": ["Hongwei Lin", "Diego Andrade", "Mini Das", "Howard C. Gifford"], "title": "Predicting Region of Interest in Human Visual Search Based on Statistical Texture and Gabor Features", "categories": ["cs.CV", "eess.IV", "eess.SP", "physics.med-ph"], "comment": "10 pages, 6 fgures", "summary": "Understanding human visual search behavior is a fundamental problem in vision science and computer vision, with direct implications for modeling how observers allocate attention in location-unknown search tasks. In this study, we investigate the relationship between Gabor-based features and gray-level co-occurrence matrix (GLCM) based texture features in modeling early-stage visual search behavior. Two feature-combination pipelines are proposed to integrate Gabor and GLCM features for narrowing the region of possible human fixations. The pipelines are evaluated using simulated digital breast tomosynthesis images. Results show qualitative agreement among fixation candidates predicted by the proposed pipelines and a threshold-based model observer. A strong correlation is observed between GLCM mean and Gabor feature responses, indicating that these features encode related image information despite their different formulations. Eye-tracking data from human observers further suggest consistency between predicted fixation regions and early-stage gaze behavior. These findings highlight the value of combining structural and texture-based features for modeling visual search and support the development of perceptually informed observer models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u878d\u5408Gabor\u4e0eGLCM\u7279\u5f81\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u9884\u6d4b\u4eba\u7c7b\u5728\u672a\u77e5\u76ee\u6807\u4f4d\u7f6e\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u65e9\u671f\u6ce8\u89c6\u533a\u57df\uff0c\u5e76\u5728\u6570\u5b57\u4e73\u817a\u65ad\u5c42\u5408\u6210\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u89c6\u89c9\u641c\u7d22\u884c\u4e3a\u5bf9\u5efa\u6a21\u6ce8\u610f\u529b\u5206\u914d\u673a\u5236\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u76ee\u6807\u4f4d\u7f6e\u672a\u77e5\u7684\u641c\u7d22\u4efb\u52a1\u4e2d\uff1b\u73b0\u6709\u65b9\u6cd5\u9700\u66f4\u597d\u5730\u6574\u5408\u7ed3\u6784\u4e0e\u7eb9\u7406\u4fe1\u606f\u4ee5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7279\u5f81\u878d\u5408\u7ba1\u9053\uff0c\u7ed3\u5408Gabor\u7279\u5f81\u4e0e\u7070\u5ea6\u5171\u751f\u77e9\u9635\uff08GLCM\uff09\u7eb9\u7406\u7279\u5f81\uff0c\u7528\u4e8e\u7f29\u5c0f\u4eba\u7c7b\u53ef\u80fd\u6ce8\u89c6\u533a\u57df\uff0c\u5e76\u5728\u6a21\u62df\u6570\u5b57\u4e73\u817a\u65ad\u5c42\u5408\u6210\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u9884\u6d4b\u7684\u6ce8\u89c6\u5019\u9009\u533a\u57df\u4e0e\u57fa\u4e8e\u9608\u503c\u7684\u6a21\u578b\u89c2\u5bdf\u8005\u5b9a\u6027\u4e00\u81f4\uff1bGLCM\u5747\u503c\u4e0eGabor\u7279\u5f81\u54cd\u5e94\u9ad8\u5ea6\u76f8\u5173\uff1b\u773c\u52a8\u6570\u636e\u8868\u660e\u9884\u6d4b\u533a\u57df\u4e0e\u4eba\u7c7b\u65e9\u671f\u6ce8\u89c6\u884c\u4e3a\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u5408\u7ed3\u6784\u4e0e\u7eb9\u7406\u7279\u5f81\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u4eba\u7c7b\u89c6\u89c9\u641c\u7d22\u884c\u4e3a\uff0c\u4e3a\u6784\u5efa\u611f\u77e5\u9a71\u52a8\u7684\u89c2\u5bdf\u8005\u6a21\u578b\u63d0\u4f9b\u652f\u6301\u3002", "summary_cn": "\u7406\u89e3\u4eba\u7c7b\u89c6\u89c9\u641c\u7d22\u884c\u4e3a\u662f\u89c6\u89c9\u79d1\u5b66\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u5bf9\u4e8e\u5efa\u6a21\u89c2\u5bdf\u8005\u5728\u76ee\u6807\u4f4d\u7f6e\u672a\u77e5\u7684\u641c\u7d22\u4efb\u52a1\u4e2d\u5982\u4f55\u5206\u914d\u6ce8\u610f\u529b\u5177\u6709\u76f4\u63a5\u610f\u4e49\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8eGabor\u7684\u7279\u5f81\u4e0e\u57fa\u4e8e\u7070\u5ea6\u5171\u751f\u77e9\u9635\uff08GLCM\uff09\u7684\u7eb9\u7406\u7279\u5f81\u5728\u5efa\u6a21\u65e9\u671f\u89c6\u89c9\u641c\u7d22\u884c\u4e3a\u4e2d\u7684\u5173\u7cfb\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u7279\u5f81\u878d\u5408\u7ba1\u9053\uff0c\u5c06Gabor\u7279\u5f81\u4e0eGLCM\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u4ee5\u7f29\u5c0f\u4eba\u7c7b\u53ef\u80fd\u6ce8\u89c6\u7684\u533a\u57df\u3002\u8fd9\u4e9b\u7ba1\u9053\u5728\u6a21\u62df\u7684\u6570\u5b57\u4e73\u817a\u65ad\u5c42\u5408\u6210\u56fe\u50cf\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7ba1\u9053\u9884\u6d4b\u7684\u6ce8\u89c6\u5019\u9009\u533a\u57df\u4e0e\u57fa\u4e8e\u9608\u503c\u7684\u6a21\u578b\u89c2\u5bdf\u8005\u4e4b\u95f4\u5b58\u5728\u5b9a\u6027\u4e00\u81f4\u6027\u3002\u540c\u65f6\u89c2\u5bdf\u5230GLCM\u5747\u503c\u4e0eGabor\u7279\u5f81\u54cd\u5e94\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u8868\u660e\u5c3d\u7ba1\u4e8c\u8005\u5f62\u5f0f\u4e0d\u540c\uff0c\u4f46\u7f16\u7801\u4e86\u76f8\u5173\u7684\u56fe\u50cf\u4fe1\u606f\u3002\u6765\u81ea\u4eba\u7c7b\u89c2\u5bdf\u8005\u7684\u773c\u52a8\u6570\u636e\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u9884\u6d4b\u7684\u6ce8\u89c6\u533a\u57df\u4e0e\u65e9\u671f\u6ce8\u89c6\u884c\u4e3a\u5177\u6709\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u7ed3\u5408\u7ed3\u6784\u4e0e\u7eb9\u7406\u7279\u5f81\u5728\u5efa\u6a21\u89c6\u89c9\u641c\u7d22\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u652f\u6301\u5f00\u53d1\u611f\u77e5\u9a71\u52a8\u7684\u89c2\u5bdf\u8005\u6a21\u578b\u3002"}}
{"id": "2601.08010", "pdf": "https://arxiv.org/pdf/2601.08010", "abs": "https://arxiv.org/abs/2601.08010", "authors": ["Chaoyu Li", "Deeparghya Dutta Barua", "Fei Tao", "Pooyan Fazli"], "title": "CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08CASHEW \u548c CASHEW-RL\uff09\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6b65\u63a8\u7406\u7684\u7a33\u5b9a\u6027\uff1a\u524d\u8005\u901a\u8fc7\u63a8\u7406\u65f6\u805a\u5408\u591a\u4e2a\u8f68\u8ff9\u5e76\u7528\u89c6\u89c9\u9a8c\u8bc1\u8fc7\u6ee4\u5e7b\u89c9\uff0c\u540e\u8005\u901a\u8fc7\u65b0\u63d0\u51fa\u7684GSPO\u7b97\u6cd5\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u5185\u90e8\u81ea\u805a\u5408\uff0c\u572813\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u5b58\u5728\u4e0d\u7a33\u5b9a\u6027\uff0c\u5bf9\u540c\u4e00\u8f93\u5165\u591a\u6b21\u91c7\u6837\u5e38\u4ea7\u751f\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u548c\u4e0d\u4e00\u81f4\u9884\u6d4b\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faCASHEW\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u8fed\u4ee3\u805a\u5408\u591a\u4e2a\u5019\u9009\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u663e\u5f0f\u89c6\u89c9\u9a8c\u8bc1\u5254\u9664\u5e7b\u89c9\u6b65\u9aa4\uff1b\u540c\u65f6\u63d0\u51faCASHEW-RL\uff0c\u5229\u7528Group Sequence Policy Optimization (GSPO)\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u5355\u6b21\u524d\u5411\u4e2d\u5185\u5316\u805a\u5408\u884c\u4e3a\uff0c\u5956\u52b1\u673a\u5236\u9f13\u52b1\u57fa\u4e8e\u6700\u5c11\u4f46\u5145\u5206\u89c6\u89c9\u8bc1\u636e\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u81ea\u9002\u5e94\u5206\u914d\u63a8\u7406\u8d44\u6e90\u3002", "result": "\u572813\u4e2a\u56fe\u50cf\u7406\u89e3\u3001\u89c6\u9891\u7406\u89e3\u548c\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u4f8b\u5982ScienceQA\u63d0\u5347+23.6\u4e2a\u767e\u5206\u70b9\uff0cEgoSchema\u63d0\u5347+8.1\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6b65\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u9a8c\u8bc1\u4e86\u6d4b\u8bd5\u65f6\u7f29\u653e\u601d\u60f3\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "summary_cn": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\u4ecd\u4e0d\u7a33\u5b9a\u3002\u5bf9\u540c\u4e00\u8f93\u5165\u8fdb\u884c\u91cd\u590d\u91c7\u6837\u5e38\u5e38\u4ea7\u751f\u4e0d\u540c\u7684\u63a8\u7406\u8def\u5f84\u548c\u4e0d\u4e00\u81f4\u7684\u6700\u7ec8\u9884\u6d4b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u53d7\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08test-time scaling\uff09\u542f\u53d1\u7684\u4e92\u8865\u65b9\u6cd5\uff1a\uff081\uff09CASHEW\uff0c\u4e00\u79cd\u63a8\u7406\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u805a\u5408\u591a\u4e2a\u5019\u9009\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u5f15\u5165\u663e\u5f0f\u89c6\u89c9\u9a8c\u8bc1\u673a\u5236\u6765\u8fc7\u6ee4\u5e7b\u89c9\u6b65\u9aa4\uff0c\u4f7f\u63a8\u7406\u624e\u6839\u4e8e\u89c6\u89c9\u8bc1\u636e\uff1b\uff082\uff09CASHEW-RL\uff0c\u4e00\u79cd\u5b66\u4e60\u578b\u53d8\u4f53\uff0c\u5c06\u4e0a\u8ff0\u805a\u5408\u884c\u4e3a\u5185\u5316\u5230\u5355\u4e00\u6a21\u578b\u4e2d\u3002CASHEW-RL\u91c7\u7528\u7fa4\u4f53\u5e8f\u5217\u7b56\u7565\u4f18\u5316\uff08Group Sequence Policy Optimization, GSPO\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5176\u590d\u5408\u5956\u52b1\u673a\u5236\u9f13\u52b1\u6a21\u578b\u57fa\u4e8e\u6700\u5c11\u4f46\u5145\u5206\u7684\u89c6\u89c9\u8bc1\u636e\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u5e76\u80fd\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u81ea\u9002\u5e94\u5730\u5206\u914d\u63a8\u7406\u8d44\u6e90\u3002\u8be5\u8bad\u7ec3\u76ee\u6807\u4f7f\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u5177\u5907\u5f3a\u5927\u7684\u81ea\u805a\u5408\u80fd\u529b\u3002\u572813\u4e2a\u56fe\u50cf\u7406\u89e3\u3001\u89c6\u9891\u7406\u89e3\u548c\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f8b\u5982\u5728ScienceQA\u4e0a\u63d0\u5347\u9ad8\u8fbe23.6\u4e2a\u767e\u5206\u70b9\uff0c\u5728EgoSchema\u4e0a\u63d0\u53478.1\u4e2a\u767e\u5206\u70b9\u3002"}}
