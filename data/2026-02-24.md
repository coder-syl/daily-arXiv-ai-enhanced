<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Replication Study: Federated Text-Driven Prompt Generation for Vision-Language Models](https://arxiv.org/abs/2602.18439)
*Suraj Prasad,Anubha Pant*

Main category: cs.CV

TL;DR: 本文对FedTPG方法进行了忠实复现，在六个视觉数据集上验证了其在联邦学习中对未见类别的良好泛化能力，结果与原论文高度一致，证实了文本驱动提示生成在跨类别泛化上的优越性。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉-语言模型虽具备强大的零样本能力，但在联邦学习场景下对未见类别的泛化仍面临挑战。FedTPG通过引入基于类别名称动态生成提示的机制来解决这一问题，本文旨在对该方法进行复现和验证。

Method: 对FedTPG方法进行忠实复现，使用其预训练模型在Caltech101、Oxford Flowers等六个不同的视觉数据集上进行评估，比较其在已见（基类）和未见（新类）类别上的准确率。

Result: 复现结果与原论文报告的准确率相差在0.2%以内，已见类平均准确率为74.58%，未见类为76.00%，泛化能力提升了+1.43个百分点。

Conclusion: 成功复现验证了FedTPG的核心主张：文本驱动的提示生成相比静态提示学习能更好地泛化到未见类别，且在不共享私有数据的情况下，联邦训练的提示生成器能在多样化的视觉领域保持高性能。该方法具有鲁棒性和可复现性。

Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot capabilities, yet their adaptation to federated learning scenarios presents significant challenges, particularly regarding generalization to unseen classes. The original FedTPG paper \cite{Qiu2024} addresses this limitation by introducing a text driven prompt generation network that dynamically creates prompts conditioned on class names, enabling better cross-class generalization in federated settings. In this work, we present a faithful replication study of FedTPG, evaluating the pre-trained model on six diverse vision datasets: Caltech101, Oxford Flowers, FGVC Aircraft, Oxford Pets, Food-101, and DTD. Our evaluation achieves results within 0.2\% of the original paper's reported accuracies, with an average accuracy of 74.58\% on seen (base) classes and 76.00\% on unseen (new) classes, demonstrating a +1.43 percentage point improvement in generalization. These results validate the original paper's core claims: (1) text-driven prompt generation enables superior generalization to unseen classes compared to static prompt learning methods, and (2) federated training of prompt generators maintains high performance across diverse visual domains without sharing private data. Our successful replication confirms the robustness and reproducibility of the FedTPG approach.

Abstract (中文翻译): 像CLIP这样的视觉-语言模型展现了卓越的零样本能力，但其在联邦学习场景中的应用，尤其是在对未见类别的泛化方面，仍面临重大挑战。原始的FedTPG论文\cite{Qiu2024}通过引入一个文本驱动的提示生成网络来解决这一局限性，该网络能够根据类别名称动态创建提示，从而在联邦环境中实现更好的跨类别泛化。在本研究中，我们对FedTPG进行了忠实的复现，并在其预训练模型上使用六个多样化的视觉数据集（Caltech101、Oxford Flowers、FGVC Aircraft、Oxford Pets、Food-101和DTD）进行了评估。我们的评估结果与原论文报告的准确率相差在0.2%以内，其中在已见（基类）类别上的平均准确率为74.58%，在未见（新类）类别上为76.00%，展示了+1.43个百分点的泛化性能提升。这些结果验证了原论文的核心主张：（1）与静态提示学习方法相比，文本驱动的提示生成能够实现对未见类别的更优泛化；（2）提示生成器的联邦训练能够在不共享私有数据的情况下，在多样化的视觉领域保持高性能。我们成功的复现确认了FedTPG方法的鲁棒性和可复现性。

</details>


### [2] [A Patient-Specific Digital Twin for Adaptive Radiotherapy of Non-Small Cell Lung Cancer](https://arxiv.org/abs/2602.18496)
*Anvi Sud,Jialu Huang,Gregory R. Hart,Keshav Saxena,John Kim,Lauren Tressel,Jun Deng*

Main category: cs.CV

TL;DR: 本文提出COMPASS系统，一种基于AI的时序数字孪生架构，利用每分次放疗中的多模态数据动态建模正常组织生物学变化，实现对非小细胞肺癌患者放疗毒性的早期预警。


<details>
  <summary>Details</summary>
Motivation: 当前放疗决策依赖静态的群体NTCP模型，忽视了个体在连续治疗中独特的动态生物学轨迹；而现代精准放疗产生高频率成像与剂量数据，适合用于构建AI驱动的时序模型以捕捉正常组织随时间演变的特征。

Method: 开发COMPASS系统，整合每次放疗分次的PET、CT、剂量组学、影像组学及累积生物等效剂量（BED）动力学数据，采用GRU自编码器学习器官特异性潜在轨迹，并通过逻辑回归分类预测CTCAE 1级及以上毒性。

Result: 在8名NSCLC患者共99个器官分次观测（涵盖24条器官轨迹）中，尽管样本量小，但密集的时序表型分析揭示了毒性发生前数次分次即可出现风险升高的早期预警窗口，并发现传统基于体积的剂量学所忽略的、具有生物学意义的空间剂量纹理特征。

Conclusion: COMPASS验证了AI赋能的自适应放疗可行性，即通过持续更新的数字孪生追踪每位患者的动态生物学响应，为个体化安全放疗提供新范式。

Abstract: Radiotherapy continues to become more precise and data dense, with current treatment regimens generating high frequency imaging and dosimetry streams ideally suited for AI driven temporal modeling to characterize how normal tissues evolve with time. Each fraction in biologically guided radiotherapy(BGRT) treated non small cell lung cancer (NSCLC) patients records new metabolic, anatomical, and dose information. However, clinical decision making is largely informed by static, population based NTCP models which overlook the dynamic, unique biological trajectories encoded in sequential data. We developed COMPASS (Comprehensive Personalized Assessment System) for safe radiotherapy, functioning as a temporal digital twin architecture utilizing per fraction PET, CT, dosiomics, radiomics, and cumulative biologically equivalent dose (BED) kinetics to model normal tissue biology as a dynamic time series process. A GRU autoencoder was employed to learn organ specific latent trajectories, which were classified via logistic regression to predict eventual CTCAE grade 1 or higher toxicity. Eight NSCLC patients undergoing BGRT contributed to the 99 organ fraction observations covering 24 organ trajectories (spinal cord, heart, and esophagus). Despite the small cohort, intensive temporal phenotyping allowed for comprehensive analysis of individual dose response dynamics. Our findings revealed a viable AI driven early warning window, as increasing risk ratings occurred from several fractions before clinical toxicity. The dense BED driven representation revealed biologically relevant spatial dose texture characteristics that occur before toxicity and are averaged out with traditional volume based dosimetry. COMPASS establishes a proof of concept for AI enabled adaptive radiotherapy, where treatment is guided by a continually updated digital twin that tracks each patients evolving biological response.

Abstract (中文翻译): 放射治疗正变得越来越精准且数据密集，当前的治疗方案可生成高频影像和剂量数据流，非常适合用于人工智能驱动的时序建模，以刻画正常组织随时间演变的过程。在接受生物引导放疗（BGRT）的非小细胞肺癌（NSCLC）患者中，每次治疗分次都会记录新的代谢、解剖和剂量信息。然而，临床决策主要依赖静态的、基于人群的NTCP模型，忽略了序列数据中蕴含的动态且独特的生物学轨迹。为此，我们开发了COMPASS（Comprehensive Personalized Assessment System，综合个性化评估系统），用于安全放疗。该系统作为一种时序数字孪生架构，利用每次分次的PET、CT、剂量组学、影像组学以及累积生物等效剂量（BED）动力学，将正常组织生物学建模为一个动态时间序列过程。研究采用GRU自编码器学习器官特异性的潜在轨迹，并通过逻辑回归对其进行分类，以预测最终是否发生CTCAE 1级或以上的毒性。共有8名接受BGRT的NSCLC患者参与研究，提供了99个器官分次观测数据，涵盖24条器官轨迹（脊髓、心脏和食管）。尽管队列规模较小，但密集的时序表型分析使得能够全面解析个体剂量-反应动力学。研究结果揭示了一个可行的AI驱动早期预警窗口：在临床毒性出现前若干次分次，风险评分已开始上升。此外，基于密集BED的表征揭示了在毒性发生前出现的、具有生物学意义的空间剂量纹理特征，而这些特征在传统的基于体积的剂量学中被平均化而丢失。COMPASS为AI赋能的自适应放疗提供了概念验证，即通过持续更新的数字孪生体追踪每位患者不断演变的生物学反应，从而指导治疗。

</details>


### [3] [Scaling Ultrasound Volumetric Reconstruction via Mobile Augmented Reality](https://arxiv.org/abs/2602.18500)
*Kian Wei Ng,Yujia Gao,Deborah Khoo,Ying Zhen Tan,Chengzheng Mao,Haojie Cheng,Andrew Makmur,Kee Yuan Ngiam,Serene Goh,Eng Tat Khoo*

Main category: cs.CV

TL;DR: MARVUS is a cost-effective, portable augmented reality system that improves the accuracy and reproducibility of lesion volume estimation using conventional 2D ultrasound, outperforming standard methods in both accuracy and inter-user consistency.


<details>
  <summary>Details</summary>
Motivation: Current 2D ultrasound suffers from high variability in volume estimation, while existing 3D ultrasound solutions are costly and less portable. There is a need for an accessible, accurate, and reproducible volumetric assessment tool compatible with standard ultrasound systems.

Method: The authors propose MARVUS, a mobile augmented reality system that works with conventional ultrasound machines. It leverages a foundation model to support cross-specialty generalization and uses AR visualization to guide clinicians during volumetric measurements.

Result: In a user study with experienced clinicians on breast phantoms, MARVUS significantly improved volume estimation accuracy (mean difference: 0.469 cm³) and reduced inter-user variability (mean difference: 0.417 cm³). AR visualization also enhanced both objective performance and subjective usability.

Conclusion: MARVUS offers a scalable, cost-effective, and resource-efficient solution to enhance cancer screening, diagnosis, and treatment planning by enabling accurate and reproducible volumetric ultrasound assessments using standard equipment.

Abstract: Accurate volumetric characterization of lesions is essential for oncologic diagnosis, risk stratification, and treatment planning. While imaging modalities such as Computed Tomography provide high-quality 3D data, 2D ultrasound (2D-US) remains the preferred first-line modality for breast and thyroid imaging due to cost, portability, and safety factors. However, volume estimates derived from 2D-US suffer from high inter-user variability even among experienced clinicians. Existing 3D ultrasound (3D-US) solutions use specialized probes or external tracking hardware, but such configurations increase costs and diminish portability, constraining widespread clinical use. To address these limitations, we present Mobile Augmented Reality Volumetric Ultrasound (MARVUS), a resource-efficient system designed to increase accessibility to accurate and reproducible volumetric assessment. MARVUS is interoperable with conventional ultrasound (US) systems, using a foundation model to enhance cross-specialty generalization while minimizing hardware requirements relative to current 3D-US solutions. In a user study involving experienced clinicians performing measurements on breast phantoms, MARVUS yielded a substantial improvement in volume estimation accuracy (mean difference: 0.469 cm3) with reduced inter-user variability (mean difference: 0.417 cm3). Additionally, we prove that augmented reality (AR) visualizations enhance objective performance metrics and clinician-reported usability. Collectively, our findings suggests that MARVUS can enhance US-based cancer screening, diagnostic workflows, and treatment planning in a scalable, cost-conscious, and resource-efficient manner. Usage video demonstration available (https://youtu.be/m4llYcZpqmM).

Abstract (中文翻译): 对病灶进行精确的体积表征对于肿瘤学诊断、风险分层和治疗规划至关重要。尽管计算机断层扫描（CT）等成像方式可提供高质量的三维数据，但由于成本、便携性和安全性等因素，二维超声（2D-US）仍是乳腺和甲状腺成像的首选一线手段。然而，基于2D-US的体积估算即使在经验丰富的临床医生之间也存在显著的用户间差异。现有的三维超声（3D-US）解决方案依赖专用探头或外部追踪硬件，但这类配置增加了成本并降低了便携性，限制了其在临床中的广泛应用。为解决上述问题，本文提出“移动增强现实体积超声”（MARVUS）——一种资源高效、旨在提升精确且可重复体积评估可及性的系统。MARVUS可与常规超声设备互操作，利用基础模型增强跨专科泛化能力，同时相较于现有3D-US方案显著降低硬件需求。在一项由经验丰富的临床医生对乳腺仿体进行测量的用户研究中，MARVUS显著提高了体积估算的准确性（平均差值：0.469 cm³），并减少了用户间变异性（平均差值：0.417 cm³）。此外，研究证明增强现实（AR）可视化不仅提升了客观性能指标，也改善了临床医生主观报告的可用性。综上所述，MARVUS能够以可扩展、注重成本且资源高效的方式，增强基于超声的癌症筛查、诊断流程和治疗规划。演示视频见：https://youtu.be/m4llYcZpqmM。

</details>


### [4] [Mitigating Shortcut Learning via Feature Disentanglement in Medical Imaging: A Benchmark Study](https://arxiv.org/abs/2602.18502)
*Sarah Müller,Philipp Berens*

Main category: cs.CV

TL;DR: 本文系统评估了医学图像中用于缓解捷径学习的特征解耦方法，发现结合数据重平衡与模型解耦策略能更有效提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像中常依赖捷径学习（如虚假相关或混杂因素），影响其在不同机构、人群和采集条件下的泛化能力，亟需有效缓解策略。

Method: 评估了基于对抗学习和基于依赖最小化的潜在空间分割等特征解耦方法，在含自然与合成混杂因子的人工及医学数据集上进行实验，分析分类性能、解耦质量、鲁棒性及计算效率。

Result: 解耦方法在强虚假相关训练条件下提升了分类性能；潜在空间分析揭示了分类指标未反映的表征差异；模型对捷径的依赖程度与训练数据中混杂程度相关；结合数据重平衡与模型解耦的方法效果最佳。

Conclusion: 单纯依赖分类指标不足以评估解耦效果，需结合潜在空间分析；最佳策略是将数据层面的重平衡与模型层面的解耦相结合，以实现更稳健的捷径缓解。

Abstract: Although deep learning models in medical imaging often achieve excellent classification performance, they can rely on shortcut learning, exploiting spurious correlations or confounding factors that are not causally related to the target task. This poses risks in clinical settings, where models must generalize across institutions, populations, and acquisition conditions. Feature disentanglement is a promising approach to mitigate shortcut learning by separating task-relevant information from confounder-related features in latent representations. In this study, we systematically evaluated feature disentanglement methods for mitigating shortcuts in medical imaging, including adversarial learning and latent space splitting based on dependence minimization. We assessed classification performance and disentanglement quality using latent space analyses across one artificial and two medical datasets with natural and synthetic confounders. We also examined robustness under varying levels of confounding and compared computational efficiency across methods. We found that shortcut mitigation methods improved classification performance under strong spurious correlations during training. Latent space analyses revealed differences in representation quality not captured by classification metrics, highlighting the strengths and limitations of each method. Model reliance on shortcuts depended on the degree of confounding in the training data. The best-performing models combine data-centric rebalancing with model-centric disentanglement, achieving stronger and more robust shortcut mitigation than rebalancing alone while maintaining similar computational efficiency.

Abstract (中文翻译): 尽管深度学习模型在医学影像中通常表现出优异的分类性能，但它们可能依赖于捷径学习，即利用与目标任务无因果关系的虚假相关性或混杂因素。这在临床环境中带来风险，因为模型必须在不同机构、人群和采集条件下具备良好的泛化能力。特征解耦是一种有前景的方法，通过在潜在表示中分离任务相关信息与混杂因素相关特征来缓解捷径学习。本研究系统评估了医学影像中用于缓解捷径学习的特征解耦方法，包括对抗学习和基于依赖最小化的潜在空间分割。我们在一个合成数据集和两个包含自然与合成混杂因子的医学数据集上，通过潜在空间分析评估了分类性能与解耦质量，并考察了不同混杂程度下的鲁棒性以及各方法的计算效率。研究发现，在训练过程中存在强虚假相关时，捷径缓解方法可提升分类性能；潜在空间分析揭示了分类指标未能捕捉的表征质量差异，凸显了各方法的优势与局限；模型对捷径的依赖程度取决于训练数据中的混杂程度；表现最佳的模型结合了以数据为中心的重平衡策略与以模型为中心的解耦方法，在保持相近计算效率的同时，实现了比单独重平衡更强且更稳健的捷径缓解效果。

</details>


### [5] [A Computer Vision Framework for Multi-Class Detection and Tracking in Soccer Broadcast Footage](https://arxiv.org/abs/2602.18504)
*Daniel Tshiani*

Main category: cs.CV

TL;DR: 本文提出一种基于单摄像头广播视频的计算机视觉系统，可自动检测和追踪足球比赛中球员、裁判、守门员及球的位置，在不依赖昂贵多摄像头或GPS设备的情况下，为低预算球队提供可行的数据分析方案。


<details>
  <summary>Details</summary>
Motivation: 高预算俱乐部可通过昂贵的多摄像头或GPS系统获取详细数据获得竞争优势，而低预算球队难以收集类似信息。本文旨在探索是否能从常规广播视频中提取同等价值的数据。

Method: 构建端到端系统，结合YOLO目标检测器与ByteTrack跟踪算法，从单路广播视频中识别并追踪球员、裁判、守门员和足球。

Result: 系统在球员和裁判的检测与追踪上表现优异，具有高精度、召回率和mAP50分数；但足球检测仍是主要挑战。

Conclusion: 尽管存在对球检测的局限，该方法证明AI可从单摄像头广播画面中提取有意义的球员空间信息，使学院、青训营和业余俱乐部也能采用此前仅限职业队使用的数据分析手段，展现了低成本计算机视觉足球分析的潜力。

Abstract: Clubs with access to expensive multi-camera setups or GPS tracking systems gain a competitive advantage through detailed data, whereas lower-budget teams are often unable to collect similar information. This paper examines whether such data can instead be extracted directly from standard broadcast footage using a single-camera computer vision pipeline. This project develops an end-to-end system that combines a YOLO object detector with the ByteTrack tracking algorithm to identify and track players, referees, goalkeepers, and the ball throughout a match. Experimental results show that the pipeline achieves high performance in detecting and tracking players and officials, with strong precision, recall, and mAP50 scores, while ball detection remains the primary challenge. Despite this limitation, our findings demonstrate that AI can extract meaningful player-level spatial information from a single broadcast camera. By reducing reliance on specialized hardware, the proposed approach enables colleges, academies, and amateur clubs to adopt scalable, data-driven analysis methods previously accessible only to professional teams, highlighting the potential for affordable computer vision-based soccer analytics.

Abstract (中文翻译): 拥有昂贵多摄像头系统或GPS追踪设备的俱乐部能够通过详尽的数据获得竞争优势，而预算较低的球队通常无法收集类似信息。本文探讨是否可以改用标准广播视频，通过单摄像头计算机视觉流程直接提取此类数据。本项目开发了一个端到端系统，结合YOLO目标检测器与ByteTrack追踪算法，在整场比赛中识别并追踪球员、裁判、守门员和足球。实验结果表明，该流程在检测和追踪球员与裁判方面表现出色，具备较高的精确率、召回率和mAP50得分，而足球检测仍是主要挑战。尽管存在这一局限，我们的研究结果表明，人工智能可以从单路广播摄像画面中提取有意义的球员层级空间信息。通过减少对专用硬件的依赖，所提出的方法使高校、青训学院和业余俱乐部能够采用此前仅限职业球队使用的可扩展、数据驱动的分析方法，凸显了基于计算机视觉的低成本足球分析的巨大潜力。

</details>


### [6] [Suppression or Deletion: A Restoration-Based Representation-Level Analysis of Machine Unlearning](https://arxiv.org/abs/2602.18505)
*Yurim Jang,Jaeung Lee,Dohyun Kim,Jaemin Jo,Simon S. Woo*

Main category: cs.CV

TL;DR: 现有机器遗忘方法大多仅在决策边界层面抑制信息，而未真正删除中间表示中的语义特征；作者提出基于稀疏自编码器和推理时干预的恢复性分析框架，揭示了当前评估指标的不足，并呼吁建立以表示层验证为核心的新评估准则。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型广泛共享，确保模型能按需“遗忘”敏感、版权或隐私信息变得至关重要。然而，当前的遗忘评估依赖输出级指标，无法区分信息是被彻底删除还是仅在表示层被抑制，而后者不足以实现真正的遗忘。

Method: 提出一种基于恢复的分析框架：利用稀疏自编码器识别中间层中类别特定的专家特征，并通过推理时干预（inference-time steering）定量区分信息是被抑制还是被删除。

Result: 在12种主流图像分类遗忘方法上的实验表明，大多数方法对已遗忘信息具有高恢复率，说明它们仅在决策边界层面抑制信息，而中间表示中的语义特征仍被保留；即使从预训练检查点重新训练，也无法消除这些源自预训练的鲁棒语义特征。

Conclusion: 表示层的信息保留构成了被输出级指标忽视的重大风险，亟需新的遗忘评估标准。作者建议在预训练模型时代，尤其在隐私敏感场景中，应优先采用表示层验证作为评估核心。

Abstract: As pretrained models are increasingly shared on the web, ensuring that models can forget or delete sensitive, copyrighted, or private information upon request has become crucial. Machine unlearning has been proposed to address this challenge. However, current evaluations for unlearning methods rely on output-based metrics, which cannot verify whether information is completely deleted or merely suppressed at the representation level, where suppression is insufficient for true unlearning. To address this gap, we propose a novel restoration-based analysis framework that uses Sparse Autoencoders to identify class-specific expert features in intermediate layers and applies inference-time steering to quantitatively distinguish between suppression and deletion. Applying our framework to 12 major unlearning methods in image classification tasks, we find that most methods achieve high restoration rates of unlearned information, indicating that they only suppress information at the decision-boundary level, while preserving semantic features in intermediate representations. Notably, even retraining from pretrained checkpoints shows high restoration, revealing that robust semantic features inherited from pretraining are not removed by retraining. These results demonstrate that representation-level retention poses significant risks overlooked by output-based metrics, highlighting the need for new unlearning evaluation criteria. We propose new evaluation guidelines that prioritize representation-level verification, especially for privacy-critical applications in the era of pre-trained models.

Abstract (中文翻译): 随着预训练模型在网络上日益广泛地共享，确保模型能够根据请求“遗忘”或删除敏感、受版权保护或私密信息变得至关重要。机器遗忘（Machine unlearning）已被提出以应对这一挑战。然而，当前对遗忘方法的评估依赖于基于输出的指标，这些指标无法验证信息是否被完全删除，还是仅在表示层被抑制——而抑制并不足以实现真正的遗忘。为填补这一空白，我们提出了一种新颖的基于恢复的分析框架，该框架利用稀疏自编码器识别中间层中的类别特定专家特征，并通过推理时干预（inference-time steering）定量区分抑制与删除。我们将该框架应用于图像分类任务中的12种主流遗忘方法，发现大多数方法对已遗忘信息具有很高的恢复率，表明它们仅在决策边界层面抑制信息，而中间表示中的语义特征仍被保留。值得注意的是，即使从预训练检查点重新训练，也表现出高恢复率，揭示出预训练所继承的鲁棒语义特征并未被重训练所移除。这些结果表明，表示层的信息保留构成了被输出级指标所忽视的重大风险，凸显了制定新遗忘评估标准的必要性。我们提出了新的评估指南，强调在预训练模型时代，尤其是在隐私关键型应用中，应优先进行表示层的验证。

</details>


### [7] [Depth from Defocus via Direct Optimization](https://arxiv.org/abs/2602.18509)
*Holly Jackson,Caleb Adams,Ignacio Lopez-Francos,Benjamin Recht*

Main category: cs.CV

TL;DR: 本文提出一种基于交替最小化的全局优化方法，用于从离焦图像中恢复深度，在分辨率上优于当前的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 从离焦图像中恢复深度是一个计算复杂度高的优化问题，尽管已有基于光学物理的前向模糊模型，但高效求解仍具挑战。

Method: 采用交替最小化策略：固定深度图时，对全清晰图像进行线性优化；固定全清晰图像时，对每个像素独立并行地搜索最优深度。结合凸优化与并行网格搜索求解。

Result: 在合成与真实离焦模糊数据集上验证了方法的有效性，相比现有方法取得了有竞争力的结果，并支持更高分辨率。

Conclusion: 利用现代优化方法和合理算力，基于交替最小化的全局优化策略可有效解决深度从离焦问题，且具备良好扩展性和实用性。

Abstract: Though there exists a reasonable forward model for blur based on optical physics, recovering depth from a collection of defocused images remains a computationally challenging optimization problem. In this paper, we show that with contemporary optimization methods and reasonable computing resources, a global optimization approach to depth from defocus is feasible. Our approach rests on alternating minimization. When holding the depth map fixed, the forward model is linear with respect to the all-in-focus image. When holding the all-in-focus image fixed, the depth at each pixel can be computed independently, enabling embarrassingly parallel computation. We show that alternating between convex optimization and parallel grid search can effectively solve the depth-from-defocus problem at higher resolutions than current deep learning methods. We demonstrate our approach on benchmark datasets with synthetic and real defocus blur and show promising results compared to prior approaches. Our code is available at github.com/hollyjackson/dfd.

Abstract (中文翻译): 尽管基于光学物理的模糊前向模型已经较为成熟，但从一组离焦图像中恢复深度仍然是一个计算上极具挑战性的优化问题。本文表明，借助当代优化方法和合理的计算资源，采用全局优化策略解决深度从离焦问题是可行的。我们的方法基于交替最小化：当深度图固定时，前向模型相对于全清晰图像呈线性关系；当全清晰图像固定时，每个像素的深度可以独立计算，从而实现高度并行化。我们证明，通过在凸优化与并行网格搜索之间交替迭代，能够以高于当前深度学习方法的分辨率有效求解深度从离焦问题。我们在包含合成与真实离焦模糊的基准数据集上验证了该方法，并与先前方法进行了对比，结果令人鼓舞。代码已开源：github.com/hollyjackson/dfd。

</details>


### [8] [Sketch2Feedback: Grammar-in-the-Loop Framework for Rubric-Aligned Feedback on Student STEM Diagrams](https://arxiv.org/abs/2602.18520)
*Aayam Bansal*

Main category: cs.CV

TL;DR: 本文提出Sketch2Feedback框架，通过结合语法规则与视觉语言模型（VLM），在学生绘制的STEM图表（如受力图和电路图）反馈任务中减少大模型幻觉，提升反馈的准确性和可操作性。


<details>
  <summary>Details</summary>
Motivation: 在STEM教育中，为学生手绘图表提供及时且符合评分标准的反馈具有挑战性；现有大型多模态模型虽能处理图像并生成解释，但其幻觉问题影响了课堂应用的可信度。

Method: 提出一个“语法闭环”框架Sketch2Feedback，将任务分解为四个阶段：混合感知、符号图构建、约束检查和受限VLM反馈，确保语言模型仅对经规则引擎验证的错误进行表述。

Result: 在FBD-10和Circuit-10两个微基准测试中，端到端LMM（如Qwen2-VL-7B）虽取得最高micro-F1分数，但幻觉率极高；结合语法规则的集成方法显著降低幻觉并保持性能；LLM-as-judge评估显示语法管道生成的电路反馈更具可操作性（4.85/5 vs 3.11/5）。

Conclusion: 语法规则与端到端模型具有互补性，将规则引擎嵌入反馈流程可有效抑制幻觉、提升反馈质量，尤其适用于对准确性要求高的教育场景。

Abstract: Providing timely, rubric-aligned feedback on student-drawn diagrams is a persistent challenge in STEM education. While large multimodal models (LMMs) can jointly parse images and generate explanations, their tendency to hallucinate undermines trust in classroom deployments. We present Sketch2Feedback, a grammar-in-the-loop framework that decomposes the problem into four stages -- hybrid perception, symbolic graph construction, constraint checking, and constrained VLM feedback -- so that the language model verbalizes only violations verified by an upstream rule engine. We evaluate on two synthetic micro-benchmarks, FBD-10 (free-body diagrams) and Circuit-10 (circuit schematics), each with 500 images spanning standard and hard noise augmentation tiers, comparing our pipeline against end-to-end LMMs (LLaVA-1.5-7B, Qwen2-VL-7B), a vision-only detector, a YOLOv8-nano learned detector, and an ensemble oracle. On n=100 test samples per benchmark with 95% bootstrap CIs, results are mixed and instructive: Qwen2-VL-7B achieves the highest micro-F1 on both FBDs (0.570) and circuits (0.528), but with extreme hallucination rates (0.78, 0.98). An ensemble oracle that selects the best prediction per sample reaches F1=0.556 with hallucination 0.320 on FBDs, demonstrating exploitable complementarity between grammar and end-to-end approaches. Confidence thresholding at tau=0.7 reduces circuit hallucination from 0.970 to 0.880 with no F1 loss. Hard noise augmentation reveals domain-dependent robustness: FBD detection is resilient while circuit detection degrades sharply. An LLM-as-judge evaluation confirms that the grammar pipeline produces more actionable circuit feedback (4.85/5) than the end-to-end LMM (3.11/5). We release all code, datasets, and evaluation scripts.

Abstract (中文翻译): 在STEM教育中，为学生手绘图表提供及时且符合评分标准的反馈一直是一项持续存在的挑战。尽管大型多模态模型（LMM）能够联合解析图像并生成解释，但其容易产生幻觉的问题削弱了其在课堂教学中的可信度。我们提出了Sketch2Feedback——一种“语法闭环”框架，将该问题分解为四个阶段：混合感知、符号图构建、约束检查和受限视觉语言模型（VLM）反馈，从而确保语言模型仅对上游规则引擎已验证的违规内容进行表述。我们在两个合成微基准数据集上进行了评估：FBD-10（受力图）和Circuit-10（电路原理图），每个数据集包含500张图像，涵盖标准和高难度噪声增强级别。我们将本方法与端到端LMM（LLaVA-1.5-7B、Qwen2-VL-7B）、纯视觉检测器、YOLOv8-nano学习型检测器以及集成Oracle进行了比较。在每项基准各100个测试样本（95%自助置信区间）上的结果显示：Qwen2-VL-7B在FBD（0.570）和电路图（0.528）上均取得最高的micro-F1分数，但其幻觉率极高（分别为0.78和0.98）；逐样本选择最优预测的集成Oracle在FBD上达到F1=0.556且幻觉率为0.320，表明语法规则方法与端到端方法之间存在可利用的互补性；将置信度阈值设为tau=0.7可在不损失F1分数的前提下，将电路图的幻觉率从0.970降至0.880；高难度噪声增强揭示了领域依赖的鲁棒性差异：FBD检测表现稳健，而电路图检测性能急剧下降；LLM-as-judge评估证实，语法规则管道生成的电路反馈更具可操作性（4.85/5），显著优于端到端LMM（3.11/5）。我们公开了全部代码、数据集和评估脚本。

</details>


### [9] [Do Generative Metrics Predict YOLO Performance? An Evaluation Across Models, Augmentation Ratios, and Dataset Complexity](https://arxiv.org/abs/2602.18525)
*Vasile Marian,Yong-Bin Kang,Alexander Buddery*

Main category: cs.CV

TL;DR: 本文系统评估了在YOLOv11目标检测任务中使用合成图像进行数据增强的效果，发现其在复杂场景（如行人、盆栽植物）中显著提升性能，但在简单或饱和场景（如交通标志）中效果有限；同时揭示了传统生成质量指标（如FID）与下游检测性能之间缺乏一致关联。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可靠的方法在训练前评估合成数据集对目标检测任务的有效性，因为常用的全局生成指标（如FID）无法准确预测下游检测性能（如mAP），因此需要系统性地研究合成数据增强的实际效果及其可预测性。

Method: 作者在三个不同难度的目标检测任务（交通标志、Cityscapes行人、COCO盆栽植物）上，使用六种基于GAN、扩散模型和混合方法的生成器，以10%至150%的比例合成数据增强YOLOv11训练集，并分别从零训练和COCO预训练两种设置下评估mAP。同时，在匹配样本量的引导协议下计算训练前的全局特征空间指标（Inception-v3和DINOv2嵌入）和以对象为中心的边界框统计分布距离，并分析这些指标与最终检测性能的相关性（包括原始相关性和控制增强比例后的残差相关性）。

Result: 在更具挑战性的检测任务（行人和盆栽植物）中，合成数据增强分别带来最高+7.6%和+30.6%的相对mAP提升；但在交通标志任务和预训练微调设置下效果不明显。此外，多数原始指标与性能的相关性在控制增强比例后显著减弱，表明指标-性能对齐高度依赖于具体任务场景。

Conclusion: 合成数据增强对目标检测的效益高度依赖于任务复杂度和训练设置，且现有生成质量指标难以可靠预测其下游性能；未来需开发更贴合检测任务的合成数据评估方法。

Abstract: Synthetic images are increasingly used to augment object-detection training sets, but reliably evaluating a synthetic dataset before training remains difficult: standard global generative metrics (e.g., FID) often do not predict downstream detection mAP. We present a controlled evaluation of synthetic augmentation for YOLOv11 across three single-class detection regimes -- Traffic Signs (sparse/near-saturated), Cityscapes Pedestrian (dense/occlusion-heavy), and COCO PottedPlant (multi-instance/high-variability). We benchmark six GAN-, diffusion-, and hybrid-based generators over augmentation ratios from 10% to 150% of the real training split, and train YOLOv11 both from scratch and with COCO-pretrained initialization, evaluating on held-out real test splits (mAP@0.50:0.95). For each dataset-generator-augmentation configuration, we compute pre-training dataset metrics under a matched-size bootstrap protocol, including (i) global feature-space metrics in both Inception-v3 and DINOv2 embeddings and (ii) object-centric distribution distances over bounding-box statistics. Synthetic augmentation yields substantial gains in the more challenging regimes (up to +7.6% and +30.6% relative mAP in Pedestrian and PottedPlant, respectively) but is marginal in Traffic Signs and under pretrained fine-tuning. To separate metric signal from augmentation quantity, we report both raw and augmentation-controlled (residualized) correlations with multiple-testing correction, showing that metric-performance alignment is strongly regime-dependent and that many apparent raw associations weaken after controlling for augmentation level.

Abstract (中文翻译): 合成图像越来越多地被用于扩充目标检测的训练集，但在训练前可靠地评估合成数据集仍然困难：标准的全局生成指标（例如FID）通常无法预测下游检测的mAP。我们针对YOLOv11在三种单类别检测场景（交通标志（稀疏/接近饱和）、Cityscapes行人（密集/遮挡严重）和COCO盆栽植物（多实例/高变异性））中对合成数据增强进行了受控评估。我们对六种基于GAN、扩散模型和混合方法的生成器在10%至150%真实训练集比例的增强水平下进行了基准测试，并分别从零开始训练和使用COCO预训练初始化训练YOLOv11，在保留的真实测试集上评估mAP@0.50:0.95。对于每种数据集-生成器-增强配置，我们在匹配样本量的自助协议下计算训练前的数据集指标，包括（i）Inception-v3和DINOv2嵌入空间中的全局特征度量，以及（ii）基于边界框统计的对象中心分布距离。合成数据增强在更具挑战性的场景中带来了显著收益（行人和盆栽植物任务中相对mAP分别最高提升+7.6%和+30.6%），但在交通标志任务和预训练微调设置下效果甚微。为区分指标信号与增强数量的影响，我们报告了经多重检验校正后的原始相关性和控制增强比例后的残差相关性，结果表明指标与性能的一致性强烈依赖于具体场景，且在控制增强水平后，许多表面上的原始关联显著减弱。

</details>


### [10] [JAEGER: Joint 3D Audio-Visual Grounding and Reasoning in Simulated Physical Environments](https://arxiv.org/abs/2602.18527)
*Zhan Liu,Changli Tang,Yuxin Wang,Zhiyuan Zhu,Youjun Chen,Yiwen Shao,Tianzi Wang,Lei Ke,Zengrui Jin,Chao Zhang*

Main category: cs.CV

TL;DR: 本文提出JAEGER框架，将音频-视觉大语言模型（AV-LLMs）从2D扩展到3D，通过融合RGB-D和多通道一阶Ambisonics音频，并引入神经强度向量（Neural IV）提升空间感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有AV-LLMs主要依赖2D RGB视频和单声道音频，导致在复杂3D环境中无法可靠进行声源定位和空间推理，存在维度不匹配的根本缺陷。

Method: 提出JAEGER框架，整合RGB-D观测与多通道一阶Ambisonics；设计神经强度向量（Neural IV）作为可学习的空间音频表示，以增强方向估计；构建包含61k样本的SpatialSceneQA指令微调数据集用于训练与评估。

Result: 在多种空间感知与推理任务上，JAEGER显著优于基于2D的基线模型，验证了显式建模3D信息对物理环境中AI系统的重要性。

Conclusion: 将AV-LLMs扩展至3D空间是提升其在真实物理环境中空间理解能力的关键，所提方法为未来具身智能和多模态3D感知提供了有效路径。

Abstract: Current audio-visual large language models (AV-LLMs) are predominantly restricted to 2D perception, relying on RGB video and monaural audio. This design choice introduces a fundamental dimensionality mismatch that precludes reliable source localization and spatial reasoning in complex 3D environments. We address this limitation by presenting JAEGER, a framework that extends AV-LLMs to 3D space, to enable joint spatial grounding and reasoning through the integration of RGB-D observations and multi-channel first-order ambisonics. A core contribution of our work is the neural intensity vector (Neural IV), a learned spatial audio representation that encodes robust directional cues to enhance direction-of-arrival estimation, even in adverse acoustic scenarios with overlapping sources. To facilitate large-scale training and systematic evaluation, we propose SpatialSceneQA, a benchmark of 61k instruction-tuning samples curated from simulated physical environments. Extensive experiments demonstrate that our approach consistently surpasses 2D-centric baselines across diverse spatial perception and reasoning tasks, underscoring the necessity of explicit 3D modelling for advancing AI in physical environments. Our source code, pre-trained model checkpoints and datasets will be released upon acceptance.

Abstract (中文翻译): 当前的音视频大语言模型（AV-LLMs）主要局限于2D感知，依赖RGB视频和单声道音频。这种设计选择引入了根本性的维度不匹配问题，阻碍了在复杂3D环境中进行可靠的声源定位和空间推理。我们通过提出JAEGER框架来解决这一局限，该框架将AV-LLMs扩展至3D空间，通过融合RGB-D观测与多通道一阶Ambisonics音频，实现联合的空间定位与推理。本工作的核心贡献是神经强度向量（Neural IV），这是一种可学习的空间音频表示，能够编码鲁棒的方向线索，即使在存在多个重叠声源的恶劣声学场景下也能提升到达方向估计性能。为支持大规模训练与系统性评估，我们提出了SpatialSceneQA基准，该基准包含从模拟物理环境中精心构建的6.1万条指令微调样本。大量实验表明，我们的方法在多种空间感知与推理任务上始终优于以2D为中心的基线模型，凸显了显式3D建模对于推动物理环境中人工智能发展的必要性。我们的源代码、预训练模型检查点和数据集将在论文被接收后公开发布。

</details>
