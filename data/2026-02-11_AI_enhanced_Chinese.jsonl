{"id": "2602.09082", "pdf": "https://arxiv.org/pdf/2602.09082", "abs": "https://arxiv.org/abs/2602.09082", "authors": ["Veuns-Team", ":", "Changlong Gao", "Zhangxuan Gu", "Yulin Liu", "Xinyu Qiu", "Shuheng Shen", "Yue Wen", "Tianyu Xia", "Zhenyu Xu", "Zhengwen Zeng", "Beitong Zhou", "Xingran Zhou", "Weizhi Chen", "Sunhao Dai", "Jingya Dou", "Yichen Gong", "Yuan Guo", "Zhenlin Guo", "Feng Li", "Qian Li", "Jinzhen Lin", "Yuqi Zhou", "Linchao Zhu", "Liang Chen", "Zhenyu Guo", "Changhua Meng", "Weiqiang Wang"], "title": "UI-Venus-1.5 Technical Report", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "AI": {"tldr": "UI-Venus-1.5 \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aef GUI \u667a\u80fd\u4f53\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230 SOTA\uff0c\u5e76\u5728\u4e2d\u6587\u79fb\u52a8\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u771f\u5b9e\u573a\u666f\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709 GUI \u667a\u80fd\u4f53\u96be\u4ee5\u540c\u65f6\u517c\u987e\u901a\u7528\u6027\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u66f4\u5f3a\u5927\u3001\u7edf\u4e00\u4e14\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u667a\u80fd\u4f53\u3002", "method": "UI-Venus-1.5 \u5f15\u5165\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\uff081\uff09\u57fa\u4e8e 30+ \u6570\u636e\u96c6\u3001100 \u4ebf token \u7684\u4e2d\u671f\u8bad\u7ec3\u4ee5\u5efa\u7acb GUI \u8bed\u4e49\u57fa\u7840\uff1b\uff082\uff09\u4f7f\u7528\u5b8c\u6574\u8f68\u8ff9\u5c55\u5f00\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u5bf9\u9f50\u957f\u89c6\u91ce\u52a8\u6001\u5bfc\u822a\u76ee\u6807\uff1b\uff083\uff09\u901a\u8fc7\u6a21\u578b\u878d\u5408\u5c06\u9886\u57df\u4e13\u7528\u6a21\u578b\uff08\u5b9a\u4f4d\u3001\u7f51\u9875\u3001\u79fb\u52a8\u7aef\uff09\u6574\u5408\u4e3a\u5355\u4e00\u7edf\u4e00\u667a\u80fd\u4f53\u3002\u6a21\u578b\u5305\u542b 2B\u30018B \u5bc6\u96c6\u7248\u672c\u548c 30B-A3B \u6df7\u5408\u4e13\u5bb6\u7248\u672c\u3002", "result": "\u5728 ScreenSpot-Pro\uff0869.6%\uff09\u3001VenusBench-GD\uff0875.0%\uff09\u548c AndroidWorld\uff0877.6%\uff09\u7b49\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u5148\u524d\u5f3a\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u591a\u79cd\u4e2d\u6587\u79fb\u52a8\u5e94\u7528\u4e2d\u5c55\u73b0\u7a33\u5065\u7684\u5bfc\u822a\u4e0e\u6307\u4ee4\u6267\u884c\u80fd\u529b\u3002", "conclusion": "UI-Venus-1.5 \u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548c\u4e09\u9879\u5173\u952e\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u901a\u7528\u6027\u4e0e\u9ad8\u6027\u80fd\u7684\u7ed3\u5408\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c GUI \u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "GUI \u667a\u80fd\u4f53\u5df2\u6210\u4e3a\u81ea\u52a8\u5316\u6570\u5b57\u73af\u5883\u4ea4\u4e92\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u4f46\u5b9e\u73b0\u5e7f\u6cdb\u7684\u901a\u7528\u6027\u548c\u6301\u7eed\u5f3a\u52b2\u7684\u4efb\u52a1\u6027\u80fd\u4ecd\u5177\u6311\u6218\u3002\u5728\u672c\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 UI-Venus-1.5\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411\u9c81\u68d2\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u7edf\u4e00\u7aef\u5230\u7aef GUI \u667a\u80fd\u4f53\u3002\u8be5\u6a21\u578b\u7cfb\u5217\u5305\u542b\u4e24\u4e2a\u5bc6\u96c6\u7248\u672c\uff082B \u548c 8B\uff09\u548c\u4e00\u4e2a\u6df7\u5408\u4e13\u5bb6\u7248\u672c\uff0830B-A3B\uff09\uff0c\u4ee5\u6ee1\u8db3\u4e0d\u540c\u7684\u4e0b\u6e38\u5e94\u7528\u573a\u666f\u3002\u76f8\u6bd4\u524d\u4e00\u7248\u672c\uff0cUI-Venus-1.5 \u5f15\u5165\u4e86\u4e09\u9879\u5173\u952e\u6280\u672f\u8fdb\u5c55\uff1a\uff081\uff09\u4e00\u4e2a\u5168\u9762\u7684\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\uff0c\u5229\u7528 30 \u591a\u4e2a\u6570\u636e\u96c6\u4e2d\u7684 100 \u4ebf token \u5efa\u7acb\u57fa\u7840 GUI \u8bed\u4e49\uff1b\uff082\uff09\u91c7\u7528\u5b8c\u6574\u8f68\u8ff9\u5c55\u5f00\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u8bad\u7ec3\u76ee\u6807\u4e0e\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u957f\u89c6\u91ce\u3001\u52a8\u6001\u5bfc\u822a\u5bf9\u9f50\uff1b\uff083\uff09\u901a\u8fc7\u6a21\u578b\u878d\u5408\u6784\u5efa\u5355\u4e00\u7edf\u4e00\u7684 GUI \u667a\u80fd\u4f53\uff0c\u5c06\u9886\u57df\u4e13\u7528\u6a21\u578b\uff08\u5b9a\u4f4d\u3001\u7f51\u9875\u548c\u79fb\u52a8\u7aef\uff09\u6574\u5408\u4e3a\u4e00\u4e2a\u8fde\u8d2f\u7684\u68c0\u67e5\u70b9\u3002\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0cUI-Venus-1.5 \u5728 ScreenSpot-Pro\uff0869.6%\uff09\u3001VenusBench-GD\uff0875.0%\uff09\u548c AndroidWorld\uff0877.6%\uff09\u7b49\u57fa\u51c6\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u6b64\u524d\u7684\u5f3a\u57fa\u7ebf\u3002\u6b64\u5916\uff0cUI-Venus-1.5 \u5728\u591a\u79cd\u4e2d\u6587\u79fb\u52a8\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u7a33\u5065\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6267\u884c\u7528\u6237\u6307\u4ee4\u3002\u4ee3\u7801\uff1ahttps://github.com/inclusionAI/UI-Venus\uff1b\u6a21\u578b\uff1ahttps://huggingface.co/collections/inclusionAI/ui-venus"}}
{"id": "2602.09084", "pdf": "https://arxiv.org/pdf/2602.09084", "abs": "https://arxiv.org/abs/2602.09084", "authors": ["Ruijie Ye", "Jiayi Zhang", "Zhuoxin Liu", "Zihao Zhu", "Siyuan Yang", "Li Li", "Tianfu Fu", "Franck Dernoncourt", "Yue Zhao", "Jiacheng Zhu", "Ryan Rossi", "Wenhao Chai", "Zhengzhong Tu"], "title": "Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling", "categories": ["cs.CV"], "comment": "Project Website: agent-banana.github.io", "summary": "We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgent Banana\u6846\u67b6\uff0c\u901a\u8fc7Context Folding\u548cImage Layer Decomposition\u673a\u5236\uff0c\u89e3\u51b3\u4e13\u4e1a\u56fe\u50cf\u7f16\u8f91\u4e2d\u8fc7\u5ea6\u7f16\u8f91\u3001\u591a\u8f6e\u7f16\u8f91\u5bf9\u8c61\u5931\u771f\u53ca\u9ad8\u6e05\u8bc4\u4f30\u7f3a\u5931\u4e09\u5927\u6311\u6218\uff0c\u5728\u65b0\u6784\u5efa\u76844K\u9ad8\u5206\u8fa8\u7387\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6HDD-Bench\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u4e13\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u9762\u4e34\u4e09\u5927\u95ee\u9898\uff1a(i) \u7f16\u8f91\u5668\u5e38\u8fc7\u5ea6\u4fee\u6539\uff0c\u8d85\u51fa\u7528\u6237\u610f\u56fe\uff1b(ii) \u591a\u6570\u6a21\u578b\u4ec5\u652f\u6301\u5355\u8f6e\u7f16\u8f91\uff0c\u800c\u591a\u8f6e\u7f16\u8f91\u6613\u5bfc\u81f4\u5bf9\u8c61\u5931\u771f\uff1b(iii) \u5f53\u524d\u8bc4\u4f30\u901a\u5e38\u5728\u7ea61K\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c\uff0c\u4e0e\u5b9e\u9645\u4f7f\u7528\u7684\u8d85\u9ad8\u6e05\uff08\u59824K\uff09\u56fe\u50cf\u5de5\u4f5c\u6d41\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faAgent Banana\uff0c\u4e00\u79cd\u5206\u5c42\u7684\u667a\u80fd\u4f53\u89c4\u5212-\u6267\u884c\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a(1) Context Folding\uff0c\u5c06\u957f\u4ea4\u4e92\u5386\u53f2\u538b\u7f29\u4e3a\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u957f\u7a0b\u63a7\u5236\uff1b(2) Image Layer Decomposition\uff0c\u901a\u8fc7\u5c40\u90e8\u56fe\u5c42\u7f16\u8f91\u4fdd\u7559\u975e\u76ee\u6807\u533a\u57df\uff0c\u5e76\u652f\u6301\u539f\u751f\u5206\u8fa8\u7387\u8f93\u51fa\u3002\u540c\u65f6\u6784\u5efa\u4e86\u9ad8\u5206\u8fa8\u7387\u5bf9\u8bdd\u57fa\u51c6HDD-Bench\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u5728HDD-Bench\u4e0a\uff0cAgent Banana\u5728\u591a\u8f6e\u4e00\u81f4\u6027\uff08IC 0.871\uff09\u548c\u80cc\u666f\u4fdd\u771f\u5ea6\uff08SSIM-OM 0.84, LPIPS-OM 0.12\uff09\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u5728\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5e76\u5728\u6807\u51c6\u5355\u8f6e\u7f16\u8f91\u57fa\u51c6\u4e0a\u4e5f\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u63d0\u51faAgent Banana\u6846\u67b6\u548cHDD-Bench\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u53ef\u9760\u3001\u4e13\u4e1a\u7ea7\u667a\u80fd\u4f53\u56fe\u50cf\u7f16\u8f91\u7684\u53d1\u5c55\uff0c\u5e76\u4fc3\u8fdb\u5176\u5728\u771f\u5b9e\u5de5\u4f5c\u6d41\u4e2d\u7684\u96c6\u6210\u3002", "summary_cn": "\u6211\u4eec\u7814\u7a76\u4e86\u4e13\u4e1a\u5de5\u4f5c\u6d41\u4e0b\u7684\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u5e76\u8bc6\u522b\u51fa\u4e09\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\uff1a(i) \u7f16\u8f91\u5668\u7ecf\u5e38\u8fc7\u5ea6\u7f16\u8f91\uff0c\u4fee\u6539\u8d85\u51fa\u7528\u6237\u610f\u56fe\u7684\u5185\u5bb9\uff1b(ii) \u73b0\u6709\u6a21\u578b\u5927\u591a\u662f\u5355\u8f6e\u7684\uff0c\u800c\u591a\u8f6e\u7f16\u8f91\u4f1a\u6539\u53d8\u5bf9\u8c61\u7684\u771f\u5b9e\u6027\uff1b(iii) \u5728\u7ea61K\u5206\u8fa8\u7387\u4e0b\u7684\u8bc4\u4f30\u4e0e\u901a\u5e38\u64cd\u4f5c\u8d85\u9ad8\u6e05\u56fe\u50cf\uff08\u4f8b\u59824K\uff09\u7684\u771f\u5b9e\u5de5\u4f5c\u6d41\u4e0d\u4e00\u81f4\u3002\u6211\u4eec\u63d0\u51fa\u4e86Agent Banana\uff0c\u4e00\u79cd\u7528\u4e8e\u9ad8\u4fdd\u771f\u3001\u5bf9\u8c61\u611f\u77e5\u3001\u6df1\u601d\u719f\u8651\u7f16\u8f91\u7684\u5206\u5c42\u667a\u80fd\u4f53\u89c4\u5212-\u6267\u884c\u6846\u67b6\u3002Agent Banana\u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a(1) \u4e0a\u4e0b\u6587\u6298\u53e0\uff08Context Folding\uff09\uff0c\u5c06\u957f\u4ea4\u4e92\u5386\u53f2\u538b\u7f29\u6210\u7ed3\u6784\u5316\u8bb0\u5fc6\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u957f\u7a0b\u63a7\u5236\uff1b(2) \u56fe\u50cf\u56fe\u5c42\u5206\u89e3\uff08Image Layer Decomposition\uff09\uff0c\u6267\u884c\u5c40\u90e8\u56fe\u5c42\u7f16\u8f91\u4ee5\u4fdd\u7559\u975e\u76ee\u6807\u533a\u57df\uff0c\u540c\u65f6\u652f\u6301\u539f\u751f\u5206\u8fa8\u7387\u8f93\u51fa\u3002\u4e3a\u4e86\u652f\u6301\u4e25\u683c\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u6784\u5efa\u4e86HDD-Bench\uff0c\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u3001\u57fa\u4e8e\u5bf9\u8bdd\u7684\u57fa\u51c6\uff0c\u5177\u6709\u53ef\u9a8c\u8bc1\u7684\u9010\u6b65\u76ee\u6807\u548c\u539f\u751f4K\u56fe\u50cf\uff081180\u4e07\u50cf\u7d20\uff09\uff0c\u7528\u4e8e\u8bca\u65ad\u957f\u7a0b\u5931\u8d25\u3002\u5728HDD-Bench\u4e0a\uff0cAgent Banana\u5728\u591a\u8f6e\u4e00\u81f4\u6027\u548c\u80cc\u666f\u4fdd\u771f\u5ea6\u65b9\u9762\u8fbe\u5230\u6700\u4f73\uff08\u4f8b\u5982\uff0cIC 0.871\uff0cSSIM-OM 0.84\uff0cLPIPS-OM 0.12\uff09\uff0c\u540c\u65f6\u5728\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5e76\u5728\u6807\u51c6\u5355\u8f6e\u7f16\u8f91\u57fa\u51c6\u4e0a\u4e5f\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\u3002\u6211\u4eec\u5e0c\u671b\u8fd9\u9879\u5de5\u4f5c\u80fd\u63a8\u52a8\u53ef\u9760\u3001\u4e13\u4e1a\u7ea7\u667a\u80fd\u4f53\u56fe\u50cf\u7f16\u8f91\u53ca\u5176\u5728\u771f\u5b9e\u5de5\u4f5c\u6d41\u4e2d\u7684\u96c6\u6210\u3002"}}
{"id": "2602.09146", "pdf": "https://arxiv.org/pdf/2602.09146", "abs": "https://arxiv.org/abs/2602.09146", "authors": ["Saar Huberman", "Kfir Goldberg", "Or Patashnik", "Sagie Benaim", "Ron Mokady"], "title": "SemanticMoments: Training-Free Motion Similarity via Third Moment Features", "categories": ["cs.CV"], "comment": null, "summary": "Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.", "AI": {"tldr": "\u73b0\u6709\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u9759\u6001\u5916\u89c2\u800c\u5ffd\u89c6\u8bed\u4e49\u8fd0\u52a8\uff0c\u672c\u6587\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684SemanticMoments\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u8bed\u4e49\u7279\u5f81\u4e0a\u8ba1\u7b97\u9ad8\u9636\u65f6\u95f4\u7edf\u8ba1\u91cf\uff0c\u5728\u65b0\u6784\u5efa\u7684SimMotion\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8868\u5f81\u65b9\u6cd5\u56e0\u8bad\u7ec3\u6570\u636e\u548c\u76ee\u6807\u7684\u504f\u5dee\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u9759\u6001\u5916\u89c2\u548c\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u800c\u4f20\u7edf\u57fa\u4e8e\u5149\u6d41\u7b49\u8fd0\u52a8\u4e2d\u5fc3\u7684\u65b9\u6cd5\u53c8\u7f3a\u4e4f\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u8bed\u4e49\u8fd0\u52a8\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSemanticMoments\u65b9\u6cd5\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u9884\u8bad\u7ec3\u8bed\u4e49\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u8ba1\u7b97\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u9ad8\u9636\u77e9\uff08\u5982\u65b9\u5dee\u3001\u504f\u5ea6\u7b49\uff09\u6765\u6355\u6349\u8bed\u4e49\u8fd0\u52a8\u4fe1\u606f\u3002", "result": "\u5728\u65b0\u6784\u5efa\u7684SimMotion\u57fa\u51c6\uff08\u5305\u542b\u5408\u6210\u6570\u636e\u4e0e\u4eba\u5de5\u6807\u6ce8\u771f\u5b9e\u6570\u636e\uff09\u4e0a\uff0cSemanticMoments\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7684RGB\u3001\u5149\u6d41\u53ca\u6587\u672c\u76d1\u7763\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u5728\u8bed\u4e49\u7279\u5f81\u7a7a\u95f4\u4e2d\u4f7f\u7528\u65f6\u95f4\u7edf\u8ba1\u91cf\uff0c\u4e3a\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u611f\u77e5\u5408\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u57fa\u4e8e\u8bed\u4e49\u8fd0\u52a8\u7684\u89c6\u9891\u68c0\u7d22\u662f\u4e00\u4e2a\u57fa\u7840\u4f46\u5c1a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u89c6\u9891\u8868\u5f81\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u9759\u6001\u5916\u89c2\u548c\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u800c\u975e\u8fd0\u52a8\u52a8\u6001\uff0c\u8fd9\u79cd\u504f\u5dee\u6e90\u4e8e\u5176\u8bad\u7ec3\u6570\u636e\u548c\u76ee\u6807\u3002\u76f8\u53cd\uff0c\u4f20\u7edf\u7684\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u8f93\u5165\uff08\u5982\u5149\u6d41\uff09\u7f3a\u4e4f\u7406\u89e3\u9ad8\u5c42\u8fd0\u52a8\u6240\u9700\u7684\u8bed\u4e49\u57fa\u7840\u3002\u4e3a\u63ed\u793a\u8fd9\u4e00\u56fa\u6709\u504f\u5dee\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SimMotion\u57fa\u51c6\uff0c\u7ed3\u5408\u4e86\u53d7\u63a7\u7684\u5408\u6210\u6570\u636e\u548c\u65b0\u7684\u4eba\u5de5\u6807\u6ce8\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u3002\u6211\u4eec\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u5e38\u65e0\u6cd5\u5c06\u8fd0\u52a8\u4e0e\u5916\u89c2\u89e3\u8026\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SemanticMoments\u2014\u2014\u4e00\u79cd\u7b80\u5355\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9884\u8bad\u7ec3\u8bed\u4e49\u6a21\u578b\u7684\u7279\u5f81\u8ba1\u7b97\u65f6\u95f4\u7edf\u8ba1\u91cf\uff08\u7279\u522b\u662f\u9ad8\u9636\u77e9\uff09\u6765\u8868\u5f81\u8bed\u4e49\u8fd0\u52a8\u3002\u5728\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemanticMoments\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684RGB\u3001\u5149\u6d41\u548c\u6587\u672c\u76d1\u7763\u65b9\u6cd5\u3002\u8fd9\u8868\u660e\uff0c\u5728\u8bed\u4e49\u7279\u5f81\u7a7a\u95f4\u4e2d\u4f7f\u7528\u65f6\u95f4\u7edf\u8ba1\u91cf\uff0c\u4e3a\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u611f\u77e5\u5408\u7406\u7684\u57fa\u77f3\u3002"}}
{"id": "2602.09154", "pdf": "https://arxiv.org/pdf/2602.09154", "abs": "https://arxiv.org/abs/2602.09154", "authors": ["Andrea Filiberto Lucas", "Dylan Seychell"], "title": "A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "7 pages, 5 figures. Accepted for publication at the 2026 IEEE Conference on Artificial Intelligence (CAI)", "summary": "The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.\n  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u6a21\u5757\u5316\u7684\u65b0\u95fb\u89c6\u9891\u4eba\u540d\u63d0\u53d6\u6846\u67b6\uff0c\u5728\u4fdd\u8bc1\u53ef\u5ba1\u8ba1\u6027\u548c\u65e0\u5e7b\u89c9\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u751f\u6210\u5f0f\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u65b0\u95fb\u5185\u5bb9\u6fc0\u589e\uff0c\u4e9f\u9700\u900f\u660e\u53ef\u9760\u7684\u65b9\u6cd5\u4ece\u5c4f\u5e55\u4e2d\u63d0\u53d6\u4fe1\u606f\uff1b\u4f46\u65b0\u95fb\u56fe\u5f62\u5e03\u5c40\u591a\u6837\u3001\u5e73\u53f0\u8bbe\u8ba1\u5dee\u5f02\u5927\uff0c\u4f7f\u5f97\u4eba\u5de5\u6807\u6ce8\u4e0d\u73b0\u5b9e\uff0c\u800c\u73b0\u6709\u751f\u6210\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u5ba1\u8ba1\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u65b0\u95fb\u4e1a\u5bf9\u6570\u636e\u6eaf\u6e90\u7684\u8981\u6c42\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u5f53\u4ee3\u65b0\u95fb\u56fe\u5f62\u591a\u6837\u6027\u7684\u6807\u6ce8\u5e27\u8bed\u6599\u5e93\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u3001\u6a21\u5757\u5316\u7684\u786e\u5b9a\u6027\u63d0\u53d6\u6d41\u7a0b\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u548c\u63d0\u53d6\u5e7f\u64ad\u53ca\u793e\u4ea4\u5a92\u4f53\u539f\u751f\u65b0\u95fb\u89c6\u9891\u4e2d\u7684\u4e2a\u4eba\u59d3\u540d\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u56fe\u5f62\u5143\u7d20\u5b9a\u4f4d\u4e0a\u8fbe\u523095.8% mAP@0.5\uff1b\u4eba\u540d\u63d0\u53d6F1\u4e3a77.08%\uff08\u7cbe\u5ea679.9%\uff0c\u53ec\u56de\u738774.4%\uff09\uff0c\u867d\u7565\u4f4e\u4e8e\u751f\u6210\u5f0f\u65b9\u6cd5\uff0884.18%\uff09\uff0c\u4f46\u5177\u5907\u5168\u7a0b\u53ef\u8ffd\u6eaf\u6027\u4e14\u65e0\u5e7b\u89c9\uff1b\u7528\u6237\u8c03\u7814\u663e\u793a59%\u53d7\u8bbf\u8005\u96be\u4ee5\u5728\u5feb\u8282\u594f\u64ad\u62a5\u4e2d\u770b\u6e05\u5c4f\u5e55\u4eba\u540d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u73b0\u4ee3\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u6df7\u5408\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u5efa\u7acb\u4e86\u65b9\u6cd5\u4e25\u8c28\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u7ebf\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u900f\u660e\u5ea6\uff0c\u66f4\u9002\u5408\u65b0\u95fb\u5206\u6790\u7b49\u9700\u5ba1\u8ba1\u7684\u5e94\u7528\u573a\u666f\u3002", "summary_cn": "\u89c6\u9891\u65b0\u95fb\u5185\u5bb9\u7684\u4e0d\u65ad\u589e\u957f\uff0c\u63d0\u9ad8\u4e86\u5bf9\u900f\u660e\u4e14\u53ef\u9760\u65b9\u6cd5\u7684\u9700\u6c42\uff0c\u4ee5\u4ece\u4e2d\u63d0\u53d6\u5c4f\u5e55\u4e0a\u7684\u4fe1\u606f\u3002\u7136\u800c\uff0c\u56fe\u5f62\u5e03\u5c40\u3001\u6392\u7248\u60ef\u4f8b\u548c\u5e73\u53f0\u7279\u5b9a\u8bbe\u8ba1\u6a21\u5f0f\u7684\u591a\u6837\u6027\uff0c\u4f7f\u5f97\u4eba\u5de5\u7d22\u5f15\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u548c\u63d0\u53d6\u5e7f\u64ad\u53ca\u793e\u4ea4\u5a92\u4f53\u539f\u751f\u65b0\u95fb\u89c6\u9891\u4e2d\u7684\u4e2a\u4eba\u59d3\u540d\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u7cbe\u5fc3\u6574\u7406\u4e14\u5747\u8861\u7684\u6807\u6ce8\u5e27\u8bed\u6599\u5e93\uff0c\u6355\u6349\u4e86\u5f53\u4ee3\u65b0\u95fb\u56fe\u5f62\u7684\u591a\u6837\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u6a21\u5757\u5316\u7684\u63d0\u53d6\u6d41\u6c34\u7ebf\uff0c\u53ef\u5728\u786e\u5b9a\u6027\u548c\u53ef\u5ba1\u8ba1\u7684\u6761\u4ef6\u4e0b\u8fd0\u884c\u3002\u8be5\u6d41\u6c34\u7ebf\u4e0e\u4e00\u7c7b\u751f\u6210\u5f0f\u591a\u6a21\u6001\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u786e\u5b9a\u6027\u53ef\u5ba1\u8ba1\u6027\u4e0e\u968f\u673a\u63a8\u7406\u4e4b\u95f4\u7684\u660e\u663e\u6743\u8861\u3002\u5e95\u5c42\u68c0\u6d4b\u5668\u5728\u56fe\u5f62\u5143\u7d20\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8695.8%\u7684mAP@0.5\uff0c\u5c55\u73b0\u51fa\u7a33\u5065\u7684\u5b9e\u7528\u6027\u80fd\u3002\u5c3d\u7ba1\u751f\u6210\u5f0f\u7cfb\u7edf\u5728\u539f\u59cb\u51c6\u786e\u7387\u4e0a\u7565\u9ad8\uff08F1\uff1a84.18% \u5bf9\u6bd4 77.08%\uff09\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u65b0\u95fb\u548c\u5206\u6790\u573a\u666f\u6240\u9700\u7684\u900f\u660e\u6570\u636e\u6eaf\u6e90\u80fd\u529b\u3002\u6240\u63d0\u51fa\u7684\u6d41\u6c34\u7ebf\u5728\u7cbe\u786e\u7387\uff0879.9%\uff09\u548c\u53ec\u56de\u7387\uff0874.4%\uff09\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u907f\u514d\u4e86\u5e7b\u89c9\uff0c\u5e76\u5728\u6bcf\u4e2a\u5904\u7406\u9636\u6bb5\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u8ffd\u6eaf\u6027\u3002\u8865\u5145\u7684\u7528\u6237\u8c03\u7814\u8868\u660e\uff0c59%\u7684\u53d7\u8bbf\u8005\u8868\u793a\u5728\u5feb\u8282\u594f\u7684\u65b0\u95fb\u64ad\u62a5\u4e2d\u96be\u4ee5\u770b\u6e05\u5c4f\u5e55\u4e0a\u7684\u59d3\u540d\uff0c\u51f8\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u5b9e\u9645\u610f\u4e49\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u73b0\u4ee3\u65b0\u95fb\u5a92\u4f53\u4e2d\u7684\u6df7\u5408\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b9\u6cd5\u4e25\u8c28\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7ebf\u3002"}}
{"id": "2602.09155", "pdf": "https://arxiv.org/pdf/2602.09155", "abs": "https://arxiv.org/abs/2602.09155", "authors": ["Ahmed Rahu", "Brian Shula", "Brandon Combs", "Aqsa Sultana", "Surendra P. Singh", "Vijayan K. Asari", "Derrick Forchetti"], "title": "Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages, 5 figures", "summary": "Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5206\u6790\u4f4e\u7ea7\u522b\u7ba1\u72b6\u817a\u7624\u7684\u5168\u5207\u7247\u56fe\u50cf\uff0c\u4ee5\u8bc6\u522b\u53ef\u9884\u6d4b\u60a3\u8005\u672a\u6765\u7f79\u60a3\u7ed3\u76f4\u80a0\u764c\u98ce\u9669\u7684\u7ec6\u5fae\u7ec4\u7ec7\u5b66\u7279\u5f81\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u7b5b\u67e5\u624b\u6bb5\u53ef\u964d\u4f4e\u7ed3\u76f4\u80a0\u764c\uff08CRC\uff09\u53d1\u75c5\u7387\uff0c\u4f46\u90e8\u5206\u6700\u521d\u88ab\u8bca\u65ad\u4e3a\u4f4e\u7ea7\u522b\u817a\u7624\u7684\u60a3\u8005\u4ecd\u4f1a\u5728\u65e5\u540e\u53d1\u5c55\u4e3aCRC\u3002\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u8bc6\u522b\u8fd9\u4e9b\u201c\u4f4e\u98ce\u9669\u201d\u60a3\u8005\u4e2d\u5b9e\u9645\u5177\u6709\u9ad8\u8fdb\u5c55\u98ce\u9669\u7684\u4e2a\u4f53\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u51c6\u76d1\u6d4b\u548c\u9884\u9632\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u4f4e\u7ea7\u522b\u7ba1\u72b6\u817a\u7624\u7684\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u5206\u6790\uff0c\u4ee5\u6316\u6398\u4f20\u7edf\u75c5\u7406\u5b66\u96be\u4ee5\u5bdf\u89c9\u7684\u3001\u4e0e\u6076\u6027\u8f6c\u5316\u76f8\u5173\u7684\u7ec6\u5fae\u7ec4\u7ec7\u7ed3\u6784\u6216\u7ec6\u80de\u5b66\u7279\u5f81\u3002", "result": "\u8bba\u6587\u5c1a\u672a\u62a5\u544a\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u6709\u671b\u4ece\u5e38\u89c4\u75c5\u7406\u56fe\u50cf\u4e2d\u63d0\u53d6\u51fa\u9884\u6d4bCRC\u957f\u671f\u98ce\u9669\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "\u7ed3\u5408\u6570\u5b57\u75c5\u7406\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u6709\u671b\u8d85\u8d8a\u4f20\u7edf\u7ec4\u7ec7\u5b66\u8bc4\u4f30\u7684\u5c40\u9650\uff0c\u4e3a\u4f4e\u7ea7\u522b\u817a\u7624\u60a3\u8005\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684\u98ce\u9669\u5206\u5c42\uff0c\u6307\u5bfc\u4e2a\u4f53\u5316\u968f\u8bbf\u548c\u5e72\u9884\u7b56\u7565\u3002", "summary_cn": "\u5c3d\u7ba1\u5df2\u5e7f\u6cdb\u5b9e\u65bd\u65e8\u5728\u68c0\u6d4b\u548c\u5207\u9664\u764c\u524d\u606f\u8089\u7684\u9884\u9632\u6027\u63aa\u65bd\uff0c\u7ed3\u76f4\u80a0\u764c\uff08CRC\uff09\u4ecd\u7136\u662f\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u7684\u91cd\u8981\u539f\u56e0\u3002\u867d\u7136\u7b5b\u67e5\u80fd\u6709\u6548\u964d\u4f4e\u53d1\u75c5\u7387\uff0c\u4f46\u76f8\u5f53\u4e00\u90e8\u5206\u6700\u521d\u88ab\u8bca\u65ad\u4e3a\u4f4e\u7ea7\u522b\u817a\u7624\u6027\u606f\u8089\u7684\u60a3\u8005\uff0c\u5373\u4f7f\u4e0d\u5b58\u5728\u5df2\u77e5\u7684\u9ad8\u5371\u7efc\u5408\u5f81\uff0c\u4ecd\u4f1a\u5728\u65e5\u540e\u53d1\u5c55\u4e3aCRC\u3002\u8bc6\u522b\u54ea\u4e9b\u4f4e\u98ce\u9669\u60a3\u8005\u5177\u6709\u66f4\u9ad8\u7684\u8fdb\u5c55\u98ce\u9669\uff0c\u662f\u5b9e\u73b0\u4e2a\u4f53\u5316\u76d1\u6d4b\u548c\u9884\u9632\u6027\u6cbb\u7597\u7b56\u7565\u7684\u5173\u952e\u672a\u6ee1\u8db3\u9700\u6c42\u3002\u4f20\u7edf\u7684\u817a\u7624\u7ec4\u7ec7\u5b66\u8bc4\u4f30\u867d\u4e3a\u57fa\u7840\uff0c\u4f46\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u9884\u793a\u6076\u6027\u6f5c\u80fd\u7684\u7ec6\u5fae\u7ed3\u6784\u6216\u7ec6\u80de\u5b66\u7279\u5f81\u3002\u6570\u5b57\u75c5\u7406\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7684\u8fdb\u6b65\u4e3a\u5168\u9762\u3001\u5ba2\u89c2\u5730\u5206\u6790\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u63d0\u4f9b\u4e86\u5951\u673a\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\uff0c\u662f\u5426\u80fd\u591f\u68c0\u6d4b\u4f4e\u7ea7\u522b\u7ba1\u72b6\u817a\u7624WSIs\u4e2d\u53ef\u9884\u6d4b\u60a3\u8005\u957f\u671fCRC\u98ce\u9669\u7684\u7ec6\u5fae\u7ec4\u7ec7\u5b66\u7279\u5f81\u3002"}}
{"id": "2602.09165", "pdf": "https://arxiv.org/pdf/2602.09165", "abs": "https://arxiv.org/abs/2602.09165", "authors": ["Hirunima Jayasekara", "Chuong Huynh", "Yixuan Ren", "Christabel Acquaye", "Abhinav Shrivastava"], "title": "All-in-One Conditioning for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u96f6\u6837\u672c\u6761\u4ef6\u673a\u5236\uff0c\u901a\u8fc7ASQL Conditioner\u5728\u63a8\u7406\u65f6\u751f\u6210\u8f6f\u89c6\u89c9\u5f15\u5bfc\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5bf9\u590d\u6742\u63d0\u793a\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0e\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u63d0\u793a\u65f6\uff0c\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\uff1b\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u5e03\u5c40\u56fe\uff0c\u9650\u5236\u4e86\u7ec4\u5408\u7075\u6d3b\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u96f6\u6837\u672c\u3001\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u6761\u4ef6\u673a\u5236\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u751f\u6210Attribute-Size-Quantity-Location\uff08ASQL\uff09\u89c6\u89c9\u6761\u4ef6\uff0c\u5e76\u7ed3\u5408\u6269\u6563\u6a21\u578b\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u8f6f\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u652f\u6301\u8f7b\u91cf\u3001\u4e00\u81f4\u4e14\u591a\u6837\u5316\u7684\u56fe\u50cf\u5408\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u590d\u6742\u63d0\u793a\u7684\u7406\u89e3\u4e0e\u53ef\u89c6\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u573a\u666f\u56fe\u7684ASQL\u6761\u4ef6\u673a\u5236\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u8bed\u4e49\u7ed3\u6784\u7684\u7ec4\u5408\u751f\u6210\u80fd\u529b\u3002", "summary_cn": "\u51c6\u786e\u7406\u89e3\u548c\u53ef\u89c6\u5316\u5305\u542b\u591a\u4e2a\u5bf9\u8c61\u3001\u5c5e\u6027\u53ca\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u63d0\u793a\u662f\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002\u5c3d\u7ba1\u8fd1\u671f\u5728\u751f\u6210\u903c\u771f\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6587\u672c\u8f93\u5165\u65f6\u4ecd\u5e38\u96be\u4ee5\u7ef4\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\u7f6e\u4e8e\u573a\u666f\u56fe\u7ed3\u6784\u6846\u67b6\u4e0b\uff0c\u4ee5\u589e\u5f3a\u73b0\u6709\u6a21\u578b\u7684\u7ec4\u5408\u80fd\u529b\u3002\u5c3d\u7ba1\u5148\u524d\u65b9\u6cd5\u5c1d\u8bd5\u901a\u8fc7\u4ece\u63d0\u793a\u4e2d\u63d0\u53d6\u7684\u9884\u5b9a\u4e49\u5e03\u5c40\u56fe\u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u6b64\u7c7b\u521a\u6027\u7ea6\u675f\u5f80\u5f80\u9650\u5236\u4e86\u7ec4\u5408\u7684\u7075\u6d3b\u6027\u4e0e\u591a\u6837\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u3001\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u6761\u4ef6\u673a\u5236\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u8f6f\u6027\u89c6\u89c9\u5f15\u5bfc\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u201c\u5c5e\u6027-\u5c3a\u5bf8-\u6570\u91cf-\u4f4d\u7f6e\u201d\uff08ASQL\uff09\u6761\u4ef6\u6a21\u5757\uff0c\u5b83\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u751f\u6210\u89c6\u89c9\u6761\u4ef6\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u4f18\u5316\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u5728\u4fdd\u6301\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5b9e\u73b0\u8f7b\u91cf\u3001\u4e00\u81f4\u4e14\u591a\u6837\u5316\u7684\u56fe\u50cf\u5408\u6210\u3002"}}
{"id": "2602.09209", "pdf": "https://arxiv.org/pdf/2602.09209", "abs": "https://arxiv.org/abs/2602.09209", "authors": ["Michael D. Murray", "James Tung", "Richard W. Nuckols"], "title": "Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain", "categories": ["cs.CV"], "comment": "19 pages excluding references and comments, 5 figures, 3 tables", "summary": "Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7CNN-RNN\u6a21\u578b\uff0c\u5229\u7528RGB-D\u76f8\u673a\u89c6\u89c9\u6570\u636e\u5728\u811a\u89e6\u5730\u524d250ms\u5185\u9884\u6d4b\u8db3\u5e95\u524d\u540e\u65b9\u5411\u7684\u538b\u529b\u4e2d\u5fc3\uff08COP\uff09\u548c\u89e6\u5730\u65f6\u95f4\uff08TOI\uff09\uff0c\u5728\u697c\u68af\u4e0a\u884c\u8fc7\u6e21\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u5e76\u53ef\u5728\u6d88\u8d39\u7ea7\u8bbe\u5907\u4e0a\u5b9e\u65f6\u8fd0\u884c\uff0860 FPS\uff09\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u73af\u5883\u5206\u7c7b\u65b9\u6cd5\u867d\u5e38\u7528\u4e8e\u8f85\u52a9\u7cfb\u7edf\u63a7\u5236\uff0c\u4f46\u5bf9\u8db3\u90e8\u5982\u4f55\u4e0e\u53d8\u5316\u73af\u5883\u63a5\u89e6\u7684\u9884\u6d4b\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u89c6\u89c9\u4fe1\u606f\u63d0\u524d\u9884\u6d4b\u8db3\u5e95\u63a5\u89e6\u7279\u6027\uff0c\u4e3a\u8f85\u52a9\u7cfb\u7edf\u7684\u524d\u77bb\u6027\u63a7\u5236\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u53d7\u8bd5\u8005\u4f69\u6234RGB-D\u76f8\u673a\uff08\u7f6e\u4e8e\u53f3\u5c0f\u817f\uff09\u548c\u4eea\u5668\u5316\u978b\u57ab\uff0c\u5728\u5e73\u5730\u5230\u697c\u68af\u4e0a\u884c\u8fc7\u6e21\u8fc7\u7a0b\u4e2d\u884c\u8d70\u3002\u7814\u7a76\u56e2\u961f\u8bad\u7ec3\u4e86\u4e00\u4e2aCNN-RNN\u6a21\u578b\uff0c\u5728\u811a\u89e6\u5730\u524d250ms\u7684\u65f6\u95f4\u7a97\u53e3\u5185\u8fde\u7eed\u9884\u6d4b\u524d\u540e\u65b9\u5411\u7684\u8db3\u5e95\u538b\u529b\u4e2d\u5fc3\uff08COP\uff09\u548c\u89e6\u5730\u65f6\u95f4\uff08TOI\uff09\u3002", "result": "\u5728150\u3001100\u548c50ms\u7684\u9884\u6d4b\u65f6\u57df\u4e0b\uff0cCOP\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a29.42mm\u300126.82mm\u548c23.72mm\uff1bTOI\u7684\u8bef\u5dee\u5206\u522b\u4e3a21.14ms\u300120.08ms\u548c17.73ms\u3002\u66f4\u5feb\u7684\u811a\u8dbe\u6446\u52a8\u901f\u5ea6\u53ef\u63d0\u9ad8COP\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5bf9TOI\u65e0\u663e\u8457\u5f71\u54cd\uff1b\u66f4\u9760\u524d\u7684\u843d\u811a\u4f4d\u7f6e\u4f1a\u964d\u4f4eCOP\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u4e0d\u5f71\u54cdTOI\u3002\u6240\u63d0\u8f7b\u91cf\u6a21\u578b\u53ef\u5728\u7b14\u8bb0\u672c\u6216\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4ee560 FPS\u8fd0\u884c\u3002", "conclusion": "\u5229\u7528\u89c6\u89c9\u6570\u636e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u9884\u6d4bCOP\u548cTOI\u662f\u53ef\u884c\u7684\uff0c\u8be5\u65b9\u6cd5\u6709\u671b\u7528\u4e8e\u63d0\u5347\u8f85\u52a9\u7cfb\u7edf\u7684\u524d\u77bb\u6027\u63a7\u5236\u80fd\u529b\u3002", "summary_cn": "\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CV\uff09\u5df2\u88ab\u7528\u4e8e\u6b65\u6001\u8fc7\u7a0b\u4e2d\u7684\u73af\u5883\u5206\u7c7b\uff0c\u5e76\u5e38\u7528\u4e8e\u8f85\u52a9\u7cfb\u7edf\u7684\u63a7\u5236\u51b3\u7b56\uff1b\u7136\u800c\uff0c\u9884\u6d4b\u8db3\u90e8\u5982\u4f55\u4e0e\u53d8\u5316\u73af\u5883\u63a5\u89e6\u7684\u80fd\u529b\u4ecd\u9c9c\u6709\u7814\u7a76\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5728\u4ece\u5e73\u5730\u8fc7\u6e21\u5230\u4e0a\u697c\u68af\u8fc7\u7a0b\u4e2d\uff0c\u4e8e\u811a\u89e6\u5730\u524d\u9884\u6d4b\u524d\u540e\u65b9\u5411\uff08AP\uff09\u8db3\u5e95\u538b\u529b\u4e2d\u5fc3\uff08COP\uff09\u548c\u89e6\u5730\u65f6\u95f4\uff08TOI\uff09\u7684\u53ef\u884c\u6027\u3002\u516b\u540d\u53d7\u8bd5\u8005\u5728\u53f3\u5c0f\u817f\u4f69\u6234RGB-D\u76f8\u673a\u5e76\u7a7f\u7740\u4eea\u5668\u5316\u978b\u57ab\uff0c\u6267\u884c\u8e0f\u4e0a\u697c\u68af\u7684\u4efb\u52a1\u3002\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2aCNN-RNN\u6a21\u578b\uff0c\u5728\u811a\u89e6\u5730\u524d250\u6beb\u79d2\u7684\u65f6\u95f4\u7a97\u53e3\uff08\u79f0\u4e3a\u9884\u6d4b\u65f6\u57df\uff0cFH\uff09\u5185\u8fde\u7eed\u9884\u6d4bCOP\u548cTOI\u3002\u5728150\u3001100\u548c50\u6beb\u79d2\u7684FH\u4e0b\uff0cCOP\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u5206\u522b\u4e3a29.42\u6beb\u7c73\u300126.82\u6beb\u7c73\u548c23.72\u6beb\u7c73\uff1bTOI\u7684MAE\u5219\u5206\u522b\u4e3a21.14\u6beb\u79d2\u300120.08\u6beb\u79d2\u548c17.73\u6beb\u79d2\u3002\u8eaf\u5e72\u901f\u5ea6\u5bf9\u4e24\u9879\u4efb\u52a1\u7684\u9884\u6d4b\u8bef\u5dee\u5747\u65e0\u5f71\u54cd\uff1b\u800c\u811a\u89e6\u5730\u524d\u66f4\u5feb\u7684\u811a\u8dbe\u6446\u52a8\u901f\u5ea6\u53ef\u63d0\u9ad8COP\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5bf9TOI\u9884\u6d4b\u65e0\u663e\u8457\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u66f4\u9760\u524d\u7684\u843d\u811a\u4f4d\u7f6e\u4f1a\u964d\u4f4eCOP\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u4e0d\u5f71\u54cdTOI\u9884\u6d4b\u7cbe\u5ea6\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u53ef\u5728\u6d88\u8d39\u7ea7\u7b14\u8bb0\u672c\u7535\u8111\u6216\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u4e0a\u4ee560\u5e27\u6bcf\u79d2\uff08FPS\uff09\u7684\u901f\u5ea6\u8fd0\u884c\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u89c6\u89c9\u6570\u636e\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u9884\u6d4bCOP\u548cTOI\u662f\u53ef\u884c\u7684\uff0c\u8fd9\u5bf9\u8f85\u52a9\u7cfb\u7edf\u4e2d\u7684\u524d\u77bb\u6027\u63a7\u5236\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2602.09214", "pdf": "https://arxiv.org/pdf/2602.09214", "abs": "https://arxiv.org/abs/2602.09214", "authors": ["Chenyu Wang", "Tianle Chen", "H. M. Sabbir Ahmad", "Kayhan Batmanghelich", "Wenchao Li"], "title": "VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VLM-UQBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u6a21\u6001\u7279\u5f02\u6027\u548c\u8de8\u6a21\u6001\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u7684\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u6270\u52a8\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u3001\u6a21\u6001\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5b89\u5168\u53ef\u9760\u8fd0\u884c\uff0c\u9700\u8981\u5bf9\u5176\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u91cf\u5316\uff08UQ\uff09\uff0c\u4f46\u5f53\u524d\u7f3a\u4e4f\u80fd\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff08\u56fe\u50cf\u3001\u6587\u672c\u6216\u4e24\u8005\u5bf9\u9f50\u95ee\u9898\uff09\u7684\u6709\u6548\u65b9\u6cd5\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86VLM-UQBench\u57fa\u51c6\uff0c\u5305\u542b600\u4e2a\u6765\u81eaVizWiz\u7684\u771f\u5b9e\u6837\u672c\uff0c\u5212\u5206\u4e3a\u5e72\u51c0\u3001\u56fe\u50cf\u4e0d\u786e\u5b9a\u6027\u3001\u6587\u672c\u4e0d\u786e\u5b9a\u6027\u548c\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u5b50\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b8\u79cd\u89c6\u89c9\u30015\u79cd\u6587\u672c\u548c3\u79cd\u8de8\u6a21\u6001\u6270\u52a8\u7684\u53ef\u6269\u5c55\u6270\u52a8\u6d41\u7a0b\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cfUQ\u5206\u6570\u5bf9\u6270\u52a8\u7684\u654f\u611f\u6027\u53ca\u5176\u4e0e\u5e7b\u89c9\u7684\u76f8\u5173\u6027\uff0c\u5e76\u57284\u4e2aVLM\u548c3\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u591a\u79cdUQ\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(i) \u73b0\u6709UQ\u65b9\u6cd5\u5177\u6709\u5f3a\u70c8\u7684\u6a21\u6001\u7279\u5f02\u6027\u4e14\u9ad8\u5ea6\u4f9d\u8d56\u5e95\u5c42VLM\uff1b(ii) \u6a21\u6001\u7279\u5f02\u6027\u4e0d\u786e\u5b9a\u6027\u5e38\u4f34\u968f\u5e7b\u89c9\uff0c\u4f46\u5f53\u524dUQ\u5206\u6570\u4ec5\u63d0\u4f9b\u5fae\u5f31\u4e14\u4e0d\u4e00\u81f4\u7684\u98ce\u9669\u4fe1\u53f7\uff1b(iii) UQ\u65b9\u6cd5\u867d\u80fd\u5728\u660e\u663e\u3001\u7fa4\u4f53\u5c42\u9762\u7684\u6a21\u7cca\u6027\u4e0a\u5ab2\u7f8e\u601d\u7ef4\u94fe\u57fa\u7ebf\uff0c\u5374\u96be\u4ee5\u68c0\u6d4b\u6270\u52a8\u5f15\u5165\u7684\u7ec6\u5fae\u3001\u5b9e\u4f8b\u7ea7\u6a21\u7cca\u6027\u3002", "conclusion": "\u5f53\u524dUQ\u5b9e\u8df5\u4e0e\u53ef\u9760\u90e8\u7f72VLM\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u3001\u6a21\u6001\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e9f\u9700\u53d1\u5c55\u66f4\u7cbe\u7ec6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "summary_cn": "\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u5bf9\u4e8e\u786e\u4fdd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b89\u5168\u53ef\u9760\u5730\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5c06\u4e0d\u786e\u5b9a\u6027\u5b9a\u4f4d\u5230\u5176\u6765\u6e90\uff0c\u5373\u5224\u65ad\u4e0d\u786e\u5b9a\u6027\u662f\u6e90\u4e8e\u56fe\u50cf\u3001\u6587\u672c\uff0c\u8fd8\u662f\u4e24\u8005\u4e4b\u95f4\u7684\u9519\u4f4d\u3002\u6211\u4eec\u63d0\u51fa\u4e86VLM-UQBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30VLM\u4e2d\u6a21\u6001\u7279\u5f02\u6027\u548c\u8de8\u6a21\u6001\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u7684\u57fa\u51c6\u3002\u5b83\u5305\u542b\u4eceVizWiz\u6570\u636e\u96c6\u4e2d\u62bd\u53d6\u7684600\u4e2a\u771f\u5b9e\u4e16\u754c\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u88ab\u6574\u7406\u4e3a\u5e72\u51c0\u3001\u56fe\u50cf\u4e0d\u786e\u5b9a\u6027\u3001\u6587\u672c\u4e0d\u786e\u5b9a\u6027\u548c\u8de8\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u5b50\u96c6\uff0c\u5e76\u914d\u5907\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6270\u52a8\u6d41\u7a0b\uff0c\u5305\u542b8\u79cd\u89c6\u89c9\u30015\u79cd\u6587\u672c\u548c3\u79cd\u8de8\u6a21\u6001\u6270\u52a8\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e24\u4e2a\u7b80\u5355\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316UQ\u5206\u6570\u5bf9\u8fd9\u4e9b\u6270\u52a8\u7684\u654f\u611f\u6027\u53ca\u5176\u4e0e\u5e7b\u89c9\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u5728\u56db\u4e2aVLM\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217UQ\u65b9\u6cd5\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff1a(i) \u73b0\u6709\u7684UQ\u65b9\u6cd5\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u6a21\u6001\u7279\u5f02\u6027\u4e13\u4e1a\u5316\uff0c\u5e76\u4e14\u4e25\u91cd\u4f9d\u8d56\u4e8e\u5e95\u5c42VLM\uff1b(ii) \u6a21\u6001\u7279\u5f02\u6027\u4e0d\u786e\u5b9a\u6027\u7ecf\u5e38\u4e0e\u5e7b\u89c9\u5171\u5b58\uff0c\u800c\u5f53\u524d\u7684UQ\u5206\u6570\u4ec5\u63d0\u4f9b\u5fae\u5f31\u4e14\u4e0d\u4e00\u81f4\u7684\u98ce\u9669\u4fe1\u53f7\uff1b(iii) \u5c3d\u7ba1UQ\u65b9\u6cd5\u5728\u660e\u663e\u7684\u3001\u7fa4\u4f53\u5c42\u9762\u7684\u6a21\u7cca\u6027\u4e0a\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u63a8\u7406\u7684\u601d\u7ef4\u94fe\u57fa\u7ebf\u76f8\u5ab2\u7f8e\uff0c\u4f46\u5b83\u4eec\u5728\u68c0\u6d4b\u7531\u6211\u4eec\u7684\u6270\u52a8\u6d41\u7a0b\u5f15\u5165\u7684\u7ec6\u5fae\u3001\u5b9e\u4f8b\u5c42\u9762\u7684\u6a21\u7cca\u6027\u65b9\u9762\u57fa\u672c\u5931\u8d25\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u5f53\u524dUQ\u5b9e\u8df5\u4e0e\u53ef\u9760\u90e8\u7f72VLM\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u3001\u6a21\u6001\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2602.09252", "pdf": "https://arxiv.org/pdf/2602.09252", "abs": "https://arxiv.org/abs/2602.09252", "authors": ["Ange Lou", "Yamin Li", "Qi Chang", "Nan Xi", "Luyuan Xie", "Zichao Li", "Tianyu Luan"], "title": "VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": null, "summary": "Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.", "AI": {"tldr": "\u63d0\u51faIR-SIS\uff1a\u9996\u4e2a\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3001\u5177\u5907\u81ea\u9002\u5e94\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\u7684\u624b\u672f\u56fe\u50cf\u5206\u5272\u7cfb\u7edf\uff0c\u5728\u57df\u5185\u548c\u57df\u5916\u6570\u636e\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\u3001\u7f3a\u4e4f\u81ea\u9002\u5e94\u4f18\u5316\u673a\u5236\uff0c\u4e14\u65e0\u6cd5\u4e0e\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u4ea4\u4e92\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u624b\u672f\u573a\u666f\u9700\u6c42\u3002", "method": "IR-SIS\u7cfb\u7edf\u7ed3\u5408\u5fae\u8c03\u540e\u7684SAM3\u8fdb\u884c\u521d\u59cb\u5206\u5272\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u5668\u68b0\u5e76\u8bc4\u4f30\u5206\u5272\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u9002\u5e94\u9009\u62e9\u4f18\u5316\u7b56\u7565\uff1b\u540c\u65f6\u652f\u6301\u533b\u751f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u53c2\u4e0e\u5206\u5272\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u57fa\u4e8eEndoVis2017/2018\u7684\u591a\u7c92\u5ea6\u8bed\u8a00\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u5728\u57df\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u5747\u53d6\u5f97\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u5f15\u5165\u4e34\u5e8a\u533b\u751f\u4ea4\u4e92\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u4e8e\u8bed\u8a00\u3001\u5177\u5907\u81ea\u9002\u5e94\u81ea\u6211\u4f18\u5316\u80fd\u529b\u7684\u624b\u672f\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u4e3a\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u548c\u672f\u4e2d\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "summary_cn": "\u624b\u672f\u56fe\u50cf\u5206\u5272\u5bf9\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u548c\u672f\u4e2d\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u4ec5\u63d0\u4f9b\u4e00\u6b21\u6027\u9884\u6d4b\u800c\u7f3a\u4e4f\u81ea\u9002\u5e94\u4f18\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u7f3a\u5c11\u4e0e\u4e34\u5e8a\u533b\u751f\u4ea4\u4e92\u7684\u673a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86IR-SIS\u2014\u2014\u4e00\u79cd\u652f\u6301\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u624b\u672f\u56fe\u50cf\u5206\u5272\u8fed\u4ee3\u4f18\u5316\u7cfb\u7edf\u3002IR-SIS\u5229\u7528\u5fae\u8c03\u540e\u7684SAM3\u8fdb\u884c\u521d\u59cb\u5206\u5272\uff0c\u91c7\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u624b\u672f\u5668\u68b0\u5e76\u8bc4\u4f30\u5206\u5272\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u9002\u5e94\u5730\u9009\u62e9\u4f18\u5316\u7b56\u7565\u3002\u8be5\u7cfb\u7edf\u652f\u6301\u4e34\u5e8a\u533b\u751f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u53c2\u4e0e\u5206\u5272\u8fc7\u7a0b\u3002\u6211\u4eec\u8fd8\u57fa\u4e8eEndoVis2017\u548cEndoVis2018\u57fa\u51c6\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u7c92\u5ea6\u8bed\u8a00\u6807\u6ce8\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57df\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e14\u4e34\u5e8a\u533b\u751f\u7684\u4ea4\u4e92\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5206\u5272\u6548\u679c\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u4e8e\u8bed\u8a00\u3001\u5177\u5907\u81ea\u9002\u5e94\u81ea\u6211\u4f18\u5316\u80fd\u529b\u7684\u624b\u672f\u56fe\u50cf\u5206\u5272\u6846\u67b6\u3002"}}
{"id": "2602.09268", "pdf": "https://arxiv.org/pdf/2602.09268", "abs": "https://arxiv.org/abs/2602.09268", "authors": ["Nikita Starodubcev", "Daniil Pakhomov", "Zongze Wu", "Ilya Drobyshevskiy", "Yuchen Liu", "Zhonghao Wang", "Yuqian Zhou", "Zhe Lin", "Dmitry Baranchuk"], "title": "Rethinking Global Text Conditioning in Diffusion Transformers", "categories": ["cs.CV"], "comment": "Accepted at ICLR26", "summary": "Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u6269\u6563Transformer\u4e2d\u57fa\u4e8e\u8c03\u5236\uff08modulation\uff09\u7684\u6587\u672c\u6761\u4ef6\u662f\u5426\u5fc5\u8981\uff0c\u53d1\u73b0\u4f20\u7edf\u7528\u6cd5\u4e0b\u5176\u8d21\u732e\u6709\u9650\uff0c\u4f46\u82e5\u5c06\u5176\u4f5c\u4e3a\u5f15\u5bfc\u4fe1\u53f7\u4ee5\u53ef\u63a7\u65b9\u5f0f\u8c03\u6574\u751f\u6210\u7ed3\u679c\uff0c\u5219\u53ef\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u591a\u79cd\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563Transformer\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u57fa\u4e8e\u6c60\u5316\u6587\u672c\u5d4c\u5165\u7684\u8c03\u5236\u673a\u5236\u5f15\u5165\u6587\u672c\u4fe1\u606f\uff0c\u4f46\u8fd1\u671f\u65b9\u6cd5\u503e\u5411\u4e8e\u4ec5\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u8c03\u5236\u673a\u5236\u662f\u5426\u4ecd\u6709\u5fc5\u8981\uff0c\u4ee5\u53ca\u662f\u5426\u80fd\u5e26\u6765\u6027\u80fd\u4f18\u52bf\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86\u4f20\u7edf\u8c03\u5236\u673a\u5236\u4e2d\u6c60\u5316\u6587\u672c\u5d4c\u5165\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u89c6\u89d2\uff1a\u5c06\u6c60\u5316\u5d4c\u5165\u7528\u4f5c\u5f15\u5bfc\u4fe1\u53f7\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u751f\u6210\u7ed3\u679c\u7684\u53ef\u63a7\u8c03\u6574\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u6613\u4e8e\u5b9e\u73b0\u4e14\u8fd0\u884c\u5f00\u9500\u6781\u5c0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4f20\u7edf\u7528\u6cd5\u4e0b\u6c60\u5316\u5d4c\u5165\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u9650\uff1b\u4f46\u4f5c\u4e3a\u5f15\u5bfc\u4fe1\u53f7\u4f7f\u7528\u65f6\uff0c\u53ef\u5728\u6587\u672c\u5230\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8c03\u5236\u673a\u5236\u5728\u4f20\u7edf\u5f62\u5f0f\u4e0b\u5e76\u975e\u5fc5\u8981\uff0c\u4f46\u82e5\u91cd\u65b0\u8bbe\u8ba1\u5176\u7528\u9014\uff08\u5982\u4f5c\u4e3a\u5f15\u5bfc\uff09\uff0c\u5219\u53ef\u6709\u6548\u589e\u5f3a\u6269\u6563\u6a21\u578b\u7684\u53ef\u63a7\u6027\u4e0e\u6027\u80fd\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002", "summary_cn": "\u6269\u6563Transformer\u901a\u5e38\u901a\u8fc7\u6ce8\u610f\u529b\u5c42\u548c\u4f7f\u7528\u6c60\u5316\u6587\u672c\u5d4c\u5165\u7684\u8c03\u5236\u673a\u5236\u6765\u878d\u5408\u6587\u672c\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd1\u671f\u65b9\u6cd5\u6452\u5f03\u4e86\u57fa\u4e8e\u8c03\u5236\u7684\u6587\u672c\u6761\u4ef6\uff0c\u4ec5\u4f9d\u8d56\u6ce8\u610f\u529b\u673a\u5236\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u8c03\u5236\u7684\u6587\u672c\u6761\u4ef6\u662f\u5426\u5fc5\u8981\uff0c\u4ee5\u53ca\u662f\u5426\u80fd\u5e26\u6765\u6027\u80fd\u4f18\u52bf\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5728\u5176\u4f20\u7edf\u7528\u6cd5\u4e2d\uff0c\u6c60\u5316\u5d4c\u5165\u5bf9\u6574\u4f53\u6027\u80fd\u8d21\u732e\u751a\u5fae\uff0c\u8bf4\u660e\u4ec5\u9760\u6ce8\u610f\u529b\u673a\u5236\u901a\u5e38\u5df2\u8db3\u4ee5\u51c6\u786e\u4f20\u9012\u63d0\u793a\u4fe1\u606f\u3002\u7136\u800c\uff0c\u6211\u4eec\u53d1\u73b0\u82e5\u4ece\u4e0d\u540c\u89c6\u89d2\u4f7f\u7528\u6c60\u5316\u5d4c\u5165\u2014\u2014\u5c06\u5176\u4f5c\u4e3a\u5f15\u5bfc\u4fe1\u53f7\uff0c\u5b9e\u73b0\u5411\u66f4\u7406\u60f3\u5c5e\u6027\u7684\u53ef\u63a7\u504f\u79fb\u2014\u2014\u5219\u53ef\u5e26\u6765\u663e\u8457\u589e\u76ca\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u3001\u5b9e\u73b0\u7b80\u5355\u3001\u8fd0\u884c\u65f6\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\uff0c\u5728\u5305\u62ec\u6587\u672c\u5230\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u548c\u56fe\u50cf\u7f16\u8f91\u5728\u5185\u7684\u591a\u79cd\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u6539\u8fdb\u3002"}}
