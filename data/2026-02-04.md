<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models](https://arxiv.org/abs/2602.02537)
*Runjie Zhou,Youbo Shao,Haoyu Lu,Bowei Xing,Tongtong Bai,Yujie Chen,Jie Zhao,Lin Sui,Haotian Yao,Zijia Zhao,Hao Yang,Haoning Wu,Zaida Zhou,Jinguo Zhu,Zhiqi Huang,Yiping Bao,Yangyang Liu,Y. Charles,Xinyu Zhou*

Main category: cs.CV

TL;DR: WorldVQA is a new benchmark that isolates and evaluates the memorized visual world knowledge of Multimodal Large Language Models (MLLMs), focusing on their ability to correctly name and ground visual entities without conflating it with reasoning.


<details>
  <summary>Details</summary>
Motivation: Current evaluations often mix visual knowledge retrieval with reasoning, making it hard to assess what visual facts MLLMs actually memorize. WorldVQA addresses this by decoupling these aspects to measure pure visual factual knowledge.

Method: WorldVQA constructs a stratified taxonomy of visual entities—from common to rare—and tests MLLMs on their atomic capability to ground and name these entities, thereby measuring memorized visual knowledge directly.

Result: The benchmark enables rigorous evaluation of visual factuality, providing insights into the encyclopedic breadth and hallucination rates of MLLMs.

Conclusion: WorldVQA establishes a standard for assessing the factual visual knowledge and reliability of current and future MLLMs by focusing on memorized visual world knowledge rather than reasoning performance.

Abstract: We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.

Abstract (中文翻译): 我们提出了WorldVQA，这是一个用于评估多模态大语言模型（MLLMs）原子级视觉世界知识的基准。与当前常将视觉知识检索与推理混为一谈的评估方法不同，WorldVQA将这两种能力解耦，以严格衡量“模型记住了什么”。该基准通过一个分层分类体系评估模型在命名和定位视觉实体方面的原子能力，涵盖从常见头部类别物体到长尾稀有物体的广泛范围。我们期望WorldVQA能成为视觉事实性的严格测试标准，从而为评估当前及下一代前沿模型的百科全书式知识广度和幻觉率奠定基础。

</details>


### [2] [AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process](https://arxiv.org/abs/2602.02676)
*Xintong Zhang,Xiaowen Zhang,Jongrong Wu,Zhi Gao,Shilin Yan,Zhenxin Diao,Kunpeng Gao,Xuanyan Chen,Yuwei Wu,Yunde Jia,Qing Li*

Main category: cs.CV

TL;DR: 本文提出了AdaptMMBench，一个用于评估视觉语言模型（VLMs）自适应多模态推理能力的新基准，通过动态难度识别和多维过程分析，揭示了自适应模式选择与最终准确率之间的解耦关系。


<details>
  <summary>Details</summary>
Motivation: 现有对自适应多模态推理的评估依赖静态难度标签和简单指标，无法反映任务难度随模型能力变化的动态特性，且混淆了自适应模式选择能力与整体性能，缺乏对推理过程的细粒度分析。

Method: 提出AdaptMMBench基准，涵盖五个领域（现实世界、OCR、GUI、知识、数学），采用基于模型能力边界的动态难度识别机制，并引入Matthews相关系数（MCC）评估推理模式选择的合理性；同时支持对关键步骤覆盖率、工具有效性及计算效率的多维过程评估。

Result: 评估发现：自适应模式选择能力随模型容量提升而增强，但与最终准确率明显解耦；关键步骤覆盖率与性能一致，而工具有效性在不同模型架构间差异显著。

Conclusion: AdaptMMBench能更精准地评估VLMs的自适应推理能力，揭示现有方法在模式选择与性能之间的脱节，为未来多模态系统设计提供新视角。

Abstract: Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.

Abstract (中文翻译): 自适应多模态推理已成为视觉语言模型（VLMs）中一个前景广阔的研究方向，旨在动态调节工具增强的视觉推理与文本推理，以提升效果与效率。然而，现有评估方法依赖静态难度标签和简化的指标，无法捕捉任务难度相对于不同模型能力的动态特性，从而模糊了自适应模式选择与整体性能之间的区别，并忽视了对推理过程的细粒度分析。本文提出AdaptMMBench——一个面向自适应多模态推理的综合性基准，覆盖现实世界、OCR、GUI、知识和数学五大领域，包含直接感知与复杂推理任务。AdaptMMBench采用Matthews相关系数（MCC）来评估不同推理模式选择的合理性，通过基于模型能力边界动态识别任务难度，从而将这种元认知能力独立出来。此外，该基准还支持在关键步骤覆盖率、工具有效性及计算效率等多个维度上进行过程评估。我们的评估表明，尽管自适应模式选择能力随模型容量增长而提升，但它与最终准确率显著解耦；相反，关键步骤覆盖率与模型性能一致，而工具有效性在不同模型架构之间仍高度不一致。

</details>


### [3] [End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning](https://arxiv.org/abs/2602.02721)
*Jinglun Yu,Yaning Wang,Wenhan Guo,Yuan Gao,Yu Sun,Jin U. Kang*

Main category: cs.CV

TL;DR: 本文提出了一种结合物理模型与深度学习的端到端正则化框架，用于OCT中的逆散射问题，可同时重建组织光学参数图和去斑点的结构图像。


<details>
  <summary>Details</summary>
Motivation: OCT中的逆散射问题因衰减、斑点噪声及参数间强耦合而极具挑战性，现有方法难以同时准确恢复结构图像和内在光学特性。

Method: 提出一个端到端深度学习框架，结合基于物理的OCT前向模型，并利用蒙特卡洛模拟数据进行训练，通过物理一致的监督信号联合重建光学参数图和去噪结构图像。

Result: 在合成角膜OCT数据集上的实验表明，该方法在噪声条件下仍能稳健恢复光学参数图，提升分辨率并增强结构保真度。

Conclusion: 该方法实现了定量多参数组织表征，展示了将物理信息建模与深度学习相结合在计算OCT中的优势。

Abstract: Inverse scattering in optical coherence tomography (OCT) seeks to recover both structural images and intrinsic tissue optical properties, including refractive index, scattering coefficient, and anisotropy. This inverse problem is challenging due to attenuation, speckle noise, and strong coupling among parameters. We propose a regularized end-to-end deep learning framework that jointly reconstructs optical parameter maps and speckle-reduced OCT structural intensity for layer visualization. Trained with Monte Carlo-simulated ground truth, our network incorporates a physics-based OCT forward model that generates predicted signals from the estimated parameters, providing physics-consistent supervision for parameter recovery and artifact suppression. Experiments on the synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity. This approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.

Abstract (中文翻译): 光学相干断层扫描（OCT）中的逆散射旨在同时恢复组织的结构图像和其固有光学特性，包括折射率、散射系数和各向异性。由于衰减、斑点噪声以及参数间的强耦合，这一逆问题极具挑战性。我们提出了一种正则化的端到端深度学习框架，能够联合重建光学参数图和经过去斑点处理的OCT结构强度图像，以实现分层可视化。该网络利用蒙特卡洛模拟生成的真实数据进行训练，并嵌入了一个基于物理的OCT前向模型，该模型根据估计的参数生成预测信号，从而为参数恢复和伪影抑制提供物理一致性的监督。在合成角膜OCT数据集上的实验表明，该方法在噪声环境下仍能稳健地恢复光学参数图，提高分辨率并增强结构保真度。该方法实现了对组织的定量多参数表征，并突显了将物理信息建模与深度学习相结合在计算OCT中的优势。

</details>


### [4] [SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?](https://arxiv.org/abs/2602.02765)
*Haruhiko Murata,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: SVD-ViT uses singular value decomposition to enhance foreground feature learning in Vision Transformers, improving classification accuracy by suppressing background noise.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers lack an explicit mechanism to distinguish foreground from background due to global self-attention, which can lead to learning irrelevant background features and degraded performance.

Method: The proposed SVD-ViT integrates three components—SPC module, SSVA, and ID-RSVD—to extract and aggregate singular vectors that capture foreground information, thereby suppressing background noise and artifacts.

Result: Experiments show that SVD-ViT improves classification accuracy and effectively learns informative foreground representations while reducing background interference.

Conclusion: Incorporating SVD into ViT enables better foreground-background separation, leading to more robust and accurate visual classification.

Abstract: Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.

Abstract (中文翻译): 视觉Transformer（ViT）已被确立为大规模基础模型。然而，由于自注意力机制是全局操作的，它们缺乏显式区分前景与背景的机制，可能导致学习到不必要的背景特征和伪影，从而降低分类性能。为解决这一问题，我们提出了SVD-ViT，该方法利用奇异值分解（SVD）来优先学习前景特征。SVD-ViT包含三个组件——SPC模块、SSVA和ID-RSVD，通过提取并聚合能够捕捉物体前景信息的奇异向量，抑制背景噪声和伪影等与任务无关的因素。实验结果表明，该方法提升了分类准确率，有效学习了具有信息量的前景表征，同时减少了背景噪声的影响。

</details>


### [5] [LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds](https://arxiv.org/abs/2602.02808)
*Matteo Bastico,Pierre Onghena,David Ryckelynck,Beatriz Marcotegui,Santiago Velasco-Forero,Laurent Corté,Caroline Robine--Decourcelle,Etienne Decencière*

Main category: cs.CV

TL;DR: 提出了一种名为Landmark Point Transformer（LmPT）的新方法，用于在点云上自动检测解剖标志点，支持跨物种学习，并在人类和狗的股骨数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统手动标记解剖标志点耗时且存在观察者间差异，而基于规则的方法通常仅适用于特定几何结构或有限的标志点集合。为克服这些局限性，作者希望开发一种通用、自动化的解剖标志点检测方法，尤其适用于跨物种研究。

Method: 将解剖表面表示为点云，并提出LmPT模型，该模型包含一种条件机制，使其能够适应不同输入类型，从而实现跨物种的解剖标志点自动检测。

Result: 在人类和新标注的狗股骨数据集上评估了LmPT，结果表明该方法在不同物种间具有良好的泛化能力和检测效果。

Conclusion: LmPT是一种有效的跨物种解剖标志点自动检测方法，具备良好的适应性和泛化能力，代码和狗股骨数据集将公开发布以促进后续研究。

Abstract: Accurate identification of anatomical landmarks is crucial for various medical applications. Traditional manual landmarking is time-consuming and prone to inter-observer variability, while rule-based methods are often tailored to specific geometries or limited sets of landmarks. In recent years, anatomical surfaces have been effectively represented as point clouds, which are lightweight structures composed of spatial coordinates. Following this strategy and to overcome the limitations of existing landmarking techniques, we propose Landmark Point Transformer (LmPT), a method for automatic anatomical landmark detection on point clouds that can leverage homologous bones from different species for translational research. The LmPT model incorporates a conditioning mechanism that enables adaptability to different input types to conduct cross-species learning. We focus the evaluation of our approach on femoral landmarking using both human and newly annotated dog femurs, demonstrating its generalization and effectiveness across species. The code and dog femur dataset will be publicly available at: https://github.com/Pierreoo/LandmarkPointTransformer.

Abstract (中文翻译): 准确识别解剖标志点对于多种医学应用至关重要。传统的人工标记方法耗时且容易受到观察者间差异的影响，而基于规则的方法通常针对特定几何结构或有限的标志点集合。近年来，解剖表面已被有效地表示为由空间坐标组成的轻量级点云结构。基于这一策略并为克服现有标记技术的局限性，我们提出了Landmark Point Transformer（LmPT）——一种用于在点云上自动检测解剖标志点的方法，该方法能够利用来自不同物种的同源骨骼进行转化研究。LmPT模型引入了一种条件机制，使其能够适应不同的输入类型，从而实现跨物种学习。我们在人类和新标注的狗股骨数据上对所提方法进行了评估，验证了其在不同物种间的泛化能力和有效性。相关代码和狗股骨数据集将在 https://github.com/Pierreoo/LandmarkPointTransformer 公开发布。

</details>


### [6] [Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room](https://arxiv.org/abs/2602.02850)
*Keqi Chen,Vinkle Srivastav,Armine Vardazaryan,Cindy Rolland,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本文提出了一种无需标注和相机标定的自监督多视角视频匿名化框架，通过利用时序和多视角上下文信息找回单视角检测中的漏检，并通过自监督域自适应提升检测与姿态估计性能，在手术室视频数据上实现了超过97%的召回率。


<details>
  <summary>Details</summary>
Motivation: 在手术室（OR）视频研究中，隐私保护至关重要，而现有匿名化方法依赖于对每个个体的完整定位。然而，当前方法存在两大可扩展性瓶颈：一是需要为每个新临床场所进行人工标注以保证精度；二是多摄像头系统在摄像头位置变动后需重新标定，限制了实际部署。

Method: 作者提出一个自监督的多视角视频匿名化框架，包含全身人体检测与姿态估计两个模块。首先在各视角使用低阈值运行现成检测器获取候选框；然后通过跟踪和无标定的多视角关联，找回与高置信度检测一致的低分漏检作为伪标签；接着用这些伪标签迭代微调检测器；最后在检测结果上进行全身姿态估计，并利用自身高置信度预测微调姿态模型。

Result: 在4D-OR模拟手术数据集和真实手术数据集上的实验表明，该方法召回率超过97%。此外，利用生成的伪标签训练的实时全身检测器性能相当，验证了方法的实用性。

Conclusion: 所提方法有效解决了手术室视频匿名化中对人工标注和相机标定的依赖，具备良好的可扩展性和实际应用价值，为医疗视频隐私保护提供了高效可行的解决方案。

Abstract: Privacy preservation is a prerequisite for using video data in Operating Room (OR) research. Effective anonymization relies on the exhaustive localization of every individual; even a single missed detection necessitates extensive manual correction. However, existing approaches face two critical scalability bottlenecks: (1) they usually require manual annotations of each new clinical site for high accuracy; (2) while multi-camera setups have been widely adopted to address single-view ambiguity, camera calibration is typically required whenever cameras are repositioned. To address these problems, we propose a novel self-supervised multi-view video anonymization framework consisting of whole-body person detection and whole-body pose estimation, without annotation or camera calibration. Our core strategy is to enhance the single-view detector by "retrieving" false negatives using temporal and multi-view context, and conducting self-supervised domain adaptation. We first run an off-the-shelf whole-body person detector in each view with a low-score threshold to gather candidate detections. Then, we retrieve the low-score false negatives that exhibit consistency with the high-score detections via tracking and self-supervised uncalibrated multi-view association. These recovered detections serve as pseudo labels to iteratively fine-tune the whole-body detector. Finally, we apply whole-body pose estimation on each detected person, and fine-tune the pose model using its own high-score predictions. Experiments on the 4D-OR dataset of simulated surgeries and our dataset of real surgeries show the effectiveness of our approach achieving over 97% recall. Moreover, we train a real-time whole-body detector using our pseudo labels, achieving comparable performance and highlighting our method's practical applicability. Code is available at https://github.com/CAMMA-public/OR_anonymization.

Abstract (中文翻译): 隐私保护是将视频数据用于手术室（OR）研究的前提条件。有效的匿名化依赖于对每个个体的全面定位；即使漏检一人，也需要大量人工修正。然而，现有方法面临两个关键的可扩展性瓶颈：（1）通常需要对每个新的临床场所进行人工标注以获得高精度；（2）尽管多摄像头设置已被广泛采用以解决单视角模糊问题，但每次摄像头位置变动后通常都需要重新标定。为解决这些问题，我们提出了一种新颖的自监督多视角视频匿名化框架，包含全身人体检测与全身姿态估计，且无需人工标注或相机标定。我们的核心策略是通过利用时序和多视角上下文“找回”单视角检测中的漏检，并进行自监督域自适应。我们首先在每个视角中以低分阈值运行现成的全身人体检测器以收集候选检测结果；然后通过跟踪和自监督的无标定多视角关联，找回与高分检测结果一致的低分漏检。这些恢复的检测结果作为伪标签，用于迭代微调全身检测器。最后，我们在每个检测到的人体上应用全身姿态估计，并利用其自身的高分预测微调姿态模型。在4D-OR模拟手术数据集和我们的真实手术数据集上的实验表明，该方法有效，召回率超过97%。此外，我们利用生成的伪标签训练了一个实时全身检测器，取得了相当的性能，突显了本方法的实际适用性。代码已开源：https://github.com/CAMMA-public/OR_anonymization。

</details>


### [7] [ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying](https://arxiv.org/abs/2602.02873)
*Weihang You,Qingchan Zhu,David Liu,Yi Pan,Geng Yuan,Hanqi Jiang*

Main category: cs.CV

TL;DR: ViThinker 是一种新型视觉-语言模型框架，通过主动查询机制动态生成任务相关的视觉特征，从而提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在链式思维（CoT）推理中表现不佳，主要因为过早将视觉信息转为文本，丢失了几何和空间布局等连续信息；同时现有方法多为被动处理预计算输入，缺乏主动感知能力。

Method: 提出 ViThinker 框架，使模型能自主生成决策（查询）token，按需合成与专家对齐的视觉特征；训练阶段内化视觉专家能力，推理阶段进行生成式心智模拟；采用两阶段课程学习：先将冻结专家蒸馏进模型参数，再通过稀疏性惩罚学习任务驱动的查询策略。

Result: 在多个以视觉为中心的基准测试中，ViThinker 均取得一致性能提升，证明其在感知定位和推理准确性方面优于被动方法。

Conclusion: 主动查询生成机制能有效提升视觉-语言模型的链式思维推理能力，ViThinker 为实现类人主动感知提供了可行路径。

Abstract: Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.

Abstract (中文翻译): 链式思维（CoT）推理在语言模型中表现出色，但在视觉-语言模型中却面临挑战，原因在于过早地将视觉信息转换为文本，从而丢失了几何结构和空间布局等连续信息。尽管近期方法通过静态枚举或基于注意力的选择机制来增强 CoT，但它们仍属于被动方式，即仅处理预先计算好的输入，而非主动探寻任务相关细节。受人类主动感知机制启发，我们提出了 ViThinker 框架，使视觉-语言模型能够自主生成决策（查询）token，按需触发与专家对齐的视觉特征合成。ViThinker 在训练阶段内化视觉专家能力，在推理阶段无需外部工具调用即可执行生成式心智模拟。该框架采用两阶段课程学习策略：首先将冻结的专家模型蒸馏到模型参数中，然后通过稀疏性惩罚学习任务驱动的查询机制，从而为每一步推理发现最小且充分的感知信息。在多个以视觉为中心的基准测试上的评估表明，ViThinker 始终取得性能提升，验证了主动查询生成在感知定位和推理准确性方面优于被动方法。

</details>


### [8] [DoubleTake: Contrastive Reasoning for Faithful Decision-Making in Medical Imaging](https://arxiv.org/abs/2602.02894)
*Daivik Patel,Shrenik Patel*

Main category: cs.CV

TL;DR: 本文提出一种面向医学图像推理的对比式、文档感知的参考选择框架，通过优化证据集的多样性与相关性提升判别能力，并引入置信度感知的反事实-对比推理方法，在MediConfusion基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像决策方法多依赖近邻检索，返回冗余证据并强化单一假设，难以有效区分视觉上相似但诊断不同的病症。此外，尽管ROCO数据集提供了大量图文对，但缺乏针对对比推理的参考选择机制，常导致从同一文档中检索出近似重复图像。

Method: 作者构建了一个对比式、文档感知的参考选择框架，利用ROCO嵌入和元数据显式平衡视觉相关性、嵌入多样性和来源出处，生成紧凑且具判别力的证据集；在此基础上，提出“反事实-对比推理”（Counterfactual-Contrastive Inference）方法，通过结构化成对视觉比较和基于边距的决策规则进行置信度感知的证据聚合，并支持可靠弃权。

Result: 在MediConfusion基准上，该方法相较先前方法将集合级准确率相对提升近15%，同时降低混淆率并提高个体准确率。

Conclusion: 通过引入文档感知的对比参考选择机制和置信度感知的推理框架，该工作显著提升了医学图像中对易混淆病症的判别能力，为可解释、可靠的医学视觉推理提供了新范式。

Abstract: Accurate decision making in medical imaging requires reasoning over subtle visual differences between confusable conditions, yet most existing approaches rely on nearest neighbor retrieval that returns redundant evidence and reinforces a single hypothesis. We introduce a contrastive, document-aware reference selection framework that constructs compact evidence sets optimized for discrimination rather than similarity by explicitly balancing visual relevance, embedding diversity, and source-level provenance using ROCO embeddings and metadata. While ROCO provides large-scale image-caption pairs, it does not specify how references should be selected for contrastive reasoning, and naive retrieval frequently yields near-duplicate figures from the same document. To address this gap, we release a reproducible reference selection protocol and curated reference bank that enable a systematic study of contrastive retrieval in medical image reasoning. Building on these contrastive evidence sets, we propose Counterfactual-Contrastive Inference, a confidence-aware reasoning framework that performs structured pairwise visual comparisons and aggregates evidence using margin-based decision rules with faithful abstention. On the MediConfusion benchmark, our approach achieves state-of-the-art performance, improving set-level accuracy by nearly 15% relative to prior methods while reducing confusion and improving individual accuracy.

Abstract (中文翻译): 医学影像中的准确决策需要对易混淆病症之间的细微视觉差异进行推理，然而大多数现有方法依赖于最近邻检索，返回冗余证据并强化单一假设。我们提出了一种对比式、文档感知的参考选择框架，该框架利用ROCO嵌入和元数据，通过显式平衡视觉相关性、嵌入多样性和来源出处，构建用于判别而非相似性优化的紧凑证据集。尽管ROCO提供了大规模的图像-标题对，但它并未指明如何为对比推理选择参考样本，而朴素检索常常从同一文档中返回近乎重复的图像。为弥补这一空白，我们发布了一个可复现的参考选择协议和经过整理的参考库，以支持对医学图像推理中对比检索的系统性研究。基于这些对比证据集，我们进一步提出了“反事实-对比推理”（Counterfactual-Contrastive Inference）——一种置信度感知的推理框架，该框架执行结构化的成对视觉比较，并使用基于边距的决策规则聚合证据，同时支持可靠的弃权机制。在MediConfusion基准上，我们的方法取得了当前最优性能，相较以往方法将集合级准确率相对提升了近15%，同时减少了混淆并提高了个体准确率。

</details>


### [9] [FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction](https://arxiv.org/abs/2602.02914)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: 现有基于像素重构指标（如PSNR、SSIM）评估隐私保护人脸识别（PPFR）系统的方法存在根本缺陷；作者提出FaceLinkGen攻击方法，无需恢复原始像素即可从受保护模板中高精度地提取身份信息并再生人脸，揭示了当前评估标准与真实隐私保护之间的结构性差距。


<details>
  <summary>Details</summary>
Motivation: 当前PPFR系统的隐私评估主要依赖于对像素级重构的抵抗能力（如PSNR和SSIM），但这种以重构为中心的视角无法真实反映系统在身份信息泄露方面的风险。作者旨在揭示这一评估盲点，并证明即使视觉上模糊或变形的人脸数据仍可能暴露身份信息。

Method: 提出名为FaceLinkGen的身份提取攻击方法，该方法直接从受保护的人脸模板中进行身份链接/匹配和人脸再生，而无需恢复原始像素。在三种最新的PPFR系统上进行了实验，并在接近零知识的设定下测试其有效性。

Result: FaceLinkGen在三个PPFR系统上实现了超过98.5%的匹配准确率和96%以上的人脸再生成功率；即使在接近零知识的设定下，匹配准确率仍超过92%，再生成功率超过94%。

Conclusion: 当前广泛使用的像素失真指标（如PSNR、SSIM）无法有效衡量PPFR系统的真实隐私保护水平；视觉上的模糊处理并不能防止身份信息被外部攻击者或不可信的服务提供商提取，表明现有评估范式存在严重不足。

Abstract: Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.

Abstract (中文翻译): 基于变换的隐私保护人脸识别（PPFR）旨在验证身份的同时，防止面部数据被攻击者和恶意服务提供商获取。现有的评估方法大多将隐私定义为对像素级重构的抵抗能力，并通过PSNR和SSIM等指标进行衡量。我们指出，这种以重构为中心的观点是失败的。我们提出了FaceLinkGen——一种身份提取攻击方法，它能够直接从受保护的模板中进行身份关联/匹配和人脸再生，而无需恢复原始像素。在三种最新的PPFR系统上，FaceLinkGen的匹配准确率超过98.5%，再生成功率超过96%；即使在接近零知识的设定下，其匹配准确率仍超过92%，再生成功率超过94%。这些结果揭示了当前PPFR评估中广泛使用的像素失真指标与真实隐私保护之间存在的结构性差距。我们证明，视觉上的模糊处理仍然使身份信息广泛暴露于外部入侵者和不可信的服务提供商面前。

</details>


### [10] [A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis](https://arxiv.org/abs/2602.02918)
*Jagan Mohan Reddy Dwarampudi,Joshua Wong,Hien Van Nguyen,Tania Banerjee*

Main category: cs.CV

TL;DR: MARBLE is a purely Mamba-based multi-scale MIL framework for whole-slide image analysis that achieves state-of-the-art performance with linear-time complexity.


<details>
  <summary>Details</summary>
Motivation: Whole-slide image (WSI) analysis is challenging due to gigapixel resolution and hierarchical magnifications. Existing MIL methods usually work at a single scale, and transformer-based models suffer from high computational cost due to quadratic attention complexity.

Method: MARBLE employs a purely Mamba-based architecture to process multiple magnification levels in parallel within a linear-time state-space model, enabling efficient cross-scale dependency modeling with minimal parameter overhead.

Result: Evaluated on five public datasets, MARBLE improves AUC by up to 6.9%, accuracy by 20.3%, and C-index by 2.3% over existing methods.

Conclusion: MARBLE offers a scalable, modular, and efficient alternative to attention-based models for multi-scale WSI analysis, demonstrating strong generalization and performance gains.

Abstract: We introduce Multi-scale Adaptive Recurrent Biomedical Linear-time Encoder (MARBLE), the first \textit{purely Mamba-based} multi-state multiple instance learning (MIL) framework for whole-slide image (WSI) analysis. MARBLE processes multiple magnification levels in parallel and integrates coarse-to-fine reasoning within a linear-time state-space model, efficiently capturing cross-scale dependencies with minimal parameter overhead. WSI analysis remains challenging due to gigapixel resolutions and hierarchical magnifications, while existing MIL methods typically operate at a single scale and transformer-based approaches suffer from quadratic attention costs. By coupling parallel multi-scale processing with linear-time sequence modeling, MARBLE provides a scalable and modular alternative to attention-based architectures. Experiments on five public datasets show improvements of up to \textbf{6.9\%} in AUC, \textbf{20.3\%} in accuracy, and \textbf{2.3\%} in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.

Abstract (中文翻译): 我们提出了多尺度自适应循环生物医学线性时间编码器（MARBLE），这是首个基于纯Mamba架构的多状态多实例学习（MIL）框架，用于全切片图像（WSI）分析。MARBLE并行处理多个放大倍率，并在线性时间状态空间模型中融合从粗到细的推理机制，以极少的参数开销高效捕捉跨尺度依赖关系。由于全切片图像具有十亿像素级分辨率和分层放大结构，其分析仍然具有挑战性；而现有MIL方法通常仅在单一尺度上操作，基于Transformer的方法则因注意力机制的二次复杂度而计算成本高昂。通过将并行多尺度处理与线性时间序列建模相结合，MARBLE为基于注意力的架构提供了一种可扩展且模块化的替代方案。在五个公开数据集上的实验表明，MARBLE在AUC上最多提升6.9%，准确率提升20.3%，C-index提升2.3%，证明其是一种高效且具有良好泛化能力的多尺度WSI分析框架。

</details>
