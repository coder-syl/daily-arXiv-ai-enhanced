{"id": "2601.15366", "pdf": "https://arxiv.org/pdf/2601.15366", "abs": "https://arxiv.org/abs/2601.15366", "authors": ["Christina Thrainer"], "title": "AI-Based Culvert-Sewer Inspection", "categories": ["cs.CV"], "comment": "Masters thesis, University of Technology Graz, 2025", "summary": "Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.\n  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\u4ee5\u63d0\u5347\u6db5\u6d1e\u4e0e\u6c61\u6c34\u7ba1\u9053\u7f3a\u9677\u7684\u81ea\u52a8\u5206\u5272\u6548\u679c\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff1a\uff081\uff09\u6709\u6548\u7684\u9884\u5904\u7406\u7b56\u7565\uff1b\uff082\uff09\u65b0\u67b6\u6784FORTRESS\uff1b\uff083\uff09\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u3002", "motivation": "\u6db5\u6d1e\u548c\u6c61\u6c34\u7ba1\u9053\u662f\u6392\u6c34\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u7f3a\u9677\u68c0\u6d4b\u5bf9\u516c\u5171\u5b89\u5168\u548c\u73af\u5883\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u6570\u636e\u91c7\u96c6\u4e0e\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\uff0c\u96be\u4ee5\u83b7\u5f97\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u4e9f\u9700\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u63d0\u5347\u81ea\u52a8\u5316\u7f3a\u9677\u5206\u5272\u6027\u80fd\u3002", "method": "1. \u8bc4\u4f30\u5305\u62ec\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u548c\u52a8\u6001\u6807\u7b7e\u6ce8\u5165\u5728\u5185\u7684\u9884\u5904\u7406\u7b56\u7565\uff1b  \n2. \u63d0\u51fa\u65b0\u67b6\u6784FORTRESS\uff0c\u878d\u5408\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u3001\u81ea\u9002\u5e94Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u548c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff1b  \n3. \u63a2\u7d22\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\uff0c\u91c7\u7528\u5e26\u6ce8\u610f\u529b\u673a\u5236\u7684\u53cc\u5411\u539f\u578b\u7f51\u7edc\u8fdb\u884c\u7f3a\u9677\u68c0\u6d4b\u3002", "result": "1. \u9884\u5904\u7406\u7b56\u7565\u663e\u8457\u63d0\u5347IoU\u548cF1\u5206\u6570\uff1b  \n2. FORTRESS\u5728\u6db5\u6d1e/\u6c61\u6c34\u7ba1\u7f3a\u9677\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u5f00\u9500\uff1b  \n3. \u5c11\u6837\u672c\u65b9\u6cd5\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u4ecd\u53d6\u5f97\u4ee4\u4eba\u6ee1\u610f\u7684\u8bc4\u4f30\u6307\u6807\u8868\u73b0\u3002", "conclusion": "\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u901a\u8fc7\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u6216\u8c03\u6574\u6a21\u578b\u67b6\u6784\uff0c\u5747\u53ef\u6709\u6548\u63d0\u5347\u6db5\u6d1e\u4e0e\u6c61\u6c34\u7ba1\u9053\u7f3a\u9677\u7684\u81ea\u52a8\u5206\u5272\u6027\u80fd\u3002\u6240\u63d0\u4e09\u79cd\u65b9\u6cd5\u4e3a\u57fa\u7840\u8bbe\u65bd\u667a\u80fd\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u6db5\u6d1e\u548c\u6c61\u6c34\u7ba1\u9053\u662f\u6392\u6c34\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u5931\u6548\u53ef\u80fd\u5bf9\u516c\u5171\u5b89\u5168\u548c\u73af\u5883\u9020\u6210\u4e25\u91cd\u98ce\u9669\u3002\u672c\u8bba\u6587\u63a2\u7d22\u4e86\u63d0\u5347\u6db5\u6d1e\u4e0e\u6c61\u6c34\u7ba1\u9053\u7f3a\u9677\u81ea\u52a8\u5206\u5272\u6548\u679c\u7684\u65b9\u6cd5\u3002\u8be5\u9886\u57df\u7684\u6570\u636e\u6536\u96c6\u4e0e\u6807\u6ce8\u8fc7\u7a0b\u7e41\u7410\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u96be\u4ee5\u83b7\u5f97\u7528\u4e8e\u7ed3\u6784\u7f3a\u9677\u68c0\u6d4b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u6211\u4eec\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002\u603b\u4f53\u800c\u8a00\uff0c\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7f3a\u9677\u5206\u5272\u80fd\u529b\u5e76\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5177\u4f53\u53ef\u901a\u8fc7\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u6216\u8c03\u6574\u6a21\u578b\u67b6\u6784\u6765\u5b9e\u73b0\u3002\u9996\u5148\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5305\u62ec\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u548c\u52a8\u6001\u6807\u7b7e\u6ce8\u5165\u5728\u5185\u7684\u9884\u5904\u7406\u7b56\u7565\uff0c\u8fd9\u4e9b\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u548cF1\u5206\u6570\u3002\u5176\u6b21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u540d\u4e3aFORTRESS\u7684\u65b0\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u3001\u81ea\u9002\u5e94Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u4ee5\u53ca\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u6db5\u6d1e\u6c61\u6c34\u7ba1\u7f3a\u9677\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u3002\u6700\u540e\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u5728\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u9002\u7528\u6027\u3002\u5c11\u6837\u672c\u5b66\u4e60\u65e8\u5728\u4ec5\u4f7f\u7528\u6709\u9650\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u91c7\u7528\u5e26\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u53cc\u5411\u539f\u578b\u7f51\u7edc\uff0c\u6a21\u578b\u83b7\u5f97\u4e86\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\uff0c\u5e76\u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002"}}
{"id": "2601.15406", "pdf": "https://arxiv.org/pdf/2601.15406", "abs": "https://arxiv.org/abs/2601.15406", "authors": ["Hatef Otroshi Shahreza", "Anjith George", "S\u00e9bastien Marcel"], "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5f02\u8d28\u4eba\u8138\u8bc6\u522b\uff08HFR\uff09\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u8de8\u5149\u8c31\u6761\u4ef6\u4e0b\u663e\u8457\u843d\u540e\u4e8e\u4f20\u7edf\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6210\u529f\uff0c\u7814\u7a76\u8005\u5f00\u59cb\u5173\u6ce8\u5176\u5728\u751f\u7269\u8bc6\u522b\uff08\u7279\u522b\u662f\u5f02\u8d28\u4eba\u8138\u8bc6\u522b\uff09\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5c1a\u7f3a\u4e4f\u5bf9\u5176\u5728\u8be5\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u5728\u591a\u79cd\u8de8\u6a21\u6001\u573a\u666f\uff08\u5982VIS-NIR\u3001VIS-SWIR\u3001VIS-THERMAL\uff09\u4e0b\uff0c\u5bf9\u591a\u4e2a\u5f00\u6e90MLLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u91c7\u7528\u751f\u7269\u8bc6\u522b\u6807\u51c6\u534f\u8bae\u548c\u6307\u6807\uff08\u5982Acquire Rate\u3001EER\u3001TAR\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1MLLM\u8fd1\u671f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u5149\u8c31\u6761\u4ef6\u4e0b\uff0c\u5176\u6027\u80fd\u4e0e\u4f20\u7edf\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u76f8\u6bd4\u4ecd\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002", "conclusion": "\u5f53\u524d\u7684MLLM\u5728\u5f02\u8d28\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\uff0c\u5728\u5c06\u5176\u90e8\u7f72\u5230\u5b9e\u9645\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u524d\uff0c\u5fc5\u987b\u8fdb\u884c\u4e25\u683c\u7684\u751f\u7269\u8bc6\u522b\u8bc4\u4f30\u3002", "summary_cn": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6700\u8fd1\u5728\u5404\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9\u5176\u5728\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u6f5c\u529b\u7684\u5173\u6ce8\u3002\u672c\u6587\u5bf9\u6700\u5148\u8fdb\u7684MLLMs\u5728\u5f02\u8d28\u4eba\u8138\u8bc6\u522b\uff08HFR\uff09\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u5176\u4e2d\u6ce8\u518c\u56fe\u50cf\u548c\u67e5\u8be2\u56fe\u50cf\u6765\u81ea\u4e0d\u540c\u7684\u4f20\u611f\u6a21\u6001\uff0c\u5305\u62ec\u53ef\u89c1\u5149\uff08VIS\uff09\u3001\u8fd1\u7ea2\u5916\uff08NIR\uff09\u3001\u77ed\u6ce2\u7ea2\u5916\uff08SWIR\uff09\u548c\u70ed\u6210\u50cf\u76f8\u673a\u3002\u6211\u4eec\u5728\u591a\u79cd\u8de8\u6a21\u6001\u573a\u666f\uff08\u5305\u62ecVIS-NIR\u3001VIS-SWIR\u548cVIS-THERMAL\u4eba\u8138\u8bc6\u522b\uff09\u4e0b\u5bf9\u591a\u4e2a\u5f00\u6e90MLLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002MLLMs\u7684\u8bc6\u522b\u6027\u80fd\u91c7\u7528\u751f\u7269\u8bc6\u522b\u534f\u8bae\u5e76\u57fa\u4e8e\u4e0d\u540c\u6307\u6807\uff08\u5305\u62ec\u83b7\u53d6\u7387\u3001\u7b49\u9519\u8bef\u7387\uff08EER\uff09\u548c\u771f\u63a5\u53d7\u7387\uff08TAR\uff09\uff09\u8fdb\u884c\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u7ed3\u679c\u63ed\u793a\u4e86MLLMs\u4e0e\u4f20\u7edf\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e4b\u95f4\u5b58\u5728\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u5149\u8c31\u6761\u4ef6\u4e0b\uff0c\u5c3d\u7ba1MLLMs\u8fd1\u671f\u5df2\u53d6\u5f97\u8fdb\u5c55\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5f53\u524dMLLMs\u5728HFR\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u5728\u8003\u8651\u5c06\u5176\u90e8\u7f72\u5230\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4e2d\u65f6\u8fdb\u884c\u4e25\u683c\u751f\u7269\u8bc6\u522b\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.15408", "pdf": "https://arxiv.org/pdf/2601.15408", "abs": "https://arxiv.org/abs/2601.15408", "authors": ["Pablo Messina", "Andr\u00e9s Villa", "Juan Le\u00f3n Alc\u00e1zar", "Karen S\u00e1nchez", "Carlos Hinojosa", "Denis Parra", "\u00c1lvaro Soto", "Bernard Ghanem"], "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "31 pages, 7 figures, submitted to CVPR 2026 (under review)", "summary": "Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure", "AI": {"tldr": "CURE is an error-aware curriculum learning framework that enhances medical vision-language models by improving visual grounding and reducing hallucinations without extra data.", "motivation": "Medical vision-language models often produce radiology reports with poor visual grounding and factual inconsistencies, undermining their reliability in clinical settings.", "method": "CURE fine-tunes a multimodal instructional model using public datasets through three tasks: phrase grounding, grounded report generation, and anatomy-grounded report generation. It employs dynamic curriculum learning that prioritizes harder samples based on model performance to enhance alignment.", "result": "CURE improves grounding accuracy by +0.37 IoU, increases report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%.", "conclusion": "CURE is a data-efficient framework that significantly boosts both grounding accuracy and factual reliability of generated radiology reports.", "summary_cn": "\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u52a8\u751f\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u4f46\u5728\u51c6\u786e\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u73b0\u6709\u6a21\u578b\u5e38\u5e38\u5c06\u6587\u672c\u53d1\u73b0\u4e0e\u89c6\u89c9\u8bc1\u636e\u9519\u8bef\u5bf9\u9f50\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u6216\u5f31\u5b9a\u4f4d\u7684\u9884\u6d4b\u3002\u6211\u4eec\u63d0\u51fa\u4e86CURE\uff0c\u4e00\u79cd\u5177\u6709\u9519\u8bef\u611f\u77e5\u80fd\u529b\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u5373\u53ef\u63d0\u5347\u5b9a\u4f4d\u80fd\u529b\u548c\u62a5\u544a\u8d28\u91cf\u3002CURE\u5229\u7528\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5728\u77ed\u8bed\u5b9a\u4f4d\u3001\u57fa\u4e8e\u5b9a\u4f4d\u7684\u62a5\u544a\u751f\u6210\u548c\u57fa\u4e8e\u89e3\u5256\u7ed3\u6784\u7684\u62a5\u544a\u751f\u6210\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5bf9\u591a\u6a21\u6001\u6307\u4ee4\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u6a21\u578b\u8868\u73b0\u52a8\u6001\u8c03\u6574\u6837\u672c\u91c7\u6837\u7b56\u7565\uff0c\u91cd\u70b9\u8bad\u7ec3\u66f4\u96be\u7684\u6837\u672c\uff0c\u4ee5\u6539\u5584\u7a7a\u95f4\u4e0e\u6587\u672c\u5bf9\u9f50\u3002CURE\u5c06\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u9ad8\u4e86+0.37 IoU\uff0c\u62a5\u544a\u8d28\u91cf\u63d0\u5347\u4e86+0.188 CXRFEScore\uff0c\u5e76\u5c06\u5e7b\u89c9\u51cf\u5c11\u4e8618.6%\u3002CURE\u662f\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u589e\u5f3a\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u62a5\u544a\u53ef\u9760\u6027\u3002\u4ee3\u7801\u89c1 https://github.com/PabloMessina/CURE\uff0c\u6a21\u578b\u6743\u91cd\u89c1 https://huggingface.co/pamessina/medgemma-4b-it-cure\u3002"}}
{"id": "2601.15416", "pdf": "https://arxiv.org/pdf/2601.15416", "abs": "https://arxiv.org/abs/2601.15416", "authors": ["Cuong Tran Van", "Trong-Thang Pham", "Ngoc-Son Nguyen", "Duy Minh Ho Nguyen", "Ngan Le"], "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Published with J2C Certification in Transactions on Machine Learning Research (TMLR)", "summary": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDuFal\uff0c\u4e00\u79cd\u53cc\u9891\u57df\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u9ad8\u9891\u589e\u5f3a\u7684\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff0c\u5728\u7a00\u758f\u89c6\u89d2\u9525\u675fCT\u91cd\u5efa\u4e2d\u663e\u8457\u63d0\u5347\u9ad8\u9891\u89e3\u5256\u7ec6\u8282\u7684\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u7a00\u758f\u89c6\u89d2\u9525\u675fCT\u91cd\u5efa\u56e0\u91c7\u6837\u4e0d\u8db3\u96be\u4ee5\u6062\u590d\u9ad8\u9891\u89e3\u5256\u7ec6\u8282\uff0c\u800c\u4f20\u7edfCNN\u65b9\u6cd5\u504f\u5411\u5b66\u4e60\u4f4e\u9891\u4fe1\u606f\uff0c\u65e0\u6cd5\u6709\u6548\u91cd\u5efa\u7cbe\u7ec6\u7ed3\u6784\u3002", "method": "\u63d0\u51faDuFal\u6846\u67b6\uff0c\u5305\u542b\u53cc\u8def\u5f84\u67b6\u6784\uff1a\u5168\u5c40\u9ad8\u9891\u589e\u5f3a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c\u5c40\u90e8\u9ad8\u9891\u589e\u5f3a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff1b\u5f15\u5165\u8c31-\u901a\u9053\u56e0\u5b50\u5316\u964d\u4f4e\u53c2\u6570\u91cf\uff0c\u5e76\u8bbe\u8ba1\u8de8\u6ce8\u610f\u529b\u9891\u7387\u878d\u5408\u6a21\u5757\u6574\u5408\u7a7a\u95f4\u4e0e\u9891\u57df\u7279\u5f81\uff0c\u6700\u7ec8\u901a\u8fc7\u7279\u5f81\u89e3\u7801\u5668\u548c\u5f3a\u5ea6\u573a\u89e3\u7801\u6d41\u7a0b\u91cd\u5efaCT\u4f53\u79ef\u3002", "result": "\u5728LUNA16\u548cToothFairy\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDuFal\u5728\u6781\u7a00\u758f\u89c6\u89d2\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4fdd\u7559\u9ad8\u9891\u89e3\u5256\u7279\u5f81\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DuFal\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u4e0e\u9891\u57df\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u89d2CT\u91cd\u5efa\u4e2d\u9ad8\u9891\u7ec6\u8282\u4e22\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "summary_cn": "\u7a00\u758f\u89c6\u89d2\u9525\u675f\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08Cone-Beam Computed Tomography, CBCT\uff09\u4ece\u6709\u9650X\u5c04\u7ebf\u6295\u5f71\u4e2d\u91cd\u5efa\u56fe\u50cf\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u4ecd\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u7ec6\u7c92\u5ea6\u89e3\u5256\u7ec6\u8282\uff08\u5bf9\u5e94\u4e8e\u9ad8\u9891\u6210\u5206\uff09\u5b58\u5728\u56fa\u6709\u7684\u6b20\u91c7\u6837\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u6062\u590d\u8fd9\u4e9b\u7cbe\u7ec6\u7ed3\u6784\uff0c\u56e0\u4e3a\u5b83\u4eec\u503e\u5411\u4e8e\u5b66\u4e60\u4f4e\u9891\u4fe1\u606f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86DuFal\uff08Dual-Frequency-Aware Learning\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u67b6\u6784\u878d\u5408\u9891\u57df\u4e0e\u7a7a\u95f4\u57df\u5904\u7406\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u7684\u9ad8-\u5c40\u90e8\u5206\u89e3\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08High-Local Factorized Fourier Neural Operator\uff09\uff0c\u8be5\u7b97\u5b50\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u5206\u652f\uff1a\u4e00\u4e2a\u5168\u5c40\u9ad8\u9891\u589e\u5f3a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u7528\u4e8e\u6355\u6349\u5168\u5c40\u9891\u7387\u6a21\u5f0f\uff0c\u53e6\u4e00\u4e2a\u5c40\u90e8\u9ad8\u9891\u589e\u5f3a\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u5219\u5bf9\u7a7a\u95f4\u5206\u5757\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u4fdd\u7559\u53ef\u80fd\u5728\u5168\u5c40\u9891\u57df\u5206\u6790\u4e2d\u4e22\u5931\u7684\u7a7a\u95f4\u5c40\u90e8\u6027\u3002\u4e3a\u63d0\u9ad8\u6548\u7387\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u8c31-\u901a\u9053\u56e0\u5b50\u5316\u65b9\u6848\u4ee5\u51cf\u5c11\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u7684\u53c2\u6570\u6570\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u8de8\u6ce8\u610f\u529b\u9891\u7387\u878d\u5408\u6a21\u5757\uff0c\u4ee5\u6709\u6548\u6574\u5408\u7a7a\u95f4\u4e0e\u9891\u57df\u7279\u5f81\u3002\u878d\u5408\u540e\u7684\u7279\u5f81\u901a\u8fc7\u7279\u5f81\u89e3\u7801\u5668\u751f\u6210\u6295\u5f71\u8868\u793a\uff0c\u5e76\u8fdb\u4e00\u6b65\u901a\u8fc7\u5f3a\u5ea6\u573a\u89e3\u7801\u6d41\u7a0b\u91cd\u5efa\u6700\u7ec8\u7684CT\u4f53\u79ef\u3002\u5728LUNA16\u548cToothFairy\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDuFal\u5728\u4fdd\u7559\u9ad8\u9891\u89e3\u5256\u7279\u5f81\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u6781\u7a00\u758f\u89c6\u89d2\u8bbe\u7f6e\u4e0b\u3002"}}
{"id": "2601.15453", "pdf": "https://arxiv.org/pdf/2601.15453", "abs": "https://arxiv.org/abs/2601.15453", "authors": ["Morteza Poudineh", "Marc Lalonde"], "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u504f\u5dee\u5f15\u5bfc\u7684\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0e\u7edf\u8ba1\u504f\u5dee\u8bc4\u5206\u673a\u5236\uff0c\u5728\u4ec5\u4f7f\u7528\u5c11\u91cf\u6b63\u5e38\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5f02\u5e38\u533a\u57df\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5c11\u6837\u672c\u6b63\u5e38\u6837\u672c\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\uff08FNSAD\uff09\u56e0\u76d1\u7763\u4fe1\u606f\u6709\u9650\u548c\u7f3a\u9677\u7c7b\u578b\u591a\u6837\u800c\u6781\u5177\u6311\u6218\uff1b\u73b0\u6709\u57fa\u4e8eCLIP\u7b49\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u6b63\u5e38\u4e0e\u5f02\u5e38\u63d0\u793a\u4e4b\u95f4\u533a\u5206\u5ea6\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u50cf\u7d20\u7ea7\u5f02\u5e38\u8bc4\u5206\u673a\u5236\u3002", "method": "\u63d0\u51fa\u504f\u5dee\u5f15\u5bfc\u7684\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff1a\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u66ff\u4ee3\u56fa\u5b9a\u63d0\u793a\u524d\u7f00\uff0c\u5171\u4eab\u4e8e\u6b63\u5e38\u4e0e\u5f02\u5e38\u63d0\u793a\uff1b\u5f15\u5165\u5f02\u5e38\u7279\u5b9a\u540e\u7f00\u4ee5\u5b9e\u73b0\u7c7b\u522b\u611f\u77e5\u5bf9\u9f50\uff1b\u7ed3\u5408Top-K\u591a\u793a\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u504f\u5dee\u635f\u5931\uff0c\u5c06\u56fe\u50cf\u5757\u7279\u5f81\u5efa\u6a21\u4e3a\u5bf9\u6b63\u5e38\u5206\u5e03\u7684\u9ad8\u65af\u504f\u5dee\uff0c\u4ece\u800c\u8d4b\u4e88\u663e\u8457\u504f\u79bb\u7684\u56fe\u50cf\u5757\u66f4\u9ad8\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5728MVTecAD\u548cVISA\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u50cf\u7d20\u7ea7\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8ePromptAD\u53ca\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff1b\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u53ef\u5b66\u4e60\u63d0\u793a\u3001\u504f\u5dee\u8bc4\u5206\u548cTop-K MIL\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u878d\u5408\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u80fd\u529b\u4e0e\u7edf\u8ba1\u504f\u5dee\u7684\u53ef\u9760\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "summary_cn": "\u5c11\u6837\u672c\u6b63\u5e38\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff08FNSAD\uff09\u65e8\u5728\u4ec5\u4f7f\u7528\u5c11\u91cf\u6b63\u5e38\u8bad\u7ec3\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\u533a\u57df\uff0c\u7531\u4e8e\u76d1\u7763\u4fe1\u606f\u6709\u9650\u4ee5\u53ca\u6f5c\u5728\u7f3a\u9677\u7684\u591a\u6837\u6027\uff0c\u8be5\u4efb\u52a1\u6781\u5177\u6311\u6218\u6027\u3002\u8fd1\u671f\u65b9\u6cd5\u5229\u7528CLIP\u7b49\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u5b66\u4e60\u6765\u5bf9\u9f50\u56fe\u50cf\u4e0e\u6587\u672c\u7279\u5f81\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u6b63\u5e38\u4e0e\u5f02\u5e38\u63d0\u793a\u4e4b\u95f4\u7f3a\u4e4f\u8db3\u591f\u7684\u5224\u522b\u80fd\u529b\uff0c\u5e76\u4e14\u7f3a\u5c11\u9488\u5bf9\u56fe\u50cf\u5757\u7ea7\u522b\u5f02\u5e38\u7684\u5408\u7406\u8bc4\u5206\u673a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u504f\u5dee\u5f15\u5bfc\u7684\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u80fd\u529b\u4e0e\u57fa\u4e8e\u504f\u5dee\u7684\u7edf\u8ba1\u53ef\u9760\u6027\u76f8\u7ed3\u5408\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7528\u53ef\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u66ff\u4ee3\u56fa\u5b9a\u7684\u63d0\u793a\u524d\u7f00\uff0c\u8fd9\u4e9b\u5411\u91cf\u5728\u6b63\u5e38\u4e0e\u5f02\u5e38\u63d0\u793a\u4e4b\u95f4\u5171\u4eab\uff0c\u540c\u65f6\u5f15\u5165\u5f02\u5e38\u7279\u5b9a\u7684\u540e\u7f00\u6807\u8bb0\u4ee5\u5b9e\u73b0\u7c7b\u522b\u611f\u77e5\u7684\u5bf9\u9f50\u3002\u4e3a\u589e\u5f3a\u53ef\u5206\u6027\uff0c\u6211\u4eec\u5f15\u5165\u7ed3\u5408Top-K\u591a\u793a\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u504f\u5dee\u635f\u5931\uff0c\u5c06\u56fe\u50cf\u5757\u7ea7\u522b\u7684\u7279\u5f81\u5efa\u6a21\u4e3a\u5bf9\u6b63\u5e38\u5206\u5e03\u7684\u9ad8\u65af\u504f\u5dee\u3002\u8fd9\u4f7f\u5f97\u7f51\u7edc\u80fd\u591f\u4e3a\u5177\u6709\u7edf\u8ba1\u663e\u8457\u504f\u5dee\u7684\u56fe\u50cf\u5757\u5206\u914d\u66f4\u9ad8\u7684\u5f02\u5e38\u5206\u6570\uff0c\u4ece\u800c\u6539\u5584\u5b9a\u4f4d\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728MVTecAD\u548cVISA\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u50cf\u7d20\u7ea7\u68c0\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8ePromptAD\u53ca\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u53ef\u5b66\u4e60\u63d0\u793a\u3001\u57fa\u4e8e\u504f\u5dee\u7684\u8bc4\u5206\u673a\u5236\u4ee5\u53caTop-K MIL\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.15475", "pdf": "https://arxiv.org/pdf/2601.15475", "abs": "https://arxiv.org/abs/2601.15475", "authors": ["Yunshan Qi", "Lin Zhu", "Nan Bao", "Yifan Zhao", "Jia Li"], "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f20\u611f\u5668\u7269\u7406\u539f\u7406\u7684\u7edf\u4e00NeRF\u6846\u67b6\uff0c\u5229\u7528\u5355\u6b21\u66dd\u5149\u7684\u6a21\u7cca\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u4e8b\u4ef6\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u6e05\u6670\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u91ce\u5916\u5e38\u89c1\u7684\u6a21\u7ccaLDR\u56fe\u50cf\u4e2d\u8fdb\u884c\u65b0\u89c6\u89d2\u5408\u6210\u65f6\uff0c\u96be\u4ee5\u6062\u590d\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\u7684HDR\u548c\u6e05\u66703D\u8868\u793a\uff1b\u4e14\u867d\u6709\u65b9\u6cd5\u5f15\u5165\u4e8b\u4ef6\u6570\u636e\uff0c\u4f46\u5ffd\u7565\u4e86\u76f8\u673a\u8f93\u51fa\u4e0e\u771f\u5b9e\u4e16\u754c\u8f90\u5c04\u4e4b\u95f4\u7684\u4f20\u611f\u5668\u7269\u7406\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4HDR\u91cd\u5efa\u548c\u53bb\u6a21\u7cca\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u4f20\u611f\u5668\u7269\u7406\u7684NeRF\u6846\u67b6\uff1a1\uff09\u7528NeRF\u76f4\u63a5\u5efa\u6a21HDR\u57df\u4e2d\u7684\u771f\u5b9e3D\u573a\u666f\u8f90\u5c04\uff0c\u5e76\u6a21\u62df\u7269\u7406\u4e16\u754c\u4e2d\u5230\u8fbe\u4f20\u611f\u5668\u50cf\u7d20\u7684\u539f\u59cbHDR\u5149\u7ebf\uff1b2\uff09\u5f15\u5165\u9010\u50cf\u7d20RGB\u6620\u5c04\u573a\uff0c\u5c06\u6e32\u67d3\u503c\u4e0e\u8f93\u5165LDR\u56fe\u50cf\u7684\u4f20\u611f\u5668\u8bb0\u5f55\u503c\u5bf9\u9f50\uff1b3\uff09\u8bbe\u8ba1\u65b0\u7684\u4e8b\u4ef6\u6620\u5c04\u573a\uff0c\u8fde\u63a5\u7269\u7406\u573a\u666f\u52a8\u6001\u4e0e\u5b9e\u9645\u4e8b\u4ef6\u4f20\u611f\u5668\u8f93\u51fa\uff1b4\uff09\u8054\u5408\u4f18\u5316\u4e24\u4e2a\u6620\u5c04\u573a\u4e0eNeRF\u7f51\u7edc\uff0c\u5229\u7528\u4e8b\u4ef6\u4e2d\u7684\u65f6\u7a7a\u52a8\u6001\u4fe1\u606f\u63d0\u5347HDR\u6e05\u66703D\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u81ea\u5efa\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u5355\u6b21\u66dd\u5149\u6a21\u7ccaLDR\u56fe\u50cf\u548c\u5bf9\u5e94\u4e8b\u4ef6\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u53bb\u6a21\u7ccaHDR\u65b0\u89c6\u89d2\u5408\u6210\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4f20\u611f\u5668\u7269\u7406\u673a\u5236\u663e\u5f0f\u5efa\u6a21\u8fdbNeRF\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u7ccaLDR\u56fe\u50cf\u5728\u6781\u7aef\u5149\u7167\u4e0b\u96be\u4ee5\u6062\u590dHDR\u548c\u6e05\u66703D\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002", "summary_cn": "\u4ece\u91ce\u5916\u5e38\u89c1\u7684\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u6a21\u7cca\u56fe\u50cf\u4e2d\u8fdb\u884c\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u5728\u6781\u7aef\u5149\u7167\u6761\u4ef6\u4e0b\u96be\u4ee5\u6062\u590d\u9ad8\u52a8\u6001\u8303\u56f4\uff08HDR\uff09\u548c\u6e05\u6670\u7684\u4e09\u7ef4\u8868\u793a\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5229\u7528\u4e8b\u4ef6\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u5ffd\u7565\u4e86\u76f8\u673a\u8f93\u51fa\u4e0e\u7269\u7406\u4e16\u754c\u8f90\u5c04\u4e4b\u95f4\u7684\u4f20\u611f\u5668\u7269\u7406\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4HDR\u91cd\u5efa\u548c\u53bb\u6a21\u7cca\u6548\u679c\u4e0d\u7406\u60f3\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u4f20\u611f\u5668\u7269\u7406\u539f\u7406\u7684NeRF\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u6b21\u66dd\u5149\u7684\u6a21\u7ccaLDR\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u4e8b\u4ef6\u6570\u636e\u4e2d\u5b9e\u73b0\u6e05\u6670HDR\u65b0\u89c6\u89d2\u5408\u6210\u3002\u6211\u4eec\u91c7\u7528NeRF\u76f4\u63a5\u8868\u793aHDR\u57df\u4e2d3D\u573a\u666f\u7684\u771f\u5b9e\u8f90\u5c04\uff0c\u5e76\u6a21\u62df\u7269\u7406\u4e16\u754c\u4e2d\u649e\u51fb\u4f20\u611f\u5668\u50cf\u7d20\u7684\u539f\u59cbHDR\u573a\u666f\u5149\u7ebf\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u9010\u50cf\u7d20\u7684RGB\u6620\u5c04\u573a\uff0c\u5c06\u4e0a\u8ff0\u6e32\u67d3\u7684\u50cf\u7d20\u503c\u4e0e\u8f93\u5165\u56fe\u50cf\u4e2d\u4f20\u611f\u5668\u8bb0\u5f55\u7684LDR\u50cf\u7d20\u503c\u5bf9\u9f50\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4e8b\u4ef6\u6620\u5c04\u573a\uff0c\u4ee5\u6865\u63a5\u7269\u7406\u573a\u666f\u52a8\u6001\u4e0e\u5b9e\u9645\u4e8b\u4ef6\u4f20\u611f\u5668\u8f93\u51fa\u3002\u8fd9\u4e24\u4e2a\u6620\u5c04\u573a\u4e0eNeRF\u7f51\u7edc\u8054\u5408\u4f18\u5316\uff0c\u5229\u7528\u4e8b\u4ef6\u4e2d\u8574\u542b\u7684\u65f6\u7a7a\u52a8\u6001\u4fe1\u606f\uff0c\u589e\u5f3a\u6e05\u6670HDR\u4e09\u7ef4\u8868\u793a\u7684\u5b66\u4e60\u3002\u5728\u81ea\u5efa\u53ca\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u6b21\u66dd\u5149\u7684\u6a21\u7ccaLDR\u56fe\u50cf\u548c\u5bf9\u5e94\u4e8b\u4ef6\u6570\u636e\uff0c\u5373\u53ef\u5b9e\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684\u53bb\u6a21\u7ccaHDR\u65b0\u89c6\u89d2\u5408\u6210\u6548\u679c\u3002"}}
{"id": "2601.15490", "pdf": "https://arxiv.org/pdf/2601.15490", "abs": "https://arxiv.org/abs/2601.15490", "authors": ["Jobeal Solomon", "Ali Mohammed Mansoor Alsahag", "Seyed Sahand Mohammadi Ziabari"], "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.", "AI": {"tldr": "\u7528Vision Transformer\u66ff\u4ee3U-Net\u4f5c\u4e3a\u5c5e\u6027\u4e2d\u548c\u6846\u67b6\u7684\u7f16\u7801\u5668\uff0c\u53ef\u5728\u4fdd\u7559\u8bca\u65ad\u51c6\u786e\u7387\u7684\u540c\u65f6\u66f4\u6709\u6548\u5730\u51cf\u5c11\u80f8\u90e8X\u5149\u56fe\u50cf\u4e2d\u7684\u6027\u522b\u4e0e\u5e74\u9f84\u5c5e\u6027\u6cc4\u9732\u3002", "motivation": "\u80f8\u90e8X\u5149\u5206\u7c7b\u5668\u5e38\u56e0\u4f9d\u8d56\u6027\u522b\u548c\u5e74\u9f84\u7b49\u6377\u5f84\u7279\u5f81\u800c\u5bf9\u5c11\u6570\u7fa4\u4f53\u9020\u6210\u7cfb\u7edf\u6027\u8bef\u8bca\uff1b\u73b0\u6709\u57fa\u4e8e\u5377\u79ef\u7f16\u7801\u5668\u7684\u50cf\u7d20\u7ea7\u5c5e\u6027\u4e2d\u548c\u65b9\u6cd5\u65e0\u6cd5\u5728\u4e34\u5e8a\u53ef\u7528\u7684\u7f16\u8f91\u5f3a\u5ea6\u4e0b\u5b8c\u5168\u6d88\u9664\u5c5e\u6027\u6cc4\u9732\u3002", "method": "\u5728\u5c5e\u6027\u4e2d\u548c\u6846\u67b6\u4e2d\uff0c\u5c06U-Net\u5377\u79ef\u7f16\u7801\u5668\u66ff\u6362\u4e3a\u6570\u636e\u9ad8\u6548\u7684DeiT-S Vision Transformer\uff0c\u5728ChestX-ray14\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5e76\u572811\u4e2a\u7f16\u8f91\u5f3a\u5ea6\u4e0b\u751f\u6210\u4e2d\u548c\u56fe\u50cf\uff1b\u4f7f\u7528\u72ec\u7acbAI\u5224\u522b\u5668\u8bc4\u4f30\u5c5e\u6027\u6cc4\u9732\u7a0b\u5ea6\uff0c\u7528ConvNet\u8bc4\u4f30\u75be\u75c5\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u4e2d\u7b49\u7f16\u8f91\u5f3a\u5ea6\uff08alpha=0.5\uff09\u4e0b\uff0cViT\u4e2d\u548c\u5668\u5c06\u6027\u522b\u8bc6\u522bAUC\u964d\u81f3\u7ea60.80\uff0c\u6bd4\u539fU-Net\u65b9\u6cd5\u4f4e\u7ea610\u4e2a\u767e\u5206\u70b9\uff08\u4e14\u8bad\u7ec3\u8f6e\u6b21\u51cf\u534a\uff09\uff1b15\u79cd\u75be\u75c5\u9884\u6d4b\u7684\u5b8f\u89c2ROC AUC\u4e0e\u539f\u59cb\u56fe\u50cf\u76f8\u5dee\u4e0d\u8d85\u8fc75\u4e2a\u767e\u5206\u70b9\uff0c\u6700\u5dee\u5b50\u7fa4\u4f53AUC\u4ecd\u63a5\u8fd10.70\u3002", "conclusion": "\u57fa\u4e8e\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u89c6\u89c9\u6a21\u578b\u80fd\u66f4\u6709\u6548\u6291\u5236\u5c5e\u6027\u6cc4\u9732\uff0c\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u7684\u80f8\u90e8X\u5149AI\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "summary_cn": "\u80f8\u90e8X\u5149\u5206\u7c7b\u5668\u4e2d\u7684\u504f\u89c1\u901a\u5e38\u6e90\u4e8e\u5bf9\u6027\u522b\u548c\u5e74\u9f84\u76f8\u5173\u6377\u5f84\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u5bfc\u81f4\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u7cfb\u7edf\u6027\u6f0f\u8bca\u3002\u4ee5\u5f80\u57fa\u4e8e\u5377\u79ef\u7f16\u7801\u5668\u7684\u50cf\u7d20\u7a7a\u95f4\u5c5e\u6027\u4e2d\u548c\u65b9\u6cd5\u867d\u80fd\u51cf\u8f7b\u4f46\u65e0\u6cd5\u5728\u4e34\u5e8a\u53ef\u7528\u7684\u7f16\u8f91\u5f3a\u5ea6\u4e0b\u5b8c\u5168\u6d88\u9664\u6b64\u7c7b\u5c5e\u6027\u6cc4\u9732\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5728\u5c5e\u6027\u4e2d\u548c\u6846\u67b6\u4e2d\uff0c\u4ee5Vision Transformer\u4e3b\u5e72\u66ff\u4ee3U-Net\u5377\u79ef\u7f16\u7801\u5668\u662f\u5426\u80fd\u5728\u4fdd\u7559\u8bca\u65ad\u51c6\u786e\u6027\u7684\u540c\u65f6\u8fdb\u4e00\u6b65\u51cf\u5c11\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u6cc4\u9732\u3002\u6211\u4eec\u5728ChestX-ray14\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u7684Image Transformer Small\uff08DeiT-S\uff09\u4e2d\u548c\u5668\uff0c\u5e76\u5728\u5341\u4e00\u4e2a\u7f16\u8f91\u5f3a\u5ea6\u7ea7\u522b\u4e0b\u751f\u6210\u4e2d\u548c\u56fe\u50cf\uff1b\u8fd9\u4e9b\u56fe\u50cf\u7531\u4e00\u4e2a\u72ec\u7acb\u7684AI\u5224\u522b\u5668\u8bc4\u4f30\u5c5e\u6027\u6cc4\u9732\u7a0b\u5ea6\uff0c\u5e76\u7531\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08ConvNet\uff09\u8bc4\u4f30\u75be\u75c5\u9884\u6d4b\u6027\u80fd\u3002\u5728\u4e2d\u7b49\u7f16\u8f91\u5f3a\u5ea6\uff08alpha = 0.5\uff09\u4e0b\uff0cVision Transformer\uff08ViT\uff09\u4e2d\u548c\u5668\u5c06\u60a3\u8005\u6027\u522b\u8bc6\u522b\u7684\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUC\uff09\u964d\u81f3\u7ea60.80\uff0c\u6bd4\u539f\u59cb\u6846\u67b6\u7684\u5377\u79efU-Net\u7f16\u7801\u5668\u4f4e\u7ea610\u4e2a\u767e\u5206\u70b9\uff0c\u5c3d\u7ba1\u5176\u8bad\u7ec3\u8f6e\u6b21\u4ec5\u4e3a\u540e\u8005\u7684\u4e00\u534a\u3002\u540c\u65f6\uff0c15\u79cd\u75be\u75c5\u53d1\u73b0\u7684\u5b8f\u89c2\u53d7\u8bd5\u8005\u5de5\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08ROC AUC\uff09\u4e0e\u672a\u7f16\u8f91\u57fa\u7ebf\u76f8\u6bd4\u504f\u5dee\u5728\u4e94\u4e2a\u767e\u5206\u70b9\u4ee5\u5185\uff0c\u6700\u5dee\u60c5\u51b5\u4e0b\u7684\u5b50\u7fa4\u4f53AUC\u4ecd\u63a5\u8fd10.70\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u89c6\u89c9\u6a21\u578b\u53ef\u5728\u4e0d\u727a\u7272\u4e34\u5e8a\u6548\u7528\u7684\u524d\u63d0\u4e0b\u8fdb\u4e00\u6b65\u6291\u5236\u5c5e\u6027\u6cc4\u9732\uff0c\u4e3a\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u80f8\u90e8X\u5149AI\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.15507", "pdf": "https://arxiv.org/pdf/2601.15507", "abs": "https://arxiv.org/abs/2601.15507", "authors": ["Jinrui Yang", "Qing Liu", "Yijun Li", "Mengwei Ren", "Letian Zhang", "Zhe Lin", "Cihang Xie", "Yuyin Zhou"], "title": "Controllable Layered Image Generation for Real-World Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.", "AI": {"tldr": "LASAGNA \u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u751f\u6210\u5177\u6709\u903c\u771f\u80cc\u666f\u548c\u9ad8\u8d28\u91cf\u900f\u660e\u524d\u666f\uff08\u542b\u9634\u5f71\u3001\u53cd\u5c04\u7b49\u6548\u679c\uff09\u7684\u56fe\u50cf\u53ca\u5176\u5206\u5c42\u8868\u793a\uff0c\u652f\u6301\u591a\u79cd\u6761\u4ef6\u8f93\u5165\uff0c\u5b9e\u73b0\u9ad8\u53ef\u63a7\u6027\u548c\u4e00\u81f4\u6027\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u7f16\u8f91\u7279\u5b9a\u56fe\u50cf\u5143\u7d20\u65f6\u96be\u4ee5\u4fdd\u6301\u53ef\u63a7\u6027\u548c\u4e00\u81f4\u6027\uff1b\u5206\u5c42\u8868\u793a\u867d\u5177\u7075\u6d3b\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u65e0\u6cd5\u751f\u6210\u5177\u6709\u5408\u7406\u5408\u6210\u5173\u7cfb\u7684\u56fe\u5c42\uff0c\u4e14\u524d\u666f\u7f3a\u4e4f\u771f\u5b9e\u89c6\u89c9\u6548\u679c\uff08\u5982\u9634\u5f71\u3001\u53cd\u5c04\uff09\u3002", "method": "\u63d0\u51fa LASAGNA \u6846\u67b6\uff0c\u4ece\u6587\u672c\u63d0\u793a\u3001\u524d\u666f\u3001\u80cc\u666f\u53ca\u4f4d\u7f6e\u63a9\u7801\u7b49\u591a\u79cd\u6761\u4ef6\u8f93\u5165\u4e2d\u5b66\u4e60\u6b63\u786e\u7684\u56fe\u50cf\u5408\u6210\u65b9\u5f0f\uff0c\u540c\u65f6\u751f\u6210\u903c\u771f\u80cc\u666f\u4e0e\u5e26\u771f\u5b9e\u89c6\u89c9\u6548\u679c\u7684\u900f\u660e\u524d\u666f\u3002\u6784\u5efa\u65b0\u6570\u636e\u96c6 LASAGNA-48K\uff08\u542b\u5e72\u51c0\u80cc\u666f\u548c\u5e26\u7269\u7406\u771f\u5b9e\u6548\u679c\u7684 RGBA \u524d\u666f\uff09\u53ca\u9996\u4e2a\u5206\u5c42\u7f16\u8f91\u57fa\u51c6 LASAGNABENCH\u3002", "result": "LASAGNA \u80fd\u5728\u591a\u4e2a\u56fe\u50cf\u5c42\u4e0a\u540c\u65f6\u751f\u6210\u9ad8\u5ea6\u4e00\u81f4\u4e14\u8fde\u8d2f\u7684\u7ed3\u679c\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u540e\u671f\u7f16\u8f91\u5e94\u7528\uff0c\u5e76\u51c6\u786e\u4fdd\u7559\u5bf9\u8c61\u8eab\u4efd\u4e0e\u89c6\u89c9\u6548\u679c\u3002", "conclusion": "LASAGNA \u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u5206\u5c42\u751f\u6210\u4e2d\u7684\u53ef\u63a7\u6027\u3001\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u611f\u95ee\u9898\uff0c\u6240\u63d0\u51fa\u7684 LASAGNA-48K \u6570\u636e\u96c6\u548c LASAGNABENCH \u57fa\u51c6\u5c06\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002", "summary_cn": "\u8fd1\u671f\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7528\u6237\u5c1d\u8bd5\u7f16\u8f91\u73b0\u6709\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u5143\u7d20\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u4ea7\u751f\u53ef\u63a7\u4e14\u4e00\u81f4\u7684\u7ed3\u679c\u3002\u5206\u5c42\u8868\u793a\u80fd\u591f\u5b9e\u73b0\u7075\u6d3b\u7684\u3001\u7531\u7528\u6237\u9a71\u52a8\u7684\u5185\u5bb9\u521b\u4f5c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u751f\u6210\u5177\u6709\u8fde\u8d2f\u5408\u6210\u5173\u7cfb\u7684\u56fe\u5c42\uff0c\u5176\u5bf9\u8c61\u56fe\u5c42\u4e5f\u901a\u5e38\u7f3a\u4e4f\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u903c\u771f\u7684\u89c6\u89c9\u6548\u679c\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 LASAGNA\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u751f\u6210\u56fe\u50cf\u53ca\u5176\u7ec4\u6210\u56fe\u5c42\u2014\u2014\u4e00\u4e2a\u903c\u771f\u7684\u80cc\u666f\u548c\u4e00\u4e2a\u5177\u6709\u5f15\u4eba\u6ce8\u76ee\u7684\u89c6\u89c9\u6548\u679c\u7684\u9ad8\u8d28\u91cf\u900f\u660e\u524d\u666f\u3002\u4e0e\u4ee5\u5f80\u5de5\u4f5c\u4e0d\u540c\uff0cLASAGNA \u80fd\u591f\u4ece\u5e7f\u6cdb\u7684\u6761\u4ef6\u8f93\u5165\uff08\u5305\u62ec\u6587\u672c\u63d0\u793a\u3001\u524d\u666f\u3001\u80cc\u666f\u548c\u4f4d\u7f6e\u63a9\u7801\uff09\u4e2d\u9ad8\u6548\u5730\u5b66\u4e60\u6b63\u786e\u7684\u56fe\u50cf\u5408\u6210\u65b9\u5f0f\uff0c\u4ece\u800c\u4e3a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u66f4\u5f3a\u7684\u53ef\u63a7\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 LASAGNA-48K\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u5e72\u51c0\u80cc\u666f\u548c\u5e26\u6709\u7269\u7406\u771f\u5b9e\u89c6\u89c9\u6548\u679c\u7684 RGBA \u524d\u666f\u7ec4\u6210\u7684\u65b0\u6570\u636e\u96c6\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86 LASAGNABENCH\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8e\u56fe\u5c42\u7f16\u8f91\u7684\u57fa\u51c6\u3002\u6211\u4eec\u8bc1\u660e\u4e86 LASAGNA \u5728\u540c\u65f6\u751f\u6210\u591a\u4e2a\u56fe\u50cf\u56fe\u5c42\u65f6\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u80fd\u591f\u652f\u6301\u591a\u6837\u5316\u7684\u540e\u671f\u7f16\u8f91\u5e94\u7528\uff0c\u5e76\u51c6\u786e\u4fdd\u7559\u5bf9\u8c61\u8eab\u4efd\u548c\u89c6\u89c9\u6548\u679c\u3002LASAGNA-48K \u548c LASAGNABENCH \u5c06\u516c\u5f00\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdb\u793e\u533a\u7684\u5f00\u653e\u7814\u7a76\u3002\u9879\u76ee\u9875\u9762\u4e3a https://rayjryang.github.io/LASAGNA-Page/\u3002"}}
{"id": "2601.15516", "pdf": "https://arxiv.org/pdf/2601.15516", "abs": "https://arxiv.org/abs/2601.15516", "authors": ["William Huang", "Siyou Pei", "Leyi Zou", "Eric J. Gonzalez", "Ishan Chatterjee", "Yang Zhang"], "title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "16 pages, 11 figures, Presented at ACM CHI 2026. For associated codebase, see https://github.com/hilab-open-source/deltadorsal", "summary": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u5229\u7528\u624b\u80cc\u56fe\u50cf\u7684\u53cc\u6d41\u589e\u91cf\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u52a8\u6001\u624b\u4e0e\u653e\u677e\u59ff\u6001\u7684\u7279\u5f81\uff0c\u5728\u624b\u6307\u4e25\u91cd\u906e\u6321\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5e76\u652f\u6301\u65b0\u578b\u4ea4\u4e92\u65b9\u5f0f\u3002", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e0b\u7684\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u5e38\u56e0\u624b\u6307\u9891\u7e41\u906e\u6321\u800c\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u624b\u90e8\u51e0\u4f55\u4fe1\u606f\u548c\u5927\u578b\u6a21\u578b\uff0c\u96be\u4ee5\u5728\u906e\u6321\u573a\u666f\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165\u53cc\u6d41\u589e\u91cf\u7f16\u7801\u5668\uff0c\u5229\u7528\u5bc6\u96c6\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5668\u6316\u6398\u624b\u80cc\u76ae\u80a4\u5f62\u53d8\u4fe1\u606f\uff0c\u901a\u8fc7\u5bf9\u6bd4\u52a8\u6001\u624b\u4e0e\u57fa\u7ebf\u653e\u677e\u59ff\u6001\u7684\u7279\u5f81\u6765\u5b66\u4e60\u59ff\u6001\u3002", "result": "\u5728\u624b\u6307\u906e\u6321\u226550%\u7684\u573a\u666f\u4e0b\uff0c\u4ec5\u4f7f\u7528\u88c1\u526a\u7684\u624b\u80cc\u56fe\u50cf\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f53\u524d\u6700\u4f18\u6280\u672f\u5c06\u5e73\u5747\u5173\u8282\u89d2\u5ea6\u8bef\u5dee\uff08MPJAE\uff09\u964d\u4f4e\u4e8618%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u906e\u6321\u573a\u666f\u4e0b\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u98df\u6307\u634f\u5408\u4e0e\u70b9\u51fb\u4f30\u8ba1\uff09\u7684\u53ef\u9760\u6027\uff0c\u8fd8\u652f\u6301\u65e0\u9700\u53ef\u89c1\u8fd0\u52a8\u7684\u7b49\u8ddd\u529b\u68c0\u6d4b\u7b49\u65b0\u4ea4\u4e92\u8303\u5f0f\uff0c\u540c\u65f6\u51cf\u5c0f\u4e86\u6a21\u578b\u89c4\u6a21\u3002", "summary_cn": "XR\u8bbe\u5907\u7684\u666e\u53ca\u4f7f\u5f97\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u6210\u4e3a\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u8be5\u89c6\u89d2\u672c\u8eab\u5e38\u56e0\u624b\u6307\u9891\u7e41\u906e\u6321\u800c\u9762\u4e34\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u8fd1\u671f\u5bc6\u96c6\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5668\u7684\u53d1\u5c55\u6240\u63ed\u793a\u7684\u624b\u80cc\u76ae\u80a4\u5f62\u53d8\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u53cc\u6d41\u589e\u91cf\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5bf9\u6bd4\u52a8\u6001\u624b\u90e8\u4e0e\u57fa\u7ebf\u653e\u677e\u59ff\u6001\u7684\u7279\u5f81\u6765\u5b66\u4e60\u59ff\u6001\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u88c1\u526a\u7684\u624b\u80cc\u56fe\u50cf\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u81ea\u906e\u6321\u573a\u666f\uff08\u624b\u6307\u906e\u6321\u226550%\uff09\u4e0b\uff0c\u76f8\u6bd4\u4f9d\u8d56\u5b8c\u6574\u624b\u90e8\u51e0\u4f55\u4fe1\u606f\u548c\u5927\u578b\u6a21\u578b\u4e3b\u5e72\u7684\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c06\u5e73\u5747\u5173\u8282\u89d2\u5ea6\u8bef\u5dee\uff08MPJAE\uff09\u964d\u4f4e\u4e8618%\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u906e\u6321\u573a\u666f\u4e0b\u8bf8\u5982\u98df\u6307\u634f\u5408\u4e0e\u70b9\u51fb\u4f30\u8ba1\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u53ef\u9760\u6027\uff0c\u8fd8\u5f00\u542f\u4e86\u65b0\u7684\u4ea4\u4e92\u8303\u5f0f\uff0c\u4f8b\u5982\u5728\u65e0\u53ef\u89c1\u8fd0\u52a8\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u7b49\u8ddd\u529b\u4ee5\u5b9e\u73b0\u8868\u9762\u201c\u70b9\u51fb\u201d\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6a21\u578b\u5c3a\u5bf8\u3002"}}
{"id": "2601.15549", "pdf": "https://arxiv.org/pdf/2601.15549", "abs": "https://arxiv.org/abs/2601.15549", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVIOLA\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u4e13\u5bb6\u6807\u6ce8\u4e0e\u5927\u91cf\u65e0\u6807\u7b7e\u89c6\u9891\u6570\u636e\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u89c6\u9891\u9886\u57df\u7684\u9ad8\u6548\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "motivation": "\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6cdb\u5316\u5230\u65b0\u7684\u89c6\u9891\u9886\u57df\u5bf9\u73b0\u5b9e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff1b\u73b0\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6837\u672c\uff0c\u5728\u5de5\u4e1a\u6216\u624b\u672f\u7b49\u4e13\u4e1a\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u65bd\u3002", "method": "\u63d0\u51faVIOLA\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u5bc6\u5ea6-\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u91c7\u6837\u7b56\u7565\uff0c\u5728\u4e25\u683c\u6807\u6ce8\u9884\u7b97\u4e0b\u9009\u62e9\u517c\u5177\u591a\u6837\u6027\u3001\u4ee3\u8868\u6027\u548c\u4fe1\u606f\u91cf\u7684\u6837\u672c\uff1b2\uff09\u6784\u5efa\u6df7\u5408\u6c60\u5e76\u5f15\u5165\u7f6e\u4fe1\u5ea6\u611f\u77e5\u68c0\u7d22\u4e0e\u63d0\u793a\u673a\u5236\uff0c\u6709\u6548\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u907f\u514d\u566a\u58f0\u4f20\u64ad\u3002", "result": "\u57289\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u548c4\u79cdMLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\uff0c\u4ee5\u6781\u4f4e\u6807\u6ce8\u6210\u672c\u5b9e\u73b0\u7a33\u5065\u9002\u5e94\u3002", "conclusion": "VIOLA\u901a\u8fc7\u6700\u5c0f\u6807\u6ce8\u4e0e\u65e0\u6807\u7b7e\u6570\u636e\u7684\u534f\u540c\u5229\u7528\uff0c\u4e3aMLLM\u5728\u65b0\u89c6\u9891\u9886\u57df\u7684\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u6cdb\u5316\u5230\u65b0\u7684\u89c6\u9891\u9886\u57df\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9002\u5e94\u8def\u5f84\uff0c\u4f46\u6807\u51c6\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6837\u672c\u6c60\uff0c\u5728\u5de5\u4e1a\u6216\u624b\u672f\u7b49\u4e13\u4e1a\u73af\u5883\u4e2d\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u4e13\u5bb6\u8fdb\u884c\u6807\u6ce8\u3002\u4e3a\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VIOLA\uff08Video In-cOntext Learning with minimal Annotation\uff09\uff0c\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5c06\u6700\u5c11\u7684\u4e13\u5bb6\u76d1\u7763\u4e0e\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u76f8\u7ed3\u5408\u3002\u9996\u5148\uff0c\u4e3a\u4e86\u5728\u4e25\u683c\u7684\u6807\u6ce8\u9884\u7b97\u4e0b\u6700\u5927\u5316\u6548\u7387\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u5bc6\u5ea6-\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u91c7\u6837\u65b9\u6cd5\u3002\u4e0e\u53ef\u80fd\u9009\u53d6\u89c6\u89c9\u5f02\u5e38\u503c\u7684\u6807\u51c6\u591a\u6837\u6027\u6216\u4e0d\u786e\u5b9a\u6027\u7b56\u7565\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5bc6\u5ea6\u4f30\u8ba1\u6765\u8bc6\u522b\u540c\u65f6\u5177\u5907\u591a\u6837\u6027\u3001\u4ee3\u8868\u6027\u548c\u4fe1\u606f\u91cf\u7684\u6837\u672c\u3002\u5176\u6b21\uff0c\u4e3a\u5728\u4e0d\u4f20\u64ad\u566a\u58f0\u7684\u524d\u63d0\u4e0b\u5229\u7528\u5269\u4f59\u7684\u672a\u6807\u6ce8\u6570\u636e\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6df7\u5408\u6c60\uff0c\u5e76\u5f15\u5165\u4e86\u7f6e\u4fe1\u5ea6\u611f\u77e5\u68c0\u7d22\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u63d0\u793a\u673a\u5236\u3002\u8fd9\u4e9b\u673a\u5236\u663e\u5f0f\u5efa\u6a21\u6807\u7b7e\u53ef\u9760\u6027\uff0c\u6839\u636e\u76f8\u4f3c\u5ea6\u4e0e\u7f6e\u4fe1\u5ea6\u7684\u7efc\u5408\u5f97\u5206\u68c0\u7d22\u793a\u4f8b\uff0c\u5e76\u4f7fMLLM\u80fd\u591f\u81ea\u9002\u5e94\u5730\u533a\u5206\u5df2\u9a8c\u8bc1\u7684\u771f\u5b9e\u6807\u7b7e\u4e0e\u542b\u566a\u7684\u4f2a\u6807\u7b7e\u3002\u5728\u56db\u4e2aMLLM\u548c\u4e5d\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u7684\u6807\u6ce8\u6210\u672c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u9002\u5e94\u6548\u679c\u3002"}}
