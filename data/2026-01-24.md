<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: 本文提出三种方法以提升涵洞与污水管道缺陷分割效果，尤其在标注数据稀缺的现实条件下：（1）有效的预处理策略；（2）新型轻量高效架构FORTRESS；（3）基于注意力机制的少样本语义分割方法。


<details>
  <summary>Details</summary>
Motivation: 涵洞和污水管道是排水系统的关键组成部分，其缺陷检测对公共安全和环境至关重要。然而，该领域数据采集与标注成本高、依赖专业知识，难以获得大规模标注数据集，因此亟需能在有限标注数据下有效工作的自动化缺陷分割方法。

Method: 1. 评估包括传统数据增强和动态标签注入在内的预处理策略；2. 提出名为FORTRESS的新架构，融合深度可分离卷积、自适应Kolmogorov-Arnold网络（KAN）和多尺度注意力机制；3. 探索少样本语义分割，采用带注意力机制的双向原型网络进行缺陷检测。

Result: 所提预处理策略显著提升了IoU和F1分数；FORTRESS在涵洞/污水管缺陷数据集上达到SOTA性能，同时大幅减少可训练参数和计算开销；少样本方法在有限标注数据下也取得了令人满意的评估指标结果。

Conclusion: 通过增强训练数据或调整模型架构，本文提出的三种方法能有效应对涵洞与污水管道缺陷分割中的数据稀缺挑战，在真实场景中具有良好的应用前景。

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

Abstract (中文翻译): 涵洞和污水管道是排水系统的关键组成部分，其失效可能对公共安全和环境造成严重风险。本论文探索了改进涵洞和污水管道自动化缺陷分割的方法。该领域的数据收集与标注过程繁琐且需要专业知识，因此难以获得用于结构缺陷检测的大规模数据集。我们所提出的方法在标注数据有限的条件下进行了测试，以证明其在现实场景中的适用性。总体而言，本论文提出了三种方法，显著增强了缺陷分割能力并应对数据稀缺问题，这可以通过增强训练数据或调整模型架构来实现。首先，我们评估了包括传统数据增强和动态标签注入在内的预处理策略，这些技术显著提升了分割性能，提高了交并比（IoU）和F1分数。其次，我们提出了名为FORTRESS的新型架构，该架构结合了深度可分离卷积、自适应科尔莫戈罗夫-阿诺德网络（KAN）和多尺度注意力机制。FORTRESS在涵洞污水管道缺陷数据集上实现了最先进的性能，同时显著减少了可训练参数数量和计算成本。最后，我们研究了少样本语义分割及其在缺陷检测中的适用性。少样本学习旨在仅使用有限数据训练模型。通过采用带有注意力机制的双向原型网络，该模型获得了更丰富的特征表示，并在各项评估指标上取得了令人满意的结果。

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文系统评估了多模态大语言模型（MLLMs）在异质人脸识别（HFR）任务中的表现，发现其在跨光谱条件下显著落后于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs在视觉-语言任务中的成功，研究者开始关注其在生物特征识别（尤其是异质人脸识别）中的潜力。然而，尚缺乏对其在该任务中性能的系统性评估。

Method: 作者在多种跨模态场景（VIS-NIR、VIS-SWIR、VIS-THERMAL）下，对多个开源MLLMs进行了基准测试，并采用生物识别协议和指标（如Acquire Rate、EER、TAR）进行评估。

Result: 实验结果表明，尽管MLLMs近期取得了进展，但在具有挑战性的跨光谱条件下，其性能与传统人脸识别系统相比仍存在显著差距。

Conclusion: 当前的MLLMs在异质人脸识别任务中仍有明显局限性，在将其部署到实际人脸识别系统前，必须进行严格的生物特征识别评估。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

Abstract (中文翻译): 多模态大语言模型（MLLMs）最近在广泛的视觉-语言任务中展现出强大的性能，引发了人们对其在生物特征识别应用中潜力的关注。在本文中，我们对最先进的MLLMs在异质人脸识别（HFR）任务中进行了系统性评估，其中注册图像和查询图像来自不同的传感模态，包括可见光（VIS）、近红外（NIR）、短波红外（SWIR）和热成像相机。我们在多种跨模态场景（包括VIS-NIR、VIS-SWIR和VIS-THERMAL人脸识别）下对多个开源MLLMs进行了基准测试。MLLMs的识别性能使用生物识别协议并基于不同指标（包括获取率（Acquire Rate）、等错误率（EER）和真接受率（TAR））进行评估。我们的结果揭示了MLLMs与经典人脸识别系统之间存在的显著性能差距，尤其是在具有挑战性的跨光谱条件下，尽管MLLMs近期已取得进展。我们的研究结果突显了当前MLLMs在HFR任务中的局限性，同时也强调了在考虑将其部署到人脸识别系统时进行严格生物特征识别评估的重要性。

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE 是一种无需额外数据的课程学习框架，通过动态调整训练样本提升医学视觉-语言模型在放射报告生成中的视觉定位准确性和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型在生成放射报告时存在视觉定位不准和文本与图像证据不一致的问题，导致结果不可靠。

Method: CURE 在短语定位、基于定位的报告生成和解剖结构引导的报告生成任务上对多模态指令模型进行微调，利用公开数据集，并根据模型表现动态调整采样策略，优先训练更难的样本以提升空间与文本对齐。

Result: CURE 将定位准确率提升 +0.37 IoU，报告质量提高 +0.188 CXRFEScore，并减少 18.6% 的幻觉现象。

Conclusion: CURE 是一种高效的数据利用框架，显著提升了医学报告生成中的定位精度与内容可靠性。

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

Abstract (中文翻译): 医学视觉-语言模型能够自动生​​成放射学报告，但在准确的视觉定位和事实一致性方面仍面临挑战。现有模型常常将文本发现与视觉证据错误对齐，导致预测结果不可靠或定位薄弱。我们提出了 CURE，一种具有错误感知能力的课程学习框架，无需额外数据即可提升定位能力和报告质量。CURE 利用公开数据集，在短语定位、基于定位的报告生成以及解剖结构引导的报告生成任务上对多模态指令模型进行微调。该方法根据模型性能动态调整样本采样策略，重点训练更难的样本，以改善空间与文本对齐。实验表明，CURE 将定位准确率（IoU）提高了 +0.37，报告质量（CXRFEScore）提升了 +0.188，并减少了 18.6% 的幻觉现象。CURE 是一种数据高效的框架，有效增强了定位准确性与报告可靠性。代码地址：https://github.com/PabloMessina/CURE，模型权重地址：https://huggingface.co/pamessina/medgemma-4b-it-cure。

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本文提出DuFal，一种双频域感知学习框架，通过结合全局与局部高频增强的傅里叶神经算子，在稀疏视角锥束CT重建中显著提升高频解剖细节的恢复能力。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角锥束CT重建因X射线投影不足而难以恢复高频解剖细节；传统CNN方法偏向低频信息，无法有效重建精细结构。

Method: 提出DuFal框架，包含双路径架构：全局高频增强傅里叶神经算子和局部高频增强傅里叶神经算子；引入谱-通道因子化降低参数量，并设计跨注意力频率融合模块整合空间与频率特征，最终通过特征解码器和强度场解码流程重建CT体积。

Result: 在LUNA16和ToothFairy数据集上的实验表明，DuFal在极稀疏视角下显著优于现有最先进方法，尤其在保留高频解剖特征方面表现突出。

Conclusion: DuFal通过联合建模空间域与频域信息，有效解决了稀疏视角CT重建中高频细节丢失的问题，为医学影像重建提供了新思路。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

Abstract (中文翻译): 稀疏视角锥束计算机断层扫描（Cone-Beam Computed Tomography, CBCT）从有限X射线投影中重建图像在医学成像中仍是一个具有挑战性的问题，因为细粒度解剖细节（对应于高频成分）存在固有的欠采样。传统的基于卷积神经网络（CNN）的方法通常难以恢复这些精细结构，因为它们往往偏向于学习低频信息。为解决这一问题，本文提出了DuFal（Dual-Frequency-Aware Learning），一种新颖的框架，通过双路径架构将频域与空间域处理相结合。其核心创新在于我们提出的高-局部分解傅里叶神经算子（High-Local Factorized Fourier Neural Operator），该算子包含两个互补分支：一个全局高频增强傅里叶神经算子用于捕捉全局频率模式，另一个局部高频增强傅里叶神经算子则对空间划分的图像块进行处理，以保留可能在全局频域分析中丢失的空间局部性。为提高效率，我们设计了一种谱-通道因子化方案，以减少傅里叶神经算子的参数数量。此外，我们还设计了一个跨注意力频率融合模块，以有效整合空间与频率特征。融合后的特征通过特征解码器生成投影表示，再经由强度场解码流程重建最终的CT体积。在LUNA16和ToothFairy数据集上的实验结果表明，DuFal在保留高频解剖特征方面显著优于现有最先进的方法，尤其是在极稀疏视角设置下。

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 本文提出一种偏差引导的提示学习框架，结合视觉-语言模型的语义能力和基于统计偏差的评分机制，在仅有少量正常样本的情况下实现更优的异常区域检测。


<details>
  <summary>Details</summary>
Motivation: 少样本正常样本异常检测（FNSAD）因训练样本极少且异常类型多样而极具挑战；现有基于CLIP等视觉-语言模型的方法在正常与异常提示之间区分度不足，且缺乏有效的逐块异常评分机制。

Method: 提出偏差引导的提示学习框架：使用可学习的上下文向量替代固定提示前缀，并引入异常特定后缀以实现类别感知对齐；通过Top-K多实例学习（MIL）构建偏差损失，将图像块特征建模为正态分布的高斯偏差，从而赋予显著偏离的图像块更高异常分数。

Result: 在MVTecAD和VISA基准上，该方法在像素级检测性能上优于PromptAD及其他基线方法；消融实验验证了可学习提示、偏差评分和Top-K MIL策略的有效性。

Conclusion: 所提方法有效提升了少样本异常检测中的定位精度与可解释性，证明了融合语义提示与统计偏差机制的优越性。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

Abstract (中文翻译): 少样本正常样本异常检测（FNSAD）旨在仅使用少量正常训练样本检测图像中的异常区域，由于监督信息有限且潜在缺陷种类繁多，该任务极具挑战性。近期方法利用CLIP等视觉-语言模型，通过基于提示的学习来对齐图像与文本特征。然而，现有方法在正常与异常提示之间的判别能力较弱，且缺乏针对图像块级别异常的合理评分机制。我们提出一种偏差引导的提示学习框架，将视觉-语言模型的语义能力与基于偏差的统计可靠性相结合。具体而言，我们用可学习的上下文向量替代固定的提示前缀，这些向量在正常与异常提示间共享，同时引入异常特定的后缀标记以实现类别感知对齐。为增强可分性，我们引入结合Top-K多实例学习（MIL）的偏差损失，将图像块特征建模为相对于正常分布的高斯偏差。这使得网络能为具有统计显著偏差的图像块分配更高的异常分数，从而提升定位精度与可解释性。在MVTecAD和VISA基准上的实验表明，该方法在像素级检测性能上优于PromptAD及其他基线方法。消融研究进一步验证了可学习提示、基于偏差的评分机制以及Top-K MIL策略的有效性。

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于传感器物理原理的统一NeRF框架，利用单次曝光的模糊低动态范围（LDR）图像和事件数据，实现高质量的高动态范围（HDR）清晰新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从野外常见的模糊LDR图像中进行新视角合成时，难以恢复极端光照条件下的HDR和清晰3D表示；且虽有方法引入事件数据，但忽略了相机输出与真实世界辐射之间的传感器物理不匹配问题，导致HDR重建和去模糊效果不佳。

Method: 作者提出一个统一的、基于传感器物理的NeRF框架：1）使用NeRF直接建模HDR域中的真实3D场景辐射，并模拟物理世界中到达传感器像素的原始HDR光线；2）引入逐像素RGB映射场，将渲染像素值与输入LDR图像的传感器记录值对齐；3）设计新的事件映射场，连接物理场景动态与实际事件传感器输出；4）联合优化两个映射场与NeRF网络，利用事件中的时空动态信息提升HDR清晰3D表示学习。

Result: 在自建和公开数据集上的实验表明，该方法仅使用单次曝光的模糊LDR图像及其对应事件数据，即可在HDR去模糊新视角合成任务上达到当前最优性能。

Conclusion: 通过将传感器物理机制显式建模进NeRF框架，并结合事件数据的时空信息，该方法有效解决了模糊LDR图像在极端光照下难以恢复HDR和清晰3D结构的问题，显著提升了新视角合成质量。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

Abstract (中文翻译): 从野外常见的低动态范围（LDR）模糊图像中进行新视角合成，在极端光照条件下难以恢复高动态范围（HDR）和清晰的三维表示。尽管现有方法利用事件数据来解决这一问题，但它们忽略了相机输出与物理世界辐射之间的传感器物理不匹配问题，导致HDR重建和去模糊效果不理想。为解决此问题，我们提出了一种统一的、基于传感器物理原理的NeRF框架，仅需单次曝光的模糊LDR图像及其对应的事件数据，即可实现清晰HDR新视角合成。我们采用NeRF直接表示HDR域中3D场景的真实辐射，并模拟物理世界中击中传感器像素的原始HDR场景光线。为此，我们引入了一个逐像素的RGB映射场，将上述渲染的像素值与输入图像中传感器记录的LDR像素值对齐；同时设计了一个新颖的事件映射场，以连接物理场景动态与实际事件传感器输出。这两个映射场与NeRF网络联合优化，利用事件中蕴含的时空动态信息，增强清晰HDR三维表示的学习。在自建及公开数据集上的实验表明，我们的方法仅使用单次曝光的模糊LDR图像和对应事件数据，即可在HDR去模糊新视角合成任务上取得当前最先进的结果。

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 用Vision Transformer替代U-Net编码器可更有效减少胸片分类器中的性别/年龄属性泄露，同时保持诊断性能。


<details>
  <summary>Details</summary>
Motivation: 胸片分类器常因依赖性别和年龄等捷径特征而对少数群体系统性误诊；现有基于卷积编码器的属性中和方法无法在临床可用强度下完全消除属性泄露。

Method: 在属性中和框架中，将U-Net卷积编码器替换为数据高效的DeiT-S Vision Transformer骨干网络，在ChestX-ray14数据集上训练，并在11个编辑强度下生成图像，由独立AI评估属性泄露、由ConvNet评估疾病预测能力。

Result: 在中等编辑强度（alpha=0.5）下，ViT中和器将性别识别AUC降至约0.80，比原U-Net方法低10个百分点（且训练轮数减半）；15种疾病的整体ROC AUC与原始基线相差不超过5个百分点，最差子群体AUC仍接近0.70。

Conclusion: 全局自注意力视觉模型能更有效地抑制属性泄露而不牺牲临床效用，为构建更公平的胸片AI提供可行路径。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

Abstract (中文翻译): 胸片分类器中的偏见通常源于对性别和年龄相关捷径特征的依赖，导致对少数群体的系统性漏诊。以往基于像素空间的属性中和方法依赖卷积编码器，虽能减轻但无法在临床可用的编辑强度下完全消除此类属性泄露。本研究评估了在属性中和框架中，用Vision Transformer主干网络替代U-Net卷积编码器是否能在保留诊断准确性的同时进一步减少人口统计学属性泄露。我们在ChestX-ray14数据集上训练了一个数据高效的Image Transformer Small（DeiT-S）中和器，并在十一个编辑强度级别下生成编辑图像，分别由独立AI评估属性泄露程度，以及由卷积神经网络（ConvNet）评估疾病预测性能。在中等编辑强度（alpha = 0.5）下，Vision Transformer（ViT）中和器将患者性别识别的曲线下面积（AUC）降至约0.80，比原始框架的卷积U-Net编码器低约10个百分点，尽管其训练轮数仅为后者的一半。同时，15种疾病发现的宏观ROC AUC与未编辑基线相比波动不超过5个百分点，最差子群体的AUC仍维持在0.70左右。这些结果表明，全局自注意力视觉模型可在不牺牲临床效用的前提下进一步抑制属性泄露，为实现更公平的胸片AI提供了切实可行的路径。

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 提出LASAGNA框架，联合生成具有真实视觉效果（如阴影、反射）的背景与透明前景图层，并配套发布新数据集LASAGNA-48K和首个图层编辑基准LASAGNABENCH。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在编辑特定元素时难以保持可控性和一致性；现有分层方法生成的图层缺乏真实感视觉效果，且合成关系不连贯。

Method: 提出统一框架LASAGNA，从文本提示、前景、背景和位置掩码等多种条件输入中学习正确图像合成；构建包含干净背景和带物理真实视觉效果RGBA前景的新数据集LASAGNA-48K；并建立首个图层编辑基准LASAGNABENCH。

Result: LASAGNA能同时生成高度一致且连贯的多图层图像，支持多样化的后期编辑应用，并准确保留对象身份和视觉效果。

Conclusion: LASAGNA显著提升了图像分层生成的可控性与真实感，所发布的数据集与基准将促进社区在该方向的开放研究。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

Abstract (中文翻译): 近期的图像生成模型虽取得显著进展，但在用户尝试编辑现有图像中的特定元素时，往往难以产生可控且一致的结果。分层表示能够实现灵活的用户驱动内容创作，但现有方法通常无法生成具有连贯合成关系的图层，其对象图层也普遍缺乏阴影和反射等逼真的视觉效果。为克服这些局限，我们提出了LASAGNA——一种新颖的统一框架，可联合生成图像及其组成图层：一个逼真的背景和一个带有引人注目的视觉效果的高质量透明前景。与以往工作不同，LASAGNA能从多种条件输入（包括文本提示、前景、背景和位置掩码）中高效学习正确的图像合成方式，从而为现实应用提供更强的可控性。为此，我们构建了LASAGNA-48K数据集，其中包含干净的背景和具有物理真实视觉效果的RGBA前景。我们还提出了LASAGNABENCH，这是首个面向图层编辑的基准。我们证明了LASAGNA在同时生成多个图像图层时表现出高度一致性和连贯性，支持多样化的后期编辑应用，并能准确保留对象身份和视觉效果。LASAGNA-48K和LASAGNABENCH将公开发布，以促进社区的开放研究。项目页面为：https://rayjryang.github.io/LASAGNA-Page/。

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: 本文提出一种利用手背皮肤形变信息的新方法，通过双流增量编码器从仅包含手背区域的图像中估计自我遮挡下的手指姿态，在高遮挡场景下显著优于现有方法，并支持新型交互方式。


<details>
  <summary>Details</summary>
Motivation: 第一人称视角（egocentric）的手势估计常因手指频繁自遮挡而性能下降，现有方法依赖完整手部几何结构和大型模型，难以在遮挡严重时保持鲁棒性。

Method: 提出一种双流增量编码器（dual-stream delta encoder），通过对比动态手部与放松基准姿态下的密集视觉特征（特别是手背皮肤形变信息）来学习手势姿态，仅使用裁剪后的手背图像进行估计。

Result: 在手指遮挡率≥50%的场景下，仅用裁剪手背图像即可将平均关节角度误差（MPJAE）比当前最优方法降低18%，同时模型更小，并提升了下游任务（如捏合、点击估计）的可靠性。

Conclusion: 该方法有效利用手背形变信息克服自遮挡问题，不仅提高了遮挡下手势估计的准确性，还支持无可见动作的等距力检测等新交互范式，兼具精度与轻量化优势。

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

Abstract (中文翻译): XR设备的普及使得以自我为中心的手势姿态估计成为一项关键任务，但该视角下手指频繁发生自遮挡，给估计带来固有挑战。为解决这一问题，我们提出一种新方法，利用近期密集视觉特征提取器所揭示的手背皮肤形变中的丰富信息。我们引入一种双流增量编码器，通过对比动态手部与基准放松姿态下的特征来学习手势姿态。评估结果表明，仅使用裁剪后的手背图像，我们的方法在手指遮挡率≥50%的自遮挡场景中，相比依赖完整手部几何结构和大型模型主干的最先进方法，将平均关节角度误差（MPJAE）降低了18%。因此，我们的方法不仅提升了遮挡场景下如食指捏合与点击等下游任务的可靠性，还实现了新的交互范式，例如在无可见动作的情况下检测等距力以实现“点击”操作，同时显著减小了模型规模。

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 本文提出VIOLA框架，通过结合少量专家标注与大量无标签视频数据，实现多模态大语言模型在新视频领域的高效上下文学习。


<details>
  <summary>Details</summary>
Motivation: 将多模态大语言模型（MLLMs）泛化到新的视频领域对实际应用至关重要，但受限于标注数据稀缺。现有上下文学习方法依赖大量标注样本，在工业或手术等专业场景中难以实现，因此需要一种更少依赖专家标注的高效适应方法。

Method: 提出VIOLA框架：1）采用密度-不确定性加权采样策略，在严格标注预算下选择多样、具代表性且信息丰富的样本；2）构建混合池并引入置信度感知检索与提示机制，利用无标签数据同时避免噪声传播，使模型能区分真实标签与伪标签。

Result: 在四个MLLM和九个多样化基准上的实验表明，该方法在低资源设定下显著优于多种基线，以极低标注成本实现稳健的领域适应。

Conclusion: VIOLA通过最小化专家标注需求并有效利用无标签数据，为多模态大语言模型在新视频领域的实际部署提供了一种高效、鲁棒的上下文学习方案。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

Abstract (中文翻译): 将多模态大语言模型（MLLMs）泛化到新的视频领域对于现实世界的应用至关重要，但由于标注数据稀缺，这一任务仍具挑战性。尽管上下文学习（ICL）提供了一种无需训练的适应路径，但标准方法依赖大量标注样本池，在工业或手术等专业环境中往往不切实际，因为这些场景需要专家进行标注。为弥合这一差距，我们提出了VIOLA（Video In-cOntext Learning with minimal Annotation），一种标签高效的框架，将最少的专家监督与大量无标签数据相结合。首先，为了在严格的标注预算下最大化效率，我们提出了密度-不确定性加权采样方法。与可能选择视觉异常值的标准多样性或不确定性策略不同，我们的方法利用密度估计来识别同时具备多样性、代表性和信息量的样本。其次，为在不传播噪声的前提下利用剩余的无标签数据，我们构建了一个混合池，并引入了置信度感知检索与置信度感知提示机制。这些机制显式建模标签可靠性，根据相似度与置信度的综合得分检索示例，并使MLLM能够自适应地区分已验证的真实标签与含噪的伪标签。在四个MLLM和九个多样化基准上的大量实验表明，该框架在低资源设置下显著优于多种基线方法，以最小的标注成本实现了稳健的适应效果。

</details>
