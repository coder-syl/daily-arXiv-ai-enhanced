<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders](https://arxiv.org/abs/2601.06067)
*Chimdi Walter Ndubuisi,Toni Kazic*

Main category: cs.CV

TL;DR: 本文提出HyperTopo-Adapters，一种轻量级、参数高效的头部模块，结合双曲+欧几里得+球面（H+E+S）乘积流形嵌入与拓扑先验（持久同调距离和可微代理损失），用于提升叶部病斑分割中的边界与拓扑结构精度，同时保持Dice/IoU性能，并提供开源可复现的训练/评估套件。


<details>
  <summary>Details</summary>
Motivation: 标准像素级损失在欧式潜在空间中对叶部病斑分割中的小合并、分裂或虚假孔洞等拓扑敏感错误惩罚不足，而这些错误在生物学上可能具有重要意义，因此需要引入几何与拓扑先验以提升分割的结构保真度。

Method: 作者提出HyperTopo-Adapters，作为冻结视觉编码器之上的轻量级头部，将特征嵌入到双曲+欧几里得+球面（H+E+S）乘积流形中，并引入拓扑先验：(i) 用持久同调（PH）距离进行评估与模型选择；(ii) 使用结合软欧拉特征匹配与全变差正则化的可微代理损失进行稳定训练。此外还包括针对双曲对比项和拓扑先验的warm-up策略、样本级结构感知指标评估，以及基于min-PD within top-K Dice的检查点选择规则。

Result: 在Kaggle叶部病斑数据集（N=2,940）上的初步结果表明，该方法在边界和拓扑指标上持续提升（如将Δβ₁孔洞误差降低9%），同时Dice/IoU保持竞争力。作者还进行了多项控制消融实验，并测试不同编码器、输入分辨率、PH权重及部分解冻策略的影响。

Conclusion: 本研究通过设计诊断性实验，验证了几何与拓扑先验对提升分割结构保真度的有效性，并开源了可复现的训练/评估套件，旨在揭示失败模式并指导未来更强大的拓扑保持架构设计。

Abstract: Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.

Abstract (中文翻译): 叶部病斑分割对拓扑结构敏感：微小的合并、分裂或虚假孔洞在生物学上可能是生化通路的重要描述符，但标准的像素级损失在欧式潜在空间中对此类错误惩罚较弱。本文探索了HyperTopo-Adapters——一种轻量级、参数高效的头部模块，它在冻结的视觉编码器之上进行训练，将特征嵌入到双曲+欧几里得+球面（H+E+S）乘积流形中，以促进层次分离（H）、局部线性细节（E）和全局闭合性（S）。拓扑先验以两种形式补充Dice/BCE损失：(i) 用于评估与模型选择的持久同调（PH）距离；(ii) 一种可微代理损失，结合软欧拉特征匹配与全变差正则化以实现稳定训练。本文还引入了针对双曲对比项和拓扑先验的预热策略、样本级结构感知指标（Boundary-F1、Betti误差、PD距离）评估，以及基于top-K Dice中最小PD距离的检查点选择规则。在Kaggle叶部病斑数据集（N=2,940）上的初步结果表明，该方法在边界和拓扑指标上持续提升（将Δβ₁孔洞误差降低9%），同时Dice/IoU保持竞争力。本研究在设计上具有诊断性：报告了多项受控消融实验（曲率学习、潜在维度、对比温度、代理设置），并正在进行不同编码器（ResNet-50、DeepLabV3、DINOv2/v3）、输入分辨率、PH权重及后期模块部分解冻的测试。本文贡献了一个开源、可复现的训练/评估套件（https://github.com/ChimdiWalter/HyperTopo-Adapters），用于分离几何/拓扑先验并揭示失败模式，以指导更强的拓扑保持架构设计。

</details>


### [2] [OptFormer: Optical Flow-Guided Attention and Phase Space Reconstruction for SST Forecasting](https://arxiv.org/abs/2601.06078)
*Yin Wang,Chunlin Gong,Zhuozhen Xu,Lehan Zhang,Xiang Wu*

Main category: cs.CV

TL;DR: 提出OptFormer模型，结合相空间重构与光流引导的运动感知注意力机制，显著提升海表温度（SST）预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 海表温度（SST）预测对气候建模和灾害预报至关重要，但由于其非线性时空动态特性及较长的预测时间跨度，仍具挑战性。

Method: 提出一种新型编码器-解码器模型OptFormer，融合相空间重构与由光流引导的运动感知注意力机制，利用帧间运动线索突出空间场中的相对变化，从而更有效地捕捉动态区域和长程时间依赖。

Result: 在多尺度NOAA SST数据集上的实验表明，OptFormer在1:1训练-预测设置下显著优于现有基线方法，在准确性和鲁棒性方面表现优异。

Conclusion: OptFormer通过引入运动感知注意力机制有效提升了SST预测性能，为复杂时空序列建模提供了新思路。

Abstract: Sea Surface Temperature (SST) prediction plays a vital role in climate modeling and disaster forecasting. However, it remains challenging due to its nonlinear spatiotemporal dynamics and extended prediction horizons. To address this, we propose OptFormer, a novel encoder-decoder model that integrates phase-space reconstruction with a motion-aware attention mechanism guided by optical flow. Unlike conventional attention, our approach leverages inter-frame motion cues to highlight relative changes in the spatial field, allowing the model to focus on dynamic regions and capture long-range temporal dependencies more effectively. Experiments on NOAA SST datasets across multiple spatial scales demonstrate that OptFormer achieves superior performance under a 1:1 training-to-prediction setting, significantly outperforming existing baselines in accuracy and robustness.

Abstract (中文翻译): 海表温度（SST）预测在气候建模和灾害预报中起着至关重要的作用。然而，由于其非线性的时空动态特性以及较长的预测时间跨度，SST预测仍然具有挑战性。为解决这一问题，我们提出了OptFormer——一种新颖的编码器-解码器模型，该模型将相空间重构与由光流引导的运动感知注意力机制相结合。与传统注意力机制不同，我们的方法利用帧间运动线索来突出空间场中的相对变化，使模型能够聚焦于动态区域，并更有效地捕捉长程时间依赖关系。在多个空间尺度的NOAA SST数据集上的实验表明，OptFormer在1:1的训练-预测设置下实现了优越的性能，显著优于现有的基线方法，在准确性和鲁棒性方面均表现出色。

</details>


### [3] [Semantic Event Graphs for Long-Form Video Question Answering](https://arxiv.org/abs/2601.06097)
*Aradhya Dixit,Tianxi Liang*

Main category: cs.CV

TL;DR: 本文提出语义事件图（SEG），一种轻量级符号化视频-语言接口，将原始视频帧转换为紧凑的时间交互日志，显著减少长视频问答所需的token数量，在保持高准确率的同时大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代视觉-语言模型在处理小时级长视频问答时面临巨大挑战，因为密集的视觉嵌入会迅速超出实际可用的token和计算预算。现有方法通常通过下采样帧或向大上下文语言模型输入密集视觉嵌入来权衡时间覆盖范围与成本，效果有限。

Method: 提出语义事件图（SEG）方法：首先使用YOLOv11检测并跟踪物体；然后将物体间的邻近模式转化为START/END形式的人-物交互事件，并组织成时间场景图（TSG）；推理时，通过查询感知剪枝模块识别锚点实体和词汇相关事件，仅返回小型子图，将其文本化后输入Gemini 2.5 Flash生成答案。

Result: 在五个YouTube视频（每个含300–500次交互）和120个自动生成的长时程问题上，SEG以每查询仅3.47k tokens达到65.0%准确率，接近使用40.39k tokens的完整日志基线（62.5%），token使用减少91.4%；而仅使用最后30秒视频的短上下文基线准确率骤降至2.5%。

Conclusion: 符号化时间图可作为现成视觉-语言模型的有效即插即用记忆层，在保留长距离推理能力的同时，显著提升长视频问答的token效率和成本效益。

Abstract: Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START/END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.

Abstract (中文翻译): 长视频问答对现代视觉-语言模型仍具挑战性，这些模型在处理小时级视频时难以在不超出实际token和计算预算的前提下进行有效推理。现有系统通常通过下采样帧或将密集视觉嵌入输入大上下文语言模型，在时间覆盖范围与成本之间做出权衡。我们提出了语义事件图（Semantic Event Graphs, SEG），这是一种轻量级的视频与语言之间的符号化接口，用紧凑的时间交互日志替代原始视频帧。我们的流程使用YOLOv11检测并跟踪物体，将邻近模式转换为START/END形式的人-物交互事件，并将其组织成时间场景图（Temporal Scene Graph, TSG）。在推理阶段，一个查询感知剪枝模块识别锚点实体和词汇相关的事件，仅返回一个小型子图，该子图被文本化后传给Gemini 2.5 Flash用于生成答案。在五个YouTube视频（每个包含300–500次交互）和120个自动生成的长时程问题上，SEG每查询仅使用3.47k个token就达到了65.0%的准确率，接近使用40.39k个token的完整日志基线（62.5%），同时将token使用量减少了91.4%。相比之下，仅限于最后30秒视频的短上下文基线准确率骤降至2.5%，凸显了显式时间记忆的必要性。这些结果表明，符号化时间图可作为现成视觉-语言模型的一种高效、即插即用的记忆层，在保留长距离推理能力的同时，使长视频问答在token和成本方面显著更高效。代码、日志和事件提取工具将公开发布以确保可复现性。

</details>


### [4] [COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control](https://arxiv.org/abs/2601.06122)
*Canming Xia,Peixi Peng,Guang Tan,Zhan Su,Haoran Xu,Zhenxian Liu,Luntong Li*

Main category: cs.CV

TL;DR: 本文提出COVR框架，通过强化学习（RL）与视觉语言模型（VLM）的协同优化，利用RL生成的数据微调VLM以提升其任务相关语义推理能力，并用增强后的VLM指导策略学习。引入两个关键模块提高微调效率，并采用渐进式微调策略降低资源消耗，在多个视觉控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注将视觉语言模型（VLM）的知识蒸馏到强化学习（RL）中，忽略了RL生成的交互数据对提升VLM本身的潜力。为解决这一问题，作者旨在建立一个VLM与RL策略相互增强的协作机制。

Method: 提出COVR协同优化框架：1）使用RL生成的数据对VLM进行微调，使其语义推理能力更贴合目标任务；2）利用增强后的VLM通过动作先验指导策略学习。为提升微调效率，设计了探索驱动的动态过滤模块和回报感知的自适应损失权重模块，并采用渐进式微调策略减少资源消耗。

Result: 在多种具有挑战性的视觉控制任务上进行了大量实验，结果表明COVR方法取得了优异的性能。

Conclusion: COVR通过VLM与RL的双向协同优化，有效提升了视觉强化学习的样本效率和整体性能，验证了利用RL交互数据增强VLM的可行性和有效性。

Abstract: Visual reinforcement learning (RL) suffers from poor sample efficiency due to high-dimensional observations in complex tasks. While existing works have shown that vision-language models (VLMs) can assist RL, they often focus on knowledge distillation from the VLM to RL, overlooking the potential of RL-generated interaction data to enhance the VLM. To address this, we propose COVR, a collaborative optimization framework that enables the mutual enhancement of the VLM and RL policies. Specifically, COVR fine-tunes the VLM with RL-generated data to enhance the semantic reasoning ability consistent with the target task, and uses the enhanced VLM to further guide policy learning via action priors. To improve fine-tuning efficiency, we introduce two key modules: (1) an Exploration-Driven Dynamic Filter module that preserves valuable exploration samples using adaptive thresholds based on the degree of exploration, and (2) a Return-Aware Adaptive Loss Weight module that improves the stability of training by quantifying the inconsistency of sampling actions via return signals of RL. We further design a progressive fine-tuning strategy to reduce resource consumption. Extensive experiments show that COVR achieves strong performance across various challenging visual control tasks.

Abstract (中文翻译): 视觉强化学习（RL）由于在复杂任务中面临高维观测，存在样本效率低下的问题。尽管已有研究表明视觉语言模型（VLM）可辅助RL，但这些工作通常聚焦于将VLM的知识蒸馏到RL中，忽视了RL生成的交互数据对增强VLM本身的潜力。为解决此问题，我们提出了COVR——一种使VLM与RL策略相互增强的协同优化框架。具体而言，COVR利用RL生成的数据对VLM进行微调，以增强其与目标任务一致的语义推理能力，并利用增强后的VLM通过动作先验进一步指导策略学习。为提高微调效率，我们引入两个关键模块：（1）探索驱动的动态过滤模块，基于探索程度的自适应阈值保留有价值的探索样本；（2）回报感知的自适应损失权重模块，通过RL的回报信号量化采样动作的不一致性，从而提升训练稳定性。此外，我们还设计了一种渐进式微调策略以降低资源消耗。大量实验表明，COVR在多种具有挑战性的视觉控制任务中均取得了卓越的性能。

</details>


### [5] [Low-Back Pain Physical Rehabilitation by Movement Analysis in Clinical Trial](https://arxiv.org/abs/2601.06138)
*Sao Mai Nguyen*

Main category: cs.CV

TL;DR: 本文提出了Keraal数据集，这是一个在临床环境中收集的患者进行腰痛康复训练的数据集，用于支持智能辅导系统（ITS）在康复中的应用，并对当前人体动作分析算法进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 开发和评估用于物理康复的智能辅导系统需要真实临床环境下的康复运动数据，而现有数据集缺乏此类信息。

Method: 构建并发布Keraal数据集，该数据集包含临床患者执行腰痛康复训练的动作，并利用当前先进的人体动作分析算法进行基准测试。

Result: 该数据集有效支持对康复训练中四个关键挑战的处理：动作评估、错误识别、空间定位和时间定位。

Conclusion: Keraal数据集填补了临床康复场景下智能辅导系统研究的数据空白，为康复运动分析提供了重要资源。

Abstract: To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we propose a medical dataset of clinical patients carrying out low back-pain rehabilitation exercises and benchmark on state of the art human movement analysis algorithms. This dataset is valuable because it includes rehabilitation motions in a clinical setting with patients in their rehabilitation program. This paper introduces the Keraal dataset, a clinically collected dataset to enable intelligent tutoring systems (ITS) for rehabilitation. It addresses four challenges in exercise monitoring: motion assessment, error recognition, spatial localization, temporal localization

Abstract (中文翻译): 为了支持智能辅导系统在物理康复中的开发与评估，我们提出了一个包含临床患者执行腰痛康复训练动作的医学数据集，并对当前先进的人体动作分析算法进行了基准测试。该数据集具有重要价值，因为它包含了处于康复计划中的患者在临床环境中进行的康复动作。本文介绍了Keraal数据集——一个在临床环境中采集的数据集，旨在推动康复领域的智能辅导系统（ITS）发展。该数据集针对运动监测中的四大挑战：动作评估、错误识别、空间定位和时间定位。

</details>


### [6] [Forget-It-All: Multi-Concept Machine Unlearning via Concept-Aware Neuron Masking](https://arxiv.org/abs/2601.06163)
*Kaiyuan Deng,Bo Hui,Gen Li,Jie Ji,Minghai Qin,Geng Yuan,Xiaolong Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Forget It All (FIA) 的新框架，用于在预训练文本到图像扩散模型中高效、可靠地同时遗忘多个概念。该方法利用模型稀疏性，通过对比概念显著性识别与目标概念相关的权重连接，并结合时空信息定位“概念敏感神经元”，进而构建统一的多概念掩码进行剪枝。FIA 无需重新训练，仅需极少超参数调整，即可在保持生成质量的同时有效移除多个敏感或受版权保护的概念。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型的广泛应用引发了对其可能生成受版权保护、不当或敏感内容的担忧。现有的机器遗忘方法大多针对单一概念，在处理需要同时遗忘多个概念的现实场景时面临效果不佳、生成质量下降以及对超参数和数据集敏感等挑战。

Method: 作者提出 Forget It All (FIA) 框架，该方法基于模型稀疏性，首先使用“对比概念显著性”量化每个权重连接对目标概念的贡献；然后结合时间和空间信息识别出“概念敏感神经元”；最后，从这些神经元构建掩码并融合成一个统一的多概念掩码，在保留支持通用内容生成的“概念无关神经元”的同时，剪枝掉与特定概念相关的神经元。

Result: 在三个不同的遗忘任务上的大量实验表明，FIA 能够更可靠地实现多概念遗忘，在提高遗忘效果的同时，保持了生成图像的语义保真度和质量。

Conclusion: FIA 提供了一种高效、即插即用的多概念遗忘解决方案，无需重新训练且对超参数不敏感，为在实际应用中安全部署大型生成模型提供了有力工具。

Abstract: The widespread adoption of text-to-image (T2I) diffusion models has raised concerns about their potential to generate copyrighted, inappropriate, or sensitive imagery learned from massive training corpora. As a practical solution, machine unlearning aims to selectively erase unwanted concepts from a pre-trained model without retraining from scratch. While most existing methods are effective for single-concept unlearning, they often struggle in real-world scenarios that require removing multiple concepts, since extending them to this setting is both non-trivial and problematic, causing significant challenges in unlearning effectiveness, generation quality, and sensitivity to hyperparameters and datasets. In this paper, we take a unique perspective on multi-concept unlearning by leveraging model sparsity and propose the Forget It All (FIA) framework. FIA first introduces Contrastive Concept Saliency to quantify each weight connection's contribution to a target concept. It then identifies Concept-Sensitive Neurons by combining temporal and spatial information, ensuring that only neurons consistently responsive to the target concept are selected. Finally, FIA constructs masks from the identified neurons and fuses them into a unified multi-concept mask, where Concept-Agnostic Neurons that broadly support general content generation are preserved while concept-specific neurons are pruned to remove the targets. FIA is training-free and requires only minimal hyperparameter tuning for new tasks, thereby promoting a plug-and-play paradigm. Extensive experiments across three distinct unlearning tasks demonstrate that FIA achieves more reliable multi-concept unlearning, improving forgetting effectiveness while maintaining semantic fidelity and image quality.

Abstract (中文翻译): 文本到图像（T2I）扩散模型的广泛采用引发了人们的担忧，即这些模型可能会从庞大的训练数据集中生成受版权保护、不适当或敏感的图像。作为一种实用的解决方案，机器遗忘旨在有选择地从预训练模型中擦除不需要的概念，而无需从头开始重新训练。尽管大多数现有方法在单概念遗忘方面是有效的，但在需要移除多个概念的真实场景中，它们往往表现不佳，因为将这些方法扩展到多概念场景既非易事又存在问题，导致在遗忘效果、生成质量和对超参数及数据集的敏感性方面面临重大挑战。在本文中，我们从模型稀疏性的独特视角出发，提出了“全部遗忘”（Forget It All, FIA）框架。FIA 首先引入“对比概念显著性”来量化每个权重连接对目标概念的贡献。然后，它通过结合时间和空间信息来识别“概念敏感神经元”，确保只选择那些对目标概念持续响应的神经元。最后，FIA 从已识别的神经元构建掩码，并将其融合成一个统一的多概念掩码，在其中保留了广泛支持通用内容生成的“概念无关神经元”，同时剪枝掉与特定概念相关的神经元以移除目标。FIA 无需训练，且对于新任务仅需极少的超参数调整，从而促进了一种即插即用的范式。在三个不同遗忘任务上的大量实验表明，FIA 实现了更可靠的多概念遗忘，在提高遗忘效果的同时保持了语义保真度和图像质量。

</details>


### [7] [What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models](https://arxiv.org/abs/2601.06165)
*Dasol Choi,Guijin Son,Hanwool Lee,Minhyuk Kim,Hyunwoo Ko,Teabin Lim,Ahn Eungyeol,Jungwhan Kim,Seunghyeok Hong,Youngsook Song*

Main category: cs.CV

TL;DR: 现实用户视觉查询常含糊不清，而当前视觉语言模型（VLM）基准多使用结构清晰的问题。本文提出HAERAE-Vision数据集，包含653个真实韩语社区视觉问题及其明确改写版本，共1,306个查询变体。评估39个VLM发现，即使顶尖模型在原始模糊查询上准确率也低于50%；仅将查询明确化即可提升8–22个百分点，小模型受益最大。即使结合网络搜索，模糊查询表现仍不如无搜索的明确查询，表明检索无法弥补信息缺失。研究揭示VLM性能瓶颈主要源于自然查询的模糊性，而非模型能力本身，凸显当前评测与实际应用间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言基准中的问题通常结构良好、提示明确，但真实用户的查询往往非正式且信息不足，依赖图像传递上下文。这种差异导致模型在基准测试中表现良好，却难以应对现实场景。因此，需要构建更贴近真实用户行为的评测基准，以衡量和提升模型在模糊查询下的理解能力。

Method: 作者从韩国在线社区收集了86,000个候选视觉问题，经过严格筛选保留653个真实用户查询（留存率0.76%），并为每个原始模糊查询提供一个人工明确改写版本，构成HAERAE-Vision基准，总计1,306个查询变体。随后在该基准上评估了39个视觉语言模型（包括GPT-5、Gemini 2.5 Pro等），比较其在原始模糊查询与明确改写查询上的性能差异，并进一步探究结合网络搜索是否能弥补模糊性带来的性能损失。

Result: 实验结果显示，即使是当前最先进的VLM在原始模糊查询上的准确率也低于50%；仅通过将查询明确化，模型性能可提升8至22个百分点，其中较小模型提升更为显著。此外，即使引入网络搜索，模糊查询的表现仍不如未使用搜索的明确查询，说明现有检索技术无法有效补偿用户未明说的信息。

Conclusion: VLM在现实应用中的主要挑战并非模型本身的能力不足，而是源于用户自然查询的模糊性和信息缺失。当前主流基准未能反映这一关键问题，导致评估结果与实际部署效果存在显著差距。因此，未来研究应更关注对模糊查询的理解与补全能力，并开发更贴近真实场景的评测方法。

Abstract: Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.

Abstract (中文翻译): 当前的视觉-语言基准主要包含结构良好、提示清晰明确的问题。然而，真实用户的查询往往是非正式且信息不足的，用户自然地省略大量信息，依赖图像来传达上下文。我们提出了HAERAE-Vision，这是一个包含653个来自韩国在线社区的真实世界视觉问题的基准（从8.6万个候选问题中筛选出，留存率仅为0.76%），每个问题均配有一个明确的改写版本，总共形成1,306个查询变体。在对39个视觉语言模型（VLM）的评估中，我们发现即使是当前最先进的模型（如GPT-5、Gemini 2.5 Pro）在原始查询上的准确率也低于50%。关键的是，仅通过将查询明确化，模型性能就能提升8到22个百分点，其中较小的模型受益最为明显。我们进一步表明，即使结合网络搜索，信息不足的查询表现仍不如不使用搜索的明确查询，这揭示了当前的检索技术无法弥补用户未言明的信息。我们的研究结果表明，VLM面临的很大一部分困难源于自然查询的信息不足，而非模型本身的能力限制，这突显了当前基准评估与现实世界部署之间存在的关键差距。

</details>


### [8] [B-FIRE: Binning-Free Diffusion Implicit Neural Representation for Hyper-Accelerated Motion-Resolved MRI](https://arxiv.org/abs/2601.06166)
*Di Xu,Hengjie Liu,Yang Yang,Mary Feng,Jin Ning,Xin Miao,Jessica E. Scholey,Alexandra E. Hotca-cho,William C. Chen,Michael Ohliger,Martina Descovich,Huiming Dong,Wensha Yang,Ke Sheng*

Main category: cs.CV

TL;DR: B-FIRE是一种无需呼吸相位分箱的扩散隐式神经表示方法，用于超加速4D MRI重建，能准确恢复瞬时腹部3D解剖结构。


<details>
  <summary>Details</summary>
Motivation: 现有4DMRI在处理呼吸运动时会产生平均相位伪影，模糊并错误表达瞬时动态信息，因此需要新方法来重建极度欠采样的非笛卡尔k空间数据。

Method: 提出B-FIRE框架，采用CNN-INR编码器-解码器结构，结合扩散优化和包含图像域保真度与频率感知约束的综合损失函数；训练使用分箱后的图像对，推理则直接处理无分箱的欠采样数据。

Result: 在T1加权StarVIBE肝脏MRI数据集上（加速倍数从RV8到RV1），B-FIRE在重建保真度、运动轨迹一致性和推理延迟方面优于NuFFT、GRASP-CS和展开式CNN方法。

Conclusion: B-FIRE能够有效实现高加速比下的高质量4D MRI重建，准确反映瞬时动态解剖信息，具有临床应用潜力。

Abstract: Accelerated dynamic volumetric magnetic resonance imaging (4DMRI) is essential for applications relying on motion resolution. Existing 4DMRI produces acceptable artifacts of averaged breathing phases, which can blur and misrepresent instantaneous dynamic information. Recovery of such information requires a new paradigm to reconstruct extremely undersampled non-Cartesian k-space data. We propose B-FIRE, a binning-free diffusion implicit neural representation framework for hyper-accelerated MR reconstruction capable of reflecting instantaneous 3D abdominal anatomy. B-FIRE employs a CNN-INR encoder-decoder backbone optimized using diffusion with a comprehensive loss that enforces image-domain fidelity and frequency-aware constraints. Motion binned image pairs were used as training references, while inference was performed on binning-free undersampled data. Experiments were conducted on a T1-weighted StarVIBE liver MRI cohort, with accelerations ranging from 8 spokes per frame (RV8) to RV1. B-FIRE was compared against direct NuFFT, GRASP-CS, and an unrolled CNN method. Reconstruction fidelity, motion trajectory consistency, and inference latency were evaluated.

Abstract (中文翻译): 加速动态容积磁共振成像（4DMRI）对于依赖运动分辨的应用至关重要。现有的4DMRI在呼吸相位平均过程中会产生可接受但有害的伪影，从而模糊并错误表达瞬时动态信息。要恢复此类信息，需要一种新范式来重建极度欠采样的非笛卡尔k空间数据。我们提出了B-FIRE——一种无需分箱的扩散隐式神经表示框架，用于超加速磁共振重建，能够反映瞬时三维腹部解剖结构。B-FIRE采用CNN-INR编码器-解码器主干网络，通过扩散过程进行优化，并结合了强制图像域保真度和频率感知约束的综合损失函数。训练阶段使用经过运动分箱的图像对作为参考，而推理阶段则直接处理无分箱的欠采样数据。实验在T1加权StarVIBE肝脏MRI队列上进行，加速倍数从每帧8条射线（RV8）到RV1不等。B-FIRE与直接NuFFT、GRASP-CS以及一种展开式CNN方法进行了对比，在重建保真度、运动轨迹一致性及推理延迟等方面进行了评估。

</details>


### [9] [Analyzing the Structure of Handwritten Digits: A Comparative Study of PCA, Factor Analysis, and UMAP](https://arxiv.org/abs/2601.06168)
*Jyotiraditya Gupta*

Main category: cs.CV

TL;DR: 本文通过PCA、因子分析和UMAP三种降维方法，揭示了MNIST手写数字在高维像素空间中实际存在于一个结构化的低维流形上，并展示了不同方法如何从不同角度刻画其内在结构。


<details>
  <summary>Details</summary>
Motivation: 手写数字图像虽处于高维像素空间，但具有明显的几何与统计结构。作者旨在探究其潜在的低维组织方式，而非追求分类性能。

Method: 采用三种互补的降维技术：主成分分析（PCA）、因子分析（FA）和均匀流形近似与投影（UMAP），分别从全局方差、可解释潜变量和非线性流形角度分析MNIST数据集。

Result: PCA能用少量主成分实现高保真重建；FA提取出对应笔画、环路和对称性的可解释潜变量；UMAP揭示了数字类别间平滑风格过渡的非线性流形。

Conclusion: 手写数字占据一个结构化的低维流形，不同统计框架能揭示该结构的不同互补方面。

Abstract: Handwritten digit images lie in a high-dimensional pixel space but exhibit strong geometric and statistical structure. This paper investigates the latent organization of handwritten digits in the MNIST dataset using three complementary dimensionality reduction techniques: Principal Component Analysis (PCA), Factor Analysis (FA), and Uniform Manifold Approximation and Projection (UMAP). Rather than focusing on classification accuracy, we study how each method characterizes intrinsic dimensionality, shared variation, and nonlinear geometry. PCA reveals dominant global variance directions and enables high-fidelity reconstructions using a small number of components. FA decomposes digits into interpretable latent handwriting primitives corresponding to strokes, loops, and symmetry. UMAP uncovers nonlinear manifolds that reflect smooth stylistic transitions between digit classes. Together, these results demonstrate that handwritten digits occupy a structured low-dimensional manifold and that different statistical frameworks expose complementary aspects of this structure.

Abstract (中文翻译): 手写数字图像位于高维像素空间中，但展现出强烈的几何与统计结构。本文利用三种互补的降维技术——主成分分析（PCA）、因子分析（FA）和均匀流形近似与投影（UMAP）——研究了MNIST数据集中手写数字的潜在组织方式。研究重点并非分类准确率，而是考察每种方法如何刻画数据的本征维度、共享变异性和非线性几何特性。PCA揭示了主导的全局方差方向，并能使用少量成分实现高保真重建；FA将数字分解为对应于笔画、环路和对称性的可解释潜变量；UMAP则揭示了反映数字类别之间平滑风格过渡的非线性流形。综合结果表明，手写数字实际上占据一个结构化的低维流形，而不同的统计框架能够揭示这一结构的互补方面。

</details>


### [10] [Think Bright, Diffuse Nice: Enhancing T2I-ICL via Inductive-Bias Hint Instruction and Query Contrastive Decoding](https://arxiv.org/abs/2601.06169)
*Zhiyong Ma,Zhenpeng Li,Yuanjie Shi,Zhengping Li,Jiahao Chen,Qingyuan Chuai*

Main category: cs.CV

TL;DR: TBDN 是一种无需训练的文本到图像上下文学习（T2I-ICL）框架，通过 Hint Instruction 和 Query Contrastive Decoding 两种机制，有效缓解了合规失败和先验主导幻觉问题，在多个基准上达到 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有 T2I-ICL 方法面临合规失败与先验主导幻觉相互加剧的问题，且依赖特定训练，限制了灵活性并增加部署成本。

Method: 提出 TBDN 框架，包含两个互补的闭环机制：Hint Instruction（HI）通过轻量级提示工程注入任务感知归纳偏置，缓解合规失败；Query Contrastive Decoding（QCD）通过对比完整输入与省略查询的分布，抑制先验主导幻觉。

Result: TBDN 在 CoBSAT 和 Text-to-Image Fast Mini-ImageNet 上达到 SOTA，对模型主干、提示设计和超参数具有强泛化能力，并在 Dreambench++ 上表现出良好的概念保留和提示跟随能力。

Conclusion: TBDN 通过打破两大瓶颈，构建了一个简单而高效的无训练 T2I-ICL 框架，实现了可靠且灵活的定制图像生成。

Abstract: Text-to-Image In-Context Learning (T2I-ICL) enables customized image synthesis via interleaved text-image examples but faces two mutually reinforcing bottlenecks, compliance failure and prior-dominated hallucination, that form a vicious cycle degrading generation quality. Existing methods rely on tailored training, which limits flexibility and raises deployment costs. To address these challenges effectively, we propose TBDN, a training-free framework integrating two complementary closed-loop mechanisms: Hint Instruction (HI) and Query Contrastive Decoding (QCD). HI injects task-aware inductive bias via lightweight prompt engineering to anchor models on contextual mapping rules, thereby mitigating compliance failure. QCD adjusts the decoding distributions of language models by contrasting full-input and query-omitted distributions, suppressing prior-dominated hallucination. TBDN achieves State-of-the-Art performance on CoBSAT and Text-to-Image Fast Mini-ImageNet, with robust generalization across model backbones, prompt designs, and hyperparameters. It also maintains promising performance in concept preservation and prompt following on Dreambench++. By breaking the two bottlenecks, TBDN establishes a simple yet effective framework for efficient and reliable T2I-ICL.

Abstract (中文翻译): 文本到图像的上下文学习（T2I-ICL）通过交错的文本-图像示例实现定制化图像合成，但面临合规失败与先验主导幻觉这两个相互加剧的瓶颈，形成恶性循环，降低生成质量。现有方法依赖专门训练，限制了灵活性并提高了部署成本。为有效应对这些挑战，我们提出了 TBDN——一种无需训练的框架，整合了两个互补的闭环机制：提示指令（Hint Instruction, HI）和查询对比解码（Query Contrastive Decoding, QCD）。HI 通过轻量级提示工程注入任务感知的归纳偏置，使模型聚焦于上下文映射规则，从而缓解合规失败；QCD 通过对比完整输入与省略查询条件下的语言模型解码分布，抑制先验主导的幻觉。TBDN 在 CoBSAT 和 Text-to-Image Fast Mini-ImageNet 基准上取得了最先进的性能，并在不同模型主干、提示设计和超参数设置下展现出强大的泛化能力。此外，在 Dreambench++ 上，它在概念保留和提示跟随方面也保持了优异表现。通过打破上述两大瓶颈，TBDN 建立了一个简洁而高效的 T2I-ICL 框架，实现了高效可靠的定制图像生成。

</details>
