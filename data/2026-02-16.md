<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring](https://arxiv.org/abs/2602.12361)
*Constantino Álvarez Casado,Mohammad Rahman,Sasan Sharifipour,Nhi Nguyen,Manuel Lage Cañellas,Xiaoting Wu,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本文利用面部热成像视频，通过信号处理流程无接触地估计皮肤电活动（EDA）、心率（HR）和呼吸率（BR），在公开数据集上验证了方法的有效性，并提供了性能基准与设计指导。


<details>
  <summary>Details</summary>
Motivation: 可见光方法虽能估计HR和BR，但无法获取作为交感神经激活标志的EDA。热红外成像可捕捉由自主神经调节引起的皮肤温度变化，有望实现对这三种生理信号的无接触同步估计。

Method: 提出一个信号处理流程：跟踪面部解剖区域、进行空间聚合，并分离缓慢的汗腺活动趋势与较快的心肺成分。对HR采用多区域正交矩阵图像变换（OMIT）分解；对BR则平均鼻部与脸颊信号后进行频谱峰值检测；对EDA评估了288种配置，包括不同区域和滤波策略。

Result: 在SIM1数据集31个会话中，最佳固定EDA配置（鼻区+指数移动平均）与手掌EDA的平均绝对相关系数为0.40±0.23（个别达0.89）；BR估计MAE为3.1±1.1 bpm；HR估计MAE为13.8±7.5 bpm（受限于7.5 Hz低帧率）。还观察到信号极性交替、短热力学延迟及条件与人口统计学对提取质量的影响。

Conclusion: 研究为基于热成像的无接触生理信号估计提供了性能基线和系统设计指导，验证了其在驾驶员监测等场景中的潜力。

Abstract: Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \pm 1.1$ bpm, while HR estimation yields $13.8 \pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.

Abstract (中文翻译): 热红外成像能够捕捉由自主神经调节驱动的皮肤温度变化，有望实现对皮肤电活动（EDA）、心率（HR）和呼吸率（BR）的无接触估计。虽然可见光方法可以处理HR和BR，但无法获取作为交感神经激活标准指标的EDA。本文提出一种信号处理流程，从面部热成像视频中提取这三种生理信号：该流程跟踪解剖区域、进行空间聚合，并将缓慢的汗腺活动趋势与较快的心肺成分分离。对于HR，我们在多个面部感兴趣区域（ROI）应用正交矩阵图像变换（OMIT）分解；对于BR，则在频谱峰值检测前对鼻部和脸颊信号取平均。我们在公开的SIMULATOR STUDY 1（SIM1）驾驶员监测数据集的31个会话中，评估了288种EDA配置以及HR/BR流程。最佳固定EDA配置（鼻区+指数移动平均）与手掌EDA的平均绝对相关系数达到0.40±0.23，个别会话高达0.89；BR估计的平均绝对误差为3.1±1.1 bpm；HR估计的平均绝对误差为13.8±7.5 bpm，受限于较低的相机帧率（7.5 Hz）。我们还报告了不同会话中信号极性的交替现象、良好追踪信号的短热力学延迟，以及提取质量受实验条件和人口统计因素的影响。这些结果为热成像无接触生理信号估计提供了性能基准和设计指导。

</details>


### [2] [LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens](https://arxiv.org/abs/2602.12370)
*Zekun Li,Sizhe An,Chengcheng Tang,Chuan Guo,Ivan Shugurov,Linguang Zhang,Amy Zhao,Srinath Sridhar,Lingling Tao,Abhay Mittal*

Main category: cs.CV

TL;DR: LLaMo is a unified motion-language model that extends pretrained LLMs with a Mixture-of-Transformers architecture, enabling high-quality bidirectional generation (text-to-motion and motion-to-text) while preserving linguistic capabilities and supporting real-time streaming motion generation.


<details>
  <summary>Details</summary>
Motivation: Existing unified motion-language models suffer from catastrophic forgetting of language skills due to limited motion-text data and introduce jitter artifacts by using discrete motion representations. There is a need for a framework that preserves LLM capabilities while enabling effective, continuous, and scalable motion-language integration.

Method: The authors propose LLaMo, which uses a modality-specific Mixture-of-Transformers (MoT) to extend pretrained LLMs without altering their original parameters. Human motion is encoded into a causal continuous latent space, and a lightweight flow-matching head is added to the decoder-only backbone to maintain next-token prediction, enabling real-time streaming motion generation.

Result: LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning, excelling particularly in zero-shot motion generation, while operating in real-time (>30 FPS).

Conclusion: LLaMo represents a significant step toward a general unified motion-language large model by effectively combining continuous motion representation with pretrained LLMs, preserving language understanding, and enabling scalable multimodal adaptation.

Abstract: Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.

Abstract (中文翻译): 近期大模型的进展在统一多模态生成与理解方面取得了显著成果。然而，统一运动-语言生成与理解的模型开发仍鲜有探索。现有方法通常在配对的运动-文本数据上微调大语言模型（LLMs），但由于可用文本-运动对数据规模有限，容易导致语言能力的灾难性遗忘。此外，以往方法通常通过量化将运动转换为离散表示以与语言模型集成，这种离散化分词会引入明显的抖动伪影。为解决这些问题，我们提出了LLaMo——一种通过模态特定的混合变换器（Mixture-of-Transformers, MoT）架构扩展预训练LLM的统一框架。该设计在本质上保留了基础模型的语言理解能力，同时支持可扩展的多模态适配。我们将人体运动编码到一个因果连续潜在空间中，并通过轻量级的流匹配头（flow-matching head）在仅解码器主干中维持下一令牌预测范式，从而实现实时流式运动生成（>30 FPS）。借助预训练LLM的全面语言理解能力和大规模运动-文本预训练，实验表明LLaMo在通用场景下实现了高保真度的文本到运动生成和运动到文本描述，尤其在零样本运动生成方面表现突出，标志着向通用统一运动-语言大模型迈出了重要一步。

</details>


### [3] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: 本文研究了基于CLIP的合成图像检测（SID）方法的有效性与泛化能力，发现其主要依赖高层次摄影属性而非生成器特有伪影，在高质量扩散模型生成图像上的性能显著下降，强调需持续更新模型并扩大训练数据多样性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能产出近乎逼真的图像，照片可信度受到挑战，促使合成图像检测（SID）成为重要研究方向。然而现有方法在面对新型生成模型时泛化能力差，且不清楚CLIP等模型是依赖视觉伪影还是语义偏差进行检测，这限制了其实际应用价值。

Method: 作者构建了一个名为SynthCLIC的新数据集，包含真实照片及其由最新扩散模型生成的高质量配对合成图像，以减少语义偏差。通过使用可解释的线性分类头（具有去相关激活）和基于文本的概念模型，分析CLIP特征中用于检测的关键线索。

Result: 基于CLIP的线性检测器在GAN基准上达到0.96 mAP，但在SynthCLIC数据集上降至0.92；跨生成器家族的泛化性能最低降至0.37 mAP。分析表明，检测器主要依赖如极简风格、镜头光晕、景深分层等高层摄影属性，而非明显的生成器伪影。

Conclusion: CLIP为基础的SID方法整体表现良好，但在不同生成架构间泛化不均，需持续更新模型并扩大训练覆盖范围。同时，该研究验证了CLIP作为构建更通用、鲁棒SID系统的基础具有重要价值。

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

Abstract (中文翻译): 近期生成模型能够生成接近真实照片的图像，这对照片的可信度构成了挑战，因此合成图像检测（SID）成为一个重要的研究领域。以往的研究指出合成图像与真实照片之间的差异，但SID方法通常难以泛化到新的生成模型，并且在实际场景中表现不佳。CLIP作为一种基础的视觉-语言模型，能够生成语义丰富的图文嵌入，在SID任务中展现出强大的准确性和泛化能力。然而，CLIP特征中所包含的相关判别线索尚不明确：目前尚不清楚基于CLIP的检测器是仅检测明显的视觉伪影，还是利用了细微的语义偏差——这两种情况都会使其在实际应用或面对高质量生成模型时失效。为此，我们提出了SynthCLIC，这是一个由真实照片及其对应高质量合成图像（来自最新扩散模型）组成的配对数据集，旨在减少SID中的语义偏差。通过采用具有去相关激活的可解释线性分类头和基于文本的概念模型，我们分析了基于CLIP的检测器所学习的内容。实验表明，基于CLIP的线性检测器在基于GAN的基准上达到0.96 mAP，但在我们的高质量扩散数据集SynthCLIC上仅为0.92；跨生成器家族的泛化性能甚至低至0.37 mAP。我们发现，这些检测器主要依赖于高层摄影属性（例如极简风格、镜头光晕或景深层次），而非明显的生成器特有伪影。总体而言，基于CLIP的检测器表现良好，但在不同生成架构间的泛化能力不均衡。这突显了持续更新模型和扩大训练数据多样性的必要性，同时也强化了CLIP方法作为构建更通用、鲁棒SID系统坚实基础的地位。

</details>


### [4] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: 本文对DragDiffusion方法进行了可复现性研究，验证了其核心主张，并揭示了其性能对少数超参数（如优化的时间步和运动监督特征层级）较为敏感。


<details>
  <summary>Details</summary>
Motivation: 验证DragDiffusion这一基于扩散模型的交互式点拖拽图像编辑方法的可复现性，并探究其关键组件和超参数对性能的影响。

Method: 使用作者发布的代码和DragBench基准，复现了关于扩散时间步选择、LoRA微调、掩码正则化强度和UNet特征监督的主要消融实验，并额外评估了一种多时间步潜在变量优化的变体。

Result: 复现实验结果与原论文报告的定性和定量趋势高度一致。研究发现性能对优化时间步和运动监督特征层级等少数超参数敏感，而其他组件则有更宽泛的有效范围。多时间步优化变体未能提升空间精度，反而大幅增加了计算成本。

Conclusion: 研究总体上支持DragDiffusion的核心主张，并明确了其可靠复现所需的具体条件。

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

Abstract (中文翻译): DragDiffusion是一种基于扩散模型的交互式点拖拽图像编辑方法，允许用户通过直接拖动选定的点来操纵图像。该方法声称，通过在中间时间步优化单个扩散潜在变量，并结合身份保持微调和空间正则化，可以实现精确的空间控制。本工作利用作者发布的实现代码和DragBench基准，对DragDiffusion进行了可复现性研究。我们复现了关于扩散时间步选择、基于LoRA的微调、掩码正则化强度和UNet特征监督的主要消融研究，观察到的结果与原作报告的定性和定量趋势非常吻合。同时，我们的实验表明，性能对少量超参数假设（特别是优化的时间步和用于运动监督的特征层级）较为敏感，而其他组件则具有更宽泛的有效操作范围。我们进一步评估了一种多时间步潜在变量优化的变体，发现它并未提高空间精度，反而显著增加了计算成本。总体而言，我们的研究结果支持DragDiffusion的核心主张，同时阐明了其可靠复现的条件。代码可在https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge获取。

</details>


### [5] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文通过“弗兰肯斯坦式”分析框架揭示，强化学习（RL）在视觉推理中的作用并非全面提升视觉感知，而是系统性地优化Transformer中后层的计算，以增强视觉到推理的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以厘清强化学习（RL）相较于监督微调（作为冷启动初始化）在视觉语言模型中具体提升了哪些能力，因为端到端基准测试的提升混杂了多种因素，无法归因于特定技能。

Method: 提出一种“弗兰肯斯坦式”分析框架，包括：(i) 通过因果探针进行功能定位；(ii) 通过参数比较刻画更新特征；(iii) 通过模型合并进行可迁移性测试。

Result: RL主要在推理阶段引起模型中后层的一致性变化；这些中后层的改进既可通过模型合并迁移，也可通过冻结验证其对RL性能提升的必要性。

Conclusion: RL在视觉推理中的可靠贡献是系统性地优化Transformer中后层的计算，以改善视觉到推理的对齐和推理性能，而非均匀提升视觉感知能力，这凸显了仅依赖基准测试评估多模态推理改进的局限性。

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

Abstract (中文翻译): 具有可验证奖励的强化学习（RL）已成为提升视觉语言模型视觉推理能力的标准后训练阶段，但尚不清楚与作为冷启动初始化（IN）的监督微调相比，RL究竟提升了哪些能力。端到端基准测试的提升混杂了多种因素，难以将改进归因于特定技能。为弥合这一差距，我们提出了一种“弗兰肯斯坦式”分析框架，包括：(i) 通过因果探针进行功能定位；(ii) 通过参数比较刻画更新特征；以及 (iii) 通过模型合并进行可迁移性测试。结果表明，RL主要在推理阶段引起模型中后层的一致性变化，且这些中后层的改进既可通过模型合并迁移，也可通过冻结验证其对RL性能提升的必要性。总体而言，我们的结果表明，RL在视觉推理中的可靠贡献并非对视觉感知的均匀增强，而是对Transformer中后层计算的系统性优化，从而改善了视觉到推理的对齐和推理性能，突显了仅依赖基准测试来理解多模态推理改进的局限性。

</details>


### [6] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: 本文提出ZeroDiff++，一种基于扩散模型的零样本学习框架，通过引入扩散增强、监督对比表示、多视角判别器以及测试时自适应生成策略，有效缓解了现有生成式ZSL方法中因样本稀缺导致的虚假视觉-语义关联问题，在多个基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有生成式零样本学习方法在训练样本稀缺的情况下容易学习到虚假的视觉-语义关联，且其生成器无法自适应地生成与真实测试样本相关的特征，限制了模型性能。

Method: 提出ZeroDiff++框架：训练阶段采用扩散增强、监督对比表示和多视角Wasserstein互学习判别器；生成阶段引入基于扩散的测试时自适应（DiffTTA）和测试时生成（DiffGen）策略，以连接真实与生成数据并缓解数据稀缺问题。

Result: 在三个ZSL基准上的实验表明，ZeroDiff++显著优于现有方法，并在训练数据稀缺时仍保持鲁棒性能。

Conclusion: 通过结合扩散模型与测试时自适应机制，ZeroDiff++有效增强了视觉-语义关联的可靠性，为生成式零样本学习提供了一种更鲁棒和高效的解决方案。

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

Abstract (中文翻译): 零样本学习（ZSL）使分类器能够识别训练过程中未见过的类别，通常通过生成式两阶段方法实现：（1）从已见类别中学习视觉与语义之间的关联；（2）利用语义信息合成未见类别的特征以训练分类器。本文指出，现有生成式ZSL方法在已见类别样本稀缺的情况下会加剧虚假的视觉-语义关联，并提出了两个指标来量化已见和未见类别中的这种虚假性。此外，我们指出一个更关键的瓶颈：现有生成器采用非自适应的全噪声生成方式，所生成的特征与真实测试样本脱节，也导致了虚假关联。为增强已见和未见类别的视觉-语义关联，我们提出了ZeroDiff++——一种基于扩散模型的生成框架。在训练阶段，ZeroDiff++采用（i）扩散增强以生成多样化的加噪样本，（ii）监督对比（SC）表示以获取实例级语义信息，以及（iii）结合Wasserstein互学习的多视角判别器来评估生成特征。在生成阶段，我们引入（iv）基于扩散的测试时自适应（DiffTTA），利用伪标签重建来调整生成器，以及（v）基于扩散的测试时生成（DiffGen），通过追踪扩散去噪路径生成部分合成特征，从而连接真实与生成数据，并进一步缓解数据稀缺问题。在三个ZSL基准上的大量实验表明，ZeroDiff++不仅显著优于现有ZSL方法，而且在训练数据稀缺的情况下仍能保持稳健性能。代码将公开。

</details>


### [7] [MonoLoss: A Training Objective for Interpretable Monosemantic Representations](https://arxiv.org/abs/2602.12403)
*Ali Nasiri-Sarvi,Anh Tien Nguyen,Hassan Rivaz,Dimitris Samaras,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: 本文提出了一种高效的单次遍历算法来计算MonoScore，并基于此设计了Monosemanticity Loss（MonoLoss），用于在训练中直接提升稀疏自编码器（SAE）学到的特征的单语义性，显著加速评估与训练过程，并在多个模型和数据集上提高了特征的可解释性和分类纯度。


<details>
  <summary>Details</summary>
Motivation: 标准稀疏自编码器的训练目标对分解多义神经表征（polysemantic representations）为单义特征（monosemantic features）的激励较弱，且现有单义性度量方法计算成本高，难以在训练中高效使用。

Method: 作者改进了近期提出的MonoScore度量方法，设计了一个单次遍历算法，使其计算复杂度从二次方降至线性；并以此为基础提出了Monosemanticity Loss（MonoLoss），作为可插拔的训练目标，直接鼓励语义一致的激活。该损失函数被用于多种SAE架构（如BatchTopK、TopK、JumpReLU）及不同视觉模型（CLIP、SigLIP2、ViT）的特征上，并进一步作为辅助正则项用于ResNet-50和CLIP-ViT-B/32的微调。

Result: 在OpenImagesV7上，新算法在评估阶段实现最高1200倍、训练阶段159倍的速度提升，每轮仅增加约4%开销。MonoLoss在大多数潜在变量上提升了MonoScore，并在所有编码器与SAE组合中一致提高了类纯度（class purity），最大提升从0.152到0.723。在ImageNet-1K微调中带来最高0.6%的准确率提升，并在标准基准数据集上产生更清晰的单义激活模式。

Conclusion: MonoLoss是一种高效且有效的训练目标，能显著提升SAE所学特征的单义性和可解释性，同时兼容多种模型架构，适用于表示学习与模型微调场景。

Abstract: Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.

Abstract (中文翻译): 稀疏自编码器（SAE）能够将多义神经表征（即单个神经元响应多个无关概念）分解为捕捉单一、可解释概念的单义特征。然而，标准训练目标对此类分解的激励作用较弱，且现有的单义性度量方法需要对数据集中所有样本进行两两比较，导致训练和评估效率低下。本文研究了近期提出的MonoScore度量方法，并推导出一种单次遍历算法，可在保持结果完全一致的前提下，将计算成本从随图像数量呈二次方增长降低为线性增长。在OpenImagesV7数据集上，该方法在评估阶段最多实现1200倍的时钟时间加速，训练阶段加速达159倍，且每轮训练仅增加约4%的额外开销。这使得我们可以将MonoScore用作训练信号：我们提出了单义性损失（Monosemanticity Loss, MonoLoss），一种即插即用的目标函数，直接奖励语义一致的激活，以学习可解释的单义表征。在CLIP、SigLIP2和预训练ViT特征上训练的各类SAE（包括BatchTopK、TopK和JumpReLU）中，MonoLoss提升了大多数潜在变量的MonoScore。此外，在所有编码器与SAE组合中，MonoLoss始终提高了类纯度（即某个潜在变量激活的图像中属于其主导类别的比例），最大提升将基线纯度从0.152提高至0.723。在ResNet-50和CLIP-ViT-B/32的微调过程中，将MonoLoss作为辅助正则项使用，可在ImageNet-1K上带来最高0.6%的准确率提升，并在标准基准数据集上产生清晰的单义激活模式。代码已公开发布于 https://github.com/AtlasAnalyticsLab/MonoLoss。

</details>


### [8] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: 本文提出PathoSpatial，一种可解释的端到端框架，用于融合配准的全切片图像（WSI）和空间转录组（ST）数据，以学习具有空间信息的预后表征，在多个生存终点上表现优异，并支持原型解释和分子风险分解。


<details>
  <summary>Details</summary>
Motivation: 随着配对的WSI-ST队列扩展到人群规模，利用其互补的空间信号进行预后建模变得至关重要，但目前缺乏针对该范式的有效跨模态融合策略。

Method: PathoSpatial采用多层级专家架构，结合任务引导的原型学习，自适应地协调模态内无监督发现与跨模态有监督聚合，整合共配准的WSI和ST数据。

Result: 在三阴性乳腺癌队列上的实验表明，PathoSpatial在五个生存终点上均表现出强劲且一致的性能，优于或媲美当前领先的单模态和多模态方法，并支持事后原型解释和分子风险分解。

Conclusion: PathoSpatial为可扩展、可解释的多模态学习提供了一个概念验证，适用于空间组学与病理学的融合。

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

Abstract (中文翻译): 全切片图像（WSI）通过多实例学习（MIL）实现弱监督预后建模。空间转录组学（ST）保留原位基因表达，提供补充形态学的空间分子背景。随着配对的WSI-ST队列扩展至人群规模，利用其互补的空间信号进行预后分析变得至关重要；然而，针对这一范式仍缺乏原则性的跨模态融合策略。为此，我们提出了PathoSpatial——一种可解释的端到端框架，用于整合共配准的WSI与ST数据，以学习具有空间信息的预后表征。PathoSpatial在多层级专家架构中采用任务引导的原型学习，自适应地协调模态内的无监督发现与跨模态的有监督聚合。该设计在保持判别能力的同时显著增强了可解释性。我们在一个包含配对ST与WSI的三阴性乳腺癌队列上评估了PathoSpatial。结果表明，PathoSpatial在五个生存终点上均表现出强劲且一致的性能，优于或媲美当前领先的单模态和多模态方法。PathoSpatial天然支持事后原型解释和分子风险分解，提供定量且具有生物学依据的解释，突出了潜在的预后因子。我们将PathoSpatial作为可扩展、可解释的多模态学习在空间组学-病理学融合中的概念验证。

</details>


### [9] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种语义感知的对抗微调方法（SAFT），通过使用语义集成攻击生成更有效的对抗样本，显著提升了CLIP模型在零样本分类任务中的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用余弦相似度生成对抗样本以增强CLIP的鲁棒性，但单一图像与手工模板之间的余弦相似度不足以准确衡量图文对的语义相似性，导致生成的对抗样本在更优相似度度量下失效，从而限制了鲁棒性提升。

Method: 作者提出语义集成攻击：首先利用基础模型生成多样化的文本描述以捕捉核心语义，再对其进行去幻觉精炼；然后基于这些描述集合的平均相似度最小化来生成语义感知的对抗样本，并以此对CLIP图像编码器进行对抗微调（SAFT）。

Result: 在16个数据集上的大量实验表明，SAFT显著优于现有方法，在零样本对抗鲁棒性方面取得大幅提升。

Conclusion: 通过引入语义更丰富、更具代表性的文本描述来生成对抗样本，能有效提升CLIP模型的对抗鲁棒性，验证了语义感知对抗训练的有效性。

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

Abstract (中文翻译): 近期研究表明，在零样本分类任务中，通过对CLIP模型的图像编码器进行对抗微调（使用对抗样本进行训练），可以增强其对抗鲁棒性。这些对抗样本通过最小化图像与手工设计模板（例如“一张{标签}的照片”）之间的余弦相似度生成。然而，已有研究指出，单张图像与单一手工模板之间的余弦相似度不足以准确衡量图像-文本对的相似性。基于此，本文发现：当将相似度度量替换为语义更丰富的替代方法时，基于余弦相似度生成的对抗样本可能无法有效欺骗CLIP模型，导致使用此类对抗样本微调后的图像编码器鲁棒性不足。为解决该问题，我们首先提出一种语义集成攻击方法，通过最小化原始图像与一组精炼文本描述之间的平均相似度，生成语义感知的对抗样本。这些文本描述首先由基础模型生成，以捕捉超越手工模板的核心语义特征，随后经过精炼以减少幻觉现象。在此基础上，我们提出了语义感知对抗微调（SAFT）方法，利用语义感知对抗样本对CLIP的图像编码器进行微调。大量实验表明，SAFT在16个数据集上均显著优于当前方法，在零样本对抗鲁棒性方面取得了实质性提升。代码已开源：https://github.com/tmlr-group/SAFT。

</details>


### [10] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: 本文提出一种基于优化DenseNet121的葡萄叶病害分类方法，通过领域特定预处理和可解释性分析，在保持高准确率（99.27%）的同时实现较低计算开销，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 当前基于YOLO等框架的自动化葡萄病害检测方法计算成本高且缺乏可解释性，难以在真实场景中应用。因此，亟需一种高效、准确且可解释的葡萄叶病害识别方法以支持葡萄园的可持续管理。

Method: 采用优化后的DenseNet121模型，结合针对葡萄叶图像的领域特定预处理，利用密集连接提取病害相关特征（如叶脉、边缘和病斑）。使用Grad-CAM进行可解释性分析，并通过迁移学习应对小样本和不平衡数据问题。

Result: 所提模型在测试中达到99.27%准确率、99.28% F1分数、99.71%特异性及98.86% Kappa系数，推理时间为9秒；交叉验证平均准确率为99.12%，优于ResNet18、VGG16、AlexNet和SqueezeNet等基线模型。

Conclusion: 该框架结合高效架构、领域预处理和可解释输出，在保证高精度的同时显著降低计算需求，具有良好的泛化能力与实际部署潜力，适用于葡萄叶病害的实时检测。

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

Abstract (中文翻译): 葡萄是全球最具经济和文化价值的水果之一，欧洲和亚洲大量生产鲜食葡萄和葡萄酒。细菌性腐烂、霜霉病和白粉病等葡萄病害严重影响其产量与品质，因此，葡萄园的可持续管理需要对这些病害进行早期且精准的识别。目前的自动化方法（尤其是基于YOLO框架的方法）通常计算成本高且缺乏可解释性，难以适用于现实场景。本研究提出一种基于优化DenseNet121的葡萄叶病害分类方法。通过领域特定的预处理和密集连接结构，有效揭示了与病害相关的特征，如叶脉、边缘和病斑。与ResNet18、VGG16、AlexNet和SqueezeNet等基准CNN模型的广泛比较表明，所提模型性能更优：准确率达99.27%，F1分数为99.28%，特异性为99.71%，Kappa系数为98.86%，推理时间为9秒。交叉验证结果显示出99.12%的平均准确率，表明模型在所有类别上均具备强健性和泛化能力。此外，研究采用Grad-CAM技术突出显示病害相关区域，确保模型关注生理上相关的特征，从而提升透明度与可信度。模型优化降低了实时部署的处理需求，而迁移学习则保障了在小规模和不平衡样本上的稳定性。综上，该框架凭借高效的架构、领域特定的预处理和可解释的输出，实现了可扩展、高精度且计算成本低廉的葡萄叶病害检测。

</details>
