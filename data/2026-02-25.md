<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography](https://arxiv.org/abs/2602.20165)
*Dorsa EPMoghaddam,Feng Gao,Drew Bernard,Kavya Sinha,Mehdi Razavi,Behnaam Aazhang*

Main category: cs.CV

TL;DR: 该研究提出一种基于人工智能的框架，利用术中常规使用的腔内超声（ICE）视频，通过3D卷积神经网络对心律失常起源进行三分类（正常窦性心律、左侧或右侧心律失常），在交叉验证中准确率达66.2%，显著优于随机基线，展示了利用ICE结合深度学习实现心律失常自动定位的可行性与临床潜力。


<details>
  <summary>Details</summary>
Motivation: 当前高密度标测技术和术前CT/MRI在心律失常定位中耗时且资源密集；而AI已在超声心动图实时分析中展现价值，因此作者希望利用术中常规采集的腔内超声（ICE）数据，开发一种AI辅助方法以快速定位心律失常起源，缩短手术时间。

Method: 将心律失常源定位建模为基于ICE视频的三分类任务（正常窦性心律、左侧心律失常、右侧心律失常），并构建一个3D卷积神经网络进行训练，在十折交叉验证中评估模型性能。

Result: 模型在四个未见过的患者数据上平均准确率达到66.2%，显著优于33.3%的随机猜测基线。

Conclusion: 结合ICE视频与深度学习可实现心律失常的自动化定位，具有临床应用前景，有望加快电生理干预并减轻导管消融手术负担；未来需扩大数据集以提升模型泛化能力。

Abstract: Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.

Abstract (中文翻译): 当代高密度标测技术及术前CT/MRI在心律失常定位方面仍耗费大量时间和资源。人工智能（AI）已被验证可作为临床决策辅助工具，对超声心动图图像提供准确、快速的实时分析。在此基础上，我们提出一种AI驱动的框架，利用电生理手术中常规使用的腔内超声（ICE）来引导临床医生定位心律失常起源区域，从而可能缩短手术时间。心律失常源定位被建模为一项三分类任务，基于ICE视频数据区分正常窦性心律、左侧起源和右侧起源的心律失常。我们开发了一个3D卷积神经网络，用于区分上述三类情况。在十折交叉验证中，该模型在四个此前未见过的患者数据上评估时，平均准确率达到66.2%（显著优于33.3%的随机基线）。这些结果表明，结合ICE视频与深度学习进行心律失常自动定位具有可行性和临床应用前景。利用ICE成像有望实现更快、更有针对性的电生理干预，并减轻心脏消融手术的操作负担。未来工作将聚焦于扩展数据集，以提升模型在不同患者群体中的鲁棒性与泛化能力。

</details>


### [2] [OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport](https://arxiv.org/abs/2602.20205)
*Xiwen Chen,Wenhui Zhu,Gen Li,Xuanzhao Dong,Yujian Xiong,Hao Wang,Peijie Qiu,Qingquan Song,Zhipeng Wang,Shao Tang,Yalin Wang,Abolfazl Razi*

Main category: cs.CV

TL;DR: OTPrune 是一种无需训练的视觉 token 剪枝方法，通过最优传输（OT）对齐完整与剪枝后 token 的分布，在降低推理成本的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 推理成本高，源于冗余视觉 token；当前剪枝方法忽略视觉表征的分布结构，导致剪枝效果不佳。

Method: 提出 OTPrune 框架，将剪枝建模为分布对齐问题，最小化完整与剪枝 token 分布间的 2-Wasserstein 距离；推导出可高效优化的子模目标函数，并证明其单调性与子模性。

Result: 在多个基准上实验表明，OTPrune 在性能与效率之间取得优于现有最先进方法的平衡。

Conclusion: OTPrune 通过分布对齐实现稳定且语义保真的剪枝，为高效 MLLM 推理提供了理论支持和实用方案。

Abstract: Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.

Abstract (中文翻译): 多模态大语言模型（MLLMs）在视觉-语言推理方面表现出色，但由于冗余的视觉 token 导致推理成本高昂。近期研究探索了视觉 token 剪枝以加速推理，但现有剪枝方法忽略了视觉表征的潜在分布结构。我们提出了 OTPrune，一种无需训练的框架，将剪枝问题形式化为通过最优传输（OT）实现分布对齐。通过最小化完整 token 与剪枝后 token 分布之间的 2-Wasserstein 距离，OTPrune 在降低推理成本的同时保留了局部多样性与全局代表性。此外，我们推导出一个易于处理的子模目标函数，使其能够高效优化，并从理论上证明了该目标函数的单调性与子模性，为稳定高效的剪枝提供了原则性基础。我们还进行了全面分析，阐释了分布对齐如何促进稳定且语义忠实的剪枝。在更广泛基准上的综合实验表明，OTPrune 相较于当前最先进的方法，在性能与效率之间取得了更优的权衡。代码已开源：https://github.com/xiwenc1/OTPrune。

</details>


### [3] [De-rendering, Reasoning, and Repairing Charts with Vision-Language Models](https://arxiv.org/abs/2602.20291)
*Valentin Bonas,Martin Sinnona,Viviana Siless,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: 本文提出一个结合图表反渲染、自动分析与迭代优化的框架，利用视觉-语言推理和可视化设计原则，为图表图像提供可操作、可解释的设计改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的可视化检查工具缺乏上下文理解，无法提供建设性修改建议；而通用大语言模型因未专门训练遵循可视化设计原则，反馈不可靠。因此需要一种能结合领域知识、自动生成高质量建议的智能系统。

Method: 该系统首先从图表图像中重构其结构（chart de-rendering），然后通过视觉-语言推理识别设计缺陷，并依据可视化研究中的既定原则提出具体修改建议。用户可选择性采纳建议并重新渲染图表，形成改进闭环。

Result: 在Chart2Code基准的1,000张图表上评估，系统生成了10,452条设计建议，聚类为10个连贯类别（如坐标轴格式、颜色无障碍性、图例一致性等）。

Conclusion: 该工作展示了基于大语言模型的推荐系统在提供结构化、基于原则的可视化设计反馈方面的潜力，为开发更智能、易用的可视化创作工具开辟了新路径。

Abstract: Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.

Abstract (中文翻译): 数据可视化在科学传播、新闻报道和日常决策中至关重要，但却常常存在错误，可能导致误读或误导受众。基于规则的可视化检查工具可以标记违规项，但缺乏上下文理解，也无法提出有意义的设计改进建议。直接向通用大语言模型（LLM）询问可视化质量并不可靠：由于缺乏遵循可视化设计原则的训练，它们经常给出不一致或错误的反馈。在本研究中，我们引入了一个结合图表反渲染、自动分析和迭代优化的框架，以提供可操作且可解释的可视化设计反馈。我们的系统从图像中重建图表结构，利用视觉-语言推理识别设计缺陷，并依据可视化研究中已确立的原则提出具体的修改建议。用户可以选择性地应用这些改进并重新渲染更新后的图表，从而形成一个既能提升可视化质量又能促进可视化素养发展的反馈闭环。在对Chart2Code基准中1,000张图表的评估中，该系统共生成了10,452条设计建议，这些建议被聚类为10个连贯的类别（例如坐标轴格式、色彩无障碍性、图例一致性等）。这些结果凸显了由大语言模型驱动的推荐系统在提供结构化、基于原则的可视化设计反馈方面的前景，为开发更智能、更易访问的可视化创作工具打开了大门。

</details>


### [4] [N4MC: Neural 4D Mesh Compression](https://arxiv.org/abs/2602.20312)
*Guodong Chen,Huanshuo Dong,Mallesham Dasari*

Main category: cs.CV

TL;DR: N4MC 是首个 4D 神经压缩框架，通过利用时间冗余高效压缩时变网格序列，在率失真性能上优于现有方法，并支持实时解码。


<details>
  <summary>Details</summary>
Motivation: 现有神经网格压缩方法通常独立处理每一帧网格，忽略了帧间的时间冗余；受 2D 视频编解码器中帧间压缩的启发，作者希望开发一种能学习网格序列中运动补偿的压缩方法。

Method: N4MC 将连续不规则网格帧转换为规则的 4D 张量，使用自解码器捕捉时空相关性以去除冗余，并引入基于 Transformer 的插值模型，根据追踪体积中心导出的潜在嵌入预测中间帧，提升时间一致性。

Result: 实验表明 N4MC 在率失真性能上优于当前最先进方法，并能实现实时解码 4D 网格序列。

Conclusion: N4MC 成功将视频帧间压缩思想引入 4D 网格压缩，显著提升了压缩效率和重建质量，为动态网格序列的高效存储与传输提供了新方案。

Abstract: We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.

Abstract (中文翻译): 我们提出了 N4MC，这是首个 4D 神经压缩框架，通过利用时间冗余高效压缩时变网格序列。与以往独立处理每帧网格的神经压缩方法不同，N4MC 借鉴了 2D 视频编解码器中的帧间压缩思想，学习在长网格序列中进行运动补偿。具体而言，N4MC 将连续的不规则网格帧转换为规则的 4D 张量，以提供统一且紧凑的表示。随后，这些张量通过一个自解码器进行压缩，该自解码器能够同时捕捉空间和时间相关性以去除冗余。为了增强时间一致性，我们引入了一种基于 Transformer 的插值模型，该模型根据从追踪体积中心导出的潜在嵌入来预测中间网格帧，从而消除运动模糊。大量实验表明，N4MC 在率失真性能上优于当前最先进方法，同时支持 4D 网格序列的实时解码。我们的方法实现代码已开源：https://github.com/frozzzen3/N4MC。

</details>


### [5] [GSNR: Graph Smooth Null-Space Representation for Inverse Problems](https://arxiv.org/abs/2602.20328)
*Romario Gualdrón-Hurtado,Roman Jacome,Rafael S. Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 该论文提出了一种名为图平滑零空间表示（GSNR）的新机制，通过在图像逆问题的零空间分量中引入图结构先验，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统图像先验（如稀疏性、平滑性或得分函数）无法约束感知矩阵零空间中的分量，导致重建结果存在偏差。因此，作者希望在重建框架中引入有意义的零空间信息。

Method: 受图上平滑图像表示的启发，作者提出了GSNR方法：给定一个图拉普拉斯算子，构建一个零空间受限的拉普拉斯算子来编码零空间信号中相邻像素的相似性，并从p个最平滑的谱图模式（最低图频率）设计一个低维投影矩阵。

Result: GSNR被集成到PnP、DIP和扩散求解器等主流逆问题求解框架中，在图像去模糊、压缩感知、去马赛克和超分辨率四个任务上，相比基线方法PSNR最高提升4.3 dB，相比端到端学习模型最高提升1 dB。

Conclusion: 通过仅在不可见分量（零空间）中施加结构约束，GSNR在理论上和实践中都具有显著优势，包括更快的收敛速度、更好的零空间方差覆盖度以及更高的模式可预测性，从而有效提升了图像重建性能。

Abstract: Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.

Abstract (中文翻译): 成像中的逆问题是不适定的，由于感知矩阵存在非平凡的零空间，导致与测量结果一致的解有无穷多个。常见的图像先验（如稀疏性、平滑性或得分函数）倾向于推广到一般图像流形上的解。然而，由于这些先验无法约束零空间分量，可能导致重建结果产生偏差。因此，我们旨在在重建框架中引入有意义的零空间信息。受图上平滑图像表示的启发，我们提出了图平滑零空间表示（GSNR）机制，该机制仅对不可见分量施加结构约束。具体而言，给定一个图拉普拉斯算子，我们构建一个零空间受限的拉普拉斯算子，以编码零空间信号中相邻像素之间的相似性，并从p个最平滑的谱图模式（即最低图频率）设计一个低维投影矩阵。该方法具有强大的理论和实践意义：i) 通过仅作用于零空间的图正则化项实现更快的收敛；ii) 更好的覆盖率，即p个模式能捕获多少零空间方差；iii) 更高的可预测性，即这些模式能从测量值中被推断得有多好。GSNR被整合到多种知名逆问题求解器中（例如PnP、DIP和扩散求解器），并在图像去模糊、压缩感知、去马赛克和图像超分辨率四种场景下进行了验证，相较于基线方法PSNR最高提升4.3 dB，相较于端到端学习模型最高提升1 dB。

</details>


### [6] [Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking](https://arxiv.org/abs/2602.20330)
*Jingcheng Yang,Tianhu Xiong,Shengyi Qian,Klara Nahrstedt,Mingyuan Wu*

Main category: cs.CV

TL;DR: 本文提出了首个用于视觉-语言模型（VLMs）的透明电路追踪框架，通过转码器、归因图和注意力方法揭示其多模态推理机制，并验证了相关视觉特征电路在数学推理和跨模态关联中的因果性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型虽功能强大，但其内部工作机制如同黑箱，缺乏可解释性。为提升模型的透明度与可靠性，亟需一种系统性方法来解析其多模态推理过程。

Method: 提出一个透明电路追踪框架，结合转码器（transcoders）、归因图（attribution graphs）和基于注意力的方法，系统分析VLM中视觉与语义概念的层次化整合方式。

Result: 发现不同的视觉特征电路能够处理数学推理任务并支持跨模态关联；通过特征操控（feature steering）和电路修补（circuit patching）验证了这些电路的因果性和可控性。

Conclusion: 该框架为构建更可解释、更可靠的视觉-语言模型奠定了基础，推动了对多模态模型内部机制的理解。

Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.

Abstract (中文翻译): 视觉-语言模型（VLMs）功能强大，但仍是不透明的黑箱。我们提出了首个用于VLMs的透明电路追踪框架，以系统分析其多模态推理过程。通过利用转码器、归因图和基于注意力的方法，我们揭示了VLMs如何分层整合视觉与语义概念。我们发现，不同的视觉特征电路能够处理数学推理并支持跨模态关联。通过特征操控和电路修补进行验证，我们的框架证明了这些电路具有因果性和可控性，为构建更可解释、更可靠的VLMs奠定了基础。

</details>


### [7] [Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques](https://arxiv.org/abs/2602.20342)
*Christos Maikos,Georgios Angelidis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出一个端到端的无人机视频流处理管道，结合3D高斯泼溅技术，实现低延迟、高保真度的实时3D重建与AR/VR可视化。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏将无人机视频流与先进3D重建技术（如3D高斯泼溅）集成到端到端实时系统中的研究，限制了其在AR/VR等沉浸式应用中的使用。

Method: 构建了一个融合RTMP视频流、传感器融合、相机位姿估计和3D高斯泼溅优化的高效架构，支持连续模型更新与低延迟交互式可视化。

Result: 相比NeRF方法，该系统在保持4–7%内离线高保真参考质量的同时，显著提升了渲染性能并大幅降低端到端延迟。

Conclusion: 所提系统适用于无人机平台上的实时、可扩展增强感知任务，具备良好的视觉保真度与低延迟特性。

Abstract: In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.

Abstract (中文翻译): 本研究提出了一种端到端的处理流程，能够将无人机捕获的视频流转为高保真度的3D重建结果，并实现极低延迟。无人机（UAV）广泛应用于空中实时感知任务，而近期3D高斯泼溅（3DGS）技术在实时神经渲染方面展现出巨大潜力。然而，将这些技术整合到基于无人机的端到端重建与可视化系统中仍鲜有探索。我们的目标是设计一种高效架构，结合通过RTMP流传输的实时视频采集、同步的传感器融合、相机位姿估计以及3DGS优化，从而在支持沉浸式增强现实与虚拟现实（AR/VR）应用的交互式可视化环境中实现模型的持续更新与低延迟部署。实验结果表明，与基于NeRF的方法相比，所提方法在视觉保真度上具有竞争力，同时显著提升了渲染性能并大幅降低了端到端延迟。其重建质量与高保真离线参考结果的差距控制在4–7%以内，验证了该系统适用于来自空中平台的实时、可扩展增强感知任务。

</details>


### [8] [BiRQA: Bidirectional Robust Quality Assessment for Images](https://arxiv.org/abs/2602.20351)
*Aleksandr Gushchin,Dmitriy S. Vatolin,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: BiRQA是一种高效且鲁棒的全参考图像质量评估模型，通过双向多尺度金字塔结构和锚定对抗训练，在保持实时速度的同时显著提升了对对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络驱动的全参考图像质量评估（FR IQA）方法存在速度慢和易受对抗扰动影响的问题，限制了其在图像压缩、恢复和生成建模等实际应用中的部署。

Method: 提出BiRQA模型，利用双向多尺度金字塔处理四种快速互补特征：自下而上的注意力模块通过不确定性感知门控将细粒度信息注入粗尺度，自上而下的交叉门控块将语义上下文回传至高分辨率。同时引入锚定对抗训练策略，结合干净“锚点”样本和排序损失，从理论上约束对抗攻击下的逐点预测误差。

Result: 在五个公开FR IQA基准上，BiRQA达到或超越了先前的SOTA性能，且推理速度约为之前SOTA模型的3倍；在未见过的白盒攻击下，KADID-10k数据集上的SROCC从0.30–0.57提升至0.60–0.84，展现出显著的鲁棒性提升。

Conclusion: BiRQA是目前唯一同时具备高精度、实时处理能力和强对抗鲁棒性的FR IQA模型，为实际应用提供了可靠解决方案。

Abstract: Full-Reference image quality assessment (FR IQA) is important for image compression, restoration and generative modeling, yet current neural metrics remain slow and vulnerable to adversarial perturbations. We present BiRQA, a compact FR IQA metric model that processes four fast complementary features within a bidirectional multiscale pyramid. A bottom-up attention module injects fine-scale cues into coarse levels through an uncertainty-aware gate, while a top-down cross-gating block routes semantic context back to high resolution. To enhance robustness, we introduce Anchored Adversarial Training, a theoretically grounded strategy that uses clean "anchor" samples and a ranking loss to bound pointwise prediction error under attacks. On five public FR IQA benchmarks BiRQA outperforms or matches the previous state of the art (SOTA) while running ~3x faster than previous SOTA models. Under unseen white-box attacks it lifts SROCC from 0.30-0.57 to 0.60-0.84 on KADID-10k, demonstrating substantial robustness gains. To our knowledge, BiRQA is the only FR IQA model combining competitive accuracy with real-time throughput and strong adversarial resilience.

Abstract (中文翻译): 全参考图像质量评估（FR IQA）在图像压缩、恢复和生成建模中具有重要意义，但现有的神经网络指标仍存在速度慢且易受对抗扰动影响的问题。本文提出了BiRQA，一种紧凑型FR IQA度量模型，它在一个双向多尺度金字塔结构中处理四种快速互补特征。自下而上的注意力模块通过一个不确定性感知门控机制，将细尺度线索注入到粗尺度层级；而自上而下的交叉门控模块则将语义上下文信息回传至高分辨率层级。为了增强模型鲁棒性，我们引入了锚定对抗训练（Anchored Adversarial Training）——一种理论上有保障的策略，利用干净的“锚点”样本和排序损失来约束对抗攻击下的逐点预测误差。在五个公开FR IQA基准测试中，BiRQA的性能达到或超越了先前的最先进水平（SOTA），同时运行速度比之前的SOTA模型快约3倍。在面对未见过的白盒攻击时，BiRQA在KADID-10k数据集上的SROCC指标从0.30–0.57显著提升至0.60–0.84，展现出强大的鲁棒性提升。据我们所知，BiRQA是目前唯一一个同时具备竞争性精度、实时吞吐能力以及强对抗鲁棒性的FR IQA模型。

</details>


### [9] [3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism](https://arxiv.org/abs/2602.20354)
*Bhavik Chandna,Kelsey R. Allen*

Main category: cs.CV

TL;DR: 提出了一种无需参考视频的自动化视频真实感评估方法3DSPA，通过融合3D点轨迹、深度线索和语义特征，有效衡量生成视频的真实性、时间一致性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频的真实性评估主要依赖人工标注或特定数据集，缺乏通用、自动化的评估手段。为支持从机器人到影视制作等广泛应用，亟需一种能自动、全面评估视频真实感的方法。

Method: 提出3DSPA（3D Spatiotemporal Point Autoencoder），将3D点轨迹、深度信息和DINO语义特征整合为统一表征，用于无参考视频的真实感评估。

Result: 实验表明，3DSPA能可靠识别违反物理规律的视频，对运动伪影更敏感，并在多个数据集上与人类对视频质量和真实感的判断高度一致。

Conclusion: 融合3D语义的轨迹表征为生成视频模型的评估提供了更坚实的基础，能够隐式捕捉物理规则的违反，推动了自动化视频评估的发展。

Abstract: AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.

Abstract (中文翻译): AI视频生成技术正在迅速发展。要使视频生成器在从机器人到电影制作等应用中发挥作用，它们必须始终如一地生成逼真的视频。然而，目前对生成视频真实性的评估在很大程度上仍依赖人工——需要人工标注或定制的评估数据集，而这些数据集的适用范围有限。本文提出了一种用于视频真实感的自动化评估框架，该框架无需参考视频，即可同时捕捉语义信息和连贯的3D结构。我们的方法名为3DSPA（3D时空点自编码器），它将3D点轨迹、深度线索和DINO语义特征整合为一个统一的视频评估表征。3DSPA建模了物体如何运动以及场景中发生了什么，从而能够对视频的真实性、时间一致性和物理合理性进行稳健评估。实验表明，3DSPA能够可靠地识别出违反物理定律的视频，对运动伪影更加敏感，并且在多个数据集上与人类对视频质量和真实感的判断更为一致。我们的结果表明，在基于轨迹的表征中融入3D语义信息，为生成视频模型的基准测试提供了更坚实的基础，并能隐式捕捉物理规则的违反。代码和预训练模型权重将在 https://github.com/TheProParadox/3dspa_code 公开。

</details>


### [10] [Aesthetic Camera Viewpoint Suggestion with 3D Aesthetic Field](https://arxiv.org/abs/2602.20363)
*Sheyang Tang,Armin Shafiee Sarvestani,Jialu Xu,Xiaoyu Xu,Zhou Wang*

Main category: cs.CV

TL;DR: 本文提出一种基于稀疏输入视图的3D美学场方法，通过将2D美学知识蒸馏到3D空间，实现高效且几何感知的相机视角推荐，避免了传统方法对密集采集或强化学习的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有美学视角推荐方法要么仅基于单张图像进行有限调整而缺乏场景几何理解，要么依赖密集采集或预建3D环境并结合昂贵的强化学习搜索。作者希望在仅使用稀疏视图的前提下，实现高效、几何感知的3D美学推理。

Method: 提出“3D美学场”概念，利用前馈式的3D Gaussian Splatting网络，将预训练2D美学模型的知识蒸馏到3D空间；在此基础上构建两阶段搜索流程：粗粒度视角采样 + 基于梯度的精细化优化。

Result: 实验表明，该方法在构图与取景方面优于现有方法，能高效推荐具有高美学质量的视角，且无需密集采集或强化学习。

Conclusion: 本工作为3D感知的美学建模开辟了新方向，展示了利用稀疏输入进行高效美学视角推荐的可行性。

Abstract: The aesthetic quality of a scene depends strongly on camera viewpoint. Existing approaches for aesthetic viewpoint suggestion are either single-view adjustments, predicting limited camera adjustments from a single image without understanding scene geometry, or 3D exploration approaches, which rely on dense captures or prebuilt 3D environments coupled with costly reinforcement learning (RL) searches. In this work, we introduce the notion of 3D aesthetic field that enables geometry-grounded aesthetic reasoning in 3D with sparse captures, allowing efficient viewpoint suggestions in contrast to costly RL searches. We opt to learn this 3D aesthetic field using a feedforward 3D Gaussian Splatting network that distills high-level aesthetic knowledge from a pretrained 2D aesthetic model into 3D space, enabling aesthetic prediction for novel viewpoints from only sparse input views. Building on this field, we propose a two-stage search pipeline that combines coarse viewpoint sampling with gradient-based refinement, efficiently identifying aesthetically appealing viewpoints without dense captures or RL exploration. Extensive experiments show that our method consistently suggests viewpoints with superior framing and composition compared to existing approaches, establishing a new direction toward 3D-aware aesthetic modeling.

Abstract (中文翻译): 场景的美学质量在很大程度上取决于相机视角。现有的美学视角推荐方法要么是单视图调整方法，即从单张图像预测有限的相机调整，但缺乏对场景几何的理解；要么是3D探索方法，依赖密集采集或预建的3D环境，并结合计算成本高昂的强化学习（RL）搜索。在本研究中，我们引入了“3D美学场”的概念，该方法能够在仅有稀疏采集的情况下实现基于几何的3D美学推理，从而高效地推荐视角，避免了昂贵的强化学习搜索。我们采用前馈式的3D Gaussian Splatting网络来学习这一3D美学场，将预训练2D美学模型中的高层美学知识蒸馏到3D空间中，从而仅凭稀疏输入视图即可对新视角进行美学预测。基于该美学场，我们提出了一种两阶段搜索流程，结合粗粒度视角采样与基于梯度的精细化优化，高效地识别出具有高美学吸引力的视角，而无需密集采集或强化学习探索。大量实验表明，我们的方法在构图与取景方面始终优于现有方法，为3D感知的美学建模开辟了新方向。

</details>
