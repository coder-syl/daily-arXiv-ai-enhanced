{"id": "2601.10781", "pdf": "https://arxiv.org/pdf/2601.10781", "abs": "https://arxiv.org/abs/2601.10781", "authors": ["Kanchana Ranasinghe", "Honglu Zhou", "Yu Fang", "Luyu Yang", "Le Xue", "Ran Xu", "Caiming Xiong", "Silvio Savarese", "Michael S Ryoo", "Juan Carlos Niebles"], "title": "Future Optical Flow Prediction Improves Robot Control & Video Generation", "categories": ["cs.CV"], "comment": "Project Site (Code, Models, Demo): https://fofpred.github.io", "summary": "Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.", "AI": {"tldr": "FOFPred \u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u6269\u6563\u67b6\u6784\u7684\u8bed\u8a00\u6761\u4ef6\u5149\u6d41\u9884\u6d4b\u6a21\u578b\uff0c\u5229\u7528\u5927\u89c4\u6a21\u7f51\u7edc\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c55\u73b0\u51fa\u5728\u673a\u5668\u4eba\u63a7\u5236\u548c\u89c6\u9891\u751f\u6210\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8de8\u9886\u57df\u901a\u7528\u6027\u3002", "motivation": "\u9884\u6d4b\u53ef\u6cdb\u5316\u7684\u7a7a\u95f4\u5bc6\u96c6\u8fd0\u52a8\u8868\u793a\uff08\u5982\u5149\u6d41\uff09\u4ecd\u5177\u6311\u6218\uff0c\u5c24\u5176\u5728\u4ece\u5608\u6742\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u5b66\u4e60\u65b9\u9762\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa FOFPred \u6a21\u578b\uff0c\u878d\u5408\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u6269\u6563\u67b6\u6784\uff0c\u901a\u8fc7\u8bed\u8a00\u6761\u4ef6\u5f15\u5bfc\uff0c\u5728\u50cf\u7d20\u7ea7\u522b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u672a\u6765\u8fd0\u52a8\u9884\u6d4b\uff1b\u4f7f\u7528\u5927\u89c4\u6a21\u7f51\u7edc\u4eba\u7c7b\u6d3b\u52a8\u89c6\u9891-\u6587\u672c\u5bf9\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u5173\u952e\u7684\u6570\u636e\u9884\u5904\u7406\u4e0e\u5f3a\u56fe\u50cf\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u8bed\u8a00\u9a71\u52a8\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0cFOFPred \u8868\u73b0\u51fa\u4f18\u5f02\u7684\u8de8\u57df\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u67b6\u6784\u548c\u4ece\u591a\u6837\u5316\u7f51\u7edc\u6570\u636e\u4e2d\u53ef\u6269\u5c55\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7edf\u4e00\u7684 VLM-\u6269\u6563\u67b6\u6784\u7ed3\u5408\u53ef\u6269\u5c55\u7684\u7f51\u7edc\u6570\u636e\u8bad\u7ec3\uff0c\u80fd\u6709\u6548\u63d0\u5347\u672a\u6765\u5149\u6d41\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u5e94\u7528\u5e7f\u5ea6\u3002", "summary_cn": "\u672a\u6765\u8fd0\u52a8\u8868\u793a\uff08\u5982\u5149\u6d41\uff09\u5728\u63a7\u5236\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u4ef7\u503c\u3002\u7136\u800c\uff0c\u9884\u6d4b\u53ef\u6cdb\u5316\u7684\u7a7a\u95f4\u5bc6\u96c6\u8fd0\u52a8\u8868\u793a\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4e14\u4ece\u5608\u6742\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u5b66\u4e60\u6b64\u7c7b\u9884\u6d4b\u65b9\u6cd5\u7684\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002\u6211\u4eec\u63d0\u51fa\u4e86 FOFPred\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u8a00\u6761\u4ef6\u5149\u6d41\u9884\u6d4b\u6a21\u578b\uff0c\u5176\u91c7\u7528\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u6269\u6563\u67b6\u6784\u3002\u8fd9\u79cd\u72ec\u7279\u7ec4\u5408\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u50cf\u7d20\u7ea7\u522b\u4e0a\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u4ee5\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\u3002\u6211\u4eec\u7684\u6a21\u578b\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\uff08\u4e00\u79cd\u9ad8\u5ea6\u53ef\u6269\u5c55\u4f46\u975e\u7ed3\u6784\u5316\u7684\u6570\u636e\u6e90\uff09\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u4ece\u8fd9\u4e9b\u542b\u566a\u7684\u89c6\u9891-\u6587\u672c\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u4fe1\u53f7\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u5173\u952e\u7684\u6570\u636e\u9884\u5904\u7406\u6280\u672f\uff0c\u5e76\u7ed3\u5408\u5177\u6709\u5f3a\u5927\u56fe\u50cf\u9884\u8bad\u7ec3\u80fd\u529b\u7684\u7edf\u4e00\u67b6\u6784\u3002\u8bad\u7ec3\u5b8c\u6210\u7684\u6a21\u578b\u8fdb\u4e00\u6b65\u88ab\u62d3\u5c55\u7528\u4e8e\u89e3\u51b3\u63a7\u5236\u4e0e\u751f\u6210\u9886\u57df\u7684\u4e24\u9879\u4e0d\u540c\u4e0b\u6e38\u4efb\u52a1\u3002\u5728\u8bed\u8a00\u9a71\u52a8\u8bbe\u7f6e\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86 FOFPred \u7684\u8de8\u9886\u57df\u901a\u7528\u6027\uff0c\u4e5f\u8bc1\u5b9e\u4e86\u7edf\u4e00\u7684 VLM-\u6269\u6563\u67b6\u6784\u4ee5\u53ca\u4ece\u591a\u6837\u5316\u7f51\u7edc\u6570\u636e\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u5b66\u4e60\u5bf9\u672a\u6765\u5149\u6d41\u9884\u6d4b\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.10802", "pdf": "https://arxiv.org/pdf/2601.10802", "abs": "https://arxiv.org/abs/2601.10802", "authors": ["Gerhard Krumpl", "Henning Avenhaus", "Horst Possegger"], "title": "ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research", "categories": ["cs.CV"], "comment": "WACV 2026, Dataset repo: https://github.com/gkrumpl/iconic-444", "summary": "Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ICONIC-444\uff0c\u4e00\u4e2a\u5305\u542b310\u4e07\u5f20\u56fe\u50cf\u3001444\u4e2a\u7c7b\u522b\u7684\u5927\u89c4\u6a21\u5de5\u4e1a\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u4e13\u4e3a\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u7814\u7a76\u8bbe\u8ba1\uff0c\u652f\u6301\u4ece\u7ec6\u7c92\u5ea6\u5230\u7c97\u7c92\u5ea6\u7684\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e8622\u79cd\u5148\u8fdbOOD\u65b9\u6cd5\u7684\u57fa\u51c6\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u7814\u7a76\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u4e14\u5177\u6709\u660e\u786e\u5b9a\u4e49OOD\u7c7b\u522b\uff08\u6db5\u76d6\u4ece\u8fd1\u5206\u5e03\u5230\u8fdc\u5206\u5e03\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\uff09\u7684\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u540c\u65f6\u652f\u6301\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u540d\u4e3aICONIC-444\u7684\u5927\u578b\u5de5\u4e1a\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7310\u4e07\u5f20RGB\u56fe\u50cf\u548c444\u4e2a\u7c7b\u522b\uff0c\u5229\u7528\u539f\u578b\u5de5\u4e1a\u5206\u62e3\u673a\u91c7\u96c6\uff0c\u8d34\u8fd1\u771f\u5b9e\u5e94\u7528\u573a\u666f\uff0c\u5e76\u5b9a\u4e49\u4e86\u56db\u4e2a\u53c2\u8003\u4efb\u52a1\u7528\u4e8eOOD\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728ICONIC-444\u4e0a\u5bf922\u79cd\u6700\u5148\u8fdb\u7684\u540e\u5904\u7406OOD\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\uff0c\u8bc1\u660e\u8be5\u6570\u636e\u96c6\u9002\u7528\u4e8e\u4e0d\u540c\u590d\u6742\u5ea6\u4efb\u52a1\u4e0b\u7684\u4e25\u683cOOD\u8bc4\u4f30\u3002", "conclusion": "ICONIC-444\u586b\u8865\u4e86\u73b0\u6709OOD\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u591a\u6837\u5316\u4e14\u8d34\u8fd1\u5de5\u4e1a\u5b9e\u9645\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u5e73\u53f0\u3002", "summary_cn": "\u5f53\u524d\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u7684\u8fdb\u5c55\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u9700\u5177\u5907\u660e\u786e\u5b9a\u4e49\u7684OOD\u7c7b\u522b\uff08\u6db5\u76d6\u4ece\u8fd1\u5206\u5e03\u5230\u8fdc\u5206\u5e03\u7684\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\uff09\uff0c\u5e76\u80fd\u540c\u65f6\u652f\u6301\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ICONIC-444\uff08Image Classification and OOD Detection with Numerous Intricate Complexities\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8eOOD\u68c0\u6d4b\u7814\u7a76\u7684\u5927\u89c4\u6a21\u5de5\u4e1a\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7310\u4e07\u5f20RGB\u56fe\u50cf\uff0c\u6db5\u76d6444\u4e2a\u7c7b\u522b\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u4e00\u53f0\u539f\u578b\u5de5\u4e1a\u5206\u62e3\u673a\u91c7\u96c6\uff0c\u9ad8\u5ea6\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u3002ICONIC-444\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\uff0c\u8865\u5145\u4e86\u73b0\u6709\u6570\u636e\u96c6\uff0c\u9002\u7528\u4e8e\u5728\u5404\u79cd\u4efb\u52a1\u590d\u6742\u5ea6\u4e0b\u8fdb\u884c\u4e25\u683c\u7684OOD\u8bc4\u4f30\u3002\u6211\u4eec\u5728ICONIC-444\u4e2d\u5b9a\u4e49\u4e86\u56db\u4e2a\u53c2\u8003\u4efb\u52a1\uff0c\u7528\u4e8e\u5bf9OOD\u68c0\u6d4b\u7814\u7a76\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e3a22\u79cd\u6700\u5148\u8fdb\u7684\u540e\u5904\u7406OOD\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\u3002"}}
{"id": "2601.10819", "pdf": "https://arxiv.org/pdf/2601.10819", "abs": "https://arxiv.org/abs/2601.10819", "authors": ["Yizhou Wang", "Sameer Pusegaonkar", "Yuxing Wang", "Anqi Li", "Vishal Kumar", "Chetan Sethi", "Ganapathy Aiyer", "Yun He", "Kartikay Thakkar", "Swapnil Rathi", "Bhushan Rupde", "Zheng Tang", "Sujit Biswas"], "title": "A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning \"inside-out\" autonomous driving models to \"outside-in\" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21\u57fa\u7840\u8bbe\u65bd\u73af\u5883\u4f18\u5316\u7684Sparse4D\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e16\u754c\u5750\u6807\u51e0\u4f55\u5148\u9a8c\u548c\u906e\u6321\u611f\u77e5ReID\u6a21\u5757\u63d0\u5347\u591a\u76f8\u673a\u8ddf\u8e2a\u6027\u80fd\uff0c\u5e76\u5229\u7528NVIDIA COSMOS\u8fdb\u884c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5728AI City Challenge 2025\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u540c\u65f6\u5f00\u53d1\u4e86TensorRT\u63d2\u4ef6\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "\u5c06\u81ea\u52a8\u9a7e\u9a76\u4e2d\u201c\u7531\u5185\u5411\u5916\u201d\u7684\u611f\u77e5\u6a21\u578b\u8fc1\u79fb\u5230\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\u4e2d\u201c\u7531\u5916\u5411\u5185\u201d\u7684\u9759\u6001\u591a\u76f8\u673a\u7f51\u7edc\u9762\u4e34\u76f8\u673a\u5e03\u5c40\u5f02\u6784\u4e0e\u4e25\u91cd\u906e\u6321\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u4e13\u4e3a\u8be5\u573a\u666f\u4f18\u5316\u76843D\u611f\u77e5\u4e0e\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eSparse4D\u6846\u67b6\uff0c\u5f15\u5165\u7edd\u5bf9\u4e16\u754c\u5750\u6807\u51e0\u4f55\u5148\u9a8c\u548c\u906e\u6321\u611f\u77e5\u7684ReID\u5d4c\u5165\u6a21\u5757\uff1b\u91c7\u7528NVIDIA COSMOS\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u7b56\u7565\u8fdb\u884c\u65e0\u6807\u7b7eSim2Real\u57df\u9002\u5e94\uff1b\u5e76\u5f00\u53d1TensorRT\u63d2\u4ef6\u4f18\u5316\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u805a\u5408\uff08MSDA\uff09\u4ee5\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728AI City Challenge 2025\u57fa\u51c6\u4e0a\uff0c\u4ec5\u4f7f\u7528\u76f8\u673a\u7684\u7cfb\u7edf\u8fbe\u523045.22\u7684HOTA\u6307\u6807\uff1b\u786c\u4ef6\u52a0\u901f\u7248\u672c\u5728\u73b0\u4ee3GPU\u4e0a\u5b9e\u73b02.15\u500d\u63d0\u901f\uff0c\u5355\u5757Blackwell\u7ea7GPU\u53ef\u652f\u630164\u8def\u4ee5\u4e0a\u5e76\u53d1\u89c6\u9891\u6d41\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9759\u6001\u591a\u76f8\u673a\u7f51\u7edc\u4e0b\u76843D\u76ee\u6807\u611f\u77e5\u4e0e\u8de8\u89c6\u89d2\u8ddf\u8e2a\u96be\u9898\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u4e0e\u5f3a\u5b9e\u65f6\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\u7684\u6570\u5b57\u5316\u90e8\u7f72\u3002", "summary_cn": "\u7cbe\u786e\u76843D\u7269\u4f53\u611f\u77e5\u4e0e\u591a\u76ee\u6807\u591a\u76f8\u673a\uff08MTMC\uff09\u8ddf\u8e2a\u662f\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\u6570\u5b57\u5316\u8f6c\u578b\u7684\u57fa\u7840\u3002\u7136\u800c\uff0c\u5c06\u201c\u7531\u5185\u5411\u5916\u201d\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u8fc1\u79fb\u5230\u201c\u7531\u5916\u5411\u5185\u201d\u7684\u9759\u6001\u76f8\u673a\u7f51\u7edc\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u4e3b\u8981\u6e90\u4e8e\u5f02\u6784\u7684\u76f8\u673a\u5e03\u8bbe\u548c\u6781\u7aef\u906e\u6321\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u5927\u89c4\u6a21\u57fa\u7840\u8bbe\u65bd\u73af\u5883\u4f18\u5316\u7684Sparse4D\u6846\u67b6\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u7edd\u5bf9\u4e16\u754c\u5750\u6807\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u906e\u6321\u611f\u77e5\u7684ReID\u5d4c\u5165\u6a21\u5757\uff0c\u4ee5\u5728\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u4e3a\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5f25\u5408\u4eff\u771f\u5230\u73b0\u5b9e\uff08Sim2Real\uff09\u7684\u57df\u5dee\u8ddd\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8eNVIDIA COSMOS\u6846\u67b6\u7684\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u73af\u5883\u98ce\u683c\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u5bf9\u8868\u89c2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u5728AI City Challenge 2025\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u4ec5\u4f7f\u7528\u76f8\u673a\u7684\u6846\u67b6\u53d6\u5f97\u4e8645.22\u7684HOTA\u6307\u6807\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u4e3a\u6ee1\u8db3\u5b9e\u65f6\u90e8\u7f72\u9700\u6c42\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u805a\u5408\uff08MSDA\uff09\u7684\u4f18\u5316TensorRT\u63d2\u4ef6\u3002\u7ecf\u786c\u4ef6\u52a0\u901f\u540e\uff0c\u8be5\u5b9e\u73b0\u5728\u73b0\u4ee3GPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e862.15\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4f7f\u5f97\u5355\u5757Blackwell\u7ea7GPU\u80fd\u591f\u652f\u6301\u8d85\u8fc764\u8def\u5e76\u53d1\u76f8\u673a\u89c6\u9891\u6d41\u3002"}}
{"id": "2601.10835", "pdf": "https://arxiv.org/pdf/2601.10835", "abs": "https://arxiv.org/abs/2601.10835", "authors": ["Hieu Bui", "Nathaniel E. Chodosh", "Arash Tavakoli"], "title": "Can Vision-Language Models Understand Construction Workers? An Exploratory Study", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u3001Florence 2 \u548c LLaVa-1.5\uff09\u5728\u5efa\u7b51\u5de5\u5730\u9759\u6001\u56fe\u50cf\u4e2d\u8bc6\u522b\u4eba\u7c7b\u884c\u4e3a\u548c\u60c5\u7eea\u7684\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0cGPT-4o \u8868\u73b0\u6700\u4f73\uff0c\u800c\u6240\u6709\u6a21\u578b\u5728\u533a\u5206\u8bed\u4e49\u76f8\u8fd1\u7c7b\u522b\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u5efa\u7b51\u6d41\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u7406\u89e3\u5e76\u54cd\u5e94\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\u5bf9\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u5efa\u7b51\u9886\u57df\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u4e14\u76d1\u6d4b\u5de5\u4eba\u884c\u4e3a\u4e0e\u60c5\u7eea\u5bf9\u5b89\u5168\u548c\u6548\u7387\u610f\u4e49\u91cd\u5927\uff0c\u65e0\u9700\u5927\u91cf\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6210\u4e3a\u6709\u6f5c\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5305\u542b1000\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u5f20\u56fe\u50cf\u6807\u6ce8\u4e8610\u79cd\u884c\u4e3a\u548c10\u79cd\u60c5\u7eea\u7c7b\u522b\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u63a8\u7406\u6d41\u7a0b\u548c\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5bf9GPT-4o\u3001Florence 2\u548cLLaVa-1.5\u4e09\u79cdVLM\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "GPT-4o\u5728\u884c\u4e3a\u8bc6\u522b\uff08F1: 0.756\uff0c\u51c6\u786e\u7387: 0.799\uff09\u548c\u60c5\u7eea\u8bc6\u522b\uff08F1: 0.712\uff0c\u51c6\u786e\u7387: 0.773\uff09\u4e0a\u5747\u8868\u73b0\u6700\u4f73\uff1bFlorence 2\u6b21\u4e4b\uff1bLLaVa-1.5\u6574\u4f53\u8868\u73b0\u6700\u5f31\u3002\u6240\u6709\u6a21\u578b\u5728\u533a\u5206\u8bed\u4e49\u76f8\u8fd1\u7c7b\u522b\uff08\u5982\u56e2\u961f\u534f\u4f5c vs. \u4e0e\u4e3b\u7ba1\u6c9f\u901a\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4e3a\u5efa\u7b51\u573a\u666f\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u8bc6\u522b\u63d0\u4f9b\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u8981\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u9760\u6027\uff0c\u4ecd\u9700\u7ed3\u5408\u9886\u57df\u9002\u914d\u3001\u65f6\u5e8f\u5efa\u6a21\u6216\u591a\u6a21\u6001\u611f\u77e5\u7b49\u6539\u8fdb\u7b56\u7565\u3002", "summary_cn": "\u968f\u7740\u673a\u5668\u4eba\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u5efa\u7b51\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5176\u7406\u89e3\u548c\u54cd\u5e94\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\u5bf9\u4e8e\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u5408\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5df2\u6210\u4e3a\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e2d\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u80fd\u591f\u5728\u65e0\u9700\u5927\u91cf\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u4eba\u7c7b\u884c\u4e3a\u3002\u8fd9\u4e00\u7279\u6027\u4f7f\u5176\u5728\u5efa\u7b51\u9886\u57df\u5c24\u4e3a\u5438\u5f15\u4eba\uff0c\u56e0\u4e3a\u8be5\u9886\u57df\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u800c\u76d1\u6d4b\u5de5\u4eba\u7684\u884c\u4e3a\u548c\u60c5\u7eea\u72b6\u6001\u5bf9\u5b89\u5168\u6027\u548c\u751f\u4ea7\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u4e3b\u6d41VLM\u2014\u2014GPT-4o\u3001Florence 2\u548cLLaVa-1.5\u2014\u2014\u5728\u9759\u6001\u5de5\u5730\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5efa\u7b51\u5de5\u4eba\u884c\u4e3a\u548c\u60c5\u7eea\u7684\u8868\u73b0\u3002\u6211\u4eec\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b1000\u5f20\u56fe\u50cf\u7684\u7cbe\u9009\u6570\u636e\u96c6\uff0c\u6bcf\u5f20\u56fe\u50cf\u5747\u6807\u6ce8\u4e8610\u79cd\u884c\u4e3a\u7c7b\u522b\u548c10\u79cd\u60c5\u7eea\u7c7b\u522b\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u5316\u63a8\u7406\u6d41\u7a0b\u548c\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u5bf9\u5404\u6a21\u578b\u8f93\u51fa\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0cGPT-4o\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u59cb\u7ec8\u5f97\u5206\u6700\u9ad8\uff1a\u884c\u4e3a\u8bc6\u522b\u7684\u5e73\u5747F1\u5f97\u5206\u4e3a0.756\uff0c\u51c6\u786e\u7387\u4e3a0.799\uff1b\u60c5\u7eea\u8bc6\u522b\u7684F1\u5f97\u5206\u4e3a0.712\uff0c\u51c6\u786e\u7387\u4e3a0.773\u3002Florence 2\u8868\u73b0\u4e2d\u7b49\uff0c\u884c\u4e3a\u548c\u60c5\u7eea\u8bc6\u522b\u7684F1\u5f97\u5206\u5206\u522b\u4e3a0.497\u548c0.414\uff1b\u800cLLaVa-1.5\u6574\u4f53\u8868\u73b0\u6700\u5dee\uff0c\u884c\u4e3a\u548c\u60c5\u7eea\u8bc6\u522b\u7684F1\u5f97\u5206\u5206\u522b\u4e3a0.466\u548c0.461\u3002\u6df7\u6dc6\u77e9\u9635\u5206\u6790\u8868\u660e\uff0c\u6240\u6709\u6a21\u578b\u5728\u533a\u5206\u8bed\u4e49\u76f8\u8fd1\u7684\u7c7b\u522b\uff08\u4f8b\u5982\u201c\u56e2\u961f\u534f\u4f5c\u201d\u4e0e\u201c\u4e0e\u4e3b\u7ba1\u6c9f\u901a\u201d\uff09\u65b9\u9762\u5747\u5b58\u5728\u56f0\u96be\u3002\u5c3d\u7ba1\u7ed3\u679c\u8868\u660e\u901a\u7528VLM\u53ef\u5728\u5efa\u7b51\u73af\u5883\u4e2d\u63d0\u4f9b\u4eba\u7c7b\u884c\u4e3a\u8bc6\u522b\u7684\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u8981\u5b9e\u73b0\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u5e94\u7528\uff0c\u53ef\u80fd\u4ecd\u9700\u5f15\u5165\u9886\u57df\u9002\u914d\u3001\u65f6\u5e8f\u5efa\u6a21\u6216\u591a\u6a21\u6001\u611f\u77e5\u7b49\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2601.10836", "pdf": "https://arxiv.org/pdf/2601.10836", "abs": "https://arxiv.org/abs/2601.10836", "authors": ["Gerhard Krumpl", "Henning Avenhaus", "Horst Possegger"], "title": "One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "WACV 2026", "summary": "Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5728\u5206\u5e03\u5185\uff08ID\uff09\u51c6\u786e\u7387\u4e0e\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u6027\u80fd\u4e4b\u95f4\u5e76\u975e\u5355\u8c03\u6b63\u76f8\u5173\uff1b\u5f53\u8bad\u7ec3\u7b56\u7565\u8fc7\u5ea6\u63d0\u5347ID\u51c6\u786e\u7387\u65f6\uff0cOOD\u6027\u80fd\u53cd\u800c\u4e0b\u964d\uff0c\u4e14\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u4e0eOOD\u68c0\u6d4b\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u5f3a\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u4e0e\u73b0\u4ee3\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\u6d41\u7a0b\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9ID\u51c6\u786e\u7387\u63d0\u5347\u662f\u5426\u771f\u6b63\u6709\u52a9\u4e8eOOD\u68c0\u6d4b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u56fa\u5b9a\u4f7f\u7528ResNet-50\u67b6\u6784\uff0c\u5bf956\u4e2a\u91c7\u7528\u4e0d\u540c\u8bad\u7ec3\u7b56\u7565\u5728ImageNet\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f3021\u79cd\u6700\u5148\u8fdb\u7684\u540e\u5904\u7406OOD\u68c0\u6d4b\u65b9\u6cd5\u57288\u4e2aOOD\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0ID\u51c6\u786e\u7387\u4e0eOOD\u6027\u80fd\u5448\u975e\u5355\u8c03\u5173\u7cfb\uff1a\u521d\u671f\u968f\u51c6\u786e\u7387\u63d0\u5347\u800c\u6539\u5584\uff0c\u4f46\u5f53\u8bad\u7ec3\u7b56\u7565\u4f7f\u51c6\u786e\u7387\u8d85\u8fc7\u57fa\u7ebf\u540e\uff0cOOD\u6027\u80fd\u53cd\u800c\u4e0b\u964d\uff1b\u540c\u65f6\uff0c\u8bad\u7ec3\u7b56\u7565\u3001\u68c0\u6d4b\u5668\u9009\u62e9\u548cOOD\u6027\u80fd\u4e09\u8005\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "\u4e0d\u5b58\u5728\u4e00\u79cd\u5728\u6240\u6709\u8bad\u7ec3\u7b56\u7565\u4e0b\u90fd\u6700\u4f18\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9700\u6839\u636e\u5177\u4f53\u8bad\u7ec3\u65b9\u5f0f\u9009\u62e9\u5408\u9002\u7684\u68c0\u6d4b\u5668\uff0c\u4e14\u76f2\u76ee\u8ffd\u6c42\u9ad8ID\u51c6\u786e\u7387\u53ef\u80fd\u635f\u5bb3OOD\u9c81\u68d2\u6027\u3002", "summary_cn": "\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u5bf9\u4e8e\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u90e8\u7f72\u9c81\u68d2\u4e14\u53ef\u9760\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1OOD\u68c0\u6d4b\u5668\u4e0d\u65ad\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u4e0e\u65e8\u5728\u6700\u5927\u5316\u5206\u5e03\u5185\uff08ID\uff09\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u7684\u73b0\u4ee3\u8bad\u7ec3\u6d41\u7a0b\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u4ecd\u7f3a\u4e4f\u6df1\u5165\u63a2\u7d22\u3002\u672c\u6587\u901a\u8fc7\u4e00\u9879\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\u63a2\u8ba8\u4e86\u8fd9\u4e00\u5173\u8054\u3002\u7814\u7a76\u56fa\u5b9a\u91c7\u7528\u5e7f\u6cdb\u4f7f\u7528\u7684ResNet-50\u67b6\u6784\uff0c\u5bf9\u901a\u8fc7\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\u83b7\u5f97\u768456\u4e2aImageNet\u8bad\u7ec3\u6a21\u578b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e8621\u79cd\u6700\u5148\u8fdb\u7684\u540e\u5904\u7406OOD\u68c0\u6d4b\u65b9\u6cd5\u57288\u4e2aOOD\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u3002\u4e0e\u201c\u66f4\u9ad8ID\u51c6\u786e\u7387\u610f\u5473\u7740\u66f4\u597dOOD\u68c0\u6d4b\u6027\u80fd\u201d\u7684\u666e\u904d\u5047\u8bbe\u76f8\u53cd\uff0c\u6211\u4eec\u53d1\u73b0\u4e8c\u8005\u4e4b\u95f4\u5b58\u5728\u975e\u5355\u8c03\u5173\u7cfb\uff1aOOD\u6027\u80fd\u8d77\u521d\u968f\u51c6\u786e\u7387\u63d0\u5347\u800c\u6539\u5584\uff0c\u4f46\u4e00\u65e6\u5148\u8fdb\u8bad\u7ec3\u65b9\u6848\u5c06\u51c6\u786e\u7387\u63a8\u9ad8\u81f3\u57fa\u7ebf\u4ee5\u4e0a\uff0cOOD\u6027\u80fd\u53cd\u800c\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u8bad\u7ec3\u7b56\u7565\u3001\u68c0\u6d4b\u5668\u9009\u62e9\u4e0e\u6700\u7ec8OOD\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f3a\u70c8\u4f9d\u8d56\u5173\u7cfb\uff0c\u8868\u660e\u5e76\u4e0d\u5b58\u5728\u4e00\u79cd\u666e\u9002\u6700\u4f18\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2601.10854", "pdf": "https://arxiv.org/pdf/2601.10854", "abs": "https://arxiv.org/abs/2601.10854", "authors": ["Mohammad Rasras", "Iuliana Marin", "Serban Radu", "Irina Mocanu"], "title": "Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification", "categories": ["cs.CV"], "comment": "18 pages, 6 figures, conference", "summary": "Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u63d0\u9ad8\u89c6\u9891\u5e27\u5206\u8fa8\u7387\u7684\u540c\u65f6\u51cf\u5c11\u65f6\u95f4\u4fe1\u606f\u5bf93D ResNet\u6a21\u578b\uff08MC3\u3001R3D\u3001R(2+1)D\uff09\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u591a\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982CBAM\u3001TCN\u3001\u591a\u5934\u6ce8\u610f\u529b\u7b49\uff09\u8fdb\u884c\u6539\u8fdb\uff0c\u5728UCF101\u6570\u636e\u96c6\u4e0a\u6700\u9ad8\u8fbe\u523088.98%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22\u5728\u63d0\u5347\u89c6\u9891\u5e27\u5206\u8fa8\u7387\u4f46\u51cf\u5c11\u65f6\u95f4\u7ef4\u5ea6\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u6765\u5f25\u8865\u7f3a\u5931\u7684\u65f6\u95f4\u7279\u5f81\uff0c\u4ece\u800c\u7ef4\u6301\u6216\u63d0\u5347\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\u3002", "method": "\u5728\u539f\u59cbMC3\u3001R3D\u548cR(2+1)D\u6a21\u578b\u57fa\u7840\u4e0a\u52a0\u5165Dropout\u5c42\uff0c\u5e76\u4e3a\u6bcf\u79cd\u6a21\u578b\u8bbe\u8ba1\u5341\u79cd\u53d8\u4f53\uff0c\u5206\u522b\u96c6\u6210CBAM\u3001TCN\u3001\u591a\u5934\u6ce8\u610f\u529b\u548c\u901a\u9053\u6ce8\u610f\u529b\u7b49\u6a21\u5757\uff0c\u4ee5\u8bc4\u4f30\u5404\u7c7b\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u53d7\u9650\u65f6\u95f4\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728UCF101\u6570\u636e\u96c6\u4e0a\uff0c\u52a0\u5165\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u6539\u8fdb\u7248R(2+1)D\u6a21\u578b\u53d6\u5f97\u4e8688.98%\u7684\u51c6\u786e\u7387\uff1b\u4e0d\u540c\u53d8\u4f53\u5728\u6574\u4f53\u6027\u80fd\u76f8\u8fd1\u7684\u60c5\u51b5\u4e0b\uff0c\u7c7b\u522b\u7ea7\u51c6\u786e\u7387\u8868\u73b0\u5404\u5f02\u3002", "conclusion": "\u7f3a\u5931\u7684\u65f6\u95f4\u7279\u5f81\u5bf9\u9ad8\u5206\u8fa8\u7387\u6a21\u578b\u7684\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u800c\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u867d\u80fd\u63d0\u5347\u6574\u4f53\u8868\u73b0\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7c7b\u522b\u4e0a\u7684\u6548\u679c\u5b58\u5728\u5dee\u5f02\u3002", "summary_cn": "\u7531\u4e8e\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u5df2\u6210\u4e3a\u91cd\u8981\u7684\u7814\u7a76\u70ed\u70b9\u3002\u57fa\u4e8e3D ResNet\u7684CNN\u6a21\u578b\uff08\u5c24\u5176\u662fMC3\u3001R3D\u548cR(2+1)D\uff09\u91c7\u7528\u4e0d\u540c\u7684\u5377\u79ef\u6ee4\u6ce2\u5668\u6765\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\u3002\u672c\u6587\u7814\u7a76\u4e86\u5728\u63d0\u9ad8\u89c6\u9891\u5e27\u5206\u8fa8\u7387\u7684\u540c\u65f6\u51cf\u5c11\u65f6\u95f4\u7ef4\u5ea6\u6240\u6355\u83b7\u4fe1\u606f\u7684\u5f71\u54cd\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e86\u4e0e\u539f\u59cb\u4e09\u79cd\u6a21\u578b\u7ed3\u6784\u76f8\u4f3c\u4f46\u5728\u6700\u7ec8\u5206\u7c7b\u5668\u524d\u52a0\u5165Dropout\u5c42\u7684\u57fa\u51c6\u6a21\u578b\uff1b\u968f\u540e\uff0c\u9488\u5bf9\u6bcf\u79cd\u6a21\u578b\u5206\u522b\u5f00\u53d1\u4e86\u5341\u79cd\u65b0\u53d8\u4f53\uff0c\u8fd9\u4e9b\u53d8\u4f53\u5728\u5176\u67b6\u6784\u4e2d\u96c6\u6210\u4e86\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5305\u62ec\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\uff08CBAM\uff09\u3001\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\uff0c\u4ee5\u53ca\u591a\u5934\u6ce8\u610f\u529b\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u3002\u76ee\u7684\u662f\u89c2\u5bdf\u8fd9\u4e9b\u6a21\u5757\u5bf9\u53d7\u9650\u65f6\u95f4\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u7a0b\u5ea6\u3002\u6240\u6709\u6a21\u578b\u5728UCF101\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u52a0\u5165\u6539\u8fdb\u540e\u7684R(2+1)D\u6a21\u578b\u53ef\u8fbe\u523088.98%\u7684\u51c6\u786e\u7387\u3002\u672c\u6587\u5f97\u51fa\u7ed3\u8bba\uff1a\u5728\u65b0\u6784\u5efa\u7684\u9ad8\u5206\u8fa8\u7387\u6a21\u578b\u4e2d\uff0c\u7f3a\u5931\u7684\u65f6\u95f4\u7279\u5f81\u5bf9\u6027\u80fd\u5177\u6709\u663e\u8457\u5f71\u54cd\u3002\u5c3d\u7ba1\u5404\u7c7b\u53d8\u4f53\u5728\u6574\u4f53\u6027\u80fd\u63d0\u5347\u65b9\u9762\u76f8\u4f3c\uff0c\u4f46\u5728\u7c7b\u522b\u7ea7\u522b\u7684\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u884c\u4e3a\u3002"}}
{"id": "2601.10880", "pdf": "https://arxiv.org/pdf/2601.10880", "abs": "https://arxiv.org/abs/2601.10880", "authors": ["Chongcong Jiang", "Tianxingjian Ding", "Chuhan Song", "Jiachen Tu", "Ziyang Yan", "Yihua Shao", "Zhenyi Wang", "Yuzhang Shang", "Tianyu Han", "Yu Tian"], "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMedical SAM3\uff0c\u901a\u8fc7\u5bf9SAM3\u5728\u5927\u89c4\u6a21\u5f02\u6784\u533b\u5b66\u5f71\u50cf\u6570\u636e\u4e0a\u8fdb\u884c\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u6a21\u7cca\u3001\u5f62\u6001\u590d\u6742\u548c\u957f\u7a0b3D\u4e0a\u4e0b\u6587\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u539f\u59cbSAM3\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u53d7\u9650\u4e8e\u4e25\u91cd\u7684\u9886\u57df\u504f\u79fb\u3001\u7f3a\u4e4f\u6709\u6548\u7684\u7a7a\u95f4\u63d0\u793a\u4ee5\u53ca\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u89e3\u5256\u7ed3\u6784\u548c\u4f53\u6570\u636e\uff0c\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002", "method": "\u5bf9SAM3\u6a21\u578b\u5728\u6db5\u76d610\u79cd\u533b\u5b66\u6210\u50cf\u6a21\u6001\u768433\u4e2a2D/3D\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u53c2\u6570\u5fae\u8c03\uff0c\u5229\u7528\u914d\u5bf9\u7684\u5206\u5272\u63a9\u7801\u548c\u6587\u672c\u63d0\u793a\uff0c\u4f7f\u5176\u83b7\u5f97\u9886\u57df\u7279\u5b9a\u8868\u793a\u5e76\u4fdd\u7559\u63d0\u793a\u9a71\u52a8\u7684\u7075\u6d3b\u6027\u3002", "result": "\u5728\u591a\u79cd\u5668\u5b98\u3001\u6210\u50cf\u6a21\u6001\u548c\u7ef4\u5ea6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedical SAM3\u76f8\u6bd4\u539f\u59cbSAM3\u6709\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u6a21\u7cca\u3001\u5f62\u6001\u590d\u6742\u548c\u957f\u7a0b3D\u4e0a\u4e0b\u6587\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002", "conclusion": "Medical SAM3\u662f\u4e00\u4e2a\u901a\u7528\u7684\u3001\u6587\u672c\u5f15\u5bfc\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5728\u4e25\u91cd\u9886\u57df\u504f\u79fb\u4e0b\uff0c\u5168\u9762\u7684\u6a21\u578b\u9002\u914d\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u63d0\u793a\u9a71\u52a8\u5206\u5272\u7684\u91cd\u8981\u6027\u3002", "summary_cn": "\u8bf8\u5982SAM3\u4e4b\u7c7b\u7684\u53ef\u63d0\u793a\u5206\u5272\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u4ea4\u4e92\u5f0f\u548c\u57fa\u4e8e\u6982\u5ff5\u7684\u63d0\u793a\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u76f4\u63a5\u9002\u7528\u6027\u4ecd\u53d7\u5230\u4e25\u91cd\u9886\u57df\u504f\u79fb\u3001\u7f3a\u4e4f\u7279\u6743\u7a7a\u95f4\u63d0\u793a\u4ee5\u53ca\u9700\u8981\u63a8\u7406\u590d\u6742\u89e3\u5256\u7ed3\u6784\u548c\u4f53\u6570\u636e\u7684\u9650\u5236\u3002\u672c\u6587\u63d0\u51fa\u4e86Medical SAM3\u2014\u2014\u4e00\u4e2a\u7528\u4e8e\u901a\u7528\u63d0\u793a\u9a71\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5bf9SAM3\u5728\u5927\u89c4\u6a21\u5f02\u67842D\u548c3D\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\uff08\u5305\u542b\u914d\u5bf9\u7684\u5206\u5272\u63a9\u7801\u548c\u6587\u672c\u63d0\u793a\uff09\u4e0a\u8fdb\u884c\u5168\u53c2\u6570\u5fae\u8c03\u800c\u83b7\u5f97\u3002\u901a\u8fc7\u5bf9\u539f\u59cbSAM3\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0\u5176\u5728\u533b\u5b66\u6570\u636e\u4e0a\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5176\u8868\u9762\u4e0a\u7684\u7ade\u4e89\u529b\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5982\u771f\u5b9e\u8fb9\u754c\u6846\u7b49\u5f3a\u51e0\u4f55\u5148\u9a8c\u3002\u8fd9\u4e9b\u53d1\u73b0\u4fc3\u4f7f\u6211\u4eec\u8d85\u8d8a\u4ec5\u9760\u63d0\u793a\u5de5\u7a0b\uff0c\u8fdb\u884c\u5b8c\u6574\u7684\u6a21\u578b\u9002\u914d\u3002\u901a\u8fc7\u5728\u6db5\u76d610\u79cd\u533b\u5b66\u6210\u50cf\u6a21\u6001\u768433\u4e2a\u6570\u636e\u96c6\u4e0a\u5fae\u8c03SAM3\u7684\u6a21\u578b\u53c2\u6570\uff0cMedical SAM3\u83b7\u5f97\u4e86\u9c81\u68d2\u7684\u9886\u57df\u7279\u5b9a\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u63d0\u793a\u9a71\u52a8\u7684\u7075\u6d3b\u6027\u3002\u5728\u591a\u79cd\u5668\u5b98\u3001\u6210\u50cf\u6a21\u6001\u548c\u7ef4\u5ea6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u59cb\u7ec8\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u6a21\u7cca\u3001\u5f62\u6001\u590d\u6742\u548c\u957f\u7a0b3D\u4e0a\u4e0b\u6587\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u3002\u6211\u4eec\u7684\u7ed3\u679c\u786e\u7acb\u4e86Medical SAM3\u4f5c\u4e3a\u533b\u5b66\u5f71\u50cf\u9886\u57df\u901a\u7528\u6587\u672c\u5f15\u5bfc\u5206\u5272\u57fa\u7840\u6a21\u578b\u7684\u5730\u4f4d\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u4e25\u91cd\u9886\u57df\u504f\u79fb\u4e0b\uff0c\u5168\u9762\u6a21\u578b\u9002\u914d\u5bf9\u5b9e\u73b0\u9c81\u68d2\u63d0\u793a\u9a71\u52a8\u5206\u5272\u7684\u91cd\u8981\u6027\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u53d1\u5e03\u4e8ehttps://github.com/AIM-Research-Lab/Medical-SAM3\u3002"}}
{"id": "2601.10909", "pdf": "https://arxiv.org/pdf/2601.10909", "abs": "https://arxiv.org/abs/2601.10909", "authors": ["Chuqiao Li", "Xianghui Xie", "Yong Cao", "Andreas Geiger", "Gerard Pons-Moll"], "title": "FrankenMotion: Part-level Human Motion Generation and Composition", "categories": ["cs.CV"], "comment": "Project page: https://coral79.github.io/frankenmotion/", "summary": "Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFrankenMotion\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5e26\u6709\u539f\u5b50\u7ea7\u3001\u65f6\u95f4\u611f\u77e5\u7684\u8eab\u4f53\u90e8\u4f4d\u6587\u672c\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u5bf9\u5404\u8eab\u4f53\u90e8\u4f4d\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7684\u72ec\u7acb\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u9a71\u52a8\u7684\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5e8f\u5217\u7ea7\u6216\u52a8\u4f5c\u7ea7\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u8eab\u4f53\u90e8\u4f4d\u8fd0\u52a8\u6807\u6ce8\uff0c\u5bfc\u81f4\u5bf9\u4e2a\u4f53\u8eab\u4f53\u90e8\u4f4d\u7684\u53ef\u63a7\u6027\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efa\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u5305\u542b\u539f\u5b50\u7ea7\u3001\u65f6\u95f4\u611f\u77e5\u7684\u8eab\u4f53\u90e8\u4f4d\u6587\u672c\u6807\u6ce8\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faFrankenMotion\u6846\u67b6\uff0c\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u6bcf\u4e2a\u8eab\u4f53\u90e8\u4f4d\u63d0\u4f9b\u72ec\u7acb\u7684\u65f6\u95f4\u7ed3\u6784\u5316\u6587\u672c\u63d0\u793a\u4ee5\u751f\u6210\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFrankenMotion\u5728\u65b0\u8bbe\u5b9a\u4e0b\u4f18\u4e8e\u6240\u6709\u9002\u914d\u5e76\u91cd\u65b0\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u80fd\u7ec4\u5408\u751f\u6210\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u52a8\u4f5c\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5b9e\u73b0\u4e86\u5177\u6709\u539f\u5b50\u7ea7\u3001\u65f6\u95f4\u611f\u77e5\u7684\u8eab\u4f53\u90e8\u4f4d\u8fd0\u52a8\u6807\u6ce8\uff0c\u5e76\u63d0\u51fa\u4e86\u652f\u6301\u7a7a\u95f4\uff08\u8eab\u4f53\u90e8\u4f4d\uff09\u4e0e\u65f6\u95f4\uff08\u539f\u5b50\u52a8\u4f5c\uff09\u53cc\u91cd\u63a7\u5236\u7684\u8fd0\u52a8\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u53ef\u63a7\u6027\u3002", "summary_cn": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u90e8\u4f4d\u7ea7\u8fd0\u52a8\u6807\u6ce8\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5e8f\u5217\u7ea7\u6216\u52a8\u4f5c\u7ea7\u63cf\u8ff0\uff0c\u9650\u5236\u4e86\u5bf9\u5404\u4e2a\u8eab\u4f53\u90e8\u4f4d\u7684\u53ef\u63a7\u6027\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u4eba\u4f53\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u539f\u5b50\u7ea7\u3001\u65f6\u95f4\u611f\u77e5\u7684\u90e8\u4f4d\u7ea7\u6587\u672c\u6807\u6ce8\u3002\u4e0e\u4ee5\u5f80\u4ec5\u63d0\u4f9b\u56fa\u5b9a\u65f6\u95f4\u6bb5\u540c\u6b65\u90e8\u4f4d\u5b57\u5e55\u6216\u4ec5\u4f9d\u8d56\u5168\u5c40\u5e8f\u5217\u6807\u7b7e\u7684\u6570\u636e\u96c6\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u6570\u636e\u96c6\u5728\u7cbe\u7ec6\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u6355\u6349\u4e86\u5f02\u6b65\u4e14\u8bed\u4e49\u4e0d\u540c\u7684\u90e8\u4f4d\u8fd0\u52a8\u3002\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u90e8\u4f4d\u611f\u77e5\u8fd0\u52a8\u751f\u6210\u6846\u67b6\u2014\u2014FrankenMotion\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8eab\u4f53\u90e8\u4f4d\u5747\u7531\u5176\u81ea\u8eab\u7684\u65f6\u95f4\u7ed3\u6784\u5316\u6587\u672c\u63d0\u793a\u8fdb\u884c\u5f15\u5bfc\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u63d0\u4f9b\u539f\u5b50\u7ea7\u3001\u65f6\u95f4\u611f\u77e5\u90e8\u4f4d\u7ea7\u8fd0\u52a8\u6807\u6ce8\u7684\u5de5\u4f5c\uff0c\u5e76\u5b9e\u73b0\u4e86\u540c\u65f6\u5177\u5907\u7a7a\u95f4\uff08\u8eab\u4f53\u90e8\u4f4d\uff09\u548c\u65f6\u95f4\uff08\u539f\u5b50\u52a8\u4f5c\uff09\u63a7\u5236\u80fd\u529b\u7684\u8fd0\u52a8\u751f\u6210\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cFrankenMotion\u5728\u6211\u4eec\u7684\u8bbe\u5b9a\u4e0b\u4f18\u4e8e\u6240\u6709\u7ecf\u8fc7\u9002\u914d\u548c\u91cd\u65b0\u8bad\u7ec3\u7684\u5148\u524d\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u80fd\u591f\u7ec4\u5408\u751f\u6210\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u52a8\u4f5c\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5728\u8bba\u6587\u53d1\u8868\u540e\u516c\u5f00\u3002"}}
{"id": "2601.10913", "pdf": "https://arxiv.org/pdf/2601.10913", "abs": "https://arxiv.org/abs/2601.10913", "authors": ["Santiago Mart\u00ednez Novoa", "Mar\u00eda Catalina Ib\u00e1\u00f1ez", "Lina G\u00f3mez Mesa", "Jeremias Kramer"], "title": "Classification of Chest XRay Diseases through image processing and analysis techniques", "categories": ["cs.CV"], "comment": null, "summary": "Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7528\u4e8e\u80f8\u90e8X\u5149\u591a\u5206\u7c7b\u4efb\u52a1\u7684\u591a\u79cd\u65b9\u6cd5\uff08\u5305\u62ecDenseNet121\uff09\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u5176\u6027\u80fd\uff0c\u5206\u6790\u6240\u63d0\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2aWeb\u5e94\u7528\u53ca\u4ee3\u7801\u3002", "motivation": "\u80f8\u90e8X\u5149\u56fe\u50cf\u662f\u8bca\u65ad\u80f8\u8154\u75be\u75c5\u6700\u5e38\u7528\u7684\u653e\u5c04\u5b66\u68c0\u67e5\u624b\u6bb5\u4e4b\u4e00\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u4e14\u53ef\u9760\u7684\u591a\u5206\u7c7b\u65b9\u6cd5\u6765\u8f85\u52a9\u8bca\u65ad\u3002", "method": "\u7efc\u8ff0\u5e76\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u591a\u79cd\u80f8\u90e8X\u5149\u56fe\u50cf\u591a\u5206\u7c7b\u65b9\u6cd5\uff08\u5982DenseNet121\uff09\uff0c\u540c\u65f6\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684Web\u5e94\u7528\u3002", "result": "\u5bf9\u4e0d\u540c\u65b9\u6cd5\u8fdb\u884c\u4e86\u6027\u80fd\u6d4b\u8bd5\uff0c\u8bc6\u522b\u51fa\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u867d\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u4f46\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u7ed3\u6784\u6216\u6570\u636e\u5904\u7406\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "summary_cn": "\u591a\u5206\u7c7b\u80f8\u90e8X\u5149\u56fe\u50cf\u662f\u4e00\u79cd\u7528\u4e8e\u8bca\u65ad\u80f8\u8154\u75be\u75c5\u6700\u5e38\u89c1\u7684\u653e\u5c04\u5b66\u68c0\u67e5\u65b9\u5f0f\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u7b80\u8981\u6982\u8ff0\u4e86\u7528\u4e8e\u89e3\u51b3\u8be5\u4efb\u52a1\u7684\u82e5\u5e72\u65b9\u6cd5\uff0c\u5305\u62ecDenseNet121\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u90e8\u7f72\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u57fa\u4e8eWeb\u7684\u5e94\u7528\u7a0b\u5e8f\u3002\u5728\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5bf9\u4e0d\u540c\u65b9\u6cd5\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u6bd4\u8f83\u5b83\u4eec\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u5f31\u70b9\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u516c\u5f00\u4e8e\uff1ahttps://github.com/AML4206-MINE20242/Proyecto_AML"}}
{"id": "2601.10917", "pdf": "https://arxiv.org/pdf/2601.10917", "abs": "https://arxiv.org/abs/2601.10917", "authors": ["Pouya Afshin", "David Helminiak", "Tianling Niu", "Julie M. Jorns", "Tina Yen", "Bing Yu", "Dong Hye Ye"], "title": "Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "This paper has been accepted for the IEEE International Symposium on Biomedical Imaging (ISBI) 2026, London, UK, and will be presented in the corresponding session", "summary": "Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6df1\u7d2b\u5916\u8367\u5149\u626b\u63cf\u663e\u5fae\u56fe\u50cf\u5408\u6210\u6570\u636e\uff0c\u4ee5\u63d0\u5347\u4e73\u817a\u4fdd\u4e73\u624b\u672f\u4e2d\u5207\u7f18\u8bc4\u4f30\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6df1\u7d2b\u5916\u8367\u5149\u626b\u63cf\u663e\u5fae\uff08DUV-FSM\uff09\u53ef\u7528\u4e8e\u4e73\u817a\u4fdd\u4e73\u624b\u672f\u4e2d\u7684\u5feb\u901f\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u4f46\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5f15\u5bfc\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\uff0c\u901a\u8fc7\u5fae\u8c03\u540e\u7684DINO\u6559\u5e08\u6a21\u578b\u5d4c\u5165\u4fe1\u606f\u6ce8\u5165\u7ec6\u80de\u7ed3\u6784\u8bed\u4e49\u7ec6\u8282\uff0c\u751f\u6210\u5408\u6210\u56fe\u50cf\uff1b\u5c06\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u7ed3\u5408\u7528\u4e8e\u5fae\u8c03Vision Transformer\uff08ViT\uff09\uff0c\u5e76\u901a\u8fc7\u56fe\u50cf\u5757\u9884\u6d4b\u805a\u5408\u5b9e\u73b0\u5168\u5207\u7247\uff08WSI\uff09\u7ea7\u522b\u5206\u7c7b\u3002", "result": "\u57285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8fbe\u523096.47%\u51c6\u786e\u7387\uff0cFID\u5206\u6570\u964d\u81f345.72\uff0c\u663e\u8457\u4f18\u4e8e\u7c7b\u522b\u6761\u4ef6\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86DUV-FSM\u56fe\u50cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5207\u7f18\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "summary_cn": "\u4fdd\u4e73\u624b\u672f\uff08BCS\uff09\u9700\u8981\u7cbe\u786e\u7684\u672f\u4e2d\u5207\u7f18\u8bc4\u4f30\u4ee5\u4fdd\u7559\u5065\u5eb7\u7ec4\u7ec7\u3002\u6df1\u7d2b\u5916\u8367\u5149\u626b\u63cf\u663e\u5fae\u955c\uff08DUV-FSM\uff09\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u8868\u9762\u6210\u50cf\u80fd\u529b\uff1b\u7136\u800c\uff0c\u6807\u6ce8\u7684DUV\u6570\u636e\u7a00\u7f3a\u963b\u788d\u4e86\u9c81\u68d2\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u7531\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5f15\u5bfc\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u5757\u3002\u901a\u8fc7\u4f7f\u7528\u5fae\u8c03\u540e\u7684DINO\u6559\u5e08\u6a21\u578b\u5d4c\u5165\u6765\u5f15\u5bfcLDM\uff0c\u6211\u4eec\u5c06\u4e30\u5bcc\u7684\u7ec6\u80de\u7ed3\u6784\u8bed\u4e49\u7ec6\u8282\u6ce8\u5165\u5408\u6210\u6570\u636e\u4e2d\u3002\u6211\u4eec\u5c06\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u5757\u7ed3\u5408\uff0c\u7528\u4e8e\u5fae\u8c03Vision Transformer\uff08ViT\uff09\uff0c\u5e76\u5229\u7528\u56fe\u50cf\u5757\u9884\u6d4b\u805a\u5408\u5b9e\u73b0\u5168\u5207\u7247\uff08WSI\uff09\u7ea7\u522b\u7684\u5206\u7c7b\u3002\u57fa\u4e8e5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e8696.47%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5c06FID\u5206\u6570\u964d\u81f345.72\uff0c\u663e\u8457\u4f18\u4e8e\u7c7b\u522b\u6761\u4ef6\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
