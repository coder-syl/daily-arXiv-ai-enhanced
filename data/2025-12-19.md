<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 提出一种结合规则掩膜变形与无配对图像翻译的两阶段生成式数据增强框架，用于提升遮挡人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 遮挡人脸识别面临数据稀缺和分布偏移两大挑战，现有方法难以生成足够真实且多样的遮挡人脸样本。

Method: 采用两阶段生成式数据增强框架：首先进行基于规则的口罩形变，再利用GAN进行无配对图像到图像的转换；引入非口罩区域保留损失和随机噪声注入以稳定训练并提升样本多样性。

Result: 相比仅使用规则形变的方法，该方法在生成质量上取得一致提升，并能有效补充如IAMGAN等现有GAN方法。

Conclusion: 所提组件在实验中验证有效，为面向人脸识别的数据增强提供了新方向。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

Abstract (中文翻译): 数据稀缺和分布偏移给遮挡人脸的检测与识别带来了重大挑战。我们提出了一种两阶段生成式数据增强框架，该框架结合了基于规则的口罩形变与基于GAN的无配对图像到图像翻译，能够生成超越纯合成变换的真实遮挡人脸样本。与仅使用规则形变的方法相比，所提出的方法在定性效果上持续提升，并可有效补充现有的基于GAN的遮挡人脸生成方法（如IAMGAN）。我们引入了非口罩区域保留损失和随机噪声注入机制，以稳定训练过程并增强样本多样性。实验结果验证了所提各组件的有效性，并为未来在人脸识别任务中开展以数据为中心的增强方法指明了方向。

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出JARVIS框架，通过引入I-JEPA自监督学习范式增强多模态大语言模型（MLLMs）的视觉理解能力，无需依赖纯语言监督，在多个视觉中心基准上提升性能且不损害多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在基础视觉推理任务上表现有限，原因在于其视觉理解主要依赖主观且不完整的文本描述作为监督信号，且多模态指令微调规模远小于纯文本预训练，导致模型过度拟合语言先验而忽略视觉细节。

Method: 提出JARVIS框架，将I-JEPA学习范式融入MLLM的标准视觉-语言对齐流程中，利用冻结的视觉基础模型作为上下文和目标编码器，并训练以LLM早期层实现的预测器，从图像中学习结构与语义规律，减少对语言监督的依赖。

Result: 在标准MLLM基准上的大量实验表明，JARVIS在不同LLM系列中均能一致提升视觉中心任务的性能，同时不损害多模态推理能力。

Conclusion: JARVIS通过自监督方式有效增强了MLLM的视觉理解能力，为解决其过度依赖语言先验的问题提供了新思路。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

Abstract (中文翻译): 多模态大语言模型（MLLMs）最近在连接视觉与语言方面展现出令人印象深刻的能力，但其在基础视觉推理任务上的表现仍显不足。这一局限性源于MLLMs主要从文本描述中学习视觉理解，而文本描述是一种主观且本质上不完整的监督信号。此外，与大规模纯文本预训练相比，多模态指令微调的数据规模较小，导致MLLMs过度拟合语言先验而忽视视觉细节。为解决这些问题，我们提出了JARVIS——一种受JEPA启发的、用于MLLM中自监督视觉增强的框架。具体而言，我们将I-JEPA学习范式整合到MLLM训练的标准视觉-语言对齐流程中。该方法利用冻结的视觉基础模型作为上下文编码器和目标编码器，同时训练以大语言模型（LLM）早期层实现的预测器，使其在不完全依赖语言监督的情况下从图像中学习结构和语义规律。在标准MLLM基准上的大量实验表明，JARVIS在不同LLM系列中均能持续提升视觉中心任务的性能，且不会削弱多模态推理能力。我们的源代码已公开：https://github.com/aimagelab/JARVIS。

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: 本文提出了一种新的任务“稀疏接地视觉导航”（Sparsely Grounded Visual Navigation）和相应基准CityNav，用于评估多模态大语言模型（MLLMs）在真实城市环境中仅依靠视觉输入进行知识密集型导航的能力，并提出了新方法Verbalization of Path（VoP）来提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 当前对具身智能体的评估主要集中在以语言为中心的任务或高度依赖模拟环境，缺乏对现实世界中所需复杂、知识密集型推理能力的有效评测。

Method: 作者构建了CityNav基准，在四个全球城市中设置超过50个决策点，要求MLLM驱动的智能体仅使用视觉输入和内部多模态推理完成导航。同时提出Verbalization of Path（VoP）方法，通过从MLLM中显式提取认知地图（关键地标和方向）来增强其导航能力。

Result: 实验表明，现有最先进的MLLM及标准推理技术（如思维链、反思）在此任务上表现不佳；而所提出的VoP方法显著提升了导航成功率。

Conclusion: 为有效评估MLLM在真实世界中的具身推理能力，需设计更贴近实际的评测任务；本文提出的CityNav基准与VoP方法为此提供了重要工具和思路。

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

Abstract (中文翻译): 利用多模态大语言模型（MLLMs）开发具身智能体在解决复杂的现实世界任务方面具有巨大潜力。然而，当前的评估基准仍主要以语言为中心，或严重依赖模拟环境，很少考察实际应用场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们提出了“稀疏接地视觉导航”（Sparsely Grounded Visual Navigation）任务，专门用于评估MLLM在具有挑战性的、知识密集型的真实世界环境中的序列决策能力。我们通过CityNav这一综合性基准对该任务进行了具体实现，该基准涵盖全球四个不同的城市，专门用于评估由原始MLLM驱动的智能体在城市导航中的表现。智能体必须仅依靠视觉输入和内部多模态推理，在没有额外环境标注或特定架构修改的情况下，依次通过50多个决策点。至关重要的是，智能体必须能够通过解读城市特有线索和识别地标实现自主定位，进行空间推理，并战略性地规划和执行通往目的地的路线。通过大量实验评估，我们证明了当前最先进的MLLM及标准推理技术（例如思维链、反思）在此类具有挑战性的设定下表现显著不足。为此，我们提出了“路径言语化”（Verbalization of Path, VoP）方法，通过从MLLM中显式探查认知地图（包括关键地标和朝向目的地的方向），从而显著提升导航成功率。项目网页：https://dwipddalal.github.io/AgentNav/

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4 是一个无需训练的框架，通过在四维时空空间中构建结构化、持久的记忆库，增强视觉语言模型（VLM）的检索与推理能力，显著提升其在动态环境中对时空信息的理解和应用。


<details>
  <summary>Details</summary>
Motivation: 人类能通过构建包含语义、空间和时间信息的四维内部表征来感知和推理环境，现有模型缺乏类似的能力。作者希望赋予视觉语言模型类似的结构化、持久记忆机制，以支持更强大的时空推理。

Method: 提出 R4 框架：在推理阶段，将自然语言查询分解为语义、空间和时间关键词，在一个持续构建的四维知识库（锚定对象级语义描述于度量空间与时间中）中进行检索，并将检索结果整合进 VLM 的推理过程，整个过程无需训练。

Result: 在具身问答和导航基准测试中，R4 相比基线方法显著提升了对时空信息的检索与推理性能。

Conclusion: R4 为动态环境中的具身智能体提供了一种新的四维推理范式，通过结构化、共享的持久世界模型实现无需训练的协作式和情景式推理。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

Abstract (中文翻译): 人类通过构建持久、结构化的内部表征来在四维时空中感知和推理周围环境，这些表征编码了语义含义、空间布局和时间动态。这种多模态记忆使人类能够回忆过去事件、推断未观测状态，并将新信息整合到依赖上下文的推理中。受此启发，我们提出了 R4——一种无需训练的框架，用于在四维时空空间中进行检索增强推理，为视觉语言模型（VLM）赋予结构化、终身记忆能力。R4 通过将对象级语义描述锚定在度量空间和时间中，持续构建一个四维知识数据库，从而形成可跨智能体共享的持久世界模型。在推理阶段，自然语言查询被分解为语义、空间和时间关键词，以检索相关观测，并将其整合进 VLM 的推理过程中。与传统的检索增强生成方法不同，R4 的检索直接在四维空间中进行，从而无需训练即可实现情景式和协作式推理。在具身问答和导航基准上的实验表明，R4 在时空信息的检索与推理方面显著优于基线方法，推动了动态环境中具身四维推理的新范式。

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出“感知观测站”（The Perceptual Observatory）框架，用于系统评估多模态大语言模型（MLLMs）在受控扰动下的感知能力，超越传统任务准确率指标，深入分析其视觉基础、鲁棒性与推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）虽然性能不断提升，但其感知能力缺乏系统评估。大多数模型仅扩展语言组件而重复使用几乎相同的视觉编码器，导致难以判断其进步是源于真正的视觉理解还是依赖大规模文本知识。现有评估方法过于关注任务准确率，忽视了鲁棒性、归因保真度及在扰动下的推理能力。

Method: 作者构建了“感知观测站”评估框架，涵盖多个垂直维度：(i) 简单视觉任务（如人脸匹配、图像中文本理解）；(ii) 从局部到全局的理解任务（如图像匹配、网格指向游戏、属性定位）。每个维度均使用带真实标签的人脸和文字数据集，并通过基于像素的增强和基于扩散模型的风格化幻觉进行系统性扰动。

Result: 该框架能够揭示MLLM在扰动条件下如何保持感知基础和关系结构，提供对模型感知能力的细粒度洞察，超越传统排行榜准确率。

Conclusion: “感知观测站”为分析当前及未来MLLM的感知优势与不足提供了原则性基础，推动对模型视觉接地能力的深入理解。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

Abstract (中文翻译): 多模态大语言模型（MLLMs）的最新进展催生了日益强大的模型，但其感知能力仍缺乏充分刻画。在实践中，大多数模型系列仅扩展语言组件，而重复使用几乎相同的视觉编码器（例如 Qwen2.5-VL 3B/7B/72B），这引发了关键问题：模型的进步究竟源于真实的视觉接地能力，还是依赖于互联网规模的文本世界知识？现有评估方法侧重于端到端任务的准确率，忽略了鲁棒性、归因保真度以及在受控扰动下的推理能力。我们提出了“感知观测站”（The Perceptual Observatory）框架，从多个垂直维度刻画MLLM的感知能力：(i) 简单视觉任务，如人脸匹配和图像中文本理解；(ii) 从局部到全局的理解，包括图像匹配、网格指向游戏和属性定位，以测试通用的视觉接地能力。每个维度均基于带真实标签的人脸和文字数据集，并通过基于像素的增强和基于扩散模型的风格化幻觉进行系统性扰动。“感知观测站”超越了排行榜准确率，深入揭示MLLM在扰动下如何保持感知基础和关系结构，为分析当前及未来模型的优势与不足提供了原则性基础。

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 本文提出CAMP-VLM，一种基于视觉语言模型的框架，用于从第三人称视角预测多个人类行为，通过结合视觉上下文和场景图的空间信息，在合成与真实数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注以自我中心视角预测单人行为，但许多机器人应用场景需要从第三人称视角理解多人行为，因此亟需新的方法和数据支持。

Method: 提出CAMP-VLM框架，融合视觉输入的上下文特征与场景图的空间感知；利用逼真模拟器生成的合成数据进行监督微调（SFT）和直接偏好优化（DPO）。

Result: CAMP-VLM在预测准确率上比最佳基线方法最高提升66.9%，并在合成与真实世界序列上验证了其泛化能力。

Conclusion: CAMP-VLM有效提升了从观察者视角对多个人类行为的预测性能，为机器人在人群环境中的行为理解提供了可行方案。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

Abstract (中文翻译): 准确预测人类行为对于在人群环境中运行的移动机器人至关重要。尽管以往的研究主要聚焦于从自我中心视角预测单人场景中的行为，但许多机器人应用需要从第三人称视角理解多个人类的行为。为此，我们提出了CAMP-VLM（上下文感知的多人行为预测）：一种基于视觉语言模型（VLM）的框架，通过融合视觉输入中的上下文特征与场景图中的空间感知信息，增强对人-场景交互的预测能力。由于缺乏适用于从观察者视角进行多人行为预测的数据集，我们利用逼真模拟器生成的合成人类行为数据对CAMP-VLM进行微调，并在合成数据和真实世界序列上评估模型的泛化能力。借助监督微调（SFT）和直接偏好优化（DPO），CAMP-VLM在预测准确率上最高比表现最好的基线方法提升66.9%。

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文探索利用视觉-语言模型（VLM）进行少样本多光谱目标检测，通过适配Grounding DINO和YOLO-World并融合文本、可见光与热成像模态，在FLIR和M3FD数据集上取得优于专用模型的性能，证明了VLM语义先验可有效迁移到新光谱模态，提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标检测在自动驾驶和监控等安全敏感应用中至关重要，但标注数据稀缺限制了深度检测器的训练。文本类别信息可作为有价值的语义监督来源，而近期视觉-语言模型（VLM）在计算机视觉中的成功激发了将其用于少样本多光谱目标检测的潜力。

Method: 作者将两种代表性VLM检测器（Grounding DINO 和 YOLO-World）适配至多光谱输入，并提出一种有效机制融合文本、可见光与热成像模态。

Result: 在FLIR和M3FD两个多光谱图像基准上的实验表明，所提方法在少样本设置下显著优于使用同等数据训练的专用多光谱模型，并在全监督设置下也达到具有竞争力甚至更优的性能。

Conclusion: 大规模VLM学习到的语义先验能有效迁移到未见过的光谱模态，为实现数据高效的多光谱感知提供了有力途径。

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

Abstract (中文翻译): 多光谱目标检测对于自动驾驶和监控等对安全性要求较高的应用至关重要，因为在各种光照条件下实现鲁棒感知是必不可少的。然而，标注多光谱数据的有限可用性严重限制了深度检测器的训练。在这种数据稀缺的情况下，文本类别信息可以作为一种宝贵的语义监督来源。受近期视觉-语言模型（VLM）在计算机视觉领域取得成功的启发，我们探索了其在少样本多光谱目标检测中的潜力。具体而言，我们对两种具有代表性的基于VLM的检测器——Grounding DINO和YOLO-World——进行了适配，使其能够处理多光谱输入，并提出了一种有效机制来融合文本、可见光与热成像模态。通过对两个流行的多光谱图像基准数据集FLIR和M3FD进行大量实验，我们证明了基于VLM的检测器不仅在少样本场景下表现出色，显著优于使用相当数据量训练的专用多光谱模型，而且在全监督设置下也能取得具有竞争力甚至更优的结果。我们的研究结果表明，大规模VLM所学习到的语义先验能够有效地迁移到未见过的光谱模态，为实现数据高效的多光谱感知提供了一条强有力的路径。

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 当前的视觉语言模型（VLM）在农业分类任务中表现远逊于专用监督模型（如YOLO11），尤其在开放式提示下准确率极低；尽管部分闭源模型（如Gemini-3 Pro）在多项选择提示下可达62%平均准确率，且开源模型Qwen-VL-72B表现最佳，但整体仍不足以作为独立的农业诊断系统，仅可在约束界面和领域适配策略下作为辅助工具。


<details>
  <summary>Details</summary>
Motivation: 评估通用视觉语言模型（VLM）在农业决策支持场景中的可靠性，因其在农业识别任务中的实际效用尚不明确。

Method: 在AgML集合的27个农业分类数据集（涵盖162个类别）上对多种开源与闭源VLM进行基准测试，采用零样本设置，并对比有监督专用模型YOLO11；同时比较多项选择与开放式提示策略，并引入基于大语言模型的语义判断以评估开放式回答。

Result: 零样本VLM整体显著落后于YOLO11；多项选择提示下最佳VLM（Gemini-3 Pro）平均准确率约62%，而开放式提示原始准确率通常低于25%；语义判断可提升开放式准确率（如从21%升至30%）并改变模型排名；开源模型中Qwen-VL-72B表现最优；植物与杂草识别比病虫害识别更容易。

Conclusion: 当前现成的VLM尚不能作为独立的农业诊断系统，但在结合约束交互界面、明确标签本体和领域感知评估策略时，可作为辅助组件发挥作用。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

Abstract (中文翻译): 视觉语言模型（VLM）正越来越多地被提出作为视觉识别任务的通用解决方案，但其在农业决策支持中的可靠性仍缺乏充分理解。我们在AgML数据集集合中的27个农业分类数据集上对多种开源和闭源VLM进行了基准测试，这些数据集涵盖162个类别，包括植物病害、虫害与损伤、以及植物与杂草物种识别。在所有任务中，零样本VLM的表现显著逊色于有监督的任务专用基线模型（YOLO11），后者始终比任何基础模型都取得明显更高的准确率。在多项选择提示下，表现最佳的VLM（Gemini-3 Pro）平均准确率约为62%，而开放式提示则导致性能大幅下降，原始准确率通常低于25%。采用基于大语言模型（LLM）的语义判断可提升开放式提示的准确率（例如，顶级模型从21%提升至30%），并改变模型排名，表明评估方法会显著影响结论。在开源模型中，Qwen-VL-72B表现最佳，在约束提示下接近闭源模型水平，但仍落后于顶尖的专有系统。任务层面分析显示，植物与杂草物种分类始终比虫害与损伤识别更容易，后者是各类模型面临的最具挑战性的类别。总体而言，这些结果表明，当前现成的VLM尚不适合作为独立的农业诊断系统，但在结合约束式交互界面、显式标签本体和领域感知评估策略的情况下，可作为辅助组件发挥作用。

</details>


### [9] [Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings](https://arxiv.org/abs/2512.15993)
*Lars Beckers,Arno Waes,Aaron Van Campenhout,Toon Goedemé*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉感知与自适应决策的机器人割草框架，通过保留视觉多样性植被斑块来主动提升花园生物多样性。


<details>
  <summary>Details</summary>
Motivation: 传统割草方式导致城市草坪单一化，缺乏生态价值；现有再野化方法多为被动，难以在日常维护中兼顾美观与生态。因此需要一种能主动识别并保护植被多样性的智能割草系统。

Method: 利用在PlantNet300K上预训练的ResNet50网络提取具有生态意义的图像嵌入，通过计算特征空间中的全局偏差度量来估计生物多样性（无需物种级别标注），并据此动态控制割草机刀片启停，实现选择性割草。

Result: 在模拟草坪和真实花园数据集上的实验表明，嵌入空间的分散程度与专家评估的生物多样性高度相关，验证了该方法的有效性。

Conclusion: 该系统可将单调无生态价值的城市草坪转变为高生物多样性的城市生境，具有广泛推广潜力。

Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.

Abstract (中文翻译): 本文提出了一种机器人割草框架，通过视觉感知与自适应决策主动提升花园生物多样性。不同于被动的再野化方法，该系统利用深度特征空间分析，在相机图像中识别并保留视觉上多样的植被斑块，并选择性地关闭割草刀片以实现保护。系统采用在PlantNet300K数据集上预训练的ResNet50网络，生成具有生态意义的嵌入表示，并通过一种全局偏差度量在无需物种级别监督的情况下估算生物多样性。这些估算结果驱动一种选择性割草算法，动态切换割草与保护行为。该系统已在改装的商用机器人割草机上实现，并在受控的模拟草坪及真实花园数据集上进行了验证。结果表明，嵌入空间的分散程度与专家对生物多样性的评估高度相关，证实了利用深度视觉多样性作为生态丰富度代理指标的可行性，以及所提割草决策方法的有效性。此类系统的广泛应用有望将生态价值低下的单一种植草坪转变为生机勃勃、富有生态价值的城市生物栖息地，从而提升城市生物多样性。

</details>


### [10] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出一种从初始图像和机器人关节状态出发、根据文本指令生成视频-动作对的方法，通过引入并行动作扩散模型、Bridge Attention机制和动作精炼模块，有效利用预训练视频扩散模型进行机器人策略学习，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用视频扩散模型进行机器人策略学习时面临动作标注缺失的问题，且多采用两阶段流程或单模态扩展方式，难以实现紧密的跨模态信息交互并充分利用预训练视频知识。

Method: 作者扩展了预训练视频扩散模型，加入一个并行的专用动作扩散模型以保留预训练知识；引入Bridge Attention机制促进视频与动作之间的跨模态交互；并设计动作精炼模块将粗略动作转化为适用于低分辨率数据集的精确控制信号。

Result: 在多个公开基准和真实世界数据集上的大量实验表明，该方法生成的视频质量更高、动作更准确，显著优于现有基线方法。

Conclusion: 该方法提供了一个可扩展的框架，能够有效利用大规模视频数据进行机器人学习。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

Abstract (中文翻译): 我们提出了一种根据文本指令生成视频-动作对的方法，该方法从初始图像观测和机器人关节状态出发。我们的方法自动为视频扩散模型提供动作标签，克服了常见动作标注缺失的问题，从而使其能充分用于机器人策略学习。现有方法要么采用两阶段流水线，限制了紧密耦合的跨模态信息共享，要么依赖于将单模态扩散模型适配为联合分布，无法充分利用预训练的视频知识。为克服这些局限，我们（1）在预训练视频扩散模型基础上扩展了一个并行的专用动作扩散模型，以保留预训练知识；（2）引入Bridge Attention机制，实现有效的跨模态交互；（3）设计了一个动作精炼模块，将粗略动作转化为适用于低分辨率数据集的精确控制。在多个公开基准和真实世界数据集上的大量评估表明，我们的方法生成的视频质量更高、动作更准确，并显著优于现有基线方法，为利用大规模视频数据进行机器人学习提供了一个可扩展的框架。

</details>
