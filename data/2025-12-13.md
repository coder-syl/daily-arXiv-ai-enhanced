<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 本文提出一种基于脉冲神经网络（SNN）的高效事件驱动眼动追踪模型，通过轻量化的LIF层和深度可分离卷积，在保持接近现有最优精度的同时，大幅降低模型大小与计算开销，适用于低功耗可穿戴AR/VR设备。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的眼动追踪方法在可穿戴系统中面临运动模糊、高计算成本和时间分辨率不足的问题，而现有SNN方案要么过于专用，要么性能不及现代ANN架构。因此需要一种兼顾高精度与高能效的神经形态解决方案。

Method: 将当前最优的事件驱动眼动追踪模型中的循环和注意力模块替换为轻量级LIF（Leaky Integrate-and-Fire）层，并采用深度可分离卷积以降低模型复杂度，构建高效的SNN版本。

Result: 所提模型平均误差为3.7–4.1像素，接近专用神经形态系统Retina（3.24像素），同时模型体积缩小20倍，理论计算量减少850倍；预计功耗为3.9–4.9毫瓦，延迟为3毫秒（1kHz下）。

Conclusion: 高性能的事件驱动眼动追踪架构可以成功重构为SNN，在显著提升能效的同时仍保持适用于实时可穿戴部署的精度。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

Abstract (中文翻译): 面向可穿戴系统的眼动追踪需要低延迟和毫瓦级功耗，但传统的基于帧的处理流程在运动模糊、高计算开销和有限的时间分辨率方面存在困难。这类能力对于增强现实（AR）和虚拟现实（VR）等新兴技术中实现无缝且响应迅速的交互至关重要，因为理解用户注视点是提升沉浸感和界面设计的关键。神经形态传感器和脉冲神经网络（SNN）提供了一种有前景的替代方案，然而现有的SNN方法要么过于专用，要么性能无法媲美现代人工神经网络（ANN）架构。本文提出了当前最优事件驱动眼动追踪模型的神经形态版本，用轻量级LIF层取代其循环和注意力模块，并利用深度可分离卷积降低模型复杂度。我们的模型取得了3.7–4.1像素的平均误差，接近专用神经形态系统Retina（3.24像素）的精度，同时相比最接近的ANN变体，模型大小减少了20倍，理论计算量减少了850倍。这些高效变体预计可在1kHz下以3.9–4.9毫瓦功耗和3毫秒延迟运行。结果表明，高性能的事件驱动眼动追踪架构可以被重新设计为SNN，在显著提升能效的同时，仍保留适用于实时可穿戴部署的精度。

</details>


### [2] [ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects](https://arxiv.org/abs/2512.10031)
*Woojin Lee,Hyugjae Chang,Jaeho Moon,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: 本文提出ABBSPO框架，通过自适应边界框缩放和对称先验角度损失，提升弱监督方向目标检测（WS-OOD）的性能，尤其在仅使用水平框标注的情况下达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于水平框（HBox）监督的弱监督方向目标检测方法存在尺度估计不准确以及在多视角增强下学习崩溃的问题，限制了其性能。

Method: 提出ABBSPO框架，包含：(i) 自适应边界框缩放（ABBS），根据预测旋转框动态调整真实水平框尺度；(ii) 基于对称先验的角度损失（SPA），利用航拍目标的对称性进行自监督学习，防止多视角预测一致错误时的学习崩溃。

Result: 大量实验表明，ABBSPO在弱监督方向目标检测任务中优于现有方法，达到最先进的性能。

Conclusion: ABBSPO有效解决了HBox监督下方向目标检测中的尺度估计不准与学习稳定性问题，显著提升了检测精度。

Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.

Abstract (中文翻译): 弱监督方向目标检测（WS-OOD）作为一种兼顾效率与高精度、且成本较低的替代方案，近年来受到广泛关注。在各类弱监督方法中，基于水平边界框（HBox）监督的OOD因其能直接利用现有的HBox标注，并在弱监督设定下实现最高精度而尤为突出。本文提出一种名为ABBSPO的新框架，用于WS-OOD任务，该框架结合了自适应边界框缩放与基于对称先验的方向预测。ABBSPO旨在解决以往HBox监督OOD方法中存在的局限性——这些方法通常直接将真实HBox与预测旋转框（RBox）的最小外接矩形进行比较，从而导致尺度估计不准确。为此，我们提出了两项关键技术：(i) 自适应边界框缩放（ABBS），可根据每个预测RBox的尺寸动态调整真实HBox的尺度，以实现更精确的尺度预测；(ii) 对称先验角度（SPA）损失，利用航拍目标固有的对称性进行自监督学习，有效缓解了以往方法在原始、旋转和翻转三种增强视图的预测全部错误时出现的学习崩溃问题。大量实验结果表明，ABBSPO在性能上超越现有方法，达到了当前最优水平。

</details>


### [3] [Diffusion Is Your Friend in Show, Suggest and Tell](https://arxiv.org/abs/2512.10038)
*Jia Cheng Hu,Roberto Cavicchioli,Alessandro Capotondi*

Main category: cs.CV

TL;DR: 本文提出将扩散模型作为自回归模型的“建议模块”，而非替代方案，在图像描述生成任务中实现SOTA性能，COCO上达到125.1 CIDEr-D，优于现有自回归和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散去噪模型在生成式计算机视觉任务中表现优异，但在离散域（如文本生成）中仍无法超越甚至仅能勉强匹敌标准自回归方法。作者旨在结合两者优势，探索一种新的范式。

Method: 提出“展示、建议与讲述”（Show, Suggest and Tell, SST）框架，利用扩散模型为自回归生成过程提供上下文建议，融合扩散模型的双向细化能力和自回归模型的语言结构建模能力。

Result: SST在COCO数据集上取得125.1 CIDEr-D分数（未使用强化学习），比当前最优的自回归模型和扩散模型分别高出1.5和2.5分；实验验证了建议模块对生成质量的正向影响。

Conclusion: 将扩散模型作为自回归生成的辅助建议机制是一种有效且有前景的新方向，值得进一步探索。

Abstract: Diffusion Denoising models demonstrated impressive results across generative Computer Vision tasks, but they still fail to outperform standard autoregressive solutions in the discrete domain, and only match them at best. In this work, we propose a different paradigm by adopting diffusion models to provide suggestions to the autoregressive generation rather than replacing them. By doing so, we combine the bidirectional and refining capabilities of the former with the strong linguistic structure provided by the latter. To showcase its effectiveness, we present Show, Suggest and Tell (SST), which achieves State-of-the-Art results on COCO, among models in a similar setting. In particular, SST achieves 125.1 CIDEr-D on the COCO dataset without Reinforcement Learning, outperforming both autoregressive and diffusion model State-of-the-Art results by 1.5 and 2.5 points. On top of the strong results, we performed extensive experiments to validate the proposal and analyze the impact of the suggestion module. Results demonstrate a positive correlation between suggestion and caption quality, overall indicating a currently underexplored but promising research direction. Code will be available at: https://github.com/jchenghu/show\_suggest\_tell.

Abstract (中文翻译): 扩散去噪模型在各类生成式计算机视觉任务中展现了令人印象深刻的效果，但在离散域（如文本生成）中仍未能超越标准的自回归方法，最多只能与之持平。本文提出了一种新范式：采用扩散模型为自回归生成过程提供建议，而非取代它。通过这种方式，我们结合了扩散模型的双向建模与细化能力，以及自回归模型强大的语言结构建模能力。为验证该方法的有效性，我们提出了“展示、建议与讲述”（Show, Suggest and Tell, SST）模型，在相似设定下于COCO数据集上取得了当前最优结果。具体而言，SST在未使用强化学习的情况下，在COCO上达到了125.1的CIDEr-D分数，分别比当前最优的自回归模型和扩散模型高出1.5分和2.5分。除强劲性能外，我们还进行了大量实验以验证所提方法，并分析建议模块的影响。结果表明，建议质量与最终描述质量呈正相关，整体显示出一个目前尚未被充分探索但极具前景的研究方向。代码将发布于：https://github.com/jchenghu/show_suggest_tell。

</details>


### [4] [MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata](https://arxiv.org/abs/2512.10041)
*Yihao Liu,Chenyu Gao,Lianrui Zuo,Michael E. Kim,Brian D. Boyd,Lisa L. Barnes,Walter A. Kukull,Lori L. Beason-Held,Susan M. Resnick,Timothy J. Hohman,Warren D. Taylor,Bennett A. Landman*

Main category: cs.CV

TL;DR: MetaVoxel 是一个联合扩散生成模型，通过建模医学影像与临床元数据的联合分布，实现多任务统一（如图像生成、年龄估计、性别预测），无需针对每个任务单独训练，并在零样本设置下支持任意输入组合的灵活推理。


<details>
  <summary>Details</summary>
Motivation: 当前大多数深度学习方法仅针对特定预测方向和输入变量建模条件分布，导致不同任务需独立模型，限制了灵活性与泛化能力。作者旨在构建一个统一框架，通过建模联合分布来整合多任务并支持灵活推理。

Method: 提出 MetaVoxel 框架，利用单一扩散过程对医学影像（如 T1 加权 MRI）与临床元数据的联合分布进行建模，从而在一个模型中统一多种任务。

Result: 在包含超过 10,000 例 T1 加权 MRI 扫描及对应临床元数据的九个数据集上，MetaVoxel 在图像生成、年龄估计和性别预测任务中达到与专用基线模型相当的性能，并展示了灵活推理能力。

Conclusion: 联合多模态扩散模型为统一医疗 AI 模型提供了新方向，有望提升模型的通用性和临床适用性。

Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.

Abstract (中文翻译): 现代深度学习方法在疾病分类、连续生物标志物估计到生成逼真医学图像等任务中取得了令人印象深刻的结果。然而，这些方法大多被训练用于建模由特定预测方向和特定输入变量定义的条件分布。本文提出了 MetaVoxel——一种生成式联合扩散建模框架，通过对成像数据与临床元数据的联合分布进行建模，学习覆盖所有变量的单一扩散过程。通过捕捉联合分布，MetaVoxel 统一了传统上需要单独条件模型的任务，并支持使用任意输入子集进行灵活的零样本推理，而无需针对具体任务重新训练。我们在来自九个数据集的超过 10,000 例 T1 加权 MRI 扫描及其配对的临床元数据上验证了该方法，结果表明单个 MetaVoxel 模型即可执行图像生成、年龄估计和性别预测，并在性能上与成熟的任务专用基线模型相当。额外实验进一步突显了其灵活推理能力。这些发现共同表明，联合多模态扩散为统一医疗人工智能模型并拓展其临床应用前景提供了一条有希望的新路径。

</details>


### [5] [Independent Density Estimation](https://arxiv.org/abs/2512.10067)
*Jiahao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为独立密度估计（IDE）的新方法，通过学习句子中每个词与图像特征之间的关联，提升视觉-语言模型在未见组合上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉-语言模型虽在图像描述和条件图像生成等任务上表现优异，但在类人组合泛化能力方面仍存在不足。

Method: 提出独立密度估计（IDE）方法，构建两个基于IDE理念的模型：一个使用完全解耦的视觉表示，另一个利用变分自编码器从原始图像中提取部分解耦特征；并设计基于熵的组合推理方法整合各词预测结果。

Result: 在多个数据集上的实验表明，所提模型在未见组合上的泛化性能优于现有模型。

Conclusion: IDE方法有效提升了视觉-语言模型的组合泛化能力，为实现更接近人类水平的理解与生成提供了新思路。

Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.

Abstract (中文翻译): 大规模视觉-语言模型在图像描述和条件图像生成等多个领域取得了显著成果。然而，这些模型在实现类人组合泛化方面仍面临挑战。本研究提出了一种名为独立密度估计（Independent Density Estimation, IDE）的新方法来应对这一问题。IDE旨在学习句子中各个词语与图像中对应特征之间的关联，从而实现组合泛化。我们基于IDE的理念构建了两个模型：第一个模型采用完全解耦的视觉表示作为输入，第二个模型则利用变分自编码器从原始图像中获取部分解耦的特征。此外，我们还提出了一种基于熵的组合推理方法，用于融合句子中每个词的预测结果。在多个数据集上的评估表明，与当前模型相比，我们的模型在未见过的组合上展现出更优的泛化能力。

</details>


### [6] [TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing](https://arxiv.org/abs/2512.10095)
*Jiachen Tao,Junyi Wu,Haoxuan Wang,Zongxin Yang,Dawen Cai,Yan Yan*

Main category: cs.CV

TL;DR: TraceFlow 是一种用于高保真渲染动态镜面场景的新框架，通过精确估计反射方向和物理准确的反射建模，在复杂动态环境中生成更清晰、逼真的镜面反射效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态镜面场景渲染中难以同时实现精确的反射方向估计与物理上准确的反射建模，导致镜面反射失真或不真实。

Method: 提出一种残差材质增强的2D高斯泼溅表示以建模动态几何与材质属性；引入动态环境高斯与混合渲染管线，将渲染分解为漫反射与镜面反射成分，并结合光栅化与光线追踪实现物理合理的镜面合成；采用由粗到细的训练策略提升优化稳定性与物理分解质量。

Result: 在多个动态场景基准上的实验表明，TraceFlow 在定量和定性指标上均优于现有方法，能生成更锐利、更真实的镜面反射。

Conclusion: TraceFlow 有效解决了动态镜面场景渲染中的关键挑战，显著提升了高保真镜面反射的生成质量。

Abstract: We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.

Abstract (中文翻译): 我们提出了 TraceFlow，这是一种用于高保真渲染动态镜面场景的新框架，旨在解决两个关键挑战：精确的反射方向估计和物理上准确的反射建模。为此，我们提出了一种残差材质增强的2D高斯泼溅表示方法，用于建模动态几何结构和材质属性，从而实现精确的反射光线计算。此外，我们引入了动态环境高斯表示和一种混合渲染管线，将渲染过程分解为漫反射和镜面反射两个部分，通过光栅化与光线追踪相结合的方式实现基于物理的镜面反射合成。最后，我们设计了一种由粗到细的训练策略，以提高优化稳定性并促进具有物理意义的反射成分分解。在多个动态场景基准上的大量实验表明，TraceFlow 在定量和定性评估中均优于现有方法，在复杂动态环境中生成了更清晰、更逼真的镜面反射效果。

</details>


### [7] [Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information](https://arxiv.org/abs/2512.10102)
*Neelima Prasad,Jarek Reynolds,Neel Karsanbhai,Tanusree Sharma,Lotus Zhang,Abigale Stangl,Yang Wang,Leah Findlater,Danna Gurari*

Main category: cs.CV

TL;DR: 本文提出了一种新任务——层次化实例跟踪，要求在视频中同时跟踪预定义类别的对象及其部件的全部实例，并保持它们之间的层次关系。作者发布了首个支持该任务的基准数据集，包含552个视频中的2,765个唯一实体，涵盖40个类别（包括对象和部件）。对四种模型的七个变体进行评估表明该数据集具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有目标跟踪方法通常只关注单一层次的对象或部件，缺乏对对象与其组成部分之间层次结构的建模能力。为填补这一空白，作者提出了层次化实例跟踪任务，以更全面地理解复杂场景中的视觉实体及其结构关系。

Method: 作者构建了一个新的基准数据集，用于支持层次化实例跟踪任务。该数据集包含552段视频，标注了2,765个唯一实体，覆盖40个类别（包括对象和其组成部分），并明确维护了这些实体之间的层次关系。此外，作者还适配并评估了四种主流模型的七个变体，以验证该任务的挑战性和数据集的有效性。

Result: 对四种模型的七个变体的评估结果表明，所提出的数据集具有较高的挑战性，现有的跟踪方法在处理层次化实例跟踪任务时仍存在明显不足。

Conclusion: 本文成功提出了层次化实例跟踪这一新任务，并发布了首个支持该任务的基准数据集。实验结果证明了该任务的难度，为未来研究提供了新的方向和资源。

Abstract: We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/

Abstract (中文翻译): 我们提出了一项新任务——层次化实例跟踪，该任务要求跟踪预定义类别中的所有对象及其部件的实例，同时保持它们之间的层次关系。我们发布了首个支持该任务的基准数据集，包含552段视频中的2,765个唯一实体，涵盖40个类别（包括对象和部件）。对四种模型的七个变体在该任务上的评估表明，新数据集具有挑战性。我们的数据集可在 https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/ 获取。

</details>


### [8] [Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization](https://arxiv.org/abs/2512.10151)
*Charles Fanning,Mehmet Emin Aktas*

Main category: cs.CV

TL;DR: 本文提出一种基于小波持久同调的条件信号，用于提升乳腺癌筛查模型在不同数据集上的泛化性能。通过拓扑数据分析提取对强度扰动稳定的多尺度空间特征图，并将其融入两阶段检测流程中，在跨域测试中显著提升了AUC。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线摄影筛查虽能降低死亡率，但在实际应用中仍面临较高的假阴性和假阳性率，且模型在不同扫描设备、成像模态和人群中的性能往往下降。因此，亟需提升模型的外部泛化能力。

Method: 作者利用拓扑数据分析（TDA）提取图像在不同强度阈值下保持稳定的结构信息，并通过小波变换将其转化为对小强度扰动具有理论稳定性的多尺度空间特征图。这些特征图通过输入通道拼接的方式整合进两阶段检测模型中。

Result: 模型在美国CBIS-DDSM胶片数字化乳腺数据集上训练和验证，并在葡萄牙（INbreast）和中国（CMMD）两个独立的全视野数字乳腺数据集上评估。在INbreast数据集上，加入小波持久性通道后，ConvNeXt Tiny模型在有限训练预算下的患者级AUC从0.55提升至0.75。

Conclusion: 所提出的基于小波持久同调的条件信号能有效提升乳腺癌检测模型在跨域场景下的性能，为解决医学影像模型泛化性差的问题提供了新思路。

Abstract: Breast cancer is the most commonly diagnosed cancer in women and a leading cause of cancer death worldwide. Screening mammography reduces mortality, yet interpretation still suffers from substantial false negatives and false positives, and model accuracy often degrades when deployed across scanners, modalities, and patient populations. We propose a simple conditioning signal aimed at improving external performance based on a wavelet based vectorization of persistent homology. Using topological data analysis, we summarize image structure that persists across intensity thresholds and convert this information into spatial, multi scale maps that are provably stable to small intensity perturbations. These maps are integrated into a two stage detection pipeline through input level channel concatenation. The model is trained and validated on the CBIS DDSM digitized film mammography cohort from the United States and evaluated on two independent full field digital mammography cohorts from Portugal (INbreast) and China (CMMD), with performance reported at the patient level. On INbreast, augmenting ConvNeXt Tiny with wavelet persistence channels increases patient level AUC from 0.55 to 0.75 under a limited training budget.

Abstract (中文翻译): 乳腺癌是女性中最常被诊断出的癌症，也是全球癌症死亡的主要原因。乳腺X线摄影筛查可降低死亡率，但其解读仍存在大量假阴性和假阳性问题，且模型在跨扫描仪、跨成像模态及不同患者群体部署时，准确率常常下降。我们提出一种基于小波持久同调向量化的简单条件信号，旨在提升模型的外部泛化性能。利用拓扑数据分析，我们总结了在不同强度阈值下持续存在的图像结构，并将该信息转化为对微小强度扰动具有理论稳定性的空间多尺度特征图。这些特征图通过输入层通道拼接的方式整合到一个两阶段检测流程中。该模型在美国CBIS-DDSM胶片数字化乳腺数据集上进行训练与验证，并在葡萄牙（INbreast）和中国（CMMD）两个独立的全视野数字乳腺数据集上进行评估，性能以患者级别指标报告。在INbreast数据集上，在有限训练预算条件下，将ConvNeXt Tiny模型结合小波持久性通道后，患者级别的AUC从0.55提升至0.75。

</details>


### [9] [Feature Coding for Scalable Machine Vision](https://arxiv.org/abs/2512.10209)
*Md Eimran Hossain Eimon,Juan Merlos,Ashan Perera,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: 本文介绍了MPEG的Feature Coding for Machines（FCM）标准及其测试模型FCTM，通过压缩DNN中间特征，在保持精度的同时平均减少85.14%的比特率，为边缘-云协同推理提供高效、可互操作的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署深度神经网络面临计算、带宽、延迟和隐私的挑战。将推理任务拆分到边缘与云端虽可平衡这些因素，但传输中间特征会带来新的带宽开销，因此需要一种高效的中间特征压缩方法。

Method: 采用MPEG制定的Feature Coding for Machines（FCM）标准，设计并实现Feature Coding Test Model（FCTM），对深度神经网络的中间特征进行专门优化的编码与压缩。

Result: 在多个视觉任务中，FCTM在保持模型精度的同时，平均实现了85.14%的比特率降低。

Conclusion: FCM标准为带宽受限和隐私敏感的消费级应用提供了可扩展、高效且可互操作的智能特征部署路径。

Abstract: Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications.

Abstract (中文翻译): 深度神经网络（DNN）推动了现代机器视觉的发展，但由于其高计算需求，在边缘设备上的部署颇具挑战性。传统方法——在设备端运行完整模型或将计算卸载至云端——在延迟、带宽和隐私方面存在权衡。将推理任务在边缘与云端之间拆分提供了一种平衡方案，但为了实现这种拆分而传输中间特征又带来了新的带宽挑战。为此，动态图像专家组（MPEG）启动了面向机器的特征编码（Feature Coding for Machines, FCM）标准，制定了专门用于压缩中间特征的比特流语法和编解码器流程。本文介绍了特征编码测试模型（Feature Coding Test Model, FCTM）的设计与性能，在多个视觉任务中平均实现了85.14%的比特率降低，同时保持了模型精度。FCM为在带宽受限和隐私敏感的消费级应用中高效、可互操作地部署智能特征提供了一条可扩展的路径。

</details>


### [10] [Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226)
*Shuhan Tan,Kashyap Chitta,Yuxiao Chen,Ran Tian,Yurong You,Yan Wang,Wenjie Luo,Yulong Cao,Philipp Krahenbuhl,Marco Pavone,Boris Ivanovic*

Main category: cs.CV

TL;DR: 本文提出LCDrive，一种在隐空间中进行推理的视觉-语言-动作模型，通过动作对齐的隐式语言替代自然语言进行链式推理，从而提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型多使用自然语言进行链式推理（CoT），但文本可能不是最高效的推理表示方式；作者旨在探索更高效、与动作对齐的隐式推理机制以提升自动驾驶的安全性和性能。

Method: 提出Latent-CoT-Drive（LCDrive）模型，在动作对齐的隐空间中统一链式推理与决策：使用动作提议token（与输出动作共享词表）和世界模型token（基于学习到的隐式世界模型，表达动作未来结果）交替进行推理；先通过真实未来轨迹监督冷启动隐式CoT，再通过闭环强化学习进行后训练。

Result: 在大规模端到端驾驶基准上，LCDrive相比无推理和基于文本推理的基线方法，实现了更快的推理速度、更优的轨迹质量，以及在交互式强化学习中更大的性能提升。

Conclusion: 将链式推理表达为动作对齐的隐式语言比使用自然语言更有效，能显著提升自动驾驶系统的推理效率与驾驶性能。

Abstract: Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.

Abstract (中文翻译): 近期用于自动驾驶的视觉-语言-动作（VLA）模型探索在推理阶段进行推理，以提升在复杂场景下的驾驶性能与安全性。以往大多数工作使用自然语言在生成驾驶动作前表达思维链（CoT）推理。然而，文本可能并非最高效的推理表示方式。本文提出Latent-CoT-Drive（LCDrive）：一种在隐式语言中表达CoT的模型，该隐式语言能够捕捉所考虑驾驶动作的可能结果。我们的方法通过在动作对齐的隐空间中统一表示CoT推理与决策过程，实现两者的融合。模型不再使用自然语言，而是交替使用两类token进行推理：（1）动作提议token，其词表与模型输出动作一致；（2）世界模型token，基于学习到的隐式世界模型，用于表达这些动作的未来结果。我们首先利用场景的真实未来轨迹对动作提议和世界模型token进行监督，以冷启动隐式CoT；随后通过闭环强化学习进行后训练，以增强推理能力。在大规模端到端驾驶基准测试中，LCDrive相比无推理和基于文本推理的基线方法，实现了更快的推理速度、更优的轨迹质量，并在交互式强化学习中获得了更大的性能提升。

</details>
