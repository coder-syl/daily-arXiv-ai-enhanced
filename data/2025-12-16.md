<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于OpenVLA-OFT框架的可解释、对抗鲁棒的视觉-语言-动作模型，通过引入Evidence-3模块检测光度扰动并生成自然语言解释，在对抗攻击下显著提升了智能农业机器人动作预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 智能农业系统常依赖RGB相机和机械臂，易受色调、光照和噪声等光度扰动影响，导致在对抗攻击下失效，因此需要提升其鲁棒性与可解释性。

Method: 提出一种基于OpenVLA-OFT框架的可解释对抗鲁棒视觉-语言-动作模型，集成Evidence-3模块用于检测光度扰动并生成扰动原因与影响的自然语言解释。

Result: 相比基线模型，所提方法将当前动作L1损失降低21.7%，下一动作L1损失降低18.4%，在对抗条件下显著提升动作预测精度与可解释性。

Conclusion: 该模型有效增强了智能农业系统在面对光度扰动和对抗攻击时的鲁棒性与可解释能力，为可靠部署提供了新思路。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

Abstract (中文翻译): 智能农业已成为通过自动化和智能控制推动现代农业发展的关键技术。然而，依赖RGB相机进行感知和机械臂进行控制的系统（这在智能农业中很常见）容易受到色调、光照和噪声变化等光度扰动的影响，从而在对抗攻击下发生故障。为解决这一问题，我们提出了一种基于OpenVLA-OFT框架的可解释、对抗鲁棒的视觉-语言-动作模型。该模型集成了Evidence-3模块，能够检测光度扰动，并生成关于其成因与影响的自然语言解释。实验表明，与基线模型相比，所提模型将当前动作的L1损失降低了21.7%，下一动作的L1损失降低了18.4%，在对抗条件下显著提升了动作预测的准确性和可解释性。

</details>


### [2] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: 本文提出Temporal-Anchor3DLane，通过多任务损失优化、轻量级时序LSTM融合模块和ESCOP式训练策略，在不增加传感器或模型规模的前提下显著提升单目3D车道检测的精度和时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 单目3D车道检测面临深度模糊、遮挡以及时序不稳定等挑战；现有Anchor3DLane方法虽表现良好，但仍存在对回归异常值敏感、全局曲线几何监督弱、多损失项平衡困难以及时序连续性利用不足等问题。

Method: 在Anchor3DLane基础上引入三项改进：(1) 多任务损失优化，包括Balanced L1回归、Chamfer点集距离、基于不确定性的损失加权，以及用于分类与可见性的focal和Dice组件；(2) 轻量级Temporal LSTM Fusion模块，替代较重的Transformer式时序融合；(3) ESCOP风格训练策略，将曲线级监督与时序一致性结合。

Result: 在OpenLane数据集上，F1分数提升+6.2，并生成更平滑的时序轨迹。

Conclusion: 通过小幅架构与损失函数优化，可在不依赖额外传感器或扩大模型规模的情况下显著增强3D车道检测的鲁棒性和时序一致性。

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

Abstract (中文翻译): 单目3D车道检测由于深度模糊、遮挡以及帧间时序不稳定性而仍然具有挑战性。基于锚点的方法（如Anchor3DLane）通过从多摄像头环绕视图中回归连续的3D车道曲线，已展现出强大的性能。然而，该基线模型仍存在以下问题：(i) 对回归异常值敏感，(ii) 全局曲线几何结构的监督较弱，(iii) 难以平衡多个损失项，以及(iv) 对时序连续性的利用有限。我们提出了Temporal-Anchor3DLane，这是一个增强的3D车道检测框架，在Anchor3DLane基础上引入三项关键改进：(1) 一组多任务损失优化，包括Balanced L1回归、Chamfer点集距离和基于不确定性的损失加权，以及用于分类和可见性预测的focal与Dice组件；(2) 一种轻量级的时序LSTM融合模块，用于跨帧聚合每个锚点的特征，替代了较重的Transformer式时序融合；(3) ESCOP风格的训练优化策略，将曲线级监督与时序一致性相结合。在OpenLane数据集上，Temporal-Anchor3DLane将F1分数提升了6.2，并生成更平滑的时序轨迹，表明小幅的架构与损失函数改进即可在无需额外传感器或扩大模型规模的情况下显著提升3D车道检测的鲁棒性。

</details>


### [3] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 本文提出一个离线优先的作物病害检测系统，专注于埃塞俄比亚提格雷地区本土仙人掌无花果（Opuntia ficus-indica）的病害识别。研究构建了包含3,587张田间图像的新数据集，并评估三种轻量级模型：自定义CNN、EfficientNet-Lite1 和 MobileViT-XS。结果表明，MobileViT-XS 凭借多头自注意力机制在准确率上最优（97.3%交叉验证准确率），而自定义CNN在推理速度和模型体积方面最具部署优势（42ms，4.8MB）。所有模型已集成至支持提格里尼亚语和阿姆哈拉语的Flutter应用中，可在低端ARM设备上完全离线运行。


<details>
  <summary>Details</summary>
Motivation: 提格雷地区80%以上人口依赖农业，但基础设施中断限制了专家诊断作物病害的可及性。因此亟需一种能在资源受限、冲突后环境中离线运行的本地化智能诊断工具，尤其针对当地重要作物仙人掌无花果。

Method: 构建包含3,587张田间图像的仙人掌无花果病害数据集，涵盖三类核心症状；在部署约束下评估三种移动端高效模型（自定义轻量CNN、EfficientNet-Lite1、MobileViT-XS）；将最优模型集成至支持提格里尼亚语和阿姆哈拉语的Flutter应用中，实现Cortex-A53级别设备上的完全离线推理。

Result: EfficientNet-Lite1测试准确率达90.7%；自定义CNN达89.5%，推理延迟仅42ms、模型大小4.8MB；MobileViT-XS交叉验证平均准确率达97.3%，其基于多头自注意力的全局推理能力显著优于CNN局部纹理核，在区分虫害簇与二维真菌病斑方面更可靠。

Conclusion: 结合Transformer架构的MobileViT-XS在准确率上表现最佳，而轻量CNN在部署效率上更具优势。所开发的多语言离线应用有效提升了边缘地区小农户获取关键作物病害诊断服务的能力，增强了粮食安全领域的包容性。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

Abstract (中文翻译): 农业支撑着埃塞俄比亚提格雷地区80%以上的人口，但基础设施中断限制了获取专家作物病害诊断的机会。我们提出了一个以离线优先的检测系统，核心是一个新构建的本土仙人掌无花果（Opuntia ficus-indica）数据集，包含3,587张田间图像，涵盖三类核心症状。鉴于在冲突后边缘环境中的部署限制，我们对三种移动端高效架构进行了基准测试：一种自定义轻量级CNN、EfficientNet-Lite1以及CNN-Transformer混合模型MobileViT-XS。尽管整个系统包含独立的马铃薯、苹果和玉米模块，但本研究聚焦于仙人掌无花果模型的性能，以单独评估注意力机制敏感性和归纳偏置在本土形态上的迁移效果。结果表明存在明显的帕累托权衡：EfficientNet-Lite1达到90.7%的测试准确率，轻量级CNN以最有利的部署特性（42毫秒推理延迟，4.8MB模型大小）达到89.5%的准确率，而MobileViT-XS则实现了97.3%的平均交叉验证准确率，表明基于多头自注意力（MHSA）的全局推理比局部纹理CNN卷积核更能可靠地区分虫害聚集与二维真菌病斑。这些ARM兼容模型已部署在一个支持提格里尼亚语和阿姆哈拉语的Flutter应用程序中，可在Cortex-A53级别设备上实现完全离线推理，从而加强了粮食安全关键诊断的包容性。

</details>


### [4] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 本文提出了一种用于小麦语义分割竞赛的自训练框架，结合两阶段混合训练策略和数据增强，基于SegFormer（MiT-B4）模型，通过迭代师生机制提升性能，在开发集和测试集上均取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 参加Global Wheat Full Semantic Segmentation Competition，解决小麦图像的全语义分割任务，提升模型在有限标注数据下的性能。

Method: 采用基于SegFormer（MiT-B4主干）的系统性自训练框架，结合两阶段混合训练策略、大量数据增强以及迭代式的师生学习机制。

Result: 在竞赛的Development和Testing Phase数据集上均取得了具有竞争力的性能表现。

Conclusion: 所提出的自训练框架能有效提升语义分割模型的准确率并充分利用数据，在小麦语义分割任务中表现优异。

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

Abstract (中文翻译): 本扩展摘要详细介绍了我们在全球小麦全语义分割竞赛中的解决方案。我们开发了一种系统性的自训练框架，该框架结合了两阶段混合训练策略与大量的数据增强方法。我们的核心模型是采用Mix Transformer（MiT-B4）作为主干网络的SegFormer。我们使用了一种迭代式的教师-学生循环机制，该机制能够逐步提升模型的准确性，并最大化数据的利用效率。我们的方法在开发阶段和测试阶段的数据集上均取得了具有竞争力的性能表现。

</details>


### [5] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: 本文在MinneApple密集苹果数据集上对比了零样本通用模型SAM3与三种微调后的YOLO11变体（nano、medium、large）在实例分割任务中的表现，发现IoU阈值选择显著影响性能评估；在IoU=0.15时YOLO整体F1更高，但SAM3在边界稳定性上远优于YOLO。


<details>
  <summary>Details</summary>
Motivation: 当前实例分割存在专用微调模型与通用零样本基础模型两种范式，缺乏系统性比较，尤其在高密度遮挡场景下。本文旨在厘清二者优劣，为实际应用提供选型依据。

Method: 在包含670张图像和28,179个标注苹果实例的MinneApple数据集上，评估零样本模式下的SAM3与三种微调YOLO11模型（nano/medium/large）的实例分割性能，重点分析不同IoU阈值对结果的影响，并比较F1分数与边界稳定性。

Result: 在IoU=0.15时，YOLO11各变体F1达68.9%–72.2%，优于SAM3的59.8%；但YOLO随IoU升高性能骤降48–50点，而SAM3仅降4点，显示其边界稳定性高出12倍。

Conclusion: YOLO11在检测完整性上占优，适合低IoU要求的密集检测；SAM3在掩码精度和边界稳定性上显著更强，更适合对分割质量要求高的零样本场景。研究提供了开源代码与方法建议，助力模型选型。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

Abstract (中文翻译): 深度学习推动了实例分割领域两种根本不同的范式：通过任务特定微调优化的专用模型，以及能够进行零样本分割的通用基础模型。本研究对以零样本模式运行的SAM3（Segment Anything Model，亦称SAMv3）与三种针对实例分割微调的Ultralytics YOLO11变体（nano、medium和large）进行了全面比较。评估在MinneApple数据集上进行，该数据集是一个密集基准，包含670张果园图像和28,179个标注的苹果实例，能够在高目标密度和遮挡条件下严格验证模型行为。我们的分析表明，IoU阈值的选择可能导致性能差距被夸大高达30%。在合适的IoU = 0.15阈值下，YOLO模型的F1分数分别达到68.9%、72.2%和71.9%，而纯零样本模式下的SAM3为59.8%。然而，YOLO在IoU范围内的性能急剧下降48–50个百分点，而SAM3仅下降4个百分点，显示出SAM3的边界稳定性高出12倍。这凸显了SAMv3在掩码精度方面的优势，而YOLO11则在检测完整性方面更具专长。我们提供了开源代码、评估流程和方法学建议，有助于深入理解在密集实例分割任务中何时应优先选择专用微调模型或通用基础模型。本项目代码库已在GitHub公开：https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [6] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: 本文提出 mmWeaver，一种基于隐式神经表示（INR）的新型框架，用于高效合成逼真、环境特定的毫米波雷达信号，显著提升下游任务性能并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在活动识别和姿态估计等应用中依赖多样且环境特定的信号数据集，但真实信号复杂、稀疏且高维，物理仿真计算开销大，因此需要一种高效生成逼真信号的方法。

Method: mmWeaver 利用隐式神经表示（INR）将毫米波信号建模为连续函数，并引入超网络，根据 RGB-D 图像提取的环境上下文和 MotionGPT 生成的人体动作特征动态生成 INR 参数，从而实现高效、自适应的信号合成。

Result: 实验表明，mmWeaver 在复杂 SSIM（0.88）和 PSNR（35 dB）指标上优于现有方法，活动识别准确率提升最多 7%，人体姿态估计误差最多降低 15%，且比基于仿真的方法快 6–35 倍，同时实现高达 49 倍的压缩率。

Conclusion: mmWeaver 能够高效生成高质量、环境自适应的毫米波信号，在保留关键相位信息的同时显著提升下游任务性能，为毫米波雷达应用的数据增强与仿真提供了实用且高效的解决方案。

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

Abstract (中文翻译): 逼真的信号生成和数据集增强对于推动毫米波雷达在活动识别和姿态估计等应用中的发展至关重要，这些应用高度依赖多样化且与环境相关的信号数据集。然而，毫米波信号本质上具有复杂性、稀疏性和高维度特性，使得物理仿真计算成本高昂。本文提出了 mmWeaver，一种新颖的框架，通过使用隐式神经表示（INR）将毫米波信号建模为连续函数，从而合成逼真且环境特定的复数毫米波信号，实现了最高达 49 倍的压缩率。mmWeaver 引入了超网络，能够根据从 RGB-D 图像中提取的环境上下文以及通过 MotionGPT 从文本生成的人体运动特征，动态生成 INR 参数，从而实现高效且自适应的信号合成。通过利用这些语义和几何先验信息，mmWeaver 能在多种分辨率下生成多样化的 I/Q 信号，并保留对点云估计和活动分类等下游任务至关重要的相位信息。大量实验表明，mmWeaver 在复杂 SSIM 上达到 0.88，PSNR 达到 35 dB，在信号逼真度方面优于现有方法，同时将活动识别准确率最多提升 7%，人体姿态估计误差最多降低 15%，并且运行速度比基于仿真的方法快 6 至 35 倍。

</details>


### [7] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: Hot Hém 是一个结合街景图像、语义分割和遥感数据的 GeoAI 工作流，用于在胡志明市估算行人热暴露并实现热感知路径规划。


<details>
  <summary>Details</summary>
Motivation: 标准路径算法常忽略热带高密度城市中微尺度热环境差异，而行人热暴露是重要的健康风险，因此需要更精细的热暴露建模方法。

Method: 该研究构建了一个空间数据科学流程：利用 Google 街景图像进行语义分割，结合遥感数据，训练两个 XGBoost 模型预测地表温度（LST），并在基于 OSMnx 提取的全部行人路网节点上以拼接方式部署模型，实现热感知路径规划。

Result: 模型成功部署于胡志明市各行政区（phường）的行人网络节点，可识别城市中热暴露显著偏高的走廊区域。

Conclusion: Hot Hém 为理解城市基础设施尺度下热暴露空间异质性提供了可操作的基础，有助于改善行人热健康风险应对策略。

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

Abstract (中文翻译): 行人热暴露是热带高密度城市中的重大健康风险，但标准路径规划算法通常忽略微尺度热环境差异。Hot Hém 是一种地理人工智能（GeoAI）工作流，用于估算并实现越南胡志明市（又称西贡）的行人热暴露评估。该空间数据科学流程融合了 Google 街景（GSV）图像、语义图像分割和遥感技术。研究在选定的行政街区（phường）中利用 GSV 数据集训练了两个 XGBoost 模型以预测地表温度（LST），并通过拼接方式将模型部署到所有基于 OSMnx 提取的行人路网节点上，从而支持热感知路径规划。该模型一旦部署，即可为精准识别城市中哪些廊道在基础设施尺度上经历异常高温，并深入理解其成因提供基础。

</details>


### [8] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 本文公开发布了利用无人机在异质、区域型城市交通条件下采集的微观车辆轨迹（MVT）数据集，提供高分辨率轨迹信息，支持仿真建模、安全评估和驾驶行为研究。


<details>
  <summary>Details</summary>
Motivation: 传统路侧视频在密集混合交通中常因遮挡、视角受限和不规则车流而失效，难以准确捕捉微观交通行为；因此需要一种更有效的数据采集方式以支持对复杂城市交通环境的研究。

Method: 使用无人机（UAV）从顶部视角采集交通视频，通过“Data from Sky”（DFS）平台提取微观车辆轨迹，并与人工计数、空间平均速度和探针轨迹进行验证；数据采自印度国家首都辖区六个路段中段位置，包含时间戳位置、速度、纵向/横向加速度及车辆分类，分辨率达每秒30帧。

Result: 数据集揭示了异质、区域型交通中的典型行为模式，如车道保持偏好、速度分布和横向变道行为；数据覆盖多样化的交通组成与密度水平。

Conclusion: 所发布的开放数据集为全球研究者提供了宝贵资源，有助于开发、测试和验证更能真实反映复杂城市交通环境的模型。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

Abstract (中文翻译): 本文提供了利用无人机（UAV）在异质、区域型城市交通条件下采集的开放式微观车辆轨迹（MVT）数据集。传统的路侧视频采集方法在密集混合交通中常因遮挡、视角受限以及不规则车辆运动而失效。基于无人机的录制方式可提供俯视视角，有效缓解上述问题，并捕捉丰富的时空动态特征。本文所述数据集通过“天空数据”（Data from Sky, DFS）平台提取，并已在先前研究中通过人工计数、空间平均速度和探针轨迹进行了验证。每个数据集均包含时间戳车辆位置、速度、纵向与横向加速度以及车辆分类信息，分辨率达到每秒30帧。数据采集自印度国家首都辖区的六个路段中段位置，涵盖多样化的交通组成与密度水平。探索性分析揭示了若干关键行为模式，包括车道保持偏好、速度分布以及在异质、区域型交通环境中典型的横向操纵行为。这些数据集旨在为全球科研社区提供资源，以支持在区域型交通条件下的仿真建模、安全评估和行为研究。通过公开发布这些实证数据集，本研究为研究人员提供了独特机会，以开发、测试和验证更能准确表征复杂城市交通环境的模型。

</details>


### [9] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 本文指出当前大视觉语言模型（LVLMs）在面对图像内误导性文本（排版攻击）时存在脆弱性，现有评估和防御方法过度强调忽略文本以提升鲁棒性，却忽视了现实场景中需同时理解物体与文本的需求。为此，作者提出新任务“读取或忽略VQA”（RIO-VQA）及配套基准RIO-Bench，通过构建同场景反事实样本（仅改变文本内容与问题类型），评估模型是否能根据上下文自适应地决定读取或忽略文本。实验表明现有LVLMs和防御方法难以兼顾排版鲁棒性与文本理解能力，而RIO-Bench支持一种新型数据驱动的自适应防御策略，超越了以往非自适应、一味忽略文本的方法。该工作揭示了当前评估范式与实际需求的根本错位，并为构建可靠LVLM提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 现有针对大视觉语言模型（LVLMs）排版攻击的评估和防御方法主要聚焦于物体识别任务，倾向于让模型忽略图像中的文本以提升鲁棒性。然而，真实世界的应用（如识别行人同时阅读交通标志）通常需要模型对图像中的物体和文本进行联合推理。这种现有方法与实际需求之间的错位促使作者提出新的评估任务和基准。

Method: 作者提出了“读取或忽略VQA”（Read-or-Ignore VQA, RIO-VQA）这一新任务，要求模型根据上下文自适应地决定在视觉问答中是读取还是忽略图像中的文本。为评估此能力，他们构建了RIO-Bench基准数据集，该数据集通过对同一真实图像仅修改其文本内容和问题类型，生成成对的“读取”和“忽略”反事实样本。此外，他们还利用该基准开发了一种新型的数据驱动防御方法，该方法能学习自适应地选择性使用文本。

Result: 通过RIO-Bench的评估发现，当前强大的LVLMs以及现有的防御方法都无法在抵抗排版攻击的鲁棒性和利用文本信息的能力之间取得良好平衡。相比之下，作者提出的基于RIO-Bench的数据驱动防御方法能够学习自适应地选择性使用文本，从而优于以往非自适应的、简单忽略文本的防御策略。

Conclusion: 本研究揭示了当前LVLM评估体系与现实世界需求之间存在的根本性错位。通过引入RIO-VQA任务和RIO-Bench基准，为评估和提升模型在需要联合理解物体与文本的复杂场景下的可靠性提供了一个原则性的新路径。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

Abstract (中文翻译): 大型视觉语言模型（LVLMs）容易受到排版攻击的影响，即图像中具有误导性的文本会覆盖模型的视觉理解能力。现有的评估协议和防御方法主要集中在物体识别上，隐含地鼓励模型忽略文本来实现鲁棒性；然而，现实世界的场景通常需要对物体和文本进行联合推理（例如，在识别行人的同时阅读交通标志）。为解决这一问题，我们引入了一项新任务——“读取或忽略VQA”（RIO-VQA），该任务在视觉问答（VQA）中形式化了对文本的选择性使用：模型必须根据上下文决定何时读取文本，何时忽略文本。为了进行评估，我们提出了“读取或忽略基准”（RIO-Bench），这是一个标准化的数据集和协议，它通过对每张真实图像仅改变其文本内容和问题类型，提供同场景的反事实样本（读取/忽略）。利用RIO-Bench，我们证明了强大的LVLMs和现有防御方法无法在排版鲁棒性和文本阅读能力之间取得平衡，凸显了改进方法的必要性。最后，RIO-Bench支持一种新颖的数据驱动防御方法，该方法能学习自适应地选择性使用文本，超越了以往非自适应的、忽略文本的防御策略。总体而言，这项工作揭示了现有评估范围与现实需求之间的根本错位，为构建可靠的LVLMs提供了原则性的路径。我们的项目页面位于 https://turingmotors.github.io/rio-vqa/。

</details>


### [10] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: CLARGA 是一种通用的多模态融合架构，能自适应地融合任意数量和类型的模态，具有高效性、对缺失模态的鲁棒性，并在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态融合方法通常针对特定模态组合设计，难以泛化到不同数量或类型的模态，且在处理缺失模态或提升跨模态一致性方面存在不足。

Method: CLARGA 通过为每个样本构建基于注意力加权的特征图，并利用多头图注意力网络（GAT）在图上传递消息，实现模态间的动态交互。该方法具有次二次复杂度，并通过可学习掩码处理缺失模态。模型采用结合监督任务损失与对比 InfoNCE 损失的混合目标进行训练。

Result: 在涵盖金融、人机交互、多媒体分类和情感计算等领域的7个数据集上，CLARGA 均优于基线模型、SOTA 方法及消融变体，同时展现出对缺失输入的鲁棒性和在小众任务上的优异性能。

Conclusion: CLARGA 是一种高效、通用且鲁棒的多模态表示学习架构，可轻松集成到各类机器学习模型中，适用于广泛的任务场景。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

Abstract (中文翻译): 我们提出了 CLARGA，这是一种用于多模态表示学习的通用多模态融合架构，能够处理任意数量和类型的模态，而无需更改底层框架。给定一个有监督数据集，CLARGA 可应用于几乎任何机器学习任务，以融合不同的多模态表示供下游层处理。CLARGA 在逐样本的基础上，通过在其特征上构建注意力加权图，并利用多头图注意力网络（Graph Attention Network）在此图上传递消息，从而学习模态之间应如何相互影响。这种方法不仅使 CLARGA 具有高度自适应性（因为它为不同样本构建独特的图），而且随着模态数量的增加，其融合效率也较高，复杂度低于二次方。此外，通过一个可学习的掩码，它还能适应缺失的模态输入。该模型采用混合目标进行训练，结合了有监督任务损失和对比 InfoNCE 损失，从而提升了跨模态一致性和对噪声输入的鲁棒性。我们在7个涵盖金融、人机交互、通用多媒体分类和情感计算等领域的多模态表示学习任务上验证了 CLARGA 的有效性。其性能始终优于基线模型、当前最先进的模型以及消融实验变体。额外的实验还表明，该方法对缺失输入具有鲁棒性，并能在小众任务上表现出色。总体而言，CLARGA 可轻松嵌入各类机器学习模型中，以高效且有效地学习适用于广泛任务的多模态表示。

</details>
