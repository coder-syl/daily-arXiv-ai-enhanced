<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HyperCLOVA X 32B Think](https://arxiv.org/abs/2601.03286)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CV

TL;DR: 本文提出了 HyperCLOVA X 32B Think，一个专注于韩语语境推理与智能体能力的视觉-语言模型，在多项韩语及多模态任务中表现优异，并已开源以促进研究与应用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对韩语语言文化背景优化、同时具备强大多模态理解与智能体行为能力的大模型，因此需要开发一个在韩语推理、视觉-语言理解和智能体任务上均表现优异的开源模型。

Method: 模型首先进行以推理能力为重点的预训练，随后通过后训练增强其多模态理解、高级推理、智能体行为以及与人类偏好的对齐。

Result: 在与同类规模模型的对比实验中，HyperCLOVA X 32B Think 在韩语文本到文本、视觉到文本基准测试以及面向智能体的评估任务中均取得了优异性能。

Conclusion: HyperCLOVA X 32B Think 是一个在韩语和多模态推理方面具有竞争力的开源视觉-语言模型，其发布有助于推动学术界和工业界在相关领域的进一步研究与创新。

Abstract: In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.

Abstract (中文翻译): 在本报告中，我们提出了 HyperCLOVA X 32B Think，这是一种视觉-语言模型，特别注重在韩语语言和文化背景下的推理能力以及智能体行为。该模型首先经过以推理能力为重点的预训练，随后通过后训练支持多模态理解、增强推理、智能体行为以及与人类偏好的对齐。在与规模相当的模型进行的实验评估中，我们的模型在韩语文本到文本、视觉到文本基准测试以及面向智能体的评估任务中均表现出色。通过开源 HyperCLOVA X 32B Think，我们旨在支持更广泛的应用，并促进学术界和工业界在相关领域的进一步研究与创新。

</details>


### [2] [CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception](https://arxiv.org/abs/2601.03302)
*Mohammad Rostami,Atik Faysal,Hongtao Xia,Hadi Kasasbeh,Ziang Gao,Huaxia Wang*

Main category: cs.CV

TL;DR: CageDroneRF (CDRF) is a large-scale, real-world and synthetically augmented RF drone detection benchmark that addresses dataset scarcity and diversity issues, offering standardized tools for classification, open-set recognition, and object detection.


<details>
  <summary>Details</summary>
Motivation: Existing RF datasets for drone detection suffer from limited scale and diversity, hindering the development of robust and generalizable perception models.

Method: CDRF combines extensive real-world RF recordings with a principled synthetic augmentation pipeline that controls SNR, injects interference, and applies frequency shifts with consistent label transformations; it also provides open-source tools for data handling and evaluation.

Result: The dataset covers diverse contemporary drone models and acquisition conditions not found in current public datasets, enabling standardized and reproducible benchmarking across multiple RF perception tasks.

Conclusion: By releasing CDRF and its associated tooling, the authors aim to foster progress in developing robust and generalizable RF-based drone detection and identification systems.

Abstract: We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models.

Abstract (中文翻译): 我们提出了CageDroneRF（CDRF），这是一个用于射频（RF）无人机检测与识别的大规模基准数据集，基于真实世界采集的数据以及系统生成的合成变体构建而成。CDRF通过将大量原始录音与一套原则性的增强流程相结合，解决了现有RF数据集稀缺且多样性不足的问题。该增强流程（i）精确控制信噪比（SNR），（ii）注入干扰信号源，（iii）在保持标签一致性的前提下进行频率偏移并相应调整检测用的边界框。该数据集涵盖了当前多种主流无人机型号（其中许多在现有公开数据集中不可获取）以及多样化的采集条件，数据来源于罗文大学校园及受控的射频屏蔽笼设施。CDRF同时发布了可互操作的开源工具，支持数据生成、预处理、增强和评估，并兼容现有公开基准。CDRF为分类、开集识别和目标检测任务提供了标准化的评测基准，支持严谨的模型比较和可复现的处理流程。通过发布这一全面的基准数据集及相关工具，CDRF旨在加速鲁棒且可泛化的射频感知模型的研究进展。

</details>


### [3] [Mass Concept Erasure in Diffusion Models with Concept Hierarchy](https://arxiv.org/abs/2601.03305)
*Jiahang Tu,Ye Li,Yiming Wu,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.CV

TL;DR: 本文提出一种基于超类型-子类型层级结构的概念擦除方法，通过将语义相近的子概念（如不同鸟类）归入同一超类型（如“鸟”），共享一组可学习参数进行联合擦除，从而提升效率与效果；同时引入 SuPLoRA 方法，在冻结下投影矩阵中编码超类型信息，仅更新上投影矩阵，以缓解因过度擦除导致的生成质量下降，并在多领域混合概念擦除的新基准上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型中的概念擦除方法在面对多个需擦除概念时效率低下且易损害整体生成质量，因为每个概念需单独微调参数，缺乏对语义相关性的利用。

Method: 构建超类型-子类型概念层级结构，将语义相近的子概念归入同一超类型，并通过共享参数实现组级联合擦除；提出 SuPLoRA 方法，在低秩适配中冻结下投影矩阵以保留超类型信息，仅更新上投影矩阵；在擦除阶段对未遮蔽区域施加标准扩散正则化。

Result: 所提方法在包含名人、物体和色情内容等多领域混合概念的同时擦除任务中表现优异，有效缓解了生成性能退化问题。

Conclusion: 通过引入概念层级结构和 SuPLoRA 机制，实现了高效、高质量的多概念联合擦除，为大规模安全内容控制提供了新思路。

Abstract: The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.

Abstract (中文翻译): 扩散模型的成功引发了对其生成不安全或有害内容的担忧，促使研究者提出概念擦除方法，通过微调模块来抑制特定概念，同时保留模型的一般生成能力。然而，随着需擦除概念数量的增加，这些方法往往变得低效且效果不佳，因为每个概念都需要单独的一组微调参数，并可能损害整体生成质量。本文提出一种超类型-子类型概念层级结构，将被擦除的概念组织为父子结构：每个被擦除的概念作为子节点，语义相关的概念（例如金刚鹦鹉和白头鹰）被归入一个共享的父节点，即超类型概念（例如“鸟”）。我们不再逐个擦除概念，而是引入一种高效且有效的组级抑制方法，将语义相似的概念分组并共享一组可学习参数进行联合擦除。在擦除阶段，对未遮蔽区域应用标准扩散正则化以保持去噪过程。为缓解因过度擦除语义相关子类型而导致的超类型生成能力下降，我们提出一种新方法——超类型保留低秩适配（SuPLoRA），该方法在冻结的下投影矩阵中编码超类型概念信息，并仅在擦除过程中更新上投影矩阵。理论分析证明了 SuPLoRA 在减轻生成性能退化方面的有效性。我们构建了一个更具挑战性的基准，要求同时擦除来自多个领域的概念，包括名人、物体和色情内容。

</details>


### [4] [VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models](https://arxiv.org/abs/2601.03309)
*Jianke Zhang,Xiaoyu Chen,Qiuyue Wang,Mingsheng Li,Yanjiang Guo,Yucheng Hu,Jiajun Zhang,Shuai Bai,Junyang Lin,Jianyu Chen*

Main category: cs.CV

TL;DR: 本文通过提出VLM4VLA这一轻量级适配框架，系统研究了视觉-语言模型（VLM）的选择与能力如何影响其在视觉-语言-动作（VLA）策略中的下游表现。研究发现：1）VLM初始化虽优于从头训练，但其通用能力并不能可靠预测下游任务性能；2）在特定具身任务上微调VLM未必提升控制性能；3）VLM中的视觉模块是性能瓶颈，向其注入控制相关监督信号可显著提升效果，即使该模块在下游微调中被冻结。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对VLM选择及其能力如何影响下游VLA策略性能的系统性分析。尽管VLA模型广泛采用预训练VLM作为策略主干，但VLM的通用能力是否足以支撑具身控制任务仍不明确。

Method: 提出VLM4VLA——一种极简适配流程，仅引入少量可学习参数将通用VLM转换为VLA策略，从而实现公平高效的比较。在此基础上，开展三类基准、多项任务的广泛实验，包括VLM初始化效果评估、七种具身辅助任务微调、以及模态级消融分析。

Result: 1）VLM初始化始终优于从头训练，但其通用能力与下游性能相关性弱；2）针对特定具身技能（如具身问答、视觉指向等）微调VLM，并未带来一致的控制性能提升；3）视觉模块是主要瓶颈，向其注入控制相关监督（即使冻结）可稳定提升下游性能。

Conclusion: 标准VLM的预训练目标与具身动作规划需求之间存在持续的领域差距。仅靠VLM的通用视觉-语言能力不足以支撑高效控制，需在视觉编码器中引入面向控制的监督信号以弥合这一差距。

Abstract: Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.

Abstract (中文翻译): 视觉-语言-动作（VLA）模型通过将预训练的大规模视觉-语言模型（VLM）整合到其策略主干中，因其出色的泛化能力而受到广泛关注。本文重新审视了一个基础但鲜有系统研究的问题：VLM的选择及其能力如何转化为下游VLA策略的性能？我们提出了VLM4VLA，这是一种极简的适配流程，仅使用少量新增的可学习参数即可将通用VLM转换为VLA策略，从而实现公平且高效的比较。尽管结构简单，VLM4VLA的表现却出人意料地与更复杂的网络设计相当。通过对三个基准上多种下游任务的大量实证研究，我们发现：虽然VLM初始化相比从头训练始终带来一致收益，但VLM的通用能力却难以有效预测其下游任务表现。这一发现挑战了普遍假设，表明标准VLM能力对于有效的具身控制而言是必要但不充分的。我们进一步通过在七项辅助具身任务（如具身问答、视觉指向、深度估计等）上微调VLM，探究特定具身能力的影响。与直觉相反，提升VLM在特定具身技能上的表现并不能保证下游控制性能的提升。最后，模态级消融实验指出，VLM中的视觉模块而非语言组件是主要的性能瓶颈。我们证明，向VLM的视觉编码器注入控制相关的监督信号能够带来稳定的性能增益，即使该编码器在下游微调阶段保持冻结。这揭示了当前VLM预训练目标与具身动作规划需求之间存在的持续性领域差距。

</details>


### [5] [Deep Learning-Based Image Recognition for Soft-Shell Shrimp Classification](https://arxiv.org/abs/2601.03317)
*Yun-Hao Zhang,I-Hsien Ting,Dario Liberona,Yun-Hsiu Liu,Kazunori Minetaki*

Main category: cs.CV

TL;DR: 本文利用深度学习图像识别技术，通过CNN模型对刚捕捞的白虾进行自动分类，以提升分选效率与准确性，从而保持虾的新鲜度并满足消费者对产品外观和品质的需求。


<details>
  <summary>Details</summary>
Motivation: 随着消费者对高品质水产品需求的增加，虾类产品在收获后新鲜度迅速下降，且软壳虾在烹饪或冷冻后易出现头身分离问题，影响外观和消费者接受度，亟需高效准确的分选方法。

Method: 采用基于卷积神经网络（CNN）的深度学习图像识别技术，对刚捕捞的白虾进行自动化分类，替代传统人工分选。

Result: 该方法提高了白虾分类的准确性、效率和一致性，缩短了处理时间，有助于维持虾的新鲜度。

Conclusion: 基于深度学习的图像识别技术可有效应用于水产加工环节，提升产品质量与客户满意度，具有良好的应用前景。

Abstract: With the integration of information technology into aquaculture, production has become more stable and continues to grow annually. As consumer demand for high-quality aquatic products rises, freshness and appearance integrity are key concerns. In shrimp-based processed foods, freshness declines rapidly post-harvest, and soft-shell shrimp often suffer from head-body separation after cooking or freezing, affecting product appearance and consumer perception. To address these issues, this study leverages deep learning-based image recognition for automated classification of white shrimp immediately after harvest. A convolutional neural network (CNN) model replaces manual sorting, enhancing classification accuracy, efficiency, and consistency. By reducing processing time, this technology helps maintain freshness and ensures that shrimp transportation businesses meet customer demands more effectively.

Abstract (中文翻译): 随着信息技术融入水产养殖业，生产变得更加稳定并逐年增长。随着消费者对高品质水产品需求的提升，产品的新鲜度和外观完整性成为关键关注点。在虾类加工食品中，虾在收获后新鲜度迅速下降，而软壳虾在烹饪或冷冻后常出现头身分离现象，影响产品外观及消费者感知。为解决这些问题，本研究利用基于深度学习的图像识别技术，在白虾刚捕捞后即进行自动分类。通过卷积神经网络（CNN）模型替代人工分选，提高了分类的准确性、效率和一致性。该技术通过缩短处理时间，有助于保持虾的新鲜度，并使虾类运输企业更有效地满足客户需求。

</details>


### [6] [Higher order PCA-like rotation-invariant features for detailed shape descriptors modulo rotation](https://arxiv.org/abs/2601.03326)
*Jarek Duda*

Main category: cs.CV

TL;DR: 该论文提出将主成分分析（PCA）推广到高阶中心矩张量或多项式乘以高斯函数的形式，以构建任意精度的旋转不变形状描述符，并应用于分子形状描述、旋转不变目标识别和高效形状相似性度量。


<details>
  <summary>Details</summary>
Motivation: 传统PCA仅利用二阶协方差矩阵近似形状为椭球体，难以准确刻画复杂真实形状；作者旨在通过引入高阶张量或更复杂的函数形式提升形状描述的精度，同时保持旋转不变性。

Method: 将PCA扩展至三阶及以上中心矩张量（如 $p_{abc}=E[(x_a-E[x_a])(x_b-E[x_b])(x_c-E[x_c])]$），或采用多项式乘以高斯函数的形式，构造可解码且任意精度的形状描述符，并推导其对应的旋转不变量（如幂迹等）。

Result: 所提出的方法能够生成高精度的旋转不变特征，适用于分子形状描述、2D/3D图像中旋转不变目标识别，以及无需昂贵旋转优化即可快速比较形状相似性的度量。

Conclusion: 通过高阶矩或多项式-高斯组合方式，可以有效构建兼具高精度与旋转不变性的形状描述符，在多个实际应用中具有显著优势。

Abstract: PCA can be used for rotation invariant features, describing a shape with its $p_{ab}=E[(x_i-E[x_a])(x_b-E[x_b])]$ covariance matrix approximating shape by ellipsoid, allowing for rotation invariants like its traces of powers. However, real shapes are usually much more complicated, hence there is proposed its extension to e.g. $p_{abc}=E[(x_a-E[x_a])(x_b-E[x_b])(x_c-E[x_c])]$ order-3 or higher tensors describing central moments, or polynomial times Gaussian allowing decodable shape descriptors of arbitrarily high accuracy, and their analogous rotation invariants. Its practical applications could be rotation-invariant features to include shape modulo rotation e.g. for molecular shape descriptors, or for up to rotation object recognition in 2D images/3D scans, or shape similarity metric allowing their inexpensive comparison (modulo rotation) without costly optimization over rotations.

Abstract (中文翻译): 主成分分析（PCA）可用于构建旋转不变特征，通过其协方差矩阵 $p_{ab}=E[(x_i-E[x_a])(x_b-E[x_b])]$ 将形状近似为椭球体，并利用其幂迹等构造旋转不变量。然而，真实形状通常更为复杂，因此本文提出将其扩展至三阶或更高阶张量（例如 $p_{abc}=E[(x_a-E[x_a])(x_b-E[x_b])(x_c-E[x_c])]$）以描述中心矩，或采用多项式乘以高斯函数的形式，从而获得可解码且任意高精度的形状描述符及其相应的旋转不变量。该方法的实际应用包括用于分子形状描述的旋转不变特征、2D图像或3D扫描中对旋转不敏感的目标识别，以及一种可在无需昂贵旋转优化的情况下高效比较形状相似性的度量方法。

</details>


### [7] [MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.03331)
*Yang Shi,Yifeng Xie,Minzhe Guo,Liangsi Lu,Mingxuan Huang,Jingchao Wang,Zhihong Zhu,Boyan Xu,Zhiqi Huang*

Main category: cs.CV

TL;DR: 本文提出了MMErroR，一个包含2013个样本的多模态基准数据集，用于评估视觉语言模型（VLMs）是否能检测并分类推理过程中的错误类型。实验显示，即使最先进的模型（如Gemini-3.0-Pro）也仅在66.47%的情况下正确识别错误，表明该任务极具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）虽在多模态任务中表现优异，但尚不清楚其是否真正理解所处理内容，尤其是能否识别推理过程中的错误及其类型。因此，需要一个专注于推理错误检测与分类的新基准。

Method: 构建MMErroR基准，包含2013个样本，每个样本嵌入一种连贯的推理错误，覆盖六大领域下的24个子领域。该基准要求模型在视觉和语言上下文中检测错误推理并分类错误类型，强调过程级而非答案正确性的评估。

Result: 对20个先进VLM进行评估，表现最好的Gemini-3.0-Pro模型仅在66.47%的样本中正确识别错误类型，表明当前模型在推理错误识别方面仍存在显著不足。

Conclusion: 准确识别推理错误的能力是衡量多模态推理模型理解深度的重要指标；MMErroR为未来研究提供了新的评估方向和挑战。

Abstract: Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io

Abstract (中文翻译): 视觉语言模型（VLMs）的最新进展提升了多模态学习的性能，但随之而来的问题是：这些模型是否真正理解其所处理的内容？尤其关键的是，VLMs能否检测到推理过程中的错误并识别其错误类型？为回答这一问题，我们提出了MMErroR——一个包含2,013个样本的多模态基准数据集，每个样本均嵌入一种连贯的推理错误。这些样本涵盖六大顶级领域下的24个子领域，确保了广泛的覆盖范围和丰富的分类结构。与现有侧重于答案正确性的基准不同，MMErroR聚焦于过程层面、以错误为中心的评估，要求模型在视觉和语言上下文中检测错误推理并对其错误类型进行分类。我们评估了20个先进的VLM，结果显示，即便是表现最佳的模型（Gemini-3.0-Pro）也仅在66.47%的情况下正确分类错误类型，凸显了识别错误推理的难度。此外，准确识别错误的能力为深入理解多模态推理模型的能力提供了宝贵洞见。

</details>


### [8] [RelightAnyone: A Generalized Relightable 3D Gaussian Head Model](https://arxiv.org/abs/2601.03357)
*Yingyan Xu,Pramod Rao,Sebastian Weiss,Gaspard Zoss,Markus Gross,Christian Theobalt,Marc Habermann,Derek Bradley*

Main category: cs.CV

TL;DR: 提出了一种通用可重光照3D高斯头部模型，仅需单视图或多视图图像（无需OLAT光照数据）即可实现高质量重光照，并支持从单张图像拟合新对象。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅（3DGS）方法在对数字人头像进行重光照时，通常需要复杂的时分复用照明（如OLAT）采集数据，限制了其通用性和实用性。因此，亟需一种无需OLAT数据即可实现高质量重光照的通用方法。

Method: 提出两阶段模型：第一阶段在无OLAT照明的多视图数据上建模平光3DGS头像，并学习数据集特定的光照编码以实现自监督光照对齐；第二阶段在较小规模的OLAT数据集上学习从平光3DGS参数到基于物理反射率参数的映射，从而实现高质量重光照。

Result: 该方法能泛化到任意未见过的对象，即使只提供单张图像也能完成拟合并实现逼真的新视角合成与重光照效果，表现如同该对象曾使用OLAT采集过一样。

Conclusion: 所提出的通用可重光照3D高斯头像模型显著降低了对复杂光照采集的依赖，提升了3D数字人头像在实际应用中的灵活性和可扩展性。

Abstract: 3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars. A major challenge is to relight the avatars to match any scene illumination. For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT). We propose a new generalized relightable 3D Gaussian head model that can relight any subject observed in a single- or multi-view images without requiring OLAT data for that subject. Our core idea is to learn a mapping from flat-lit 3DGS avatars to corresponding relightable Gaussian parameters for that avatar. Our model consists of two stages: a first stage that models flat-lit 3DGS avatars without OLAT lighting, and a second stage that learns the mapping to physically-based reflectance parameters for high-quality relighting. This two-stage design allows us to train the first stage across diverse existing multi-view datasets without OLAT lighting ensuring cross-subject generalization, where we learn a dataset-specific lighting code for self-supervised lighting alignment. Subsequently, the second stage can be trained on a significantly smaller dataset of subjects captured under OLAT illumination. Together, this allows our method to generalize well and relight any subject from the first stage as if we had captured them under OLAT lighting. Furthermore, we can fit our model to unseen subjects from as little as a single image, allowing several applications in novel view synthesis and relighting for digital avatars.

Abstract (中文翻译): 3D高斯泼溅（3DGS）已成为重建和渲染逼真3D头部化身的标准方法。一个主要挑战是如何对这些化身进行重光照，使其匹配任意场景照明。为了实现高质量的重光照，现有方法通常要求在复杂的时分复用照明（例如一次一灯，OLAT）下采集对象数据。我们提出了一种新的通用可重光照3D高斯头部模型，该模型能够对仅通过单视图或多视图图像观察到的任意对象进行重光照，而无需该对象的OLAT数据。我们的核心思想是学习一个从平光3DGS化身到对应可重光照高斯参数的映射。该模型包含两个阶段：第一阶段在没有OLAT照明的情况下建模平光3DGS化身；第二阶段则学习映射到基于物理的反射参数，以实现高质量重光照。这种两阶段设计使我们能够在大量不含OLAT照明的现有多视图数据集上训练第一阶段，确保跨对象泛化能力，并通过学习数据集特定的光照编码实现自监督光照对齐。随后，第二阶段可在规模小得多的OLAT采集数据集上进行训练。结合两者，我们的方法能够很好地泛化，并对第一阶段中的任意对象进行重光照，效果如同该对象曾使用OLAT采集过一样。此外，我们的模型甚至可以从单张图像拟合未见过的对象，为数字化身的新视角合成与重光照提供了多种应用可能。

</details>


### [9] [Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views](https://arxiv.org/abs/2601.03362)
*Xiang Zhang,Yang Zhang,Lukas Mehl,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: 本文提出HairGuard框架，通过新数据处理流程和深度修复网络，有效提升3D视觉任务中软边界（如细发）区域的细节恢复能力，在深度估计、立体转换和新视角合成方面达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 软边界（如细发）在自然与合成图像中普遍存在，但由于前景与背景信息混杂，给3D视觉任务带来挑战，现有方法难以精确恢复其细节。

Method: 提出名为HairGuard的框架：1）利用图像抠图数据集构建训练数据；2）设计带门控残差模块的深度修复网络，精准优化软边界区域深度；3）在视图合成中采用基于深度的前向扭曲保留纹理，并用生成式场景绘制器填补遮挡区域；4）通过颜色融合模块自适应整合结果。

Result: 在单目深度估计、立体图像/视频转换和新视角合成任务中均取得SOTA性能，尤其在软边界区域有显著提升。

Conclusion: HairGuard能有效解决3D视觉中软边界细节恢复难题，具备即插即用特性，可广泛集成于现有深度模型，提升整体几何一致性和细节保真度。

Abstract: Soft boundaries, like thin hairs, are commonly observed in natural and computer-generated imagery, but they remain challenging for 3D vision due to the ambiguous mixing of foreground and background cues. This paper introduces Guardians of the Hair (HairGuard), a framework designed to recover fine-grained soft boundary details in 3D vision tasks. Specifically, we first propose a novel data curation pipeline that leverages image matting datasets for training and design a depth fixer network to automatically identify soft boundary regions. With a gated residual module, the depth fixer refines depth precisely around soft boundaries while maintaining global depth quality, allowing plug-and-play integration with state-of-the-art depth models. For view synthesis, we perform depth-based forward warping to retain high-fidelity textures, followed by a generative scene painter that fills disoccluded regions and eliminates redundant background artifacts within soft boundaries. Finally, a color fuser adaptively combines warped and inpainted results to produce novel views with consistent geometry and fine-grained details. Extensive experiments demonstrate that HairGuard achieves state-of-the-art performance across monocular depth estimation, stereo image/video conversion, and novel view synthesis, with significant improvements in soft boundary regions.

Abstract (中文翻译): 软边界（如细发）在自然图像和计算机生成图像中十分常见，但由于前景与背景线索的模糊混合，它们在3D视觉任务中仍具挑战性。本文提出了“发丝守护者”（HairGuard）框架，旨在恢复3D视觉任务中细粒度的软边界细节。具体而言，我们首先提出了一种新颖的数据整理流程，利用图像抠图数据集进行训练，并设计了一个深度修复网络以自动识别软边界区域。该网络通过门控残差模块，在保持全局深度质量的同时，精准优化软边界周围的深度，从而实现与当前先进深度模型的即插即用集成。在视图合成方面，我们采用基于深度的前向扭曲以保留高保真纹理，随后使用一个生成式场景绘制器填充因视角变化而产生的空洞区域，并消除软边界内部的冗余背景伪影。最后，颜色融合模块自适应地结合扭曲与修复结果，生成具有几何一致性与细粒度细节的新视角图像。大量实验表明，HairGuard在单目深度估计、立体图像/视频转换以及新视角合成等多个任务上均达到领先水平，尤其在软边界区域表现显著优于现有方法。

</details>


### [10] [RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models](https://arxiv.org/abs/2601.03369)
*Sha Luo,Yogesh Prabhu,Tim Ossowski,Kaiping Chen,Junjie Hu*

Main category: cs.CV

TL;DR: 本文提出新基准RiskCueBench，聚焦于从视频中最早的风险信号预测未来危险事件，揭示现有模型在此任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频风险评估数据集通常包含完整视频（含事故本身），难以反映现实场景中仅凭早期视觉线索预测风险的挑战。

Method: 构建新视频理解基准RiskCueBench，对视频进行精细标注，标识出最早预示潜在安全问题的“风险信号片段”。

Result: 实验表明当前系统在基于早期视觉信号解读动态情境并预测未来风险事件方面存在显著性能差距。

Conclusion: 该研究凸显了在实际部署视频风险预测模型时所面临的重要挑战，强调需提升模型对早期风险线索的理解能力。

Abstract: With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.

Abstract (中文翻译): 随着以视频为中心的社交媒体迅速发展，从视觉数据中预测风险事件成为保障公共安全和预防现实事故的一个有前景的方向。以往的研究已在驾驶、抗议活动和自然灾害等多个领域广泛探讨了监督式视频风险评估。然而，许多现有数据集向模型提供了包含事故本身的完整视频序列，这大大降低了任务难度。为更好地反映现实条件，我们提出了一个新的视频理解基准RiskCueBench，其中视频经过仔细标注，以识别出“风险信号片段”——即最早表明潜在安全隐患的时间点。实验结果揭示了当前系统在解读动态情境并从早期视觉信号中预测未来风险事件方面存在显著差距，突显了在实践中部署视频风险预测模型所面临的重要挑战。

</details>
