{"id": "2512.13731", "pdf": "https://arxiv.org/pdf/2512.13731", "abs": "https://arxiv.org/abs/2512.13731", "authors": ["Weikang Bai", "Yongkun Du", "Yuchen Su", "Yazhen Xie", "Zhineng Chen"], "title": "Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCMER-Bench\u57fa\u51c6\u3001\u5927\u89c4\u6a21\u590d\u6742\u6570\u5b66\u8868\u8fbe\u5f0f\u6570\u636e\u96c6MER-17M\u548cCMER-3M\uff0c\u4ee5\u53ca\u65b0\u8868\u793a\u65b9\u6cd5Structured Mathematical Language\u548c\u4e13\u7528\u6a21\u578bCMERNet\uff0c\u5728\u590d\u6742\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08MER\uff09\u5728\u5904\u7406\u5305\u542b\u5927\u91cf\u7b26\u53f7\u548c\u591a\u884c\u7ed3\u6784\u7684\u590d\u6742\u8868\u8fbe\u5f0f\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u73b0\u6709\u516c\u5f00\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7b80\u5355\u6837\u672c\u5360\u4e3b\u5bfc\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u8868\u8fbe\u5f0f\u7684\u8986\u76d6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u6309\u96be\u5ea6\u5206\u7ea7\u7684CMER-Bench\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u5f3a\u8c03\u590d\u6742\u8868\u8fbe\u5f0f\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6MER-17M\u548cCMER-3M\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u8fbe\u5f0f\u5206\u8bcd\u5668\u548c\u540d\u4e3aStructured Mathematical Language\u7684\u7ed3\u6784\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u663e\u5f0f\u5efa\u6a21\u8868\u8fbe\u5f0f\u7684\u5c42\u6b21\u4e0e\u7a7a\u95f4\u5e03\u5c40\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u4e13\u7528\u6a21\u578bCMERNet\uff0c\u5e76\u5728CMER-3M\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u542b1.25\u4ebf\u53c2\u6570\u7684CMERNet\u5728CMER-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684MER\u6a21\u578b\u548c\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u8868\u8fbe\u5f0f\u8bc6\u522b\u4efb\u52a1\u4e0a\u3002", "conclusion": "\u9488\u5bf9\u590d\u6742\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u7684\u6311\u6218\uff0c\u672c\u6587\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u57fa\u51c6\u4e0e\u6570\u636e\u96c6\u3001\u63d0\u51fa\u7ed3\u6784\u5316\u8868\u793a\u65b9\u6cd5\u53ca\u4e13\u7528\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86MER\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "summary_cn": "\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08MER\uff09\u5728\u8bc6\u522b\u7b80\u5355\u8868\u8fbe\u5f0f\u65b9\u9762\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5bf9\u5305\u542b\u5927\u91cf\u7b26\u53f7\u548c\u591a\u884c\u7ed3\u6784\u7684\u590d\u6742\u6570\u5b66\u8868\u8fbe\u5f0f\u7684\u9c81\u68d2\u8bc6\u522b\u4ecd\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86CMER-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u7cbe\u5fc3\u6784\u5efa\u7684\u57fa\u51c6\uff0c\u5c06\u8868\u8fbe\u5f0f\u5212\u5206\u4e3a\u6613\u3001\u4e2d\u3001\u96be\u4e09\u4e2a\u96be\u5ea6\u7b49\u7ea7\u3002\u5229\u7528CMER-Bench\uff0c\u6211\u4eec\u5bf9\u73b0\u6709\u7684MER\u6a21\u578b\u548c\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u6613\u3001\u4e2d\u7b49\u96be\u5ea6\u8868\u8fbe\u5f0f\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u8868\u8fbe\u5f0f\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u73b0\u6709\u516c\u5f00\u8bad\u7ec3\u6570\u636e\u96c6\u4e3b\u8981\u7531\u7b80\u5355\u6837\u672c\u6784\u6210\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MER-17M\u548cCMER-3M\u4e24\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91cd\u70b9\u5f3a\u8c03\u590d\u6742\u6570\u5b66\u8868\u8fbe\u5f0f\u7684\u8bc6\u522b\uff0c\u4e3a\u5f00\u53d1\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u7684\u590d\u6742MER\u6a21\u578b\u63d0\u4f9b\u4e30\u5bcc\u591a\u6837\u7684\u6837\u672c\u652f\u6301\u3002\u6b64\u5916\uff0c\u4e3a\u5e94\u5bf9\u590d\u6742\u8868\u8fbe\u5f0f\u590d\u6742\u7a7a\u95f4\u5e03\u5c40\u5e26\u6765\u7684\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8868\u8fbe\u5f0f\u5206\u8bcd\u5668\u548c\u4e00\u79cd\u79f0\u4e3a\u201c\u7ed3\u6784\u5316\u6570\u5b66\u8bed\u8a00\u201d\uff08Structured Mathematical Language\uff09\u7684\u65b0\u8868\u793a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u5f0f\u5efa\u6a21\u8d85\u8d8aLaTeX\u683c\u5f0f\u7684\u8868\u8fbe\u5f0f\u5c42\u6b21\u7ed3\u6784\u4e0e\u7a7a\u95f4\u5173\u7cfb\u3002\u57fa\u4e8e\u4e0a\u8ff0\u6210\u679c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMERNet\u7684\u4e13\u7528\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5e76\u5728CMER-3M\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u542b1.25\u4ebf\u53c2\u6570\u7684CMERNet\u5728CMER-Bench\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684MER\u6a21\u578b\u548cMLLMs\u3002"}}
{"id": "2512.13739", "pdf": "https://arxiv.org/pdf/2512.13739", "abs": "https://arxiv.org/abs/2512.13739", "authors": ["Yajie Yang", "Yuqing Zhao", "Xiaochao Xi", "Yinan Zhu"], "title": "Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI-AISI 2026", "summary": "Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque \"black boxes,\" hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u65b0\u95fb\u7279\u7a3f\u4e2d\u53ef\u63a7AIGC\u56fe\u50cf\u751f\u6210\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u63ed\u793a\u73b0\u6709\u5de5\u5177\u5728\u8bed\u4e49\u5bf9\u9f50\u3001\u6587\u5316\u7279\u5f02\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9ad8\u7cbe\u5ea6\u5206\u5272\u3001\u8bed\u4e49\u5bf9\u9f50\u4e0e\u98ce\u683c\u8c03\u63a7\u7684\u4eba\u673a\u534f\u540c\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u4ee5\u63d0\u5347\u5185\u5bb9\u51c6\u786e\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002", "motivation": "AIGC\u8f85\u52a9\u56fe\u50cf\u751f\u6210\u5728\u65b0\u95fb\u4e1a\u5f15\u53d1\u4e89\u8bae\uff0c\u4e3b\u8981\u56e0\u5176\u201c\u9ed1\u7bb1\u201d\u7279\u6027\u96be\u4ee5\u6ee1\u8db3\u65b0\u95fb\u5bf9\u5185\u5bb9\u51c6\u786e\u6027\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u53cc\u91cd\u9700\u6c42\uff0c\u5e76\u5e26\u6765\u4f26\u7406\u3001\u793e\u4f1a\u6280\u672f\u53ca\u4fe1\u4efb\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u5f00\u5c55\u4e24\u9879\u5b9e\u9a8c\uff1a(1) \u901a\u8fc7\u6807\u51c6\u5316\u63d0\u793a\u6d4b\u8bd5\u4e09\u5927\u5e73\u53f0\u5728\u4e09\u7c7b\u573a\u666f\u4e0b\u7684\u8de8\u5e73\u53f0\u9002\u5e94\u6027\uff1b(2) \u6784\u5efa\u4eba\u673a\u534f\u540c\u7684\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u6574\u5408SAM/GroundingDINO\uff08\u9ad8\u7cbe\u5ea6\u5206\u5272\uff09\u3001BrushNet\uff08\u8bed\u4e49\u5bf9\u9f50\uff09\u548cStyle-LoRA/Prompt-to-Prompt\uff08\u98ce\u683c\u8c03\u63a7\uff09\uff0c\u5e76\u901a\u8fc7CLIP\u8bed\u4e49\u8bc4\u5206\u3001NSFW/OCR/YOLO\u8fc7\u6ee4\u53ca\u53ef\u9a8c\u8bc1\u5185\u5bb9\u51ed\u8bc1\u4fdd\u969c\u7f16\u8f91\u5fe0\u5b9e\u5ea6\u3002", "result": "\u5b9e\u9a8c1\u53d1\u73b0\u8bad\u7ec3\u8bed\u6599\u504f\u5dee\u548c\u5e73\u53f0\u7ea7\u8fc7\u6ee4\u5bfc\u81f4\u8bed\u4e49\u5bf9\u9f50\u3001\u6587\u5316\u7279\u5f02\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u6027\u7684\u663e\u8457\u5dee\u5f02\uff1b\u5b9e\u9a8c2\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u5757\u5316\u6d41\u7a0b\u80fd\u6709\u6548\u4fdd\u7559\u8bed\u4e49\u8868\u5f81\u5e76\u5b9e\u73b0\u53ef\u8ffd\u6eaf\u90e8\u7f72\u3002", "conclusion": "\u63d0\u51fa\u9002\u7528\u4e8e\u65b0\u95fb\u7279\u7a3f\u7684\u4eba-AI\u534f\u4f5c\u673a\u5236\uff0c\u5e76\u5efa\u8bae\u4ece\u89d2\u8272\u8eab\u4efd\u7a33\u5b9a\u6027\uff08CIS\uff09\u3001\u6587\u5316\u8868\u8fbe\u51c6\u786e\u6027\uff08CEA\uff09\u548c\u516c\u4f17\u7528\u6237\u9002\u5b9c\u6027\uff08U-PA\uff09\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30AIGC\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "summary_cn": "\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u8f85\u52a9\u56fe\u50cf\u5236\u4f5c\u5728\u65b0\u95fb\u4e1a\u4e2d\u5f15\u53d1\u4e89\u8bae\uff0c\u540c\u65f6\u53d7\u5230\u5a92\u4f53\u673a\u6784\u7684\u5173\u6ce8\u3002\u5173\u952e\u95ee\u9898\u5305\u62ec\u865a\u5047\u4fe1\u606f\u3001\u771f\u5b9e\u6027\u3001\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5927\u591a\u6570AIGC\u5de5\u5177\u662f\u4e0d\u900f\u660e\u7684\u201c\u9ed1\u7bb1\u201d\uff0c\u96be\u4ee5\u6ee1\u8db3\u5185\u5bb9\u51c6\u786e\u6027\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u53cc\u91cd\u9700\u6c42\uff0c\u4ece\u800c\u5f15\u53d1\u4f26\u7406\u3001\u793e\u4f1a\u6280\u672f\u53ca\u4fe1\u4efb\u56f0\u5883\u3002\u672c\u6587\u63a2\u7d22\u4e86\u5728\u65b0\u95fb\u7279\u7a3f\u4e2d\u5b9e\u73b0\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u7684\u8def\u5f84\uff0c\u5e76\u57fa\u4e8e\u4e2d\u56fd\u5a92\u4f53\u673a\u6784\u7684\u9879\u76ee\u5f00\u5c55\u4e86\u4e24\u9879\u5b9e\u9a8c\uff1a\uff081\uff09\u5b9e\u9a8c\u4e00\u901a\u8fc7\u5728\u4e09\u79cd\u573a\u666f\u4e2d\u4f7f\u7528\u6807\u51c6\u5316\u63d0\u793a\u6d4b\u8bd5\u8de8\u5e73\u53f0\u9002\u5e94\u6027\uff0c\u63ed\u793a\u4e86\u7531\u8bad\u7ec3\u8bed\u6599\u504f\u5dee\u548c\u5e73\u53f0\u7ea7\u8fc7\u6ee4\u6240\u5bfc\u81f4\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u6587\u5316\u7279\u5f02\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff1b\uff082\uff09\u5b9e\u9a8c\u4e8c\u6784\u5efa\u4e86\u4e00\u79cd\u4eba\u5728\u56de\u8def\u7684\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408\u9ad8\u7cbe\u5ea6\u5206\u5272\uff08SAM\u3001GroundingDINO\uff09\u3001\u8bed\u4e49\u5bf9\u9f50\uff08BrushNet\uff09\u548c\u98ce\u683c\u8c03\u63a7\uff08Style-LoRA\u3001Prompt-to-Prompt\uff09\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u8bc4\u5206\u3001NSFW/OCR/YOLO\u8fc7\u6ee4\u4ee5\u53ca\u53ef\u9a8c\u8bc1\u7684\u5185\u5bb9\u51ed\u8bc1\u786e\u4fdd\u7f16\u8f91\u5fe0\u5b9e\u5ea6\uff0c\u5b9e\u73b0\u53ef\u8ffd\u6eaf\u7684\u8bed\u4e49\u8868\u5f81\u4fdd\u7559\u3002\u636e\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u65b0\u95fb\u7279\u7a3f\u7684AIGC\u8f85\u52a9\u56fe\u50cf\u751f\u6210\u4eba\u673a\u534f\u4f5c\u673a\u5236\uff0c\u5e76\u5efa\u8bae\u4ece\u89d2\u8272\u8eab\u4efd\u7a33\u5b9a\u6027\uff08CIS\uff09\u3001\u6587\u5316\u8868\u8fbe\u51c6\u786e\u6027\uff08CEA\uff09\u548c\u7528\u6237-\u516c\u4f17\u9002\u5b9c\u6027\uff08U-PA\uff09\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002"}}
{"id": "2512.13742", "pdf": "https://arxiv.org/pdf/2512.13742", "abs": "https://arxiv.org/abs/2512.13742", "authors": ["Md. Najib Hasan", "Imran Ahmad", "Sourav Basak Shuvo", "Md. Mahadi Hasan Ankon", "Sunanda Das", "Nazmul Siddique", "Hui Wang"], "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u5206\u7c7b\u5668\uff08MobileCoAtNet\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u63a8\u7406\u89e3\u91ca\u3002\u5c3d\u7ba1\u5f3a\u5206\u7c7b\u6027\u80fd\u63d0\u5347\u4e86LLM\u89e3\u91ca\u8d28\u91cf\uff0c\u4f46\u6240\u6709\u6d4b\u8bd5\u768432\u4e2aLLM\u5728\u4e13\u5bb6\u9a8c\u8bc1\u7684\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5747\u672a\u8fbe\u5230\u4eba\u7c7b\u7a33\u5b9a\u6027\u6c34\u5e73\uff0c\u63d0\u793a\u5f53\u524dLLM\u5c1a\u4e0d\u53ef\u9760\u7528\u4e8e\u9ad8\u98ce\u9669\u533b\u7597\u51b3\u7b56\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u751f\u6210\u4e34\u5e8a\u6587\u672c\uff0c\u4f46\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u4e14\u89e3\u91ca\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u4e0e\u4e34\u5e8a\u533b\u751f\u671f\u671b\u7684\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u6a21\u578bMobileCoAtNet\u7528\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u7c7b\uff0c\u5176\u8f93\u51fa\u9a71\u52a8\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e34\u5e8a\u63a8\u7406\uff1b\u6784\u5efa\u4e24\u4e2a\u7531\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6db5\u76d6\u75c5\u56e0\u3001\u75c7\u72b6\u3001\u6cbb\u7597\u3001\u751f\u6d3b\u65b9\u5f0f\u548c\u968f\u8bbf\u7684\u63a8\u7406\u57fa\u51c6\uff0c\u8bc4\u4f3032\u4e2aLLM\u7684\u8868\u73b0\u3002", "result": "MobileCoAtNet\u5728\u516b\u7c7b\u80c3\u90e8\u75be\u75c5\u5206\u7c7b\u4e2d\u8fbe\u5230\u9ad8\u51c6\u786e\u7387\uff1b\u5f3a\u5206\u7c7b\u7ed3\u679c\u63d0\u5347\u4e86LLM\u751f\u6210\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u4f46\u6240\u6709LLM\u5728\u63a8\u7406\u7a33\u5b9a\u6027\u4e0a\u5747\u672a\u8fbe\u4eba\u7c7b\u6c34\u5e73\uff0c\u4e14\u5bf9\u63d0\u793a\u53d8\u5316\u654f\u611f\u3002", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u751f\u6210\u6709\u7528\u7684\u4e34\u5e8a\u53d9\u8ff0\uff0c\u4f46\u5f53\u524dLLM\u5728\u9ad8\u98ce\u9669\u533b\u7597\u51b3\u7b56\u4e2d\u4ecd\u4e0d\u53ef\u9760\uff1b\u6240\u63d0\u6846\u67b6\u6709\u52a9\u4e8e\u63ed\u793a\u5176\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u8def\u5f84\u3002", "summary_cn": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u5728\u68c0\u6d4b\u80c3\u80a0\u9053\u75be\u75c5\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u65e0\u6cd5\u89e3\u91ca\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u751f\u6210\u4e34\u5e8a\u6587\u672c\uff0c\u5374\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u5e38\u4ea7\u751f\u4e0d\u7a33\u5b9a\u6216\u9519\u8bef\u7684\u89e3\u91ca\u3002\u8fd9\u5bfc\u81f4\u6a21\u578b\u6240\u201c\u770b\u5230\u201d\u7684\u5185\u5bb9\u4e0e\u4e34\u5e8a\u533b\u751f\u6240\u671f\u671b\u7684\u63a8\u7406\u7c7b\u578b\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u5c06\u56fe\u50cf\u5206\u7c7b\u4e0e\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u76f8\u7ed3\u5408\u7684\u6846\u67b6\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7528\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u65b0\u6df7\u5408\u6a21\u578bMobileCoAtNet\uff0c\u5728\u516b\u7c7b\u80c3\u90e8\u76f8\u5173\u75be\u75c5\u5206\u7c7b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u3002\u8be5\u6a21\u578b\u7684\u8f93\u51fa\u968f\u540e\u7528\u4e8e\u9a71\u52a8\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002\u4e3a\u8bc4\u4f30\u8fd9\u4e9b\u63a8\u7406\u8d28\u91cf\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e24\u4e2a\u7ecf\u4e13\u5bb6\u9a8c\u8bc1\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u75c5\u56e0\u3001\u75c7\u72b6\u3001\u6cbb\u7597\u3001\u751f\u6d3b\u65b9\u5f0f\u53ca\u968f\u8bbf\u62a4\u7406\u7b49\u65b9\u9762\u3002\u6211\u4eec\u5bf932\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u91d1\u6807\u51c6\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u5927\u7684\u5206\u7c7b\u80fd\u529b\u53ef\u63d0\u5347\u5176\u89e3\u91ca\u8d28\u91cf\uff0c\u4f46\u6ca1\u6709\u4efb\u4f55\u6a21\u578b\u8fbe\u5230\u4eba\u7c7b\u7ea7\u522b\u7684\u7a33\u5b9a\u6027\u3002\u5373\u4f7f\u8868\u73b0\u6700\u597d\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u63d0\u793a\u8bcd\u53d8\u5316\u65f6\u4e5f\u4f1a\u6539\u53d8\u5176\u63a8\u7406\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u53ef\u751f\u6210\u6709\u7528\u7684\u4e34\u5e8a\u53d9\u8ff0\uff0c\u4f46\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u533b\u7597\u51b3\u7b56\u4e2d\u4ecd\u4e0d\u53ef\u9760\u3002\u8be5\u6846\u67b6\u66f4\u6e05\u6670\u5730\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002\u672c\u7814\u7a76\u4f7f\u7528\u7684\u5b8c\u6574\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5728https://github.com/souravbasakshuvo/DL3M\u516c\u5f00\u3002"}}
{"id": "2512.13747", "pdf": "https://arxiv.org/pdf/2512.13747", "abs": "https://arxiv.org/abs/2512.13747", "authors": ["Siyuan Dai", "Lunxiao Li", "Kun Zhao", "Eardi Lila", "Paul K. Crane", "Heng Huang", "Dongkuan Xu", "Haoteng Tang", "Liang Zhan"], "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICDM 2025 the Workshop on Synergy of AI and Multimodal Biomedical Data Mining", "summary": "With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.", "AI": {"tldr": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u751f\u7269\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u7eaf\u6587\u672c\u63a8\u7406\uff1b\u4f5c\u8005\u901a\u8fc7\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u7c7b\u548c\u80f8\u90e8X\u5149\u8bca\u65ad\u4e24\u4e2a\u6570\u636e\u96c6\u9a8c\u8bc1\u8be5\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u6539\u8fdb\u7b56\u7565\uff0c\u63ed\u793a\u4e86MLLMs\u7f3a\u4e4f\u5177\u8eab\u89c6\u89c9\u7406\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\uff0c\u5373\u4fbf\u662f\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u7840\u7684\u533b\u7597\u51b3\u7b56\u4efb\u52a1\u4e0a\u4ecd\u8868\u73b0\u4e0d\u4f73\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u8fd9\u4e00\u5c40\u9650\u6027\u53ca\u5176\u539f\u56e0\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff1a(1) \u4e09\u9636\u6bb5\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5206\u7c7b\uff08\u6b63\u5e38\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u3001\u75f4\u5446\uff09\uff0c\u5176\u7c7b\u522b\u95f4\u89c6\u89c9\u5dee\u5f02\u7ec6\u5fae\uff1b(2) MIMIC-CXR\u80f8\u90e8X\u5149\u7247\u5206\u7c7b\uff0c\u5305\u542b14\u79cd\u975e\u4e92\u65a5\u75be\u75c5\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u7eaf\u6587\u672c\u3001\u7eaf\u89c6\u89c9\u53ca\u591a\u6a21\u6001\u8f93\u5165\u7684\u8868\u73b0\uff0c\u5e76\u5c1d\u8bd5\u4e09\u79cd\u7f13\u89e3\u7b56\u7565\uff1a(1) \u4f7f\u7528\u5e26\u63a8\u7406\u6ce8\u91ca\u7684\u793a\u4f8b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1b(2) \u5148\u5bf9\u56fe\u50cf\u751f\u6210\u63cf\u8ff0\u518d\u8fdb\u884c\u7eaf\u6587\u672c\u63a8\u7406\uff1b(3) \u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u5e76\u8f85\u4ee5\u5206\u7c7b\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u533b\u7597\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0c\u7eaf\u6587\u672c\u63a8\u7406\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u89c6\u89c9\u6216\u591a\u6a21\u6001\u8f93\u5165\uff0c\u4e14\u591a\u6a21\u6001\u8f93\u5165\u5e38\u5e38\u6bd4\u4ec5\u7528\u6587\u672c\u6548\u679c\u66f4\u5dee\u3002\u6240\u63d0\u51fa\u7684\u4e09\u79cd\u7b56\u7565\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u4e86\u8be5\u95ee\u9898\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u533b\u7597\u573a\u666f\u4e2d\u7f3a\u4e4f\u624e\u5b9e\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u672a\u6765\u9700\u52a0\u5f3a\u89c6\u89c9\u4e0e\u8bed\u4e49\u7684\u5bf9\u9f50\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u533b\u7597\u51b3\u7b56\u6027\u80fd\u3002", "summary_cn": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\uff0c\u5373\u4fbf\u662f\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u7684\u533b\u7597\u51b3\u7b56\uff08MDM\uff09\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002\u6211\u4eec\u5229\u7528\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u5bf9\u6b64\u5c40\u9650\u6027\u5c55\u5f00\u7814\u7a76\uff1a(1) \u4e09\u9636\u6bb5\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5206\u7c7b\uff08\u6b63\u5e38\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u3001\u75f4\u5446\uff09\uff0c\u5176\u4e2d\u5404\u7c7b\u522b\u4e4b\u95f4\u7684\u89c6\u89c9\u5dee\u5f02\u975e\u5e38\u7ec6\u5fae\uff1b(2) MIMIC-CXR\u80f8\u90e8X\u5149\u7247\u5206\u7c7b\uff0c\u5305\u542b14\u79cd\u975e\u4e92\u65a5\u7684\u75c5\u7406\u72b6\u51b5\u3002\u6211\u4eec\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u7eaf\u6587\u672c\u63a8\u7406\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u89c6\u89c9\u6216\u89c6\u89c9-\u6587\u672c\u8054\u5408\u8f93\u5165\uff0c\u800c\u591a\u6a21\u6001\u8f93\u5165\u7684\u8868\u73b0\u5f80\u5f80\u6bd4\u4ec5\u4f7f\u7528\u6587\u672c\u66f4\u5dee\u3002\u4e3a\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e09\u79cd\u7b56\u7565\uff1a(1) \u5229\u7528\u5e26\u6709\u63a8\u7406\u6ce8\u91ca\u7684\u793a\u4f8b\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1b(2) \u5148\u5bf9\u56fe\u50cf\u751f\u6210\u6587\u5b57\u63cf\u8ff0\uff0c\u518d\u8fdb\u884c\u7eaf\u6587\u672c\u63a8\u7406\uff1b(3) \u5728\u5206\u7c7b\u76d1\u7763\u4e0b\u5bf9\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u5c11\u6837\u672c\u5fae\u8c03\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u7f3a\u4e4f\u5177\u8eab\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u63d0\u5347\u533b\u7597\u9886\u57df\u591a\u6a21\u6001\u51b3\u7b56\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2512.13752", "pdf": "https://arxiv.org/pdf/2512.13752", "abs": "https://arxiv.org/abs/2512.13752", "authors": ["Jie Qin", "Jiancheng Huang", "Limeng Qiao", "Lin Ma"], "title": "STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.", "AI": {"tldr": "STAR\u662f\u4e00\u79cd\u5806\u53e0\u81ea\u56de\u5f52\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u5904\u7406\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\uff0c\u5728\u4e0d\u5e72\u6270\u5df2\u6709\u80fd\u529b\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u76ee\u6807\u65b9\u9762\u9762\u4e34\u4f18\u5316\u51b2\u7a81\u4e0e\u6027\u80fd\u6743\u8861\u7684\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u65e2\u80fd\u589e\u5f3a\u751f\u6210\u80fd\u529b\u53c8\u4e0d\u635f\u5bb3\u7406\u89e3\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSTAR\u65b9\u6cd5\uff1a\u5c06\u591a\u6a21\u6001\u5b66\u4e60\u5206\u89e3\u4e3a\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u591a\u4e2a\u9636\u6bb5\uff1b\u51bb\u7ed3\u57fa\u7840\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u9010\u5c42\u5806\u53e0\u540c\u6784AR\u6a21\u5757\u4ee5\u907f\u514d\u4efb\u52a1\u95f4\u5e72\u6270\uff1b\u5f15\u5165\u9ad8\u5bb9\u91cfVQ\u63d0\u5347\u56fe\u50cf\u8868\u5f81\u7c92\u5ea6\uff0c\u5e76\u91c7\u7528\u9690\u5f0f\u63a8\u7406\u673a\u5236\u6539\u5584\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u3002", "result": "STAR\u5728GenEval\uff080.91\uff09\u3001DPG-Bench\uff0887.44\uff09\u548cImgEdit\uff084.34\uff09\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "STAR\u6709\u6548\u5b9e\u73b0\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u7406\u89e3\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6027\u80fd\u3002", "summary_cn": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u63a8\u52a8\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u4e2d\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4f18\u5316\u51b2\u7a81\u548c\u6027\u80fd\u6743\u8861\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\u76ee\u6807\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u4e86\u5728\u4fdd\u7559\u73b0\u6709\u7406\u89e3\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u751f\u6210\u6027\u80fd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86STAR\uff1a\u4e00\u79cd\u7528\u4e8e\u4efb\u52a1\u6e10\u8fdb\u5f0f\u7edf\u4e00\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5806\u53e0\u81ea\u56de\u5f52\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u5c06\u591a\u6a21\u6001\u5b66\u4e60\u5206\u89e3\u4e3a\u591a\u4e2a\u9636\u6bb5\uff1a\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u3002\u901a\u8fc7\u51bb\u7ed3\u57fa\u7840\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u7684\u53c2\u6570\u5e76\u9010\u6b65\u5806\u53e0\u540c\u6784\u7684AR\u6a21\u5757\uff0c\u8be5\u65b9\u6cd5\u5728\u6269\u5c55\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u907f\u514d\u4e86\u8de8\u4efb\u52a1\u5e72\u6270\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u5bb9\u91cf\u7684\u77e2\u91cf\u91cf\u5316\uff08VQ\uff09\u6a21\u5757\u4ee5\u589e\u5f3a\u56fe\u50cf\u8868\u5f81\u7684\u7c92\u5ea6\uff0c\u5e76\u91c7\u7528\u9690\u5f0f\u63a8\u7406\u673a\u5236\u6765\u63d0\u5347\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u751f\u6210\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSTAR\u5728GenEval\uff080.91\uff09\u3001DPG-Bench\uff0887.44\uff09\u548cImgEdit\uff084.34\uff09\u4e0a\u5747\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7edf\u4e00\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.13753", "pdf": "https://arxiv.org/pdf/2512.13753", "abs": "https://arxiv.org/abs/2512.13753", "authors": ["Mika Sipil\u00e4", "Sabrina Maggio", "Sandra De Iaco", "Klaus Nordhausen", "Monica Palma", "Sara Taskinen"], "title": "Time-aware UNet and super-resolution deep residual networks for spatial downscaling", "categories": ["cs.CV", "cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728SRDRN\u548cUNet\u4e2d\u52a0\u5165\u8f7b\u91cf\u7ea7\u65f6\u95f4\u6a21\u5757\uff08\u4f7f\u7528\u6b63\u5f26\u6216RBF\u7f16\u7801\uff09\u4ee5\u63d0\u5347\u5bf9\u6d41\u5c42\u81ed\u6c27\u536b\u661f\u6570\u636e\u7684\u7a7a\u95f4\u964d\u5c3a\u5ea6\u6027\u80fd\uff0c\u5728\u610f\u5927\u5229\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u536b\u661f\u5927\u6c14\u6c61\u67d3\u7269\u6570\u636e\u7a7a\u95f4\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5728\u5c40\u5730\u73af\u5883\u5206\u6790\u4e0e\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u7a7a\u95f4\u964d\u5c3a\u5ea6\u65b9\u6cd5\u3002", "method": "\u5728SRDRN\u548cUNet\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u5f15\u5165\u8f7b\u91cf\u7ea7\u65f6\u95f4\u6a21\u5757\uff0c\u5229\u7528\u6b63\u5f26\u6216\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u5bf9\u89c2\u6d4b\u65f6\u95f4\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u5c06\u5176\u4e0e\u7f51\u7edc\u4e2d\u7684\u7a7a\u95f4\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u610f\u5927\u5229\u81ed\u6c27\u964d\u5c3a\u5ea6\u6848\u4f8b\u4e2d\uff0c\u52a0\u5165\u65f6\u95f4\u6a21\u5757\u7684\u65b9\u6cd5\u5728\u4ec5\u7565\u5fae\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u964d\u5c3a\u5ea6\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u5f15\u5165\u65f6\u95f4\u611f\u77e5\u6a21\u5757\u80fd\u6709\u6548\u589e\u5f3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u81ed\u6c27\u536b\u661f\u6570\u636e\u7a7a\u95f4\u964d\u5c3a\u5ea6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "summary_cn": "\u5927\u6c14\u6c61\u67d3\u7269\u7684\u536b\u661f\u6570\u636e\u901a\u5e38\u4ec5\u5177\u6709\u8f83\u7c97\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u9650\u5236\u4e86\u5176\u5728\u5c40\u5730\u5c3a\u5ea6\u73af\u5883\u5206\u6790\u548c\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002\u7a7a\u95f4\u964d\u5c3a\u5ea6\u65b9\u6cd5\u65e8\u5728\u5c06\u7c97\u5206\u8fa8\u7387\u7684\u536b\u661f\u6570\u636e\u8f6c\u5316\u4e3a\u9ad8\u5206\u8fa8\u7387\u573a\u3002\u672c\u7814\u7a76\u8003\u8651\u4e86\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u2014\u2014\u8d85\u5206\u8fa8\u7387\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff08SRDRN\uff09\u548c\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\u7684UNet\uff0c\u7528\u4e8e\u5bf9\u6d41\u5c42\u81ed\u6c27\u7684\u7a7a\u95f4\u964d\u5c3a\u5ea6\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u6269\u5c55\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u65f6\u95f4\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4f7f\u7528\u6b63\u5f26\u6216\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u5bf9\u89c2\u6d4b\u65f6\u95f4\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u5c06\u65f6\u95f4\u7279\u5f81\u4e0e\u7f51\u7edc\u4e2d\u7684\u7a7a\u95f4\u8868\u793a\u8fdb\u884c\u878d\u5408\u3002\u6240\u63d0\u51fa\u7684\u65f6\u95f4\u611f\u77e5\u6269\u5c55\u65b9\u6cd5\u5728\u610f\u5927\u5229\u81ed\u6c27\u964d\u5c3a\u5ea6\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u4e0e\u5176\u539f\u59cb\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u4ec5\u7565\u5fae\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u65f6\u95f4\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u964d\u5c3a\u5ea6\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2512.13796", "pdf": "https://arxiv.org/pdf/2512.13796", "abs": "https://arxiv.org/abs/2512.13796", "authors": ["Victor Rong", "Jan Held", "Victor Chu", "Daniel Rebain", "Marc Van Droogenbroeck", "Kiriakos N. Kutulakos", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries", "categories": ["cs.CV"], "comment": "Webpage at https://lessvrong.com/cs/nexels", "summary": "Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06\u51e0\u4f55\u4e0e\u5916\u89c2\u89e3\u8026\u7684\u65b0\u8868\u793a\u65b9\u6cd5\uff0c\u4f7f\u7528surfels\u8868\u793a\u51e0\u4f55\uff0c\u5e76\u7ed3\u5408\u5168\u5c40\u795e\u7ecf\u573a\u4e0e\u6bcf\u4e2a\u56fe\u5143\u7684\u989c\u8272\u6765\u8868\u793a\u5916\u89c2\uff0c\u5728\u663e\u8457\u51cf\u5c11\u56fe\u5143\u6570\u91cf\u548c\u5185\u5b58\u5360\u7528\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e0e3D Gaussian Splatting\u76f8\u5f53\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u5b9e\u73b0\u66f4\u5feb\u7684\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "Gaussian Splatting\u867d\u7136\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u6548\u679c\u51fa\u8272\uff0c\u4f46\u5373\u4f7f\u573a\u666f\u51e0\u4f55\u7ed3\u6784\u7b80\u5355\uff0c\u4ecd\u9700\u6570\u767e\u4e07\u4e2a\u56fe\u5143\u6765\u5efa\u6a21\u9ad8\u7eb9\u7406\u573a\u666f\uff0c\u5bfc\u81f4\u8868\u793a\u5197\u4f59\u3001\u5185\u5b58\u5360\u7528\u9ad8\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u57fa\u4e8e\u70b9\u7684\u6e32\u67d3\uff0c\u5c06\u51e0\u4f55\u4e0e\u5916\u89c2\u89e3\u8026\uff1a\u4f7f\u7528surfels\u8868\u793a\u51e0\u4f55\uff0c\u5916\u89c2\u5219\u7531\u4e00\u4e2a\u5168\u5c40\u795e\u7ecf\u573a\u4e0e\u6bcf\u4e2a\u56fe\u5143\u7684\u989c\u8272\u5171\u540c\u5efa\u6a21\uff1b\u795e\u7ecf\u573a\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u56fa\u5b9a\u6570\u91cf\u7684\u56fe\u5143\u63d0\u4f9b\u7eb9\u7406\uff0c\u4ece\u800c\u63a7\u5236\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u5ba4\u5916\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f7f\u75289.7\u500d\u66f4\u5c11\u7684\u56fe\u5143\u548c5.5\u500d\u66f4\u5c11\u7684\u5185\u5b58\uff1b\u5728\u5ba4\u5185\u573a\u666f\u4e2d\uff0c\u56fe\u5143\u51cf\u5c1131\u500d\u3001\u5185\u5b58\u51cf\u5c113.7\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e3D Gaussian Splatting\u76f8\u5f53\u7684\u611f\u77e5\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u6e32\u67d3\u901f\u5ea6\u662f\u73b0\u6709\u7eb9\u7406\u56fe\u5143\u65b9\u6cd5\u7684\u4e24\u500d\uff0c\u4e14\u89c6\u89c9\u8d28\u91cf\u66f4\u4f18\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u51e0\u4f55-\u5916\u89c2\u89e3\u8026\u8868\u793a\u65b9\u6cd5\u5728\u5927\u5e45\u538b\u7f29\u56fe\u5143\u6570\u91cf\u548c\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u56fe\u5143\u7684\u6e32\u67d3\u65b9\u6cd5\u3002", "summary_cn": "\u5c3d\u7ba1\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian splatting\uff09\u5728\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u9700\u8981\u6570\u767e\u4e07\u4e2a\u56fe\u5143\u624d\u80fd\u5bf9\u9ad8\u5ea6\u7eb9\u7406\u5316\u7684\u573a\u666f\u8fdb\u884c\u5efa\u6a21\uff0c\u5373\u4f7f\u573a\u666f\u7684\u51e0\u4f55\u7ed3\u6784\u5f88\u7b80\u5355\u4e5f\u662f\u5982\u6b64\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u57fa\u4e8e\u70b9\u6e32\u67d3\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u51e0\u4f55\u4e0e\u5916\u89c2\u89e3\u8026\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7d27\u51d1\u7684\u8868\u793a\u3002\u6211\u4eec\u4f7f\u7528surfels\u8868\u793a\u51e0\u4f55\uff0c\u5e76\u7ed3\u5408\u5168\u5c40\u795e\u7ecf\u573a\u4e0e\u6bcf\u4e2a\u56fe\u5143\u7684\u989c\u8272\u6765\u8868\u793a\u5916\u89c2\u3002\u795e\u7ecf\u573a\u4e3a\u6bcf\u4e2a\u50cf\u7d20\u56fa\u5b9a\u6570\u91cf\u7684\u56fe\u5143\u63d0\u4f9b\u7eb9\u7406\uff0c\u4ece\u800c\u786e\u4fdd\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u8f83\u4f4e\u3002\u6211\u4eec\u7684\u8868\u793a\u65b9\u6cd5\u5728\u5ba4\u5916\u573a\u666f\u4e2d\u4f7f\u7528\u4e869.7\u500d\u66f4\u5c11\u7684\u56fe\u5143\u548c5.5\u500d\u66f4\u5c11\u7684\u5185\u5b58\uff0c\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u4f7f\u7528\u4e8631\u500d\u66f4\u5c11\u7684\u56fe\u5143\u548c3.7\u500d\u66f4\u5c11\u7684\u5185\u5b58\uff0c\u540c\u65f6\u8fbe\u5230\u4e86\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u76f8\u5f53\u7684\u611f\u77e5\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6e32\u67d3\u901f\u5ea6\u662f\u73b0\u6709\u7eb9\u7406\u5316\u56fe\u5143\u65b9\u6cd5\u7684\u4e24\u500d\uff0c\u5e76\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u6709\u6240\u63d0\u5347\u3002"}}
{"id": "2512.13834", "pdf": "https://arxiv.org/pdf/2512.13834", "abs": "https://arxiv.org/abs/2512.13834", "authors": ["Naman Balbir Singh Makkar"], "title": "VajraV1 -- The most accurate Real Time Object Detector of the YOLO family", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report. 20 Pages, 7 figures", "summary": "Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.\n  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.", "AI": {"tldr": "VajraV1 \u662f\u4e00\u79cd\u65b0\u578b\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u4e0e\u73b0\u6709 YOLO \u6a21\u578b\u76f8\u5f53\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u5728 COCO \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f18\u7684\u7cbe\u5ea6\u3002", "motivation": "\u8fd1\u5e74\u6765 YOLO \u7cfb\u5217\u6a21\u578b\uff08\u5982 YOLOv10 \u81f3 YOLOv13\uff09\u5728\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7cbe\u5ea6\u4e0e\u6548\u7387\u5e73\u8861\u7684\u7a7a\u95f4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408\u5e76\u4f18\u5316\u5df2\u6709 YOLO \u67b6\u6784\u4e2d\u7684\u6709\u6548\u8bbe\u8ba1\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u5b9e\u65f6\u68c0\u6d4b\u6027\u80fd\u8fb9\u754c\u3002", "method": "\u63d0\u51fa VajraV1 \u6a21\u578b\u67b6\u6784\uff0c\u878d\u5408\u5148\u524d YOLO \u6a21\u578b\u4e2d\u7684\u9ad8\u6548\u8bbe\u8ba1\u9009\u62e9\uff0c\u6784\u5efa\u4e00\u7cfb\u5217\u4e0d\u540c\u89c4\u6a21\uff08Nano\u3001Small\u3001Medium\u3001Large\u3001Xlarge\uff09\u7684\u68c0\u6d4b\u5668\u3002", "result": "\u5728 COCO \u9a8c\u8bc1\u96c6\u4e0a\uff0cVajraV1 \u5404\u7248\u672c\u5747\u8d85\u8d8a\u5bf9\u5e94\u89c4\u6a21\u7684 YOLOv12 \u548c YOLOv13\uff1aVajraV1-Nano \u8fbe\u5230 44.3% mAP\uff0cVajraV1-Small \u4e3a 50.4% mAP\uff0cVajraV1-Medium \u4e3a 52.7% mAP\uff0cVajraV1-Large \u4e3a 53.7% mAP\uff0cVajraV1-Xlarge \u8fbe\u5230 56.2% mAP\uff0c\u4e14\u63a8\u7406\u5ef6\u8fdf\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "VajraV1 \u67b6\u6784\u901a\u8fc7\u6709\u6548\u6574\u5408\u73b0\u6709 YOLO \u6a21\u578b\u7684\u8bbe\u8ba1\u4f18\u52bf\uff0c\u5728\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u548c\u6709\u7ade\u4e89\u529b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u6210\u4e3a\u5f53\u524d\u6027\u80fd\u6700\u5f3a\u7684\u5b9e\u65f6\u68c0\u6d4b\u5668\u3002", "summary_cn": "\u8fd1\u5e74\u6765\uff0c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c2024\u81f32025\u5e74\u95f4\u76f8\u7ee7\u53d1\u5e03\u4e86YOLOv10\u3001YOLO11\u3001YOLOv12\u548cYOLOv13\u3002\u672c\u6280\u672f\u62a5\u544a\u63d0\u51fa\u4e86VajraV1\u6a21\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5728\u73b0\u6709YOLO\u7cfb\u5217\u68c0\u6d4b\u5668\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u67b6\u6784\u589e\u5f3a\u3002VajraV1\u878d\u5408\u4e86\u5148\u524dYOLO\u6a21\u578b\u4e2d\u7684\u6709\u6548\u8bbe\u8ba1\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u7684\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u5728\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u5668\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002\u5728COCO\u9a8c\u8bc1\u96c6\u4e0a\uff0cVajraV1-Nano\u8fbe\u5230\u4e8644.3%\u7684mAP\uff0c\u6bd4YOLOv12-N\u9ad8\u51fa3.7%\uff0c\u6bd4YOLOv13-N\u9ad8\u51fa2.7%\uff0c\u4e14\u5176\u5ef6\u8fdf\u4e0eYOLOv12-N\u548cYOLOv11-N\u76f8\u5f53\u3002VajraV1-Small\u8fbe\u5230\u4e8650.4%\u7684mAP\uff0c\u8d85\u8fc7YOLOv12-S\u548cYOLOv13-S\u8fbe2.4%\u3002VajraV1-Medium\u8fbe\u5230\u4e8652.7%\u7684mAP\uff0c\u4f18\u4e8eYOLOv12-M\u8fbe0.2%\u3002VajraV1-Large\u8fbe\u5230\u4e8653.7%\u7684mAP\uff0c\u8d85\u8fc7YOLOv13-L\u8fbe0.3%\u3002VajraV1-Xlarge\u8fbe\u5230\u4e8656.2%\u7684mAP\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u5668\u3002"}}
{"id": "2512.13840", "pdf": "https://arxiv.org/pdf/2512.13840", "abs": "https://arxiv.org/abs/2512.13840", "authors": ["Yannan He", "Garvita Tiwari", "Xiaohan Zhang", "Pankaj Bora", "Tolga Birdal", "Jan Eric Lenssen", "Gerard Pons-Moll"], "title": "MoLingo: Motion-Language Alignment for Text-to-Motion Generation", "categories": ["cs.CV"], "comment": "Project page: https://hynann.github.io/molingo/MoLingo.html", "summary": "We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.", "AI": {"tldr": "MoLingo \u662f\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u53bb\u566a\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u591a\u4ee4\u724c\u4ea4\u53c9\u6ce8\u610f\u529b\u6587\u672c\u6761\u4ef6\u673a\u5236\uff0c\u5728\u751f\u6210\u903c\u771f\u4e14\u4e0e\u6587\u672c\u9ad8\u5ea6\u4e00\u81f4\u7684\u4eba\u4f53\u52a8\u4f5c\u65b9\u9762\u8fbe\u5230\u65b0 SOTA\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u7b56\u7565\u548c\u6587\u672c\u6761\u4ef6\u6ce8\u5165\u65b9\u5f0f\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u4f7f\u8fde\u7eed\u52a8\u4f5c\u6f5c\u5728\u7a7a\u95f4\u4e0a\u7684\u6269\u6563\u66f4\u6709\u6548\uff0c\u5e76\u63d0\u5347\u751f\u6210\u52a8\u4f5c\u4e0e\u6587\u672c\u63cf\u8ff0\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u5bf9\u9f50\u7684\u52a8\u4f5c\u7f16\u7801\u5668\uff0c\u5229\u7528\u5e27\u7ea7\u6587\u672c\u6807\u7b7e\u8bad\u7ec3\uff0c\u4f7f\u8bed\u4e49\u76f8\u8fd1\u7684\u52a8\u4f5c\u6f5c\u5728\u8868\u793a\u66f4\u63a5\u8fd1\uff1b\u540c\u65f6\u6bd4\u8f83\u5355\u4ee4\u724c\u4e0e\u591a\u4ee4\u724c\u4ea4\u53c9\u6ce8\u610f\u529b\u6587\u672c\u6761\u4ef6\u673a\u5236\uff0c\u91c7\u7528\u540e\u8005\u4ee5\u589e\u5f3a\u52a8\u4f5c\u771f\u5b9e\u611f\u4e0e\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u3002", "result": "\u5728\u6807\u51c6\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u53d6\u5f97\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u6f5c\u5728\u7a7a\u95f4\u3001\u81ea\u56de\u5f52\u751f\u6210\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6587\u672c\u6761\u4ef6\u673a\u5236\uff0cMoLingo \u5728\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6280\u672f\u7a81\u7834\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u4e0e\u6a21\u578b\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86 MoLingo\uff0c\u4e00\u79cd\u6587\u672c\u5230\u52a8\u4f5c\uff08T2M\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u53bb\u566a\u6765\u751f\u6210\u903c\u771f\u3001\u81ea\u7136\u7684\u4eba\u4f53\u52a8\u4f5c\u3002\u8fd1\u671f\u5de5\u4f5c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6269\u6563\uff0c\u8981\u4e48\u4e00\u6b21\u6027\u5904\u7406\u6574\u4e2a\u6f5c\u5728\u8868\u793a\uff0c\u8981\u4e48\u5728\u591a\u4e2a\u6f5c\u5728\u8868\u793a\u4e0a\u81ea\u56de\u5f52\u5730\u8fdb\u884c\u3002\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u4f7f\u8fde\u7eed\u52a8\u4f5c\u6f5c\u5728\u7a7a\u95f4\u4e0a\u7684\u6269\u6563\u6548\u679c\u6700\u4f73\u3002\u6211\u4eec\u805a\u7126\u4e8e\u4e24\u4e2a\u95ee\u9898\uff1a\uff081\uff09\u5982\u4f55\u6784\u5efa\u8bed\u4e49\u5bf9\u9f50\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f7f\u6269\u6563\u66f4\u6709\u6548\uff1b\uff082\uff09\u5982\u4f55\u6700\u597d\u5730\u6ce8\u5165\u6587\u672c\u6761\u4ef6\uff0c\u4f7f\u751f\u6210\u7684\u52a8\u4f5c\u7d27\u5bc6\u9075\u5faa\u6587\u672c\u63cf\u8ff0\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5bf9\u9f50\u7684\u52a8\u4f5c\u7f16\u7801\u5668\uff0c\u5229\u7528\u5e27\u7ea7\u6587\u672c\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u5f97\u5177\u6709\u76f8\u4f3c\u6587\u672c\u8bed\u4e49\u7684\u6f5c\u5728\u8868\u793a\u5f7c\u6b64\u9760\u8fd1\uff0c\u4ece\u800c\u4f7f\u6f5c\u5728\u7a7a\u95f4\u66f4\u6709\u5229\u4e8e\u6269\u6563\u8fc7\u7a0b\u3002\u6211\u4eec\u8fd8\u6bd4\u8f83\u4e86\u5355\u4ee4\u724c\u6761\u4ef6\u673a\u5236\u4e0e\u591a\u4ee4\u724c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53d1\u73b0\u4ea4\u53c9\u6ce8\u610f\u529b\u80fd\u5e26\u6765\u66f4\u597d\u7684\u52a8\u4f5c\u771f\u5b9e\u611f\u548c\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u6548\u679c\u3002\u7ed3\u5408\u8bed\u4e49\u5bf9\u9f50\u7684\u6f5c\u5728\u8868\u793a\u3001\u81ea\u56de\u5f52\u751f\u6210\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6587\u672c\u6761\u4ef6\u673a\u5236\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u8fbe\u5230\u4e86\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u7684\u65b0\u524d\u6cbf\u6c34\u5e73\u3002\u6211\u4eec\u5c06\u53d1\u5e03\u4ee3\u7801\u548c\u6a21\u578b\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2512.13855", "pdf": "https://arxiv.org/pdf/2512.13855", "abs": "https://arxiv.org/abs/2512.13855", "authors": ["Ujjwal Mishra", "Vinita Shukla", "Praful Hambarde", "Amit Shukla"], "title": "Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the IEEE/CVF winter conference on applications of computer vision (WACV 2026)", "summary": "Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cTelescopic Adapters\u201d\u7684\u65b0\u578b\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728CLIPSeg\u6a21\u578b\u4e2d\u6309\u5c42\u6df1\u5ea6\u52a8\u6001\u589e\u52a0\u9002\u914d\u5668\u5bb9\u91cf\uff0c\u5728\u4ec5\u4f7f\u7528613k\u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u6bd4\u5168\u5fae\u8c03\u5c11244\u500d\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e94\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\uff08VLSM\uff09\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u8ba1\u7b97\u5f00\u9500\u5927\uff1b\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5bf9\u6240\u6709Transformer\u5c42\u4f7f\u7528\u7edf\u4e00\u7684\u9002\u914d\u5668\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u53c2\u6570\u5206\u914d\u6b21\u4f18\u3001\u9002\u5e94\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faTelescopic Adapters\u6846\u67b6\uff0c\u5728CLIPSeg\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u8f7b\u91cf\u7ea7\u74f6\u9888\u6a21\u5757\uff0c\u5e76\u6839\u636e\u5c42\u6df1\u5ea6\u548c\u8bed\u4e49\u76f8\u5173\u6027\u52a8\u6001\u7f29\u653e\u9002\u914d\u5668\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u4ece\u6d45\u5c42\u5230\u6df1\u5c42\u9010\u6b65\u589e\u52a0\u9002\u914d\u5668\u5bb9\u91cf\u3002", "result": "\u5728\u5305\u62ec\u606f\u8089\u5206\u5272\u3001\u76ae\u80a4\u75c5\u53d8\u68c0\u6d4b\u548c\u4e73\u817a\u8d85\u58f0\u6210\u50cf\u5728\u5185\u7684\u4e94\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u7528613k\u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u6bd4\u7aef\u5230\u7aef\u5fae\u8c03\u5c11244\u500d\uff09\u5c31\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff1b\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6df1\u5c42\u9700\u8981\u66f4\u591a\u9002\u914d\u5bb9\u91cf\u3002", "conclusion": "Telescopic Adapters\u4e3a\u533b\u5b66VLSM\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u53ef\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "summary_cn": "\u5c06\u89c6\u89c9\u8bed\u8a00\u5206\u5272\u6a21\u578b\uff08VLSM\uff09\u9002\u914d\u5230\u533b\u5b66\u5f71\u50cf\u9886\u57df\u65f6\uff0c\u91c7\u7528\u4f20\u7edf\u7684\u5fae\u8c03\u65b9\u6cd5\u4f1a\u5e26\u6765\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5728\u6240\u6709Transformer\u5c42\u4e2d\u4f7f\u7528\u7edf\u4e00\u7684\u9002\u914d\u5668\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u53c2\u6570\u5206\u914d\u6b21\u4f18\u5e76\u964d\u4f4e\u9002\u914d\u6548\u7387\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u671b\u8fdc\u955c\u5f0f\u9002\u914d\u5668\u201d\uff08Telescopic Adapters\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684PEFT\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u7f29\u653e\u7b56\u7565\uff0c\u4ece\u6d45\u5c42\u5230\u6df1\u5c42\u9010\u6b65\u589e\u52a0\u9002\u914d\u5668\u5bb9\u91cf\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728CLIPSeg\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u4e2d\u96c6\u6210\u8f7b\u91cf\u7ea7\u74f6\u9888\u6a21\u5757\uff0c\u9002\u914d\u5668\u7ef4\u5ea6\u6839\u636e\u5c42\u6df1\u5ea6\u548c\u8bed\u4e49\u76f8\u5173\u6027\u52a8\u6001\u8c03\u6574\u3002\u4ec5\u4f7f\u752861.3\u4e07\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u6bd4\u7aef\u5230\u7aef\u5fae\u8c03\u5c11244\u500d\uff09\uff0cTelescopic Adapters\u5728\u6db5\u76d6\u606f\u8089\u5206\u5272\u3001\u76ae\u80a4\u75c5\u53d8\u68c0\u6d4b\u548c\u4e73\u817a\u8d85\u58f0\u6210\u50cf\u7684\u4e94\u4e2a\u591a\u6837\u5316\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u8bc1\u660e\uff0c\u6df1\u5c42\u7f51\u7edc\u6bd4\u6d45\u5c42\u9700\u8981\u663e\u8457\u66f4\u591a\u7684\u9002\u914d\u5bb9\u91cf\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u671b\u8fdc\u955c\u5f0f\u7f29\u653e\u5047\u8bbe\u3002\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66VLSM\u7684\u9ad8\u6548\u5fae\u8c03\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u540c\u65f6\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u5272\u7cbe\u5ea6\u3002"}}
