<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SOSControl: Enhancing Human Motion Generation through Saliency-Aware Symbolic Orientation and Timing Control](https://arxiv.org/abs/2601.14258)
*Ho Yin Au,Junkun Jiang,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SOS（Salient Orientation Symbolic）脚本的可编程符号化框架，用于在关键帧上精确指定身体部位朝向和运动时序，并配套开发了自动提取SOS脚本的流程和SOSControl生成框架，显著提升了文本到动作生成的可控性、质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统文本到动作生成方法缺乏精确控制；现有基于关节关键帧位置的方法仅提供位置引导，难以直观地指定身体部位朝向和运动时序。

Method: 1. 提出SOS脚本：一种可编程符号化框架，用于在关键帧指定身体部位朝向和运动时序。2. 设计自动SOS提取流程：采用时间约束的凝聚聚类进行帧显著性检测，并通过基于显著性的掩码方案（SMS）从动作数据中生成稀疏、可解释的SOS脚本。3. 构建SOSControl框架：将SOS脚本中的朝向符号视为显著约束，在动作生成中优先满足；结合SMS数据增强和基于梯度的迭代优化以提升对用户约束的对齐；使用基于ControlNet的ACTOR-PAE解码器确保动作输出的平滑自然。

Result: 实验表明，SOS提取流程能生成在显著关键帧上带有符号注释的、人类可解释的脚本；SOSControl框架在动作质量、可控性和泛化能力方面均优于现有基线，特别是在运动时序和身体部位朝向控制上。

Conclusion: 所提出的SOS脚本及其配套的SOSControl框架为文本到动作生成任务提供了一种更精确、直观且可解释的控制范式，有效解决了现有方法在朝向和时序控制上的不足。

Abstract: Traditional text-to-motion frameworks often lack precise control, and existing approaches based on joint keyframe locations provide only positional guidance, making it challenging and unintuitive to specify body part orientations and motion timing. To address these limitations, we introduce the Salient Orientation Symbolic (SOS) script, a programmable symbolic framework for specifying body part orientations and motion timing at keyframes. We further propose an automatic SOS extraction pipeline that employs temporally-constrained agglomerative clustering for frame saliency detection and a Saliency-based Masking Scheme (SMS) to generate sparse, interpretable SOS scripts directly from motion data. Moreover, we present the SOSControl framework, which treats the available orientation symbols in the sparse SOS script as salient and prioritizes satisfying these constraints during motion generation. By incorporating SMS-based data augmentation and gradient-based iterative optimization, the framework enhances alignment with user-specified constraints. Additionally, it employs a ControlNet-based ACTOR-PAE Decoder to ensure smooth and natural motion outputs. Extensive experiments demonstrate that the SOS extraction pipeline generates human-interpretable scripts with symbolic annotations at salient keyframes, while the SOSControl framework outperforms existing baselines in motion quality, controllability, and generalizability with respect to motion timing and body part orientation control.

Abstract (中文翻译): 传统的文本到动作生成框架通常缺乏精确控制，而现有的基于关节关键帧位置的方法仅提供位置引导，难以直观地指定身体部位的朝向和运动时序。为解决这些局限性，我们提出了显著朝向符号（Salient Orientation Symbolic, SOS）脚本，这是一种可编程的符号化框架，用于在关键帧上指定身体部位的朝向和运动时序。我们进一步提出了一种自动SOS提取流程，该流程采用时间约束的凝聚聚类进行帧显著性检测，并通过一种基于显著性的掩码方案（Saliency-based Masking Scheme, SMS）直接从动作数据中生成稀疏且可解释的SOS脚本。此外，我们提出了SOSControl框架，该框架将稀疏SOS脚本中可用的朝向符号视为显著约束，并在动作生成过程中优先满足这些约束。通过结合基于SMS的数据增强和基于梯度的迭代优化，该框架增强了与用户指定约束的对齐。同时，它采用基于ControlNet的ACTOR-PAE解码器来确保输出动作的平滑与自然。大量实验表明，SOS提取流程能够生成在显著关键帧上带有符号注释的人类可解释脚本，而SOSControl框架在动作质量、可控性和泛化能力方面均优于现有基线，尤其是在运动时序和身体部位朝向控制方面。

</details>


### [2] [A Cloud-Based Cross-Modal Transformer for Emotion Recognition and Adaptive Human-Computer Interaction](https://arxiv.org/abs/2601.14259)
*Ziwen Zhong,Zhitao Shu,Yue Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于云的跨模态Transformer（CMT）框架，用于多模态情绪识别，通过融合视觉、听觉和文本信号，在多个基准数据集上实现了SOTA性能，并在云环境中实现了低延迟、可扩展的实时情感交互。


<details>
  <summary>Details</summary>
Motivation: 现有情绪识别系统多依赖单一模态（如面部表情、语音语调或文本情感），在真实场景中鲁棒性差、泛化能力弱；同时缺乏高效、可扩展的部署方案以支持大规模人机交互。

Method: 采用预训练编码器（ViT、Wav2Vec2、BERT）分别提取视觉、听觉和文本特征，通过跨模态注意力机制建模异构特征间的复杂依赖关系，并利用Kubernetes与TensorFlow Serving构建云端分布式训练与推理架构。

Result: 在IEMOCAP、MELD和AffectNet数据集上，CMT相比强多模态基线提升F1分数3.0%，降低交叉熵损失12.9%；云端部署平均响应延迟为128ms，较传统Transformer融合系统降低35%。

Conclusion: 所提CMT框架有效实现了高效、实时的多模态情绪识别与自适应反馈，为智能客服、虚拟教学和情感计算接口等应用提供了可行方案，推动了云原生情感计算的发展。

Abstract: Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.

Abstract (中文翻译): 情绪识别是下一代人机交互（HCI）的核心组成部分，使机器能够感知、理解并响应用户的情感状态。然而，现有系统通常依赖单一模态分析（如面部表情、语音语调或文本情感），导致在现实环境中鲁棒性有限、泛化能力较差。为应对这些挑战，本研究提出了一种基于云的跨模态Transformer（CMT）框架，用于多模态情绪识别与自适应人机交互。该模型利用预训练编码器（Vision Transformer、Wav2Vec2和BERT）整合视觉、听觉和文本信号，并采用跨模态注意力机制捕捉异构特征之间的复杂相互依赖关系。通过利用云计算基础设施，结合Kubernetes上的分布式训练与TensorFlow Serving，该系统实现了面向大规模用户交互的可扩展、低延迟情绪识别。在IEMOCAP、MELD和AffectNet等基准数据集上的实验表明，CMT达到了当前最优性能，相较强大的多模态基线模型，F1分数提升了3.0%，交叉熵损失降低了12.9%。此外，云端部署评估显示其平均响应延迟为128毫秒，比传统基于Transformer的融合系统降低了35%。这些结果证实，所提出的框架能够在智能客服、虚拟辅导系统和情感计算界面等应用中实现高效、实时的情绪识别与自适应反馈，标志着向云原生情感计算和情感智能交互系统迈出了重要一步。

</details>


### [3] [Intelligent Power Grid Design Review via Active Perception-Enabled Multimodal Large Language Models](https://arxiv.org/abs/2601.14261)
*Taoliang Tan,Chengwei Ma,Zhen Tian,Zhao Lin,Dongdong Li,Si Shi*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型（MLLM）和提示工程的三阶段框架，用于智能审阅电网工程图纸，有效解决了超高分辨率图纸带来的计算负担、信息丢失及缺乏整体语义理解等问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化系统在审阅超高分辨率电网工程图纸时面临计算开销大、信息丢失以及缺乏整体语义理解能力等挑战，难以准确识别设计错误，因此亟需一种更高效、可靠的智能审图方法。

Method: 提出一个三阶段框架：第一阶段利用MLLM从低分辨率概览图中进行全局语义理解并提出关键语义区域；第二阶段在这些区域中进行高分辨率细粒度识别，并获取带置信度的详细信息；第三阶段通过决策模块整合置信度感知的结果，诊断设计错误并评估可靠性。

Result: 在真实电网图纸上的初步实验表明，该方法显著提升了MLLM对宏观语义信息的理解能力与设计错误定位精度，在缺陷发现准确率和审图判断可靠性方面优于传统被动式MLLM推理。

Conclusion: 本研究提供了一种新颖的、由提示驱动的智能电网图纸审阅范式，具有更高的准确性和可靠性。

Abstract: The intelligent review of power grid engineering design drawings is crucial for power system safety. However, current automated systems struggle with ultra-high-resolution drawings due to high computational demands, information loss, and a lack of holistic semantic understanding for design error identification. This paper proposes a novel three-stage framework for intelligent power grid drawing review, driven by pre-trained Multimodal Large Language Models (MLLMs) through advanced prompt engineering. Mimicking the human expert review process, the first stage leverages an MLLM for global semantic understanding to intelligently propose domain-specific semantic regions from a low-resolution overview. The second stage then performs high-resolution, fine-grained recognition within these proposed regions, acquiring detailed information with associated confidence scores. In the final stage, a comprehensive decision-making module integrates these confidence-aware results to accurately diagnose design errors and provide a reliability assessment. Preliminary results on real-world power grid drawings demonstrate our approach significantly enhances MLLM's ability to grasp macroscopic semantic information and pinpoint design errors, showing improved defect discovery accuracy and greater reliability in review judgments compared to traditional passive MLLM inference. This research offers a novel, prompt-driven paradigm for intelligent and reliable power grid drawing review.

Abstract (中文翻译): 电网工程设计图纸的智能审阅对电力系统安全至关重要。然而，当前的自动化系统由于计算需求高、信息丢失以及缺乏用于识别设计错误的整体语义理解能力，难以处理超高清图纸。本文提出了一种新颖的三阶段框架，通过先进的提示工程驱动预训练的多模态大语言模型（MLLM）进行智能电网图纸审阅。该框架模拟人类专家的审阅流程：第一阶段利用MLLM从低分辨率概览图中进行全局语义理解，智能地提出特定领域的语义区域；第二阶段在这些区域中进行高分辨率、细粒度的识别，获取带有置信度评分的详细信息；第三阶段通过一个综合决策模块整合这些置信度感知的结果，准确诊断设计错误并提供可靠性评估。在真实电网图纸上的初步结果表明，我们的方法显著增强了MLLM把握宏观语义信息和定位设计错误的能力，在缺陷发现准确率和审阅判断可靠性方面优于传统的被动式MLLM推理。本研究为智能、可靠的电网图纸审阅提供了一种新颖的、由提示驱动的新范式。

</details>


### [4] [LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models](https://arxiv.org/abs/2601.14330)
*Mengyu Sun,Ziyuan Yang,Andrew Beng Jin Teoh,Junxu Liu,Haibo Hu,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出LURE方法，通过重构潜在空间和引导采样轨迹，在扩散模型中高效地重新唤醒被擦除的多个概念。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法存在漏洞，被擦除的概念仍可被重新唤醒；而当前的重唤醒方法仅关注提示词层面的优化，忽略了生成过程中的其他关键因素，导致对擦除机制的理解不全面。

Method: 将扩散模型的生成过程建模为隐函数，理论分析文本条件、模型参数和潜在状态等因素对概念重唤醒的影响；提出LURE方法，包括语义重绑定机制、梯度场正交化（解决多概念下的梯度冲突）和潜在语义识别引导采样（LSIS）以确保重唤醒稳定性。

Result: 实验表明，LURE能在多种擦除任务和方法下，同时高保真地重唤醒多个被擦除的概念。

Conclusion: 该工作揭示了当前概念擦除方法的根本性缺陷，并提供了一种系统性的重唤醒框架，强调了在潜在空间操作的重要性，对提升擦除鲁棒性具有指导意义。

Abstract: Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.

Abstract (中文翻译): 概念擦除旨在抑制扩散模型中的敏感内容，但近期研究表明，被擦除的概念仍可能被重新唤醒，暴露出擦除方法的脆弱性。现有的重唤醒方法主要依赖于提示词层面的优化来操纵采样轨迹，忽视了其他生成因素，从而限制了对底层动态机制的全面理解。本文将生成过程建模为一个隐函数，以实现对包括文本条件、模型参数和潜在状态在内的多种因素的综合理论分析。我们从理论上证明，扰动任一因素均可重新唤醒被擦除的概念。基于此洞察，我们提出了一种新颖的概念重唤醒方法：用于概念重唤醒的潜在空间解封（LURE），该方法通过重构潜在空间并引导采样轨迹来重唤醒被擦除的概念。具体而言，我们的语义重绑定机制通过将去噪预测与目标分布对齐来重构潜在空间，从而重建被切断的文本-视觉关联。然而，在多概念场景中，简单的重构可能导致梯度冲突和特征纠缠。为解决此问题，我们引入了梯度场正交化，通过强制特征正交性来防止相互干扰。此外，我们的潜在语义识别引导采样（LSIS）通过后验密度验证确保重唤醒过程的稳定性。大量实验表明，LURE能够在多种擦除任务和方法下，同时高保真地重唤醒多个被擦除的概念。

</details>


### [5] [CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments](https://arxiv.org/abs/2601.14339)
*Haotian Xu,Yue Hu,Zhengqiu Zhu,Chen Gao,Ziyou Wang,Junreng Rao,Wenhao Lu,Weishi Li,Quanjun Yin,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了CityCube，一个用于评估视觉语言模型（VLMs）在城市环境中跨视角空间推理能力的新基准，揭示了当前VLMs与人类在该任务上的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注室内或街道场景，忽视了开放城市空间中丰富的语义、复杂几何结构和视角变化所带来的独特挑战，因此需要一个专门针对城市环境的跨视角推理基准。

Method: 作者构建了CityCube基准，整合了四种视角动态以模拟相机运动，并涵盖来自车辆、无人机和卫星等多种平台的广泛视角。该基准包含5,022个精心标注的多视角问答对，分为五个认知维度和三种空间关系表达形式，并对33个VLMs进行了全面评估。

Result: 评估结果显示，即使大规模VLMs的准确率也未超过54.1%，比人类表现低34.2%；而小规模微调VLMs则能达到60.0%以上的准确率。进一步分析揭示了任务间的相关性以及VLMs与人类推理之间的根本认知差异。

Conclusion: CityCube有效揭示了当前VLMs在城市跨视角空间推理方面的不足，强调了开发更贴近人类认知能力模型的必要性，并为未来研究提供了重要基准。

Abstract: Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.

Abstract (中文翻译): 跨视角空间推理对于具身人工智能至关重要，是复杂环境中空间理解、心理模拟和规划的基础。现有基准主要侧重于室内或街道场景，忽视了开放式城市空间所特有的丰富语义、复杂几何结构和视角变化带来的独特挑战。为此，我们提出了CityCube——一个系统性的基准，旨在探究当前视觉语言模型（VLMs）在城市环境中的跨视角推理能力。CityCube整合了四种视角动态以模拟相机运动，并涵盖来自多种平台（如车辆、无人机和卫星）的广泛视角。为实现全面评估，该基准包含5,022个精心标注的多视角问答对，按五个认知维度和三种空间关系表达方式进行分类。对33个VLMs的综合评估显示，其性能与人类存在显著差距：即使是大规模模型，准确率也未能超过54.1%，比人类表现低34.2%。相比之下，小规模微调的VLMs准确率可超过60.0%，凸显了本基准的必要性。进一步分析揭示了任务间的相关性以及VLMs与类人推理之间存在的根本认知差异。

</details>


### [6] [Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data](https://arxiv.org/abs/2601.14406)
*Yixiong Chen,Zongwei Zhou,Wenxuan Li,Alan Yuille*

Main category: cs.CV

TL;DR: 提出SegAE，一种轻量级视觉-语言模型，用于自动评估医学图像分割标签质量，提升数据效率与训练性能。


<details>
  <summary>Details</summary>
Motivation: 大规模医学分割数据集常混合人工标注和伪标签，质量参差不齐，影响模型训练和评估的可靠性。

Method: 开发SegAE（Segmentation Assessment Engine），一种基于超过400万带质量评分图像-标签对训练的轻量级视觉-语言模型，可自动预测142种解剖结构的标签质量。

Result: SegAE与真实Dice相似度的相关系数达0.902，单个3D掩码评估仅需0.06秒；在公开数据集中发现普遍存在低质量标注；在主动学习和半监督学习中减少三分之一标注成本和70%的质量检查时间。

Conclusion: SegAE为大规模医学分割数据集提供了一种高效、简便的标签质量控制工具，显著提升数据利用效率与模型训练鲁棒性。

Abstract: Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.

Abstract (中文翻译): 大规模医学分割数据集通常结合了人工标注和质量不一的伪标签，这可能损害模型的训练与评估效果。低质量标签会阻碍模型性能，并降低训练的鲁棒性。为解决这一问题，我们提出了SegAE（分割评估引擎），一种轻量级的视觉-语言模型（VLM），可自动预测142种解剖结构的标签质量。该模型在超过四百万个带有质量评分的图像-标签对上进行训练，与真实Dice相似度的相关系数高达0.902，并可在0.06秒内评估一个3D掩码。SegAE展现出多项实用优势：（I）我们的分析揭示了多个公开数据集中普遍存在低质量标注；（II）SegAE在主动学习和半监督学习中提升了数据效率和训练性能，将数据集标注成本降低三分之一，每个标签的质量检查时间减少70%。该工具为大规模医学分割数据集的质量控制提供了一种简单而有效的解决方案。相关数据集、模型权重和代码已发布于 https://github.com/Schuture/SegAE。

</details>


### [7] [Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation](https://arxiv.org/abs/2601.14438)
*Danial Sadrian Zadeh,Otman A. Basir,Behzad Moshiri*

Main category: cs.CV

TL;DR: 本文提出一种新框架，将单张前视摄像头图像转化为自然语言描述，以捕捉交通场景的空间布局、语义关系和驾驶相关线索，并构建了基于BDD100K的新数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要准确感知和理解交通场景以确保安全行驶，而现有方法在从单视角图像生成丰富语义描述方面仍存在不足，且缺乏专门的数据集和评估标准。

Method: 提出一种结合混合注意力机制的模型，增强空间与语义特征提取，并融合这些特征生成上下文丰富的自然语言描述；同时构建了一个基于BDD100K的新数据集，并讨论了适用于该任务的评估指标。

Result: 在新构建的数据集上，通过CIDEr、SPICE等自动指标及人工评估，验证了所提模型在生成准确、详细交通场景描述方面的优越性能。

Conclusion: 该研究为交通场景理解提供了一种有效的方法，不仅提升了从单视角图像生成自然语言描述的能力，还为后续研究提供了新的数据资源和评估基准。

Abstract: Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.

Abstract (中文翻译): 交通场景理解对于使自动驾驶车辆能够准确感知和解释其环境，从而确保安全导航至关重要。本文提出了一种新颖的框架，可将单张前视摄像头图像转化为简洁的自然语言描述，有效捕捉空间布局、语义关系以及与驾驶相关的线索。所提出的模型利用混合注意力机制，增强空间和语义特征的提取，并融合这些特征以生成上下文丰富且详细的场景描述。为解决该领域专用数据集稀缺的问题，本文基于BDD100K数据集构建了一个新数据集，并提供了详尽的构建指南。此外，研究还深入探讨了相关评估指标，确定了最适合该任务的度量方法。通过CIDEr和SPICE等定量指标以及人工判断评估的大量实验表明，所提出的模型在新构建的数据集上表现优异，有效实现了预期目标。

</details>


### [8] [Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2601.14448)
*A. Enes Doruk*

Main category: cs.CV

TL;DR: 本文提出一种基于高斯的自适应相机-LiDAR多模态3D占据预测模型，通过四个关键组件有效融合语义与几何信息，在降低计算复杂度的同时提升动态环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中稀疏目标检测难以应对长尾安全挑战，而现有密集3D语义占据预测方法在体素化过程中存在计算复杂度高、融合过程脆弱且静态、难以适应动态环境等问题。

Method: 提出一种基于高斯的自适应多模态3D占据预测模型，包含四个核心组件：(1) LiDAR深度特征聚合（LDFA），采用深度可变形采样处理几何稀疏性；(2) 基于熵的特征平滑，利用交叉熵抑制域特异性噪声；(3) 自适应相机-LiDAR融合，根据模型输出动态重校准传感器数据；(4) Gauss-Mamba Head，采用选择性状态空间模型实现线性复杂度的全局上下文解码。

Result: 所提方法通过高效融合相机的语义优势与LiDAR的几何优势，在保持内存效率的同时显著提升了在动态环境中的3D占据预测性能和鲁棒性。

Conclusion: 该研究为解决自动驾驶中长尾安全挑战提供了一种高效、鲁棒的3D语义占据预测新范式，其自适应融合机制和线性复杂度设计具有实际部署潜力。

Abstract: The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.

Abstract (中文翻译): 面向自动驾驶中的长尾安全挑战，从稀疏目标检测范式转向密集3D语义占据预测是必要的。然而，当前的体素化方法通常面临过高的计算复杂度，其融合过程脆弱、静态，且在动态环境设置下容易失效。为此，本研究提出一种新颖的基于高斯的自适应相机-LiDAR多模态3D占据预测模型，通过内存高效的3D高斯模型无缝结合相机模态的语义优势与LiDAR模态的几何优势。所提方案包含四个关键组件：(1) LiDAR深度特征聚合（LDFA），采用深度方向的可变形采样以应对几何稀疏性；(2) 基于熵的特征平滑，利用交叉熵处理特定域的噪声；(3) 自适应相机-LiDAR融合，根据模型输出对传感器输出进行动态重校准；(4) Gauss-Mamba Head，采用选择性状态空间模型进行全局上下文解码，并享有线性计算复杂度。

</details>


### [9] [Real-Time Wildfire Localization on the NASA Autonomous Modular Sensor using Deep Learning](https://arxiv.org/abs/2601.14475)
*Yajvan Ravan,Aref Malek,Chester Dolph,Nikhil Behari*

Main category: cs.CV

TL;DR: 本文提出了一种基于NASA多光谱高空气球图像的新型野火检测数据集，并开发了一个结合分类与分割的深度学习模型，在夜间和有云遮挡条件下仍能高效准确地识别活跃野火区域。


<details>
  <summary>Details</summary>
Motivation: 高海拔多光谱航空图像稀缺且昂贵，但对野火检测等高影响力任务中的机器学习算法发展至关重要。现有方法（如卫星数据或基于颜色规则的传统算法）在复杂场景下表现有限，亟需更鲁棒、高效的解决方案。

Method: 作者构建了一个包含12通道（含红外、短波红外和热红外）的高空气球野火图像数据集，涵盖20次任务、4000余张带标注的小图块。在此基础上训练两个深度神经网络：一个用于图像分类，另一个用于像素级分割，并将二者融合为实时分割模型以定位活跃野火。

Result: 该模型在分类准确率达96%，IoU为74%，召回率为84%，优于以往基于卫星数据或传统颜色规则的方法。模型能有效在夜间及云层遮挡下检测野火，并区分易混淆的假阳性样本。研究还发现SWIR、IR和热红外波段对火场边界识别最为关键。

Conclusion: 利用多光谱高空图像训练的深度学习模型可显著提升野火检测的准确性与鲁棒性，尤其适用于复杂环境下的实时应用。所提出的模型和公开数据集为未来相关研究提供了重要基础。

Abstract: High-altitude, multi-spectral, aerial imagery is scarce and expensive to acquire, yet it is necessary for algorithmic advances and application of machine learning models to high-impact problems such as wildfire detection. We introduce a human-annotated dataset from the NASA Autonomous Modular Sensor (AMS) using 12-channel, medium to high altitude (3 - 50 km) aerial wildfire images similar to those used in current US wildfire missions. Our dataset combines spectral data from 12 different channels, including infrared (IR), short-wave IR (SWIR), and thermal. We take imagery from 20 wildfire missions and randomly sample small patches to generate over 4000 images with high variability, including occlusions by smoke/clouds, easily-confused false positives, and nighttime imagery.
  We demonstrate results from a deep-learning model to automate the human-intensive process of fire perimeter determination. We train two deep neural networks, one for image classification and the other for pixel-level segmentation. The networks are combined into a unique real-time segmentation model to efficiently localize active wildfire on an incoming image feed. Our model achieves 96% classification accuracy, 74% Intersection-over-Union(IoU), and 84% recall surpassing past methods, including models trained on satellite data and classical color-rule algorithms. By leveraging a multi-spectral dataset, our model is able to detect active wildfire at nighttime and behind clouds, while distinguishing between false positives. We find that data from the SWIR, IR, and thermal bands is the most important to distinguish fire perimeters. Our code and dataset can be found here: https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main and https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link

Abstract (中文翻译): 高海拔多光谱航空图像获取困难且成本高昂，但对于推动算法进步并将机器学习模型应用于野火检测等高影响力问题至关重要。我们引入了一个由NASA自主模块化传感器（AMS）采集的人工标注数据集，包含12通道、中高海拔（3–50公里）的航空野火图像，与当前美国野火任务所用图像类似。该数据集融合了来自12个不同通道的光谱数据，包括红外（IR）、短波红外（SWIR）和热红外波段。我们从20次野火任务中提取图像，并随机采样小图块，生成了超过4000张具有高度多样性的图像，涵盖烟雾/云层遮挡、易混淆的假阳性样本以及夜间图像。我们展示了一个深度学习模型，用于自动化原本依赖人工的火场边界判定过程。我们训练了两个深度神经网络，一个用于图像分类，另一个用于像素级分割，并将二者结合成一种独特的实时分割模型，以高效地在输入图像流中定位活跃野火区域。我们的模型实现了96%的分类准确率、74%的交并比（IoU）和84%的召回率，优于以往方法，包括基于卫星数据训练的模型和传统的颜色规则算法。通过利用多光谱数据集，我们的模型能够在夜间和云层遮挡下检测活跃野火，同时有效区分假阳性样本。我们发现，SWIR、IR和热红外波段的数据对于区分火场边界最为关键。我们的代码和数据集可通过以下链接获取：https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main 和 https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link。

</details>


### [10] [XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping](https://arxiv.org/abs/2601.14477)
*Frank Bieder,Hendrik Königshof,Haohao Hu,Fabian Immel,Yinzhe Shen,Jan-Hendrik Pauls,Christoph Stiller*

Main category: cs.CV

TL;DR: 本文提出了一种名为XD-MAP的新方法，通过利用图像数据集中的神经网络检测结果构建语义参数化地图，从而在无需人工标注的情况下将相机图像中的语义知识迁移到LiDAR点云域，显著提升了2D/3D语义和全景分割性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的性能高度依赖于训练数据与目标域（包括对象类别、传感器特性及模态）的一致性。现有数据集往往难以覆盖实际部署场景，因此需要有效的跨域迁移方法来弥合这一差距。

Method: 提出XD-MAP方法：利用相机图像上预训练神经网络的检测结果生成语义参数化地图；该地图可自动为LiDAR目标域生成伪标签，无需人工标注；且不要求传感器之间存在直接重叠，还能将前视相机的感知范围扩展至360度全景。

Result: 在大规模道路特征数据集上，XD-MAP相比单次检测基线方法，在2D语义分割上提升+19.5 mIoU，2D全景分割提升+19.5 PQth，3D语义分割提升+32.3 mIoU。

Conclusion: XD-MAP能有效实现从图像到LiDAR的跨模态知识迁移，在无需任何人工标注的前提下，在LiDAR数据上取得优异的分割性能，验证了其在开放世界感知任务中的潜力。

Abstract: Until open-world foundation models match the performance of specialized approaches, the effectiveness of deep learning models remains heavily dependent on dataset availability. Training data must align not only with the target object categories but also with the sensor characteristics and modalities. To bridge the gap between available datasets and deployment domains, domain adaptation strategies are widely used. In this work, we propose a novel approach to transferring sensor-specific knowledge from an image dataset to LiDAR, an entirely different sensing domain. Our method XD-MAP leverages detections from a neural network on camera images to create a semantic parametric map. The map elements are modeled to produce pseudo labels in the target domain without any manual annotation effort. Unlike previous domain transfer approaches, our method does not require direct overlap between sensors and enables extending the angular perception range from a front-view camera to a full 360 view. On our large-scale road feature dataset, XD-MAP outperforms single shot baseline approaches by +19.5 mIoU for 2D semantic segmentation, +19.5 PQth for 2D panoptic segmentation, and +32.3 mIoU in 3D semantic segmentation. The results demonstrate the effectiveness of our approach achieving strong performance on LiDAR data without any manual labeling.

Abstract (中文翻译): 在开放世界基础模型达到专用方法的性能水平之前，深度学习模型的有效性仍严重依赖于数据集的可用性。训练数据不仅需与目标物体类别一致，还需匹配传感器特性和模态。为弥合现有数据集与部署领域之间的差距，领域自适应策略被广泛采用。本文提出了一种新颖的方法，将图像数据集中特定于传感器的知识迁移到完全不同的感知域——LiDAR。我们的方法XD-MAP利用神经网络在相机图像上的检测结果构建语义参数化地图，并据此在目标域中生成伪标签，无需任何人工标注。与以往的域迁移方法不同，本方法不要求传感器之间存在直接重叠，并能将前视相机的感知角度扩展至完整的360度视野。在我们构建的大规模道路特征数据集上，XD-MAP在2D语义分割上比单次检测基线方法高出+19.5 mIoU，在2D全景分割上提升+19.5 PQth，在3D语义分割上提升+32.3 mIoU。实验结果表明，该方法在无需人工标注的情况下，在LiDAR数据上实现了优异的性能。

</details>
