<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR是一种基于语义解耦的无限制人脸隐私保护框架，通过在扩散模型潜在空间中进行语义引导重组，在保持机器可识别身份一致性的同时生成视觉上匿名但自然的人脸图像，并支持密码授权下的原始图像恢复。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别技术深度融入在线银行、身份验证等网络服务，如何在图像存储与传输过程中有效将身份信息与视觉表征解耦，成为隐私保护的关键挑战。

Method: SIDeR将人脸图像分解为机器可识别的身份特征向量和视觉可感知的语义外观成分，利用扩散模型潜在空间中的语义引导重组机制生成对抗性匿名人脸；引入动量驱动的无限制扰动优化和语义-视觉平衡因子，合成多样且自然的对抗样本；同时支持通过正确密码恢复原始图像。

Result: 在CelebA-HQ和FFHQ数据集上的实验表明，SIDeR在黑盒场景下攻击成功率达99%，在基于PSNR的图像恢复质量上比基线方法提升41.28%。

Conclusion: SIDeR在保障人脸图像视觉匿名性的同时维持了身份一致性，并实现了高质量的可逆恢复，为人脸隐私保护提供了一种高效且实用的新范式。

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

Abstract (中文翻译): 随着人脸识别技术深度融入在线银行、身份验证等联网服务，在图像存储与传输过程中实现身份信息与视觉表征的有效解耦已成为隐私保护的关键挑战。为此，我们提出了SIDeR——一种基于语义解耦驱动的无限制人脸隐私保护框架。SIDeR将人脸图像分解为机器可识别的身份特征向量和视觉可感知的语义外观成分，通过在扩散模型潜在空间中进行语义引导的重组，生成视觉上匿名但保持机器级身份一致性的对抗人脸。该框架结合动量驱动的无限制扰动优化策略与语义-视觉平衡因子，能够合成多种视觉多样且高度自然的对抗样本。此外，对于授权访问，当提供正确密码时，受保护图像可被还原为原始形式。在CelebA-HQ和FFHQ数据集上的大量实验表明，SIDeR在黑盒场景下攻击成功率达到99%，在基于PSNR的恢复质量方面比基线方法高出41.28%。

</details>


### [2] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: UniTrack 是一种即插即用的图论损失函数，通过端到端可微学习统一优化多目标跟踪中的检测、身份保持和时空一致性，显著提升各类 MOT 模型性能，无需修改原有架构。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的多目标跟踪方法通常需要重新设计跟踪架构，限制了通用性和部署灵活性。作者旨在提出一种通用、可插拔的训练目标，直接优化跟踪任务的核心指标，同时兼容现有系统。

Method: UniTrack 将检测精度、身份保持和时空一致性整合为一个统一的、端到端可微的图论损失函数，通过可微图表示学习，使网络能学习跨帧的运动连续性和身份关系的整体表示。

Result: 在多个主流 MOT 模型（如 Trackformer、MOTR、FairMOT、ByteTrack、GTR、MOTE）和挑战性基准上验证，UniTrack 一致提升性能，最多减少 53% 的 ID 切换，IDF1 提升达 12%，其中 GTR 在 SportsMOT 上 MOTA 提高 9.7%。

Conclusion: UniTrack 作为一种通用、即插即用的损失函数，有效提升了多目标跟踪系统的性能，无需修改模型结构，具有良好的泛化能力和实用价值。

Abstract: We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

Abstract (中文翻译): 我们提出了 UniTrack，一种即插即用的图论损失函数，旨在通过统一的可微学习直接优化多目标跟踪（MOT）特定目标，从而显著提升 MOT 性能。与以往需要重新设计跟踪架构的基于图的 MOT 方法不同，UniTrack 提供了一种通用的训练目标，将检测精度、身份保持和时空一致性整合到单一的端到端可训练损失函数中，能够无缝集成到现有 MOT 系统中而无需修改架构。通过可微图表示学习，UniTrack 使网络能够学习跨帧的运动连续性和身份关系的整体表示。我们在多种跟踪模型和多个具有挑战性的基准上验证了 UniTrack，结果表明其在所有测试架构和数据集（包括 Trackformer、MOTR、FairMOT、ByteTrack、GTR 和 MOTE）上均带来一致的性能提升。大量实验显示，在具有挑战性的基准上，身份切换最多减少了 53%，IDF1 指标提升了 12%，其中 GTR 在 SportsMOT 数据集上实现了高达 9.7% 的 MOTA 性能增益。

</details>


### [3] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: 本文提出一种训练框架，通过增强视觉条件依赖来改善Vision-Language-Action（VLA）模型中的视觉-动作对齐问题，在不修改架构或增加数据的前提下提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有将大规模预训练视觉-语言模型（VLMs）扩展到动作空间的方法存在视觉-动作不对齐问题，即动作预测对当前视觉状态依赖较弱，导致输出不可靠。作者观察到成功执行轨迹比失败轨迹具有更强的视觉依赖性，因此希望显式增强VLA模型中的视觉条件作用。

Method: 方法分为两步：首先在轨迹跟随代理任务上通过偏好优化使动作预测与视觉输入对齐；然后在监督微调阶段通过潜在空间蒸馏将增强的对齐能力迁移到指令跟随任务中。

Result: 该方法在离散OpenVLA模型上同时提升了视觉条件性和任务性能，并在连续动作空间的OpenVLA-OFT设置下也取得一致改进。

Conclusion: 显式强化VLA模型中的视觉条件依赖可有效缓解视觉-动作不对齐问题，提升策略可靠性，且无需架构改动或额外数据收集。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

Abstract (中文翻译): 视觉-语言-动作（VLA）模型已在多种机器人操作任务中展现出强大性能。然而，尽管取得了成功，将大规模预训练的视觉-语言模型（VLMs）扩展到动作空间可能导致视觉-动作错位问题，即动作预测对当前视觉状态的依赖较弱，从而产生不可靠的动作输出。本文从视觉条件依赖的角度研究VLA模型，并通过实验表明，成功的执行轨迹始终比失败轨迹表现出更强的视觉依赖性。受此启发，我们提出一种训练框架，显式增强VLA模型中的视觉条件作用。该方法首先在轨迹跟随代理任务上通过偏好优化实现动作预测与视觉输入的对齐，然后在监督微调阶段通过潜在空间蒸馏将这种增强的对齐能力迁移到指令跟随任务中。在不引入架构修改或额外数据收集的情况下，我们的方法提升了离散OpenVLA模型的视觉条件性和任务性能，并在扩展到连续动作空间的OpenVLA-OFT设置时也持续带来性能增益。项目网站：https://vista-vla.github.io/。

</details>


### [4] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文综述了基于图像的饮食评估中用于精确估算食物份量的不同策略，重点解决从2D图像估计3D食物尺寸的挑战。


<details>
  <summary>Details</summary>
Motivation: 基于图像的饮食评估在慢性病和肥胖防控中具有重要作用，但其核心难点在于从二维图像准确估计食物的三维体积。

Method: 综述了多种克服该限制的策略，包括使用深度图、多视角输入、模板匹配等辅助信息，以及利用深度学习结合单目图像或辅助输入进行份量预测。

Result: 系统梳理了当前用于提升食物份量估计准确性的主要技术路径。

Conclusion: 尽管存在挑战，融合深度学习与多模态输入的方法展现出改善图像基饮食评估精度的潜力。

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

Abstract (中文翻译): 依赖图像进行饮食评估是一种重要策略，能够准确且便捷地监测个体健康状况，从而在慢性疾病和肥胖的预防与照护中发挥关键作用。然而，基于图像的饮食评估面临一个核心难题：从二维图像输入中估算食物的三维尺寸。为克服这一关键限制，研究者提出了多种策略，例如使用深度图、多视角输入等辅助信息，或采用基于模型的方法（如模板匹配）。深度学习技术也有助于弥合这一差距，可通过单目图像或结合图像与辅助输入，精确预测图像中的食物份量。本文探讨了用于实现准确份量估算的各种策略。

</details>


### [5] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出了一种名为视觉概念排序（VCR）的方法，用于识别大型多模态模型（LMMs）中的关键视觉概念，并揭示其在医疗任务（如皮肤癌分类）中对不同人口群体表现不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，确保机器学习模型的可靠性至关重要。现有方法难以有效揭示模型在处理医学图像时的潜在缺陷，尤其是在不同人群间的性能差异方面，因此需要一种可解释且可验证的审计方法。

Method: 作者提出“视觉概念排序”（Visual Concept Ranking, VCR）方法，通过分析LMM在医学任务提示下的行为，识别其依赖的关键视觉特征，并生成可手动验证的假设。

Result: 研究发现，LMM在使用示例提示进行皮肤病变分类时，在不同人口亚群之间存在意外的性能差距；VCR成功识别出模型所依赖的视觉概念，并通过人工干预验证了这些依赖关系。

Conclusion: VCR为审计大型多模态模型在医疗任务中的行为提供了一种有效工具，有助于揭示模型偏差并提升其在安全关键场景中的可靠性。

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

Abstract (中文翻译): 确保机器学习模型在医疗等安全关键领域的可靠性，需要能够揭示模型缺陷的审计方法。我们提出了一种用于识别大型多模态模型（LMMs）中重要视觉概念的方法，并利用该方法研究这些模型在面对医疗任务提示时所表现出的行为。我们主要聚焦于从临床皮肤科图像中分类恶性皮肤病变的任务，并辅以胸部X光片和自然图像的补充实验。在展示LMM在使用示例提示时在不同人口亚群之间存在意外的性能差距后，我们将所提出的“视觉概念排序”（Visual Concept Ranking, VCR）方法应用于这些模型和提示。VCR能够生成与不同视觉特征依赖相关的假设，随后我们通过人工干预对这些假设进行了验证。

</details>


### [6] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: 提出CLEAR-HPV框架，在无需概念标签的情况下，通过注意力机制在MIL模型中自动发现可解释的形态学概念（如角化、基底样、间质），将高维特征压缩为仅10个可解释概念，并在多个数据集上验证其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的多实例学习（MIL）方法虽能有效预测HPV状态，但缺乏形态学层面的可解释性，限制了临床理解和应用。

Method: 提出CLEAR-HPV框架，在注意力加权的潜在空间中自动发现形态学概念，生成空间概念图，并用紧凑的概念比例向量表示整张切片，无需训练时提供概念标签。

Result: CLEAR-HPV在TCGA-HNSCC、TCGA-CESC和CPTAC-HNSCC等多个数据集上表现稳定，将1536维的高维特征压缩至仅10个可解释概念，同时保留原始MIL嵌入的预测信息。

Conclusion: CLEAR-HPV为基于注意力的MIL模型提供了一种通用、与骨干网络无关的可解释性方法，实现了全切片病理图像分析中概念级别的紧凑且可解释的表示。

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

Abstract (中文翻译): 人乳头瘤病毒（HPV）状态是头颈癌和宫颈癌预后及治疗反应的关键决定因素。尽管基于注意力机制的多实例学习（MIL）在HPV相关全切片病理图像的切片级预测方面表现出色，但其在形态学可解释性方面仍存在局限。为解决这一问题，我们提出了CLEAR-HPV（Concept-Level Explainable Attention-guided Representation for HPV）框架，该框架利用注意力机制重构MIL潜在空间，无需在训练过程中提供概念标签即可实现概念发现。CLEAR-HPV在注意力加权的潜在空间中自动识别出角化、基底样和间质等形态学概念，生成空间概念图，并使用紧凑的概念比例向量表示每张切片。CLEAR-HPV的概念比例向量在保留原始MIL嵌入预测信息的同时，将高维特征空间（例如1536维）压缩至仅10个可解释的概念。CLEAR-HPV在TCGA-HNSCC、TCGA-CESC和CPTAC-HNSCC等多个数据集上均展现出良好的泛化能力，为全切片病理图像的注意力型MIL模型提供了一种通用、与骨干网络无关的紧凑且概念级可解释的框架。

</details>


### [7] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文提出ARGaze，一种基于自回归机制的在线第一人称视线估计方法，通过结合当前视觉特征和近期视线历史，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有第一人称（自我中心）视线估计缺乏明确的头部或眼部信号，需依赖稀疏、间接的线索（如手-物交互）推断注意力；同时，目标导向活动中视线具有强时间连续性，可作为预测先验，但现有方法未充分利用这一特性。

Method: 提出ARGaze模型，将视线估计重构为序列预测任务：使用Transformer解码器，在每个时间步以（i）当前视觉特征和（ii）固定长度的近期视线目标估计窗口（Gaze Context Window）为条件，进行因果、流式推理。

Result: 在多个自我中心视线估计基准的在线评估设置下达到最先进性能；消融实验证明，利用有限历史的自回归建模对鲁棒预测至关重要。

Conclusion: ARGaze有效利用视线的时间连续性，通过自回归方式实现高效准确的在线第一人称视线估计，为AR与辅助技术提供实用解决方案；作者将开源代码与预训练模型。

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

Abstract (中文翻译): 在线自我中心视线估计旨在仅使用过去和当前帧的第一人称视频，预测佩戴摄像头者当前注视的位置，这项任务对增强现实和辅助技术至关重要。与第三人称视线估计不同，该场景缺乏明确的头部或眼部信号，要求模型从手-物交互和显著场景内容等稀疏、间接线索中推断当前视觉注意力。我们观察到，在目标导向活动中，视线表现出强烈的时间连续性：了解用户最近的注视位置可为预测下一步注视提供有力先验。受视觉-语言模型中视觉条件自回归解码的启发，我们提出ARGaze方法，将视线估计重构为序列预测任务：在每个时间步，Transformer解码器通过结合（i）当前视觉特征和（ii）一个固定长度的近期视线目标估计窗口（Gaze Context Window）来预测当前视线。该设计保证了因果性，并支持资源受限的流式推理。我们在多个自我中心视线基准的在线评估设置下取得了最先进的性能，大量消融实验验证了利用有限视线历史的自回归建模对鲁棒预测至关重要。我们将发布源代码和预训练模型。

</details>


### [8] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: 本文首次系统评估了基于视觉的手部追踪模型在戴手套场景下的表现，发现现有裸手模型因外观差异在手套上性能显著下降，并提出AirGlove方法，利用已有手套数据提升对新类型手套的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于传感器的手套追踪方法受限于信号质量和校准精度，而基于视觉的方法虽在裸手上表现优异，但在不同外观的手套上效果尚未充分研究。因此，亟需探索视觉模型在戴手套场景中的适用性并提升其泛化能力。

Method: 作者首先对现有视觉手部追踪模型在手套数据上的零样本和微调设置下进行系统评估；随后提出AirGlove方法，通过利用已有的多种传感手套数据，学习可泛化的手套表征，从而在仅有少量新手套数据的情况下提升模型性能。

Result: 实验表明，现有裸手模型在手套上性能大幅下降；而AirGlove在多种传感手套上均能有效泛化，显著优于基线方法。

Conclusion: 视觉手部追踪模型在手套场景中面临外观差异带来的挑战，但通过引入如AirGlove这样的泛化策略，可以在有限数据下有效提升对新类型手套的追踪性能。

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

Abstract (中文翻译): 传感手套已成为遥操作和机器人策略学习的重要工具，因其能够提供速度、加速度和触觉反馈等丰富信号。一种常见的追踪戴手套手部的方法是直接使用传感器信号（如角速度、重力方向）来估计三维手部姿态。然而，基于传感器的追踪在实践中往往受限，因为其精度常受传感器信号质量和校准质量的影响。近期基于视觉的方法通过大规模预训练在人类裸手上取得了优异性能，但它们在具有显著不同视觉外观的戴手套手部上的表现仍缺乏深入探索。本文首次系统评估了基于视觉的手部追踪模型在戴手套手部上的表现，涵盖零样本和微调两种设置。分析表明，由于裸手与手套设计之间存在显著外观差异，现有裸手模型在传感手套上性能大幅下降。为此，我们提出了AirGlove方法，该方法利用已有的手套数据，将所学的手套表征泛化至仅有少量数据的新类型手套上。在多种传感手套上的实验表明，AirGlove能有效将手部姿态模型泛化至新手套设计，并显著优于对比方案。

</details>


### [9] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: 本文提出SHaSaM方法，通过子模硬样本挖掘提升模型公平性与性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练中易从标注数据继承社会和人口统计偏见，导致对敏感属性（如种族、年龄、性别）的不公平预测；现有方法因数据不平衡问题反而加剧了不公平性。

Method: 提出SHaSaM（Submodular Hard Sample Mining），包含两个阶段：SHaSaM-MINE利用子模子集选择策略挖掘难分正负样本以缓解数据不平衡；SHaSaM-LEARN基于子模条件互信息设计组合损失函数，在最大化目标类别决策边界的同时最小化敏感属性影响。

Result: 在CelebA和UTKFace数据集上，SHaSaM在更少训练轮次内实现SOTA效果，公平性（Equalized Odds）提升最多2.7点，准确率提高3.5%。

Conclusion: SHaSaM通过统一的子模优化框架有效抑制模型学习敏感属性相关特征，在不牺牲性能的前提下显著提升公平性。

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

Abstract (中文翻译): 深度神经网络在模型训练过程中常常从标注数据中继承社会和人口统计偏见，导致不公平的预测结果，尤其在存在种族、年龄、性别等敏感属性的情况下更为严重。现有方法容易受到属性组之间固有数据不平衡的影响，无意中强化了对敏感属性的关注，从而加剧了不公平性并损害模型性能。为克服这些挑战，我们提出了SHaSaM（Submodular Hard Sample Mining），一种新颖的组合优化方法，将面向公平性的表征学习建模为子模硬样本挖掘问题。我们的两阶段方法包括：SHaSaM-MINE，引入子模子集选择策略来挖掘难分的正负样本，有效缓解数据不平衡；以及SHaSaM-LEARN，基于子模条件互信息构建一类组合损失函数，在最大化目标类别间决策边界的同时最小化敏感属性的影响。这一统一框架限制模型学习与敏感属性相关的特征，从而在不牺牲性能的前提下显著提升公平性。在CelebA和UTKFace数据集上的实验表明，SHaSaM取得了当前最优结果，在更少训练轮次内，模型公平性（Equalized Odds）最多提升2.7点，准确率提高3.5%。

</details>


### [10] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的图像到图像方法，用于自动修复水下摄影中的对比度下降、模糊和颜色失真问题，在小规模高质量数据集上训练后实现了良好的视觉效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 水下摄影存在固有挑战，如对比度降低、空间模糊和波长相关的色彩失真，严重影响海洋生物的视觉表现力；摄影师通常需依赖繁重的后期处理来校正这些问题，因此亟需自动化且高效的修复方法。

Method: 作者构建了一个合成退化流程来模拟水下图像退化，并利用基于扩散生成模型的图像到图像管道学习逆转这些退化效果；模型在约2.5k张高质量水下意识摄影图像上从头训练，参数量约为1100万。

Result: 该方法在合成512x768分辨率图像时表现出高感知一致性与强泛化能力，验证了其在小数据集上的有效性。

Conclusion: 所提出的扩散模型驱动的图像修复方法能有效应对水下摄影的典型退化问题，为自动化高质量水下图像增强提供了一种可行方案。

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

Abstract (中文翻译): 水下摄影面临显著的固有挑战，包括对比度降低、空间模糊以及与波长相关的色彩失真。这些效应会掩盖海洋生物的生动性，尤其对注重生态意识的摄影师而言，他们常常需要依赖繁重的后期处理流程来校正这些失真。我们开发了一种图像到图像的处理流程，通过引入一个合成退化流程，并利用基于扩散的生成模型学习逆转其影响，从而实现对水下图像退化的自动修复。训练与评估均基于Keith Ellenbogen提供的小规模高质量意识摄影图像数据集。所提出的方法在从头训练约2500张图像后，使用约1100万参数的模型即可合成512×768分辨率的图像，并展现出高度的感知一致性和强大的泛化能力。

</details>
