<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 本文首次系统评估了开放词汇目标检测（OVD）模型在航拍图像上的零样本迁移能力，发现现有方法在航拍场景中性能严重下降，主要瓶颈是语义混淆而非视觉定位。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测（OVD）在自然图像上表现良好，但其在航拍图像中的可迁移性尚未被研究。作者旨在填补这一空白，系统评估当前先进OVD模型在航拍数据上的零样本性能，并识别其失败原因。

Method: 作者在LAE-80C航拍数据集（3,592张图像，80个类别）上，对五个最先进的OVD模型进行了严格的零样本评估。通过Global、Oracle和Single-Category三种推理模式，将语义混淆与视觉定位问题解耦。此外，还测试了提示工程（如领域前缀、同义词扩展）等策略，并在DIOR和FAIR1M等不同航拍数据集上进行了对比实验。

Result: 实验结果表明，OVD模型在航拍图像上存在严重的领域迁移失败：最佳模型OWLv2的F1分数仅为27.6%，且假阳性率高达69%。将词汇量从80类减少到3.2类可使性能提升15倍，证明语义混淆是主要瓶颈。提示工程策略未能带来显著改进，且模型在不同航拍数据集（DIOR F1: 0.53 vs FAIR1M F1: 0.12）上表现差异巨大，显示出对成像条件的脆弱性。

Conclusion: 该研究为航拍图像中的开放词汇目标检测建立了首个基准，揭示了现有模型在该领域的严重局限性，并强调了开发领域自适应方法的必要性。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

Abstract (中文翻译): 开放词汇目标检测（OVD）通过视觉-语言模型实现了对新类别的零样本识别，在自然图像上取得了优异性能。然而，其在航拍图像上的可迁移性仍未被探索。我们提出了首个系统性基准，评估了五种最先进的OVD模型在LAE-80C航拍数据集（3,592张图像，80个类别）上的严格零样本性能。我们的实验方案通过全局（Global）、神谕（Oracle）和单类别（Single-Category）三种推理模式，将语义混淆与视觉定位问题分离开来。结果揭示了严重的领域迁移失败：最佳模型（OWLv2）仅达到27.6%的F1分数，且假阳性率高达69%。关键的是，将词汇量从80类减少到3.2类可带来15倍的性能提升，表明语义混淆是主要瓶颈。诸如领域特定前缀和同义词扩展等提示工程策略未能提供有意义的性能增益。模型在不同数据集上的性能差异巨大（DIOR上F1为0.53，FAIR1M上为0.12），暴露了其对成像条件的脆弱性。这些发现为航拍OVD设定了基线预期，并突显了开发领域自适应方法的必要性。

</details>


### [2] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: 本文提出并构建了一个专注于科学图表的新型VQA基准数据集，强调图表与其底层数据之间并非一一对应，要求模型具备更深层次的数据推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集多关注真实图像或简单图表，缺乏针对复杂科学图表的理解，且通常假设图表标记与底层数据一一对应，忽略了图表是对数据进行分析、简化或变换后的产物，无法评估模型对这种非直接映射关系的推理能力。

Method: 作者首先调研现有VQA数据集并指出其局限性；然后基于真实数据生成合成的直方图，并设计问题，这些问题的精确答案依赖于对底层数据的访问；最后发布一个包含图表、底层数据、生成数据的分布参数以及所有图表标记和文本的边界框的开源数据集。

Result: 成功构建并开源了一个新的VQA数据集，该数据集专门用于评估模型在处理图表与底层数据非一一对应情况下的推理能力。

Conclusion: 为科学图表理解建立专门的VQA基准是必要的，新数据集为未来研究提供了重要资源，以推动大模型在复杂数据可视化推理方面的发展。

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

Abstract (中文翻译): 视觉问答（VQA）已成为评估大型多模态模型（LMMs）如何解读图像的重要基准。然而，大多数VQA数据集聚焦于现实世界图像或简单的图表分析，很少有研究专注于解读复杂的科学图表。事实上，许多用于分析图表的VQA数据集要么不包含这些图表背后的底层数据，要么假设图表标记与底层数据之间存在一一对应关系。而实际上，图表是对数据进行变换（即分析、简化、修改）后的产物。这种区别在VQA中引入了一种当前数据集未能捕捉到的推理挑战。在本文中，我们主张为科学图表建立一个专门的VQA基准，其中图表标记与底层数据之间不存在一一对应关系。为此，我们调研了现有的VQA数据集，并指出了当前领域的局限性。接着，我们基于真实数据生成合成的直方图，并向人类和一个大型推理模型提出问题，这些问题的精确答案依赖于对底层数据的访问。我们发布了该开源数据集，其中包括图表、底层数据、用于生成数据的分布参数，以及所有图表标记和文本的边界框，以供未来研究使用。

</details>


### [3] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: 本文指出视觉语言模型（VLMs）在3D空间理解方面存在明显不足，通过相对相机姿态估计（RCPE）任务进行评估，发现即使是先进模型也远逊于经典几何方法和人类表现。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在2D感知和语义推理上的优势与其在3D空间结构理解上的局限性之间的差距。

Method: 构建两个基准：VRRPI-Bench（基于真实场景的无标签第一人称视频与口头描述）和VRRPI-Diag（隔离单一运动自由度的诊断性基准），用于评估VLMs在相对相机姿态估计任务中的表现。

Result: 大多数VLMs无法超越浅层2D启发式方法，尤其在深度变化和绕光轴旋转（roll）方面表现差；GPT-5得分仅0.64，远低于经典几何基线（0.97）和人类（0.92）；多图像空间推理能力弱，跨帧整合线索时表现不一致（最高仅59.7%）。

Conclusion: 当前VLMs在3D空间理解和多视角推理方面存在根本性局限，难以有效建模真实世界中的三维几何关系。

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

Abstract (中文翻译): 视觉语言模型（VLMs）在2D感知和语义推理方面表现优异，但在理解3D空间结构方面仍显不足。本文通过相对相机姿态估计（RCPE）这一基础视觉任务来探究这一差距，该任务要求从一对图像中推断相对相机的平移和旋转。我们提出了VRRPI-Bench基准，该基准源自带有相对相机运动口头标注的无标签第一人称视频，反映了围绕共享物体同时发生平移和旋转的真实场景。我们还提出了VRRPI-Diag诊断性基准，用于隔离各个运动自由度。尽管RCPE任务看似简单，但大多数VLMs仍无法超越浅层的2D启发式方法，尤其在深度变化和绕光轴的滚转（roll）变换方面表现不佳。即便是最先进的模型如GPT-5（得分0.64）也远逊于经典几何基线（0.97）和人类表现（0.92）。此外，VLMs在多图像推理方面也存在困难，在跨帧整合空间线索时表现不一致（最佳仅为59.7%）。我们的研究揭示了VLMs在3D空间和多视角空间推理方面的局限性。

</details>


### [4] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: 本文从几何视角重新审视了视觉Transformer（ViT）中位置编码（PE）的作用，发现PE不仅是标记索引，更是塑造表征空间结构的几何先验，并通过多视图一致性实验验证了其对空间推理的关键影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对ViT中位置编码（PE）的作用理解不足，通常将其视为简单的标记索引。本文旨在从几何角度深入探究PE在构建ViT表征空间结构中的真实作用。

Method: 提出了一种token-level的诊断方法，用于衡量ViT表征中的多视图几何一致性如何依赖于一致的位置编码（PE），并在14个基础ViT模型上进行了大量实验。

Result: 实验揭示了位置编码（PE）对ViT表征中多视图几何和空间推理能力具有显著影响，证明PE是控制空间结构的因果机制。

Conclusion: 位置编码（PE）在ViT中扮演着几何先验的角色，是决定表征空间结构的关键因果因素，而不仅仅是标记的索引。

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

Abstract (中文翻译): 本文从几何视角重新审视了视觉Transformer（ViTs）中位置编码（PEs）的作用。我们表明，PEs并非仅仅是标记索引，而是有效地作为空间结构的几何先验，塑造了表征的空间结构。我们引入了token级别的诊断方法，用于衡量ViT表征中的多视图几何一致性如何依赖于一致的PEs。通过对14个基础ViT模型进行大量实验，我们揭示了PEs如何影响多视图几何和空间推理能力。我们的研究结果阐明了PEs作为控制ViT表征中空间结构的因果机制所扮演的角色。代码已开源：https://github.com/shijianjian/vit-geometry-probes

</details>


### [5] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 本文研究表明，在匹配表征容量并缓解码本坍塌的前提下，单层VQ-VAE可以达到与分层VQ-VAE相当的重建保真度，挑战了“分层结构在重建质量上必然更优”的普遍假设。


<details>
  <summary>Details</summary>
Motivation: 质疑分层VQ-VAE（如VQ-VAE2）在重建保真度上优于单层模型的根本原因，探究其优势是否真正源于分层结构本身，还是由码本利用不足和表征容量差异等其他因素导致。

Method: 在高分辨率ImageNet数据集上，对比一个两层的分层VQ-VAE和一个表征容量相匹配的单层VQ-VAE。通过采用轻量级干预措施（如从数据初始化、定期重置不活跃码本向量、系统性调整超参数）来缓解单层模型中的码本坍塌问题。

Result: 研究发现，当表征预算匹配且有效缓解码本坍塌后，单层VQ-VAE的重建保真度可以与分层变体相媲美。同时验证了码本利用不足和嵌入维度太高是导致单层模型性能不佳的关键原因。

Conclusion: 分层量化结构对于实现高保真重建并非不可或缺。单层VQ-VAE在适当的设计和训练下，足以达到与复杂分层模型相当的重建性能，这为简化模型架构提供了依据。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

Abstract (中文翻译): 矢量量化变分自编码器（VQ-VAE）是依赖高重建保真度模型的核心，从神经压缩到生成流水线皆是如此。其分层扩展版本（如VQ-VAE2）常被认为具有更优的重建性能，因为它将全局和局部特征分布在多个层级上。然而，由于高层级的所有信息都源自低层级，因此其不应包含超出低层级已编码内容的额外重建信息。结合近期在训练目标和量化机制方面的进展，我们提出疑问：一个具有匹配表征预算且无码本坍塌的单层级VQ-VAE，能否达到与其分层对应物相当的重建保真度？尽管分层模型的多尺度结构可能提升下游任务的感知质量，但分层结构本身对重建精度的影响（在排除码本利用率和总体表征容量等因素后）仍缺乏实证研究。我们通过在高分辨率ImageNet图像上比较一个两层级VQ-VAE和一个容量匹配的单层级模型来重新审视这一问题。与先前观察一致，我们确认了码本利用不足会限制单层级VQ-VAE的性能，而过高的嵌入维度会使量化不稳定并加剧码本坍塌。我们证明，诸如从数据初始化、定期重置不活跃的码本向量以及系统性地调整码本超参数等轻量级干预措施能显著减少坍塌。我们的结果表明，当表征预算匹配且码本坍塌得到缓解时，单层级VQ-VAE可以匹敌分层变体的重建保真度，从而挑战了“分层量化在高质量重建上具有内在优越性”的假设。

</details>


### [6] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: VMonarch 是一种基于 Monarch 矩阵的新型注意力机制，通过结构化稀疏性显著降低 Video DiT 中注意力计算的复杂度，在保持生成质量的同时实现超过 5 倍的加速和 17.5 倍的 FLOPs 减少。


<details>
  <summary>Details</summary>
Motivation: 视频扩散 Transformer（Video DiTs）中的注意力机制具有二次复杂度，严重限制了其处理长视频上下文的能力。作者观察到视频 DiTs 中存在高度稀疏的时空注意力模式，这启发他们探索更高效的稀疏注意力方法。

Method: 提出 VMonarch 方法：1）采用时空 Monarch 分解以显式建模帧内与帧间相关性；2）引入重计算策略缓解交替最小化过程中的不稳定性；3）设计一种融合到 FlashAttention 中的在线熵算法，用于快速更新长序列的 Monarch 矩阵。

Result: 在 VBench 上仅需少量调优，VMonarch 即可达到与全注意力相当甚至更优的生成质量；相比现有稀疏注意力方法，在 90% 稀疏度下实现超过 5 倍的注意力计算加速和 17.5 倍的 FLOPs 降低。

Conclusion: VMonarch 成功克服了 Video DiTs 中的注意力瓶颈，为高效长视频生成提供了可行方案，并在效率与生成质量之间取得了优异平衡。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

Abstract (中文翻译): 注意力机制的二次复杂度严重限制了视频扩散 Transformer（Video DiTs）的上下文可扩展性。我们发现，Video DiTs 中表现出的高度稀疏的时空注意力模式可以自然地用 Monarch 矩阵表示。Monarch 矩阵是一类具有灵活稀疏性的结构化矩阵，可通过交替最小化算法实现次二次的注意力计算。为此，我们提出了 VMonarch——一种用于 Video DiTs 的新型注意力机制，利用结构化的 Monarch 矩阵在动态稀疏模式上实现高效计算。首先，我们对时空 Monarch 分解进行适配，以显式捕捉视频数据的帧内和帧间相关性；其次，引入一种重计算策略，以缓解 Monarch 矩阵交替最小化过程中因不稳定性而产生的伪影；第三，我们提出一种新颖的在线熵算法，并将其融合到 FlashAttention 中，从而实现对长序列的快速 Monarch 矩阵更新。大量实验表明，VMonarch 在 VBench 上经过少量调优后，即可达到与全注意力相当甚至更优的生成质量。它克服了 Video DiTs 中的注意力瓶颈，将注意力计算的 FLOPs 降低了 17.5 倍，并在长视频上实现了超过 5 倍的注意力计算加速，在 90% 稀疏度下超越了当前最先进的稀疏注意力方法。

</details>


### [7] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: C2R 是一个从粗略3D模拟生成逼真城市人群视频的生成式渲染框架，结合文本提示与神经渲染，在缺乏配对数据的情况下实现可控、一致且真实的视频合成。


<details>
  <summary>Details</summary>
Motivation: 传统渲染流程在生成大规模动态人群场景时面临可扩展性与真实感的挑战，且依赖复杂资产、精确材质光照和大量计算资源。作者希望构建一个仅需简单3D输入即可生成高质量、可控且逼真城市人群视频的系统。

Method: 提出 C2R（Coarse-to-Real）框架：利用粗略3D渲染控制场景布局、摄像机运动和人物轨迹，通过文本引导的神经渲染器生成逼真的外观、光照和细节动态；采用两阶段混合CG-真实训练策略，从大规模真实视频中学习生成先验，并通过跨域共享的隐式时空特征引入可控性。

Result: 该系统支持从极简3D输入生成时间一致、可控且逼真的城市场景视频，能泛化到多种CG和游戏输入，并实现从粗到细的控制。

Conclusion: C2R 有效弥合了粗略模拟与真实视频之间的鸿沟，在无需配对数据的前提下实现了高质量、可控的生成式渲染，为动态人群场景的高效创作提供了新途径。

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

Abstract (中文翻译): 传统的渲染管线依赖复杂的资产、精确的材质与光照以及大量的计算资源来生成逼真图像，但在处理包含大量动态人物的场景时，仍面临可扩展性和真实感方面的挑战。我们提出了 C2R（Coarse-to-Real），一种生成式渲染框架，能够从粗略的3D模拟中合成具有真实风格的城市人群视频。我们的方法利用粗略的3D渲染显式控制场景布局、摄像机运动和人物轨迹，同时通过一个由文本提示引导的神经渲染器生成逼真的外观、光照和细粒度动态效果。为克服粗略模拟与真实视频之间缺乏配对训练数据的问题，我们采用了一种两阶段的混合CG-真实训练策略：首先从大规模真实视频中学习强大的生成先验，再通过跨域共享的隐式时空特征引入可控性。所构建的系统支持从粗到细的控制，能够泛化至多种CG和游戏输入，并仅需极少的3D输入即可生成时间一致、可控且逼真的城市场景视频。我们将在 https://gonzalognogales.github.io/coarse2real/ 发布模型和项目网页。

</details>


### [8] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap 是一种新型高精地图构建方法，无需固定相机配置或显式几何投影，能适应不同摄像头设置并保持对传感器缺失和变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有高精地图构建方法依赖校准的多摄像头系统和2D到鸟瞰图（BEV）的转换，在传感器失效或车辆间摄像头配置不一致时表现脆弱，难以在实际中部署。

Method: 提出 FlexMap 方法，利用具有跨帧注意力机制的几何感知基础模型，在特征空间中隐式编码三维场景理解，避免显式几何投影；包含时空增强模块和带潜在相机标记的相机感知解码器，实现无需投影矩阵的视图自适应注意力。

Result: 实验表明，FlexMap 在多种摄像头配置下均优于现有方法，并对缺失视角和传感器变化具有强鲁棒性。

Conclusion: FlexMap 为高精地图构建提供了一种更灵活、实用且鲁棒的解决方案，适用于真实世界中多样化的车载传感器配置。

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

Abstract (中文翻译): 高精（HD）地图为自动驾驶系统提供了道路结构的关键语义信息，但当前的高精地图构建方法依赖于经过标定的多摄像头系统，并采用隐式或显式的2D到鸟瞰图（BEV）转换，当传感器失效或车队中摄像头配置不一致时，这些方法往往变得脆弱。我们提出了 FlexMap，与以往固定于特定N摄像头装置的方法不同，我们的方法无需任何架构修改或针对每种配置重新训练，即可适应可变的摄像头配置。我们的核心创新在于摒弃了显式的几何投影，转而使用一个具备跨帧注意力机制的几何感知基础模型，在特征空间中隐式地编码三维场景理解。FlexMap 包含两个核心组件：一个时空增强模块，将跨视角的空间推理与时间动态解耦；以及一个带有潜在相机标记的相机感知解码器，可在无需投影矩阵的情况下实现视图自适应注意力。实验表明，FlexMap 在多种配置下均优于现有方法，同时对缺失视角和传感器变化具有良好的鲁棒性，从而支持更实用的真实世界部署。

</details>


### [9] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 本文提出一种结合后训练思维链（CoT）提示和ReAct驱动自适应加噪机制的越狱框架，用于生成可绕过视觉-语言模型安全过滤器的隐蔽提示，显著提升攻击成功率并保持文本与图像的自然性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）对提示变化高度敏感，这种敏感性可能暴露其在安全对齐方面的漏洞。因此，作者旨在探索如何利用这种脆弱性，设计有效且隐蔽的攻击方法以绕过现有安全机制。

Method: 该方法包含两个核心策略：1）利用后训练阶段的思维链（CoT）提示构建隐蔽越狱提示；2）引入基于ReAct范式的自适应图像加噪机制，根据模型反馈迭代扰动输入图像中易触发安全防御的区域，以增强攻击的隐蔽性和规避能力。

Result: 实验结果表明，所提出的双策略方法在文本和视觉领域均显著提高了攻击成功率（ASR），同时保持了输出的自然性。

Conclusion: 该研究揭示了当前视觉-语言模型在安全对齐方面的潜在弱点，并通过结合CoT提示与ReAct驱动的自适应扰动，提供了一种高效且隐蔽的越狱攻击框架，对模型安全性提出了新的挑战。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

Abstract (中文翻译): 视觉-语言模型（VLMs）已成为视觉问答、图像描述和文本到图像生成等任务的核心工具。然而，其输出对提示语的变化极为敏感，这种敏感性可能暴露出其在安全对齐方面的漏洞。在本研究中，我们提出了一种越狱框架，该框架利用后训练阶段的思维链（Chain-of-Thought, CoT）提示来构建能够绕过安全过滤器的隐蔽提示。为进一步提高攻击成功率（ASR），我们提出了一种基于ReAct范式的自适应加噪机制，该机制根据模型反馈迭代地对输入图像进行扰动。该方法利用ReAct范式，在最可能激活安全防御的图像区域中优化对抗性噪声，从而增强攻击的隐蔽性和规避能力。实验结果表明，所提出的双重策略在显著提升攻击成功率的同时，在文本和视觉领域均保持了良好的自然性。

</details>


### [10] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: 本文评估了一种基于变分推断的多视角无标记运动捕捉（MMMC）系统的置信区间校准与可靠性，结果表明该概率模型能有效量化认知不确定性，并在无需真实值设备的情况下识别不可靠输出。


<details>
  <summary>Details</summary>
Motivation: 临床实践中需要可靠且可信赖的无标记运动捕捉系统，不仅要求高精度，还需提供准确反映个体预测不确定性的置信区间，以增强临床采纳和信任。

Method: 基于先前利用变分推断估计关节角度后验分布的工作，本研究在68名受试者数据上验证了该概率性MMMC方法，使用预期校准误差（ECE）评估置信区间校准情况，并与仪器化步道和标准标记式运动捕捉进行对比。

Result: 模型在校准方面表现良好（ECE通常<0.1），步长和跨步长中位误差分别为约16 mm和12 mm，下肢各关节校正后运动学误差中位数为1.5–3.8度，且预测不确定性与实际误差高度相关。

Conclusion: 所提出的概率模型能有效量化认知不确定性，从而在没有同步真实值设备的情况下识别不可靠的输出，具备临床应用潜力。

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

Abstract (中文翻译): 基于视频的人体运动分析在临床实践和研究中的运动评估方面具有潜力。然而，多视角无标记运动捕捉（MMMC）系统的临床应用和可信度不仅要求其具备高准确性，还需要能够生成可靠的置信区间，以指示对任何个体预测的准确程度。本研究在我们先前利用变分推断估计关节角度后验分布工作的基础上，评估了一种概率性MMMC方法的校准性和可靠性。我们分析了来自两个机构共68名参与者的数据，将该模型与仪器化步道和标准标记式运动捕捉系统进行对比验证。我们采用预期校准误差（ECE）来衡量置信区间的校准程度。结果显示，该模型在校准方面表现可靠，步长和跨步长以及偏差校正后的步态运动学参数的ECE值普遍小于0.1。观察到的步长和跨步长中位误差分别约为16毫米和12毫米，下肢各关节的偏差校正后运动学误差中位数在1.5至3.8度之间。与校准良好的ECE一致，模型预测的不确定性大小与观测到的误差指标高度相关。这些结果表明，该概率模型如设计所预期，能够量化认知不确定性，从而在无需同步真实值设备的情况下识别不可靠的输出。

</details>
