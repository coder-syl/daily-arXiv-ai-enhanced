{"id": "2512.09969", "pdf": "https://arxiv.org/pdf/2512.09969", "abs": "https://arxiv.org/abs/2512.09969", "authors": ["Paul Hueber", "Luca Peres", "Florian Pitters", "Alejandro Gloriani", "Oliver Rhodes"], "title": "Neuromorphic Eye Tracking for Low-Latency Pupil Detection", "categories": ["cs.CV", "cs.NE"], "comment": "8 pages, 2 figures, conference", "summary": "Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment."}
{"id": "2512.10031", "pdf": "https://arxiv.org/pdf/2512.10031", "abs": "https://arxiv.org/abs/2512.10031", "authors": ["Woojin Lee", "Hyugjae Chang", "Jaeho Moon", "Jaehyup Lee", "Munchurl Kim"], "title": "ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 11 figures, 8 tables, supplementary included. Accepted to CVPR 2025. Please visit our project page at https://kaist-viclab.github.io/ABBSPO_site/", "summary": "Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods."}
{"id": "2512.10038", "pdf": "https://arxiv.org/pdf/2512.10038", "abs": "https://arxiv.org/abs/2512.10038", "authors": ["Jia Cheng Hu", "Roberto Cavicchioli", "Alessandro Capotondi"], "title": "Diffusion Is Your Friend in Show, Suggest and Tell", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Diffusion Denoising models demonstrated impressive results across generative Computer Vision tasks, but they still fail to outperform standard autoregressive solutions in the discrete domain, and only match them at best. In this work, we propose a different paradigm by adopting diffusion models to provide suggestions to the autoregressive generation rather than replacing them. By doing so, we combine the bidirectional and refining capabilities of the former with the strong linguistic structure provided by the latter. To showcase its effectiveness, we present Show, Suggest and Tell (SST), which achieves State-of-the-Art results on COCO, among models in a similar setting. In particular, SST achieves 125.1 CIDEr-D on the COCO dataset without Reinforcement Learning, outperforming both autoregressive and diffusion model State-of-the-Art results by 1.5 and 2.5 points. On top of the strong results, we performed extensive experiments to validate the proposal and analyze the impact of the suggestion module. Results demonstrate a positive correlation between suggestion and caption quality, overall indicating a currently underexplored but promising research direction. Code will be available at: https://github.com/jchenghu/show\\_suggest\\_tell."}
{"id": "2512.10041", "pdf": "https://arxiv.org/pdf/2512.10041", "abs": "https://arxiv.org/abs/2512.10041", "authors": ["Yihao Liu", "Chenyu Gao", "Lianrui Zuo", "Michael E. Kim", "Brian D. Boyd", "Lisa L. Barnes", "Walter A. Kukull", "Lori L. Beason-Held", "Susan M. Resnick", "Timothy J. Hohman", "Warren D. Taylor", "Bennett A. Landman"], "title": "MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability."}
{"id": "2512.10067", "pdf": "https://arxiv.org/pdf/2512.10067", "abs": "https://arxiv.org/abs/2512.10067", "authors": ["Jiahao Liu"], "title": "Independent Density Estimation", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 1 table, 4 figures", "summary": "Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets."}
{"id": "2512.10095", "pdf": "https://arxiv.org/pdf/2512.10095", "abs": "https://arxiv.org/abs/2512.10095", "authors": ["Jiachen Tao", "Junyi Wu", "Haoxuan Wang", "Zongxin Yang", "Dawen Cai", "Yan Yan"], "title": "TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing", "categories": ["cs.CV"], "comment": null, "summary": "We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments."}
{"id": "2512.10102", "pdf": "https://arxiv.org/pdf/2512.10102", "abs": "https://arxiv.org/abs/2512.10102", "authors": ["Neelima Prasad", "Jarek Reynolds", "Neel Karsanbhai", "Tanusree Sharma", "Lotus Zhang", "Abigale Stangl", "Yang Wang", "Leah Findlater", "Danna Gurari"], "title": "Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information", "categories": ["cs.CV"], "comment": "Accepted at WACV 2026", "summary": "We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/"}
{"id": "2512.10151", "pdf": "https://arxiv.org/pdf/2512.10151", "abs": "https://arxiv.org/abs/2512.10151", "authors": ["Charles Fanning", "Mehmet Emin Aktas"], "title": "Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization", "categories": ["cs.CV"], "comment": "8 Pages, 2 Figures, submitted to IEEE Transactions on Medical Imaging", "summary": "Breast cancer is the most commonly diagnosed cancer in women and a leading cause of cancer death worldwide. Screening mammography reduces mortality, yet interpretation still suffers from substantial false negatives and false positives, and model accuracy often degrades when deployed across scanners, modalities, and patient populations. We propose a simple conditioning signal aimed at improving external performance based on a wavelet based vectorization of persistent homology. Using topological data analysis, we summarize image structure that persists across intensity thresholds and convert this information into spatial, multi scale maps that are provably stable to small intensity perturbations. These maps are integrated into a two stage detection pipeline through input level channel concatenation. The model is trained and validated on the CBIS DDSM digitized film mammography cohort from the United States and evaluated on two independent full field digital mammography cohorts from Portugal (INbreast) and China (CMMD), with performance reported at the patient level. On INbreast, augmenting ConvNeXt Tiny with wavelet persistence channels increases patient level AUC from 0.55 to 0.75 under a limited training budget."}
{"id": "2512.10209", "pdf": "https://arxiv.org/pdf/2512.10209", "abs": "https://arxiv.org/abs/2512.10209", "authors": ["Md Eimran Hossain Eimon", "Juan Merlos", "Ashan Perera", "Hari Kalva", "Velibor Adzic", "Borko Furht"], "title": "Feature Coding for Scalable Machine Vision", "categories": ["cs.CV"], "comment": "This article has been accepted for publication in IEEE Consumer Electronics Magazine", "summary": "Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications."}
{"id": "2512.10226", "pdf": "https://arxiv.org/pdf/2512.10226", "abs": "https://arxiv.org/abs/2512.10226", "authors": ["Shuhan Tan", "Kashyap Chitta", "Yuxiao Chen", "Ran Tian", "Yurong You", "Yan Wang", "Wenjie Luo", "Yulong Cao", "Philipp Krahenbuhl", "Marco Pavone", "Boris Ivanovic"], "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving", "categories": ["cs.CV", "cs.RO"], "comment": "Technical Report", "summary": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines."}
