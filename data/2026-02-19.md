<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Egocentric Bias in Vision-Language Models](https://arxiv.org/abs/2602.15892)
*Maijunxian Wang,Yijiang Li,Bingyang Wang,Tianwei Zhao,Ran Ji,Qingying Gao,Emmy Liu,Hokin Deng,Dezhi Luo*

Main category: cs.CV

TL;DR: 本文提出FlipSet基准，用于评估视觉语言模型在二级视觉视角采择（L2 VPT）任务中的能力。研究发现大多数模型存在显著的自我中心偏差，无法将心理旋转与社会认知有效结合，表明当前模型缺乏整合空间操作与社会意识的机制。


<details>
  <summary>Details</summary>
Motivation: 视觉视角采择是社会认知的基础，但现有视觉语言模型（VLMs）在此方面的能力尚不明确。为系统评估模型是否具备从他人视角理解二维场景的空间变换能力，并排除三维场景复杂性干扰，作者构建了专门的诊断基准。

Method: 提出FlipSet诊断基准，要求模型模拟180度旋转后的2D字符字符串以反映另一观察者的视角；对103个VLM进行评估，并通过控制实验分别测试其心理旋转能力与心理理论（ToM）准确性。

Result: 绝大多数模型表现低于随机水平，约75%的错误直接复制了相机视角；尽管模型在单独的心理旋转和ToM任务中表现良好，但在需要两者结合的任务中表现灾难性失败。

Conclusion: 当前视觉语言模型缺乏将社会意识与空间操作绑定的机制，表明其在基于模型的空间推理方面存在根本性局限；FlipSet可作为多模态系统视角采择能力的认知基础测试平台。

Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D character strings from another agent's perspective, isolating spatial transformation from 3D scene complexity. Evaluating 103 VLMs reveals systematic egocentric bias: the vast majority perform below chance, with roughly three-quarters of errors reproducing the camera viewpoint. Control experiments expose a compositional deficit--models achieve high theory-of-mind accuracy and above-chance mental rotation in isolation, yet fail catastrophically when integration is required. This dissociation indicates that current VLMs lack the mechanisms needed to bind social awareness to spatial operations, suggesting fundamental limitations in model-based spatial reasoning. FlipSet provides a cognitively grounded testbed for diagnosing perspective-taking capabilities in multimodal systems.

Abstract (中文翻译): 视觉视角采择——即推断世界在他人视角下如何呈现——是社会认知的基础。我们提出了FlipSet，这是一个用于诊断视觉语言模型二级视觉视角采择（L2 VPT）能力的基准。该任务要求模型从另一主体的视角模拟2D字符字符串的180度旋转，从而将空间变换与3D场景复杂性分离开来。对103个视觉语言模型的评估揭示了系统性的自我中心偏差：绝大多数模型的表现低于随机水平，约四分之三的错误直接复现了相机视角。控制实验进一步暴露了模型的组合性缺陷——模型在单独执行心理理论任务和心理旋转任务时准确率较高，但当需要将二者整合时却出现灾难性失败。这种分离现象表明，当前的视觉语言模型缺乏将社会意识与空间操作相结合的机制，暗示其在基于模型的空间推理方面存在根本性局限。FlipSet为诊断多模态系统中的视角采择能力提供了一个具有认知基础的测试平台。

</details>


### [2] [Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment](https://arxiv.org/abs/2602.15903)
*Jingwei Li,Jiaxin Tong,Pengfei Wu*

Main category: cs.CV

TL;DR: 提出MSBA-CLIP框架，通过多变量软混合增强和CLIP引导的伪造强度估计，显著提升深度伪造检测的准确性和跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法因不同伪造技术产生的样本分布差异大，导致准确率有限且泛化能力差。

Method: 利用CLIP的多模态对齐能力捕捉细微伪造痕迹；提出多变量软混合增强（MSBA）策略，通过随机权重混合多种伪造图像以学习通用模式；设计多变量伪造强度估计（MFIE）模块，显式引导模型学习不同伪造模式和强度的特征。

Result: 在域内测试中，准确率和AUC分别比最佳基线提升3.32%和4.02%；在五个数据集的跨域评估中，平均AUC提升3.27%；消融实验证明了所提组件的有效性。

Conclusion: 尽管依赖大型视觉语言模型带来较高计算成本，但该方法在实现更通用、鲁棒的深度伪造检测方面迈出了重要一步。

Abstract: The proliferation of highly realistic facial forgeries necessitates robust detection methods. However, existing approaches often suffer from limited accuracy and poor generalization due to significant distribution shifts among samples generated by diverse forgery techniques. To address these challenges, we propose a novel Multivariate and Soft Blending Augmentation with CLIP-guided Forgery Intensity Estimation (MSBA-CLIP) framework. Our method leverages the multimodal alignment capabilities of CLIP to capture subtle forgery traces. We introduce a Multivariate and Soft Blending Augmentation (MSBA) strategy that synthesizes images by blending forgeries from multiple methods with random weights, forcing the model to learn generalizable patterns. Furthermore, a dedicated Multivariate Forgery Intensity Estimation (MFIE) module is designed to explicitly guide the model in learning features related to varied forgery modes and intensities. Extensive experiments demonstrate state-of-the-art performance. On in-domain tests, our method improves Accuracy and AUC by 3.32\% and 4.02\%, respectively, over the best baseline. In cross-domain evaluations across five datasets, it achieves an average AUC gain of 3.27\%. Ablation studies confirm the efficacy of both proposed components. While the reliance on a large vision-language model entails higher computational cost, our work presents a significant step towards more generalizable and robust deepfake detection.

Abstract (中文翻译): 高度逼真的面部伪造技术日益泛滥，亟需鲁棒的检测方法。然而，现有方法常因不同伪造技术生成的样本之间存在显著分布偏移，而面临准确率有限和泛化能力差的问题。为应对这些挑战，我们提出了一种新颖的“基于CLIP引导伪造强度估计的多变量软混合增强”（MSBA-CLIP）框架。该方法利用CLIP的多模态对齐能力来捕捉细微的伪造痕迹。我们引入了一种多变量软混合增强（MSBA）策略，通过以随机权重混合多种伪造方法生成的图像，迫使模型学习可泛化的模式。此外，我们还设计了一个专门的多变量伪造强度估计（MFIE）模块，显式引导模型学习与不同伪造模式和强度相关的特征。大量实验表明，该方法达到了当前最优性能：在域内测试中，准确率和AUC分别比最佳基线提升了3.32%和4.02%；在五个数据集上的跨域评估中，平均AUC增益达3.27%。消融研究证实了所提出两个组件的有效性。尽管依赖大型视觉语言模型带来了较高的计算成本，但本工作在实现更具泛化性和鲁棒性的深度伪造检测方面迈出了重要一步。

</details>


### [3] [A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving](https://arxiv.org/abs/2602.15904)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: 本文首次对面向自动驾驶的LiDAR超分辨率方法进行了全面综述，系统分类现有方法、梳理关键技术，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高分辨率LiDAR成本高昂，而低成本低分辨率LiDAR生成的点云稀疏、缺乏关键细节；尽管LiDAR超分辨率对实际部署至关重要，但此前尚无系统性综述。

Method: 将现有LiDAR超分辨率方法分为四类：基于CNN的架构、基于模型的深度展开、隐式表示方法以及基于Transformer和Mamba的方法；并梳理了数据表示、问题定义、基准数据集和评估指标等基础概念。

Result: 总结了当前趋势，包括采用距离图像表示以提升效率、极端模型压缩、分辨率灵活架构设计，以及强调实时推理与跨传感器泛化能力。

Conclusion: 指出了LiDAR超分辨率领域仍存在的开放挑战和未来研究方向，以推动该技术在自动驾驶中的实际应用。

Abstract: LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.

Abstract (中文翻译): LiDAR传感器常被视为自动驾驶的关键组件，但高分辨率传感器价格昂贵，而价格亲民的低分辨率传感器所生成的点云过于稀疏，难以捕捉关键细节。LiDAR超分辨率技术利用深度学习增强稀疏点云，弥合不同传感器类型之间的差距，从而实现真实场景中的跨传感器兼容性。本文首次对面向自动驾驶的LiDAR超分辨率方法进行了全面综述。尽管该技术对实际部署至关重要，但此前尚未有系统性的回顾研究。我们将现有方法划分为四类：基于CNN的架构、基于模型的深度展开方法、隐式表示方法，以及基于Transformer和Mamba的方法。同时，我们梳理了包括数据表示、问题定义、基准数据集和评估指标在内的基础概念。当前的研究趋势包括采用距离图像表示以实现高效处理、极端模型压缩，以及开发分辨率灵活的架构。近期工作更注重实时推理能力和跨传感器泛化能力，以满足实际部署需求。最后，我们指出了该领域尚存的开放挑战和未来研究方向，以推动LiDAR超分辨率技术的进一步发展。

</details>


### [4] [MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2602.15915)
*Xianwei Mao,Kai Ye,Sheng Zhou,Nan Zhang,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.CV

TL;DR: 提出MaS-VQA框架，通过Mask-and-Select机制联合筛选图像区域与外部知识片段，有效融合显式与隐式知识以提升KB-VQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有KB-VQA方法在整合视觉信息与外部知识时面临检索知识噪声大、与图像内容不一致，以及模型内部知识难以控制和解释的问题，简单聚合限制了推理效果。

Method: MaS-VQA框架首先检索候选知识段落，利用Mask-and-Select机制联合剪枝无关图像区域和弱相关知识片段，生成高信噪比的多模态知识；随后在约束语义空间中引导激活模型内部知识，实现显式与隐式知识的互补协同建模。

Result: 在Encyclopedic-VQA和InfoSeek数据集上，MaS-VQA在多个MLLM主干网络上均取得一致性能提升，消融实验证明其选择机制能有效降噪并提升知识利用率。

Conclusion: 通过紧密耦合显式知识过滤与隐式知识推理，MaS-VQA显著提升了知识驱动的视觉问答准确性和鲁棒性。

Abstract: Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to control and interpret. Naive aggregation of these sources limits reasoning effectiveness and reduces answer accuracy. To address this, we propose MaS-VQA, a selection-driven framework that tightly couples explicit knowledge filtering with implicit knowledge reasoning. MaS-VQA first retrieves candidate passages and applies a Mask-and-Select mechanism to jointly prune irrelevant image regions and weakly relevant knowledge fragments, producing compact, high-signal multimodal knowledge . This filtered knowledge then guides the activation of internal knowledge in a constrained semantic space, enabling complementary co-modeling of explicit and implicit knowledge for robust answer prediction. Experiments on Encyclopedic-VQA and InfoSeek demonstrate consistent performance gains across multiple MLLM backbones, and ablations verify that the selection mechanism effectively reduces noise and enhances knowledge utilization.

Abstract (中文翻译): 基于知识的视觉问答（KB-VQA）要求模型通过整合视觉信息与外部知识来回答问题。然而，检索到的知识通常包含噪声、部分无关或与视觉内容不一致，而模型内部知识则难以控制和解释。对这些知识源的简单聚合限制了推理效果并降低了答案准确性。为解决这一问题，我们提出了MaS-VQA，一种由选择驱动的框架，将显式知识过滤与隐式知识推理紧密结合。MaS-VQA首先检索候选段落，并应用Mask-and-Select机制联合剪枝无关的图像区域和弱相关的知识片段，生成紧凑且高信噪比的多模态知识。随后，该过滤后的知识在受限的语义空间中引导模型内部知识的激活，从而实现显式与隐式知识的互补协同建模，以进行鲁棒的答案预测。在Encyclopedic-VQA和InfoSeek上的实验表明，该方法在多个多模态大语言模型（MLLM）主干网络上均带来一致的性能提升，消融实验也验证了所提选择机制能有效降低噪声并增强知识利用效率。

</details>


### [5] [EarthSpatialBench: Benchmarking Spatial Reasoning Capabilities of Multimodal LLMs on Earth Imagery](https://arxiv.org/abs/2602.15918)
*Zelin Xu,Yupu Zhang,Saugat Adhikari,Saiful Islam,Tingsong Xiao,Zibo Liu,Shigang Chen,Da Yan,Zhe Jiang*

Main category: cs.CV

TL;DR: 本文提出了 EarthSpatialBench，一个针对地球影像中多模态大语言模型（MLLMs）空间推理能力的综合性评测基准，涵盖距离、方向、拓扑关系及复杂几何对象的定量与定性推理。


<details>
  <summary>Details</summary>
Motivation: 现有地球影像的评测基准主要关注2D空间定位、图像描述和粗略空间关系，缺乏对定量方向/距离推理、系统性拓扑关系以及超越边界框的复杂几何对象的支持，因此需要构建更全面的评测体系。

Method: 构建包含32.5万问答对的 EarthSpatialBench 基准，覆盖四类任务：(1) 空间距离与方向的定性与定量推理；(2) 系统性拓扑关系；(3) 单对象、对象对及组合聚合查询；(4) 通过文本描述、视觉叠加和显式几何坐标（如边界框、折线、多边形）引用对象。

Result: 在开源和闭源 MLLM 上的大量实验揭示了当前模型在地球影像空间推理方面的局限性。

Conclusion: EarthSpatialBench 为评估和推动 MLLM 在地球影像上的空间推理能力提供了重要工具，填补了现有基准的空白。

Abstract: Benchmarking spatial reasoning in multimodal large language models (MLLMs) has attracted growing interest in computer vision due to its importance for embodied AI and other agentic systems that require precise interaction with the physical world. However, spatial reasoning on Earth imagery has lagged behind, as it uniquely involves grounding objects in georeferenced images and quantitatively reasoning about distances, directions, and topological relations using both visual cues and vector geometry coordinates (e.g., 2D bounding boxes, polylines, and polygons). Existing benchmarks for Earth imagery primarily focus on 2D spatial grounding, image captioning, and coarse spatial relations (e.g., simple directional or proximity cues). They lack support for quantitative direction and distance reasoning, systematic topological relations, and complex object geometries beyond bounding boxes. To fill this gap, we propose \textbf{EarthSpatialBench}, a comprehensive benchmark for evaluating spatial reasoning in MLLMs on Earth imagery. The benchmark contains over 325K question-answer pairs spanning: (1) qualitative and quantitative reasoning about spatial distance and direction; (2) systematic topological relations; (3) single-object queries, object-pair queries, and compositional aggregate group queries; and (4) object references expressed via textual descriptions, visual overlays, and explicit geometry coordinates, including 2D bounding boxes, polylines, and polygons. We conducted extensive experiments on both open-source and proprietary models to identify limitations in the spatial reasoning of MLLMs.

Abstract (中文翻译): 由于空间推理在具身智能和其他需要与物理世界精确交互的智能体系统中的重要性，评测多模态大语言模型（MLLMs）的空间推理能力在计算机视觉领域引起了越来越多的关注。然而，地球影像上的空间推理研究相对滞后，因其独特地涉及在具有地理参考的图像中定位物体，并结合视觉线索与矢量几何坐标（例如二维边界框、折线和多边形）对距离、方向和拓扑关系进行定量推理。现有的地球影像评测基准主要聚焦于二维空间定位、图像描述和粗略的空间关系（如简单的方向或邻近提示），缺乏对定量方向与距离推理、系统性拓扑关系以及超越边界框的复杂物体几何结构的支持。为填补这一空白，我们提出了 EarthSpatialBench——一个用于评测 MLLM 在地球影像上空间推理能力的综合性基准。该基准包含超过32.5万个问答对，涵盖以下四个方面：(1) 空间距离与方向的定性与定量推理；(2) 系统性拓扑关系；(3) 单对象查询、对象对查询以及组合聚合群组查询；(4) 通过文本描述、视觉叠加和显式几何坐标（包括二维边界框、折线和多边形）表达的对象引用。我们在多个开源和闭源模型上进行了广泛实验，揭示了当前 MLLM 在空间推理方面的局限性。

</details>


### [6] [A Study on Real-time Object Detection using Deep Learning](https://arxiv.org/abs/2602.15926)
*Ankita Bose,Jayasravani Bhumireddy,Naveen N*

Main category: cs.CV

TL;DR: 本文综述了深度学习在实时目标检测中的应用，涵盖主流模型、基准数据集、实际应用场景、对比研究及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 目标检测在多个领域具有重要应用价值，而实时目标检测能支持动态视觉分析与即时决策。随着深度学习的发展，更准确高效的目标检测方法不断涌现，亟需系统性综述以梳理现状并指导未来研究。

Method: 文章详细回顾了多种主流深度学习目标检测算法（如Faster R-CNN、YOLO、SSD等），介绍了公开基准数据集，并对不同模型在各类应用中的使用进行了调研和受控比较实验。

Result: 通过对比分析，文章揭示了不同目标检测策略的性能差异，并提供了具有启发性的发现，为算法选择和优化提供参考。

Conclusion: 深度学习显著推动了实时目标检测的发展，但仍存在诸多挑战。文章提出了若干有前景的研究方向，鼓励在相关深度学习方法和目标识别技术上进一步探索。

Abstract: Object detection has compelling applications over a range of domains, including human-computer interfaces, security and video surveillance, navigation and road traffic monitoring, transportation systems, industrial automation healthcare, the world of Augmented Reality (AR) and Virtual Reality (VR), environment monitoring and activity identification. Applications of real time object detection in all these areas provide dynamic analysis of the visual information that helps in immediate decision making. Furthermore, advanced deep learning algorithms leverage the progress in the field of object detection providing more accurate and efficient solutions. There are some outstanding deep learning algorithms for object detection which includes, Faster R CNN(Region-based Convolutional Neural Network),Mask R-CNN, Cascade R-CNN, YOLO (You Only Look Once), SSD (Single Shot Multibox Detector), RetinaNet etc. This article goes into great detail on how deep learning algorithms are used to enhance real time object recognition. It provides information on the different object detection models available, open benchmark datasets, and studies on the use of object detection models in a range of applications. Additionally, controlled studies are provided to compare various strategies and produce some illuminating findings. Last but not least, a number of encouraging challenges and approaches are offered as suggestions for further investigation in both relevant deep learning approaches and object recognition.

Abstract (中文翻译): 目标检测在众多领域具有引人注目的应用，包括人机交互、安全与视频监控、导航与道路交通监控、交通系统、工业自动化医疗、增强现实（AR）与虚拟现实（VR）世界、环境监测以及活动识别。在所有这些领域中，实时目标检测的应用能够对视觉信息进行动态分析，从而辅助即时决策。此外，先进的深度学习算法利用目标检测领域的进展，提供了更加准确和高效的解决方案。目前存在一些杰出的深度学习目标检测算法，包括Faster R-CNN（基于区域的卷积神经网络）、Mask R-CNN、Cascade R-CNN、YOLO（You Only Look Once）、SSD（单次多框检测器）、RetinaNet等。本文深入详尽地探讨了深度学习算法如何用于增强实时目标识别。文章提供了关于现有不同目标检测模型、公开基准数据集的信息，并调研了目标检测模型在各种应用中的使用情况。此外，还提供了受控研究以比较不同策略，并得出了一些富有启发性的发现。最后，文章提出了一些有前景的挑战和方法，作为对未来相关深度学习方法和目标识别研究的建议。

</details>


### [7] [Visual Memory Injection Attacks for Multi-Turn Conversations](https://arxiv.org/abs/2602.15927)
*Christian Schlarmann,Matthias Hein*

Main category: cs.CV

TL;DR: 本文提出了一种名为视觉记忆注入（VMI）的新型隐蔽攻击方法，可在多轮对话中通过被篡改的图像，在特定触发提示下使大视觉语言模型（LVLM）输出预设的恶意信息，从而实现对用户的操纵。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型（LVLM）在长上下文多轮对话场景下的安全性尚未得到充分研究。作者旨在探索一种现实可行的攻击方式：攻击者上传经过篡改的图像，用户下载后将其作为LVLM输入，从而在特定条件下被操纵。

Method: 提出一种隐蔽的视觉记忆注入（VMI）攻击方法：在正常提示下模型行为正常，但当用户提供特定触发提示时，模型会输出攻击者预设的目标消息。该方法适用于多轮对话场景，并在多个开源LVLM上进行了验证。

Result: 实验表明，VMI攻击在经过长时间多轮对话后依然有效，能够在用户不知情的情况下成功注入并触发恶意信息，证明了利用扰动图像在多轮对话中大规模操纵用户的可行性。

Conclusion: 该研究揭示了当前LVLM在多轮对话设置下面临的安全风险，强调亟需提升模型对此类视觉记忆注入攻击的鲁棒性。作者已公开攻击代码以促进后续防御研究。

Abstract: Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection

Abstract (中文翻译): 近期，生成式大视觉语言模型（LVLM）取得了令人瞩目的性能提升，其用户群体也迅速扩大。然而，特别是在长上下文多轮对话场景下，LVLM的安全性仍鲜有研究。本文考虑一种现实场景：攻击者将经过篡改的图像上传至网络或社交媒体，良性用户下载该图像并将其作为LVLM的输入。我们提出了一种新颖且隐蔽的视觉记忆注入（VMI）攻击方法：在常规提示下，LVLM表现正常；但一旦用户给出特定的触发提示，LVLM便会输出攻击者预设的目标消息，以实现对用户的操纵（例如用于恶意营销或政治劝说）。与以往仅关注单轮攻击的研究不同，VMI即使在与用户进行长时间多轮对话后依然有效。我们在多个近期开源的LVLM上验证了该攻击的有效性。本文由此表明，在多轮对话场景中，通过扰动图像对用户进行大规模操纵是可行的，这呼吁提升LVLM对此类攻击的鲁棒性。我们已在 https://github.com/chs20/visual-memory-injection 公开源代码。

</details>


### [8] [Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families](https://arxiv.org/abs/2602.15950)
*Yuval Levental*

Main category: cs.CV

TL;DR: 本文通过一个简单实验揭示了视觉语言模型（VLMs）在处理无文本标识的纯视觉元素时存在严重空间定位缺陷：当二值网格中的填充单元格以纯色块而非文本符号（如“#”）呈现时，主流VLMs的转录准确率和F1分数大幅下降。


<details>
  <summary>Details</summary>
Motivation: 探究当前前沿视觉语言模型是否真正具备稳健的视觉空间推理能力，特别是在缺乏文本线索的情况下能否准确识别和定位图像中的非文本视觉元素。

Method: 生成15个不同密度（10.7%-41.8%）的15x15二值网格，分别渲染为两种图像类型：含文本符号（"."和"#"）和不含网格线的纯填充方块。要求Claude Opus、ChatGPT 5.2和Gemini 3 Thinking三个VLM对图像进行转录，并比较其在两种条件下的性能差异。

Result: 在文本符号条件下，Claude和ChatGPT达到约91%的单元格准确率和84%的F1分数，Gemini为84%准确率和63% F1；而在纯色块条件下，三者性能均显著下降至60-73%准确率和29-39% F1。各模型在色块条件下表现出不同的失败模式（系统性少计、大量多计、模板幻觉），但根源相同：对非文本视觉元素的空间定位能力严重退化。

Conclusion: 当前VLMs严重依赖文本识别通路进行空间推理，其原生视觉通路在处理无文本标识的视觉元素时表现远逊于文本通路，暴露出模型在纯粹视觉空间理解方面的根本性局限。

Abstract: We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.

Abstract (中文翻译): 我们提出了一项简单实验，揭示了视觉语言模型（VLMs）的一个根本性局限：当二值网格中的填充单元格缺乏文本标识时，模型无法准确对其进行定位。我们生成了十五个15x15的网格，填充密度在10.7%至41.8%之间，并将每个网格渲染为两种图像类型——文本符号（“.”和“#”）以及无网格线的纯填充方块——然后要求三个前沿VLM（Claude Opus、ChatGPT 5.2和Gemini 3 Thinking）对它们进行转录。在文本符号条件下，Claude和ChatGPT的单元格准确率约为91%，F1分数为84%；Gemini的准确率为84%，F1分数为63%。而在纯填充方块条件下，所有三个模型的性能均急剧下降，准确率降至60%-73%，F1分数降至29%-39%。关键在于，所有条件都经过相同的视觉编码器处理——文本符号是以图像形式输入的，而非被分词的文本。各模型在文本与方块条件下的F1分数差距达34至54个百分点，表明VLMs的行为仿佛拥有一条高保真度的文本识别通路用于空间推理，其性能远超其原生的视觉通路。每个模型在方块条件下都表现出独特的失败模式——系统性少计（Claude）、大量多计（ChatGPT）和模板幻觉（Gemini）——但它们都共享同一个根本缺陷：对非文本视觉元素的空间定位能力严重退化。

</details>


### [9] [Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration](https://arxiv.org/abs/2602.15959)
*Yiwen Wang,Jiahao Qin*

Main category: cs.CV

TL;DR: 提出GPEReg-Net，通过解耦场景与外观特征并引入全局位置编码，在高速双向光声显微成像中实现高质量帧间配准。


<details>
  <summary>Details</summary>
Motivation: 高速双向光栅扫描的光声显微成像（OR-PAM）存在前向与后向扫描线之间的域偏移和几何错位问题；现有方法受限于亮度恒定假设或缺乏时间一致性建模，难以实现高质量配准。

Method: 提出GPEReg-Net框架：1）利用AdaIN将图像解耦为域不变的场景特征和域特定的外观编码，实现无需显式形变场估计的图像到图像配准；2）设计全局位置编码（GPE）模块，融合可学习位置嵌入、正弦编码与跨帧注意力机制，利用相邻帧上下文提升时间一致性。

Result: 在OR-PAM-Reg-4K基准（432个测试样本）上，GPEReg-Net取得NCC 0.953、SSIM 0. 932和PSNR 34.49dB，SSIM和PSNR分别比当前最优方法提升3.8%和1.99dB，同时保持有竞争力的NCC。

Conclusion: GPEReg-Net有效解决了高速双向OR-PAM中的域偏移与几何错位问题，通过场景-外观解耦和时间上下文建模显著提升了配准质量，为动态光声成像提供了新思路。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at https://github.com/JiahaoQin/GPEReg-Net.

Abstract (中文翻译): 具有双向光栅扫描的高速光学分辨率光声显微成像（OR-PAM）可将成像速度提高一倍，但会引入前向与后向扫描线之间的耦合域偏移和几何错位。现有配准方法受限于亮度恒定假设，配准质量有限；而近期基于生成模型的方法虽通过复杂架构缓解域偏移，却缺乏对帧间时间一致性的建模。为此，我们提出GPEReg-Net——一种场景与外观解耦的配准框架，该框架通过自适应实例归一化（AdaIN）将域不变的场景特征与域特定的外观编码分离，从而实现无需显式估计形变场的端到端图像配准。为进一步利用序列采集中的时间结构，我们引入全局位置编码（GPE）模块，结合可学习的位置嵌入、正弦编码与跨帧注意力机制，使网络能够借助邻近帧的上下文信息提升时间一致性。在包含432个测试样本的OR-PAM-Reg-4K基准上，GPEReg-Net取得了0.953的NCC、0.932的SSIM和34.49dB的PSNR，相较当前最优方法，SSIM提升3.8%，PSNR提升1.99dB，同时保持了具有竞争力的NCC性能。代码已开源：https://github.com/JiahaoQin/GPEReg-Net。

</details>


### [10] [Automated Re-Identification of Holstein-Friesian Cattle in Dense Crowds](https://arxiv.org/abs/2602.15962)
*Phoenix Yu,Tilo Burghardt,Andrew W Dowsey,Neill W Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种新的检测-分割-识别（detect-segment-identify）流程，结合开放词汇无权重定位（Open-Vocabulary Weight-free Localisation）和Segment Anything模型（SAM），有效解决了荷斯坦-弗里生牛在密集聚集时因斑纹干扰导致的检测与重识别失败问题，在真实农场CCTV数据上实现了98.93%的检测准确率和94.82%的Re-ID准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于YOLO等方法的奶牛检测与重识别技术在个体空间分离时表现良好，但在奶牛紧密聚集、尤其具有破坏轮廓的斑纹（如荷斯坦牛）时性能显著下降。因此，亟需一种在密集群体中仍能保持高准确率且具备良好迁移能力的方法。

Method: 提出一种三阶段pipeline：首先利用开放词汇无权重定位模型进行初步定位，再通过Segment Anything Model（SAM）进行精细分割，最后将分割结果输入重识别（Re-ID）网络；同时采用无监督对比学习优化Re-ID性能。

Result: 在自建的9天真实奶牛场CCTV数据集上，所提方法检测准确率达98.93%，较当前基于定向边界框的方法和SAM基线分别提升47.52%和27.13%；Re-ID准确率达到94.82%。

Conclusion: 该方法在无需人工干预的情况下，显著提升了密集场景下奶牛重识别的实用性与可靠性，适用于实际农场环境，并开源了代码与数据集以支持复现。

Abstract: Holstein-Friesian detection and re-identification (Re-ID) methods capture individuals well when targets are spatially separate. However, existing approaches, including YOLO-based species detection, break down when cows group closely together. This is particularly prevalent for species which have outline-breaking coat patterns. To boost both effectiveness and transferability in this setting, we propose a new detect-segment-identify pipeline that leverages the Open-Vocabulary Weight-free Localisation and the Segment Anything models as pre-processing stages alongside Re-ID networks. To evaluate our approach, we publish a collection of nine days CCTV data filmed on a working dairy farm. Our methodology overcomes detection breakdown in dense animal groupings, resulting in a 98.93% accuracy. This significantly outperforms current oriented bounding box-driven, as well as SAM species detection baselines with accuracy improvements of 47.52% and 27.13%, respectively. We show that unsupervised contrastive learning can build on this to yield 94.82% Re-ID accuracy on our test data. Our work demonstrates that Re-ID in crowded scenarios is both practical as well as reliable in working farm settings with no manual intervention. Code and dataset are provided for reproducibility.

Abstract (中文翻译): 荷斯坦-弗里生牛的检测与重识别（Re-ID）方法在目标彼此空间分离时表现良好，但当奶牛紧密聚集时，包括基于YOLO的物种检测在内的现有方法会失效，这一问题在具有破坏轮廓斑纹的物种中尤为突出。为在此类场景下同时提升有效性与可迁移性，我们提出了一种新的“检测-分割-识别”流程，该流程将开放词汇无权重定位（Open-Vocabulary Weight-free Localisation）和Segment Anything模型（SAM）作为预处理阶段，与Re-ID网络协同工作。为评估所提方法，我们发布了在运营中奶牛场连续九天拍摄的CCTV数据集。我们的方法有效克服了密集动物群中的检测失效问题，检测准确率达到98.93%，显著优于当前基于定向边界框的方法和SAM物种检测基线，准确率分别提升了47.52%和27.13%。此外，我们证明无监督对比学习可在此基础上进一步实现94.82%的Re-ID准确率。本研究表明，在拥挤场景下的重识别在实际农场环境中既实用又可靠，且无需人工干预。为确保可复现性，我们已开源代码与数据集。

</details>
