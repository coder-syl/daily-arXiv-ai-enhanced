<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文首次系统综述了3D高斯泼溅（3DGS）的知识产权保护研究，提出了一个自下而上的分析框架，并指出了未来六大研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着3D高斯泼溅在实时3D场景合成中的广泛应用及其商业价值提升，其显式的参数化结构引发了对知识产权（IP）保护的迫切需求，但当前研究分散，缺乏统一视角。

Method: 提出一个自下而上的分析框架，从（i）基于高斯的扰动机制、（ii）被动与主动保护范式、（iii）生成式AI时代下的鲁棒性威胁三个方面系统梳理现有工作。

Result: 揭示了当前3DGS IP保护在技术基础和鲁棒性表征方面的不足，并识别出若干值得深入探索的研究空白。

Conclusion: 为实现可靠可信的3DGS资产IP保护，未来应聚焦于鲁棒性、效率和保护范式等六个方向开展研究。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

Abstract (中文翻译): 3D高斯泼溅（3DGS）已成为实时3D场景合成的主流表示方法，广泛应用于虚拟现实、增强现实、机器人和3D内容创作等领域。其日益增长的商业价值和显式的参数化结构引发了新兴的知识产权（IP）保护问题，促使大量关于3DGS知识产权保护的研究涌现。然而，当前的研究仍较为零散，缺乏对底层机制、保护范式及鲁棒性挑战的统一认识。为填补这一空白，本文首次对3DGS知识产权保护进行了系统性综述，并提出一个自下而上的分析框架，从（i）基于高斯的扰动机制、（ii）被动与主动保护范式，以及（iii）生成式人工智能时代下的鲁棒性威胁三个方面进行深入探讨，揭示了当前在技术基础和鲁棒性表征方面的不足，并指出了进一步深入研究的机会。最后，本文提出了涵盖鲁棒性、效率和保护范式等六个方面的未来研究方向，为构建可靠且可信的3DGS资产知识产权保护体系提供了路线图。

</details>


### [2] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: 提出TruKAN，一种基于截断幂函数的KAN新架构，在保持表达能力的同时提升准确率、训练速度和可解释性，并在视觉任务中优于其他KAN变体。


<details>
  <summary>Details</summary>
Motivation: 现有KAN模型在计算效率与遵循Kolmogorov-Arnold表示定理原则之间存在权衡，且可解释性不足。作者旨在设计一种兼顾逼近能力、计算效率与透明度的新架构。

Method: 将KAN中的B样条基替换为源自k阶样条理论的截断幂函数族；每层结合截断幂项与多项式项，并采用共享或独立节点；构建TruKAN-EfficientNet-V2模型，并与MLP、KAN、SineKAN等变体在相同框架下进行公平比较；使用混合优化策略训练，并研究层归一化及节点配置的影响。

Result: TruKAN在计算机视觉基准数据集上，在准确率、训练时间和内存占用方面均优于其他KAN变体，尤其在小型和深层架构中表现突出。

Conclusion: TruKAN通过简化基函数和节点配置，在保持KAN表达能力的同时显著提升了效率与可解释性，验证了其在复杂视觉任务中的实用优势，超越了以往KAN研究的局限场景。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

Abstract (中文翻译): 为解决计算效率与遵循Kolmogorov-Arnold网络（KAN）原则之间的权衡问题，我们提出了TruKAN——一种基于KAN结构和可学习激活函数的新架构。TruKAN用源自k阶样条理论的一族截断幂函数替代了KAN中的B样条基。这一改变在保持KAN表达能力的同时，提高了准确率和训练速度。每个TruKAN层结合了一个截断幂项与一个多项式项，并采用共享或独立的节点（knots）。由于其简化的基函数和节点配置，TruKAN相比其他KAN变体具有更强的可解释性。通过优先考虑可解释的基函数，TruKAN力求在逼近效能与透明度之间取得平衡。我们开发了TruKAN模型并将其集成到先进的EfficientNet-V2框架中，随后在计算机视觉基准数据集上进行评估。为确保公平比较，我们构建了多种模型：基于MLP、KAN、SineKAN和TruKAN的EfficientNet框架，并在小型和深层架构中评估它们的训练时间和准确率。训练阶段采用混合优化策略以提升收敛稳定性。此外，我们还研究了所有模型的层归一化技术，并评估了TruKAN中共享节点与独立节点的影响。总体而言，TruKAN在复杂视觉任务中，在准确率、计算效率和内存使用方面均优于其他KAN模型，展现出超越以往KAN研究有限场景的优势。

</details>


### [3] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 提出了一种名为DiGAN的新方法，结合扩散模型与注意力卷积网络，用于从有限且不规则的纵向神经影像数据中增强时间上下文并提升早期阿尔茨海默病检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在早期阿尔茨海默病诊断中受限于对大规模纵向数据的依赖，且难以处理临床数据中固有的时间连续性和模态不规则性问题。

Method: 提出Diffusion-Guided Attention Network（DiGAN），将潜在扩散模型与注意力引导的卷积网络相结合：扩散模型从有限数据合成逼真的纵向神经影像轨迹，注意力卷积层则捕捉区分认知正常、轻度认知障碍和主观认知下降个体的结构-时间模式。

Result: 在合成数据集和ADNI数据集上的实验表明，DiGAN优于当前最先进的基线方法。

Conclusion: DiGAN在应对有限且不规则临床数据方面具有优势，展现出在阿尔茨海默病早期检测中的应用潜力。

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

Abstract (中文翻译): 由于前驱期阶段脑结构变化细微且时间进程不规则，阿尔茨海默病（AD）的早期诊断仍面临重大挑战。现有的深度学习方法通常需要大量纵向数据集，并且往往无法有效建模真实临床数据中固有的时间连续性和模态不规则性。为解决这些问题，我们提出了扩散引导注意力网络（DiGAN），该方法将潜在扩散建模与注意力引导的卷积网络相结合。扩散模型能够从有限的训练数据中合成逼真的纵向神经影像轨迹，从而丰富时间上下文信息并提高对访视时间间隔不均的鲁棒性。随后，注意力卷积层可捕捉区分认知正常个体与轻度认知障碍及主观认知下降个体的关键结构-时间模式。在合成数据集和ADNI数据集上的实验表明，DiGAN优于当前最先进的基线方法，展现出其在AD早期检测中的潜力。

</details>


### [4] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: PriorProbe 是一种基于 MCMC 与人类结合的新方法，能有效提取个体细粒度认知先验，并用于提升神经网络在模糊刺激下的个性化预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确、无偏地获取个体层面的认知先验，限制了神经网络的个性化能力。

Method: 提出 PriorProbe 方法，基于 Markov Chain Monte Carlo with People（MCMCP），从个体参与者中恢复其特定的认知先验，并将其整合到先进神经网络中。

Result: 在面部表情识别任务中，结合 PriorProbe 提取的先验显著提升了模型对个体在模糊刺激下分类行为的预测能力，优于仅用神经网络或其它先验来源的方法，且不影响对真实标签的判断。

Conclusion: PriorProbe 为个性化深度神经网络提供了一个通用且可解释的框架。

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

Abstract (中文翻译): 引入个体层面的认知先验是实现神经网络个性化的重要途径，但准确获取此类先验仍具挑战性：现有方法要么无法唯一识别这些先验，要么引入系统性偏差。本文提出 PriorProbe，一种基于“与人结合的马尔可夫链蒙特卡洛”（Markov Chain Monte Carlo with People）的新颖先验提取方法，能够恢复细粒度的个体特异性先验。我们以面部表情识别任务为例，对个体参与者应用 PriorProbe，并检验将所恢复的先验与当前最先进的神经网络相结合是否能提升模型对个体在模糊刺激下分类行为的预测能力。结果表明，由 PriorProbe 获取的先验显著提升了模型性能，不仅优于单独使用神经网络，也优于其他先验来源，同时保持了模型对真实标签的推理能力。综上，本研究证明 PriorProbe 为个性化深度神经网络提供了一个通用且可解释的框架。

</details>


### [5] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的计算机视觉框架，用于在增材制造部件的三维断层图像中检测孔隙并评估其临界性。研究发现，孔隙到表面的归一化距离是预测临界性的主导因素，远超其他几何特征的影响。


<details>
  <summary>Details</summary>
Motivation: 增材制造部件中的内部孔隙是影响结构性能的关键缺陷，但现有自动检测方法缺乏可解释性，使工程师难以理解临界性预测的物理依据。

Method: 将连续灰度切片重建为三维数据集，通过基于强度的阈值分割和连通成分分析识别出500个独立孔隙；利用几何描述符（尺寸、长宽比、范围、距边界位置）表征每个孔隙，并基于百分位欧氏距离构建孔隙交互网络（24,950条连接）；使用机器学习模型预测孔隙临界性，并通过SHAP分析量化各特征贡献。

Result: 归一化表面距离对模型预测的贡献比其他所有描述符高出一个数量级以上；孔隙尺寸影响微弱，其他几何参数几乎无影响；表面邻近性与临界性呈强负相关，揭示了边界驱动的失效机制。

Conclusion: 该可解释框架实现了透明的缺陷评估，为增材制造的工艺优化和质量控制提供了可操作的见解。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

Abstract (中文翻译): 内部孔隙仍是增材制造部件中的关键缺陷模式，会损害结构性能并限制其工业应用。尽管已有自动化缺陷检测方法，但其缺乏可解释性，使工程师无法理解临界性预测的物理基础。本研究提出了一种可解释的计算机视觉框架，用于在三维断层扫描体积中进行孔隙检测与临界性评估。通过将连续灰度切片重建为体数据集，并结合基于强度的阈值分割与连通成分分析，识别出500个独立孔隙。每个孔隙均以几何描述符（包括尺寸、长宽比、范围以及相对于试样边界的几何位置）进行表征。基于百分位数的欧氏距离准则构建了孔隙相互作用网络，共形成24,950条孔隙间连接。利用提取的特征训练机器学习模型以预测孔隙临界性评分，并通过SHAP分析量化各特征的贡献。结果表明，归一化表面距离在模型预测中占主导地位，其重要性比其他所有描述符高出一个数量级以上；孔隙尺寸影响极小，而其他几何参数的影响可忽略不计。表面邻近性与临界性之间存在强烈的负相关关系，揭示了由边界驱动的失效机制。该可解释框架实现了透明的缺陷评估，为增材制造的工艺优化和质量控制提供了可操作的见解。

</details>


### [6] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 本文提出了4DPC²hat，首个面向动态点云理解的多模态大语言模型，并构建了包含20万问答对的大规模跨模态数据集4DPC²hat-200K，通过Mamba增强的时序推理架构和失败感知的自举学习策略，显著提升了动作理解和时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态点云对象，缺乏对动态点云序列的理解，原因在于缺少大规模跨模态数据集以及在时空上下文中建模运动的困难。

Method: 构建了4DPC²hat-200K数据集，包含44K动态对象序列、700K点云帧和200K问答对；提出基于Mamba的时序推理MLLM以捕捉长程依赖和动态模式；设计失败感知的自举学习策略，迭代生成针对性QA监督以增强模型推理能力。

Result: 实验表明，4DPC²hat在动作理解和时序推理方面显著优于现有模型，为4D动态点云理解奠定了坚实基础。

Conclusion: 本文通过构建首个大规模动态点云跨模态数据集和专用MLLM架构，有效推动了4D动态点云理解的发展，为未来相关研究提供了重要资源和方法基础。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

Abstract (中文翻译): 点云为三维物体提供了一种紧凑且富有表现力的表示方式，近期已被整合到多模态大语言模型（MLLMs）中。然而，现有方法主要聚焦于静态物体，而对动态点云序列的理解仍鲜有探索。这一局限性主要源于缺乏大规模跨模态数据集，以及在时空上下文中建模运动的困难。为弥合这一差距，我们提出了4DPC²hat——首个专为动态点云理解定制的多模态大语言模型。为此，我们通过一个精细的两阶段流程（包括拓扑一致的4D点云构建和两级描述生成）构建了大规模跨模态数据集4DPC²hat-200K。该数据集包含超过44K个动态物体序列、700K个点云帧和200K个精心整理的问题-答案（QA）对，支持关于计数、时序关系、动作、空间关系和外观等方面的查询。在框架核心部分，我们引入了一种Mamba增强的时序推理MLLM，以捕捉点云序列中的长程依赖和动态模式。此外，我们提出了一种失败感知的自举学习策略，可迭代识别模型缺陷并生成针对性的QA监督信号，持续强化相应的推理能力。大量实验表明，与现有模型相比，我们的4DPC²hat在动作理解和时序推理方面显著提升，为4D动态点云理解奠定了坚实基础。

</details>


### [7] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本文提出MQA-RefAVS任务，用于在无真实标注的情况下评估语言引导的音视频分割（Ref-AVS）中候选分割掩码的质量，并构建了包含多种错误类型的基准数据集MQ-RAVSBench，同时提出了基于多模态大语言模型的评估器MQ-Auditor，可有效检测分割失败并辅助改进下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导的音视频分割（Ref-AVS）方法主要关注生成分割掩码，但缺乏对掩码质量的丰富且可解释的诊断能力。在推理阶段无法依赖真实标注的情况下，如何评估掩码质量、识别错误类型并给出改进建议，是一个尚未充分探索的问题。

Method: 作者提出了MQA-RefAVS新任务，要求模型基于音视频-语言输入和给定的分割掩码，估计其与未知真实掩码的IoU、识别错误类型并给出质量控制决策。为此构建了包含几何与语义错误的基准数据集MQ-RAVSBench，并设计了基于多模态大语言模型（MLLM）的MQ-Auditor，通过显式推理多模态线索和掩码信息进行质量评估。

Result: 实验表明，MQ-Auditor在掩码质量评估任务上优于现有的开源和商用多模态大语言模型，并能有效集成到现有Ref-AVS系统中，用于检测分割失败并支持下游分割性能的提升。

Conclusion: 本文首次系统性地研究了Ref-AVS中的掩码质量评估问题，提出的MQA-RefAVS任务、MQ-RAVSBench基准和MQ-Auditor模型为该领域提供了新的方向和实用工具，有助于提升音视频分割系统的鲁棒性和可解释性。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

Abstract (中文翻译): 语言引导的音视频分割（Ref-AVS）旨在通过联合推理视频、音频和文本，分割出自然语言所描述的目标对象。除了生成分割掩码外，如何提供丰富且可解释的掩码质量诊断仍鲜有研究。本文提出了Ref-AVS场景下的掩码质量评估任务（MQA-RefAVS），该任务在推理阶段无需依赖真实标注即可评估候选分割掩码的质量。给定音视频-语言输入及每个提供的分割掩码，该任务需估计其与未观测到的真实掩码之间的IoU，识别对应的错误类型，并推荐可操作的质量控制决策。为支持该任务，我们构建了MQ-RAVSBench基准数据集，其中包含涵盖几何与语义问题的多样化且具代表性的掩码错误模式。我们进一步提出了MQ-Auditor——一种基于多模态大语言模型（MLLM）的评估器，能够显式地对多模态线索和掩码信息进行推理，从而生成定量与定性的掩码质量评估。大量实验表明，MQ-Auditor优于强大的开源和商业MLLM，并可与现有Ref-AVS系统集成，用于检测分割失败并支持下游分割性能的提升。相关数据与代码将发布于：https://github.com/jasongief/MQA-RefAVS。

</details>


### [8] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: 提出了一种名为GPAIR的超快速三维光声迭代重建方法，通过高斯核和GPU加速实现亚秒级重建，极大提升三维光声成像的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统迭代重建（IR）算法在三维光声计算机断层成像（PACT）中计算耗时过长（数百秒至数小时），严重限制了其实际应用。

Method: 提出Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction（GPAIR）方法，使用连续各向同性高斯核替代传统空间网格，并推导压力波的解析闭式表达，结合GPU加速的可微Triton算子实现高效计算。

Result: 在动物实验中，GPAIR对包含840万体素的三维目标实现了亚秒级重建速度，比传统方法快几个数量级。

Conclusion: 该方法实现了近实时的大规模三维光声重建，显著推动三维PACT向临床应用迈进。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

Abstract (中文翻译): 尽管迭代重建（IR）算法能够显著校正光声（PA）计算机断层成像（PACT）中的重建伪影，但其重建时间过长，尤其在大规模三维（3D）成像中，IR算法需耗时数百秒甚至数小时。这种计算负担严重限制了IR算法的实际应用。本研究提出了一种用于三维PACT的超快速IR方法，称为基于高斯核的超快速三维光声迭代重建（GPAIR），实现了数量级的计算加速。GPAIR采用连续各向同性的高斯核替代传统的空间网格，并推导出压力波的解析闭式表达，同时利用强大的GPU加速可微Triton算子。在动物实验中，GPAIR对包含840万体素的三维目标实现了亚秒级的超快重建速度。这一革命性的超快图像重建技术实现了近实时的大规模三维光声重建，显著推动了三维PACT向临床应用的发展。

</details>


### [9] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 该研究评估了基于Vision Transformer（ViT）的无监督和半监督方法在将大量未标记动物图像聚类到物种级别甚至亚种级别（如性别、年龄）的可行性，发现DINOv3结合t-SNE和层次聚类可实现接近完美的物种级聚类（V-measure: 0.958），且无需专家标注。


<details>
  <summary>Details</summary>
Motivation: 生态学研究中手动标注动物图像成本高昂，限制了生物多样性监测的规模与效率。作者希望探索利用最新的视觉基础模型（ViT）直接对未标记图像进行物种级甚至亚种级聚类，以减轻人工标注负担。

Method: 构建了一个包含5种ViT模型、5种降维方法和4种聚类算法（2种有监督、2种无监督）的综合基准框架，在60个物种（30种哺乳动物+30种鸟类）上测试，每种使用200张已验证图像。评估不同组合在物种级聚类及揭示种内变异（如年龄、性别、表型）方面的性能。

Result: 最佳方法（DINOv3 + t-SNE + 有监督层次聚类）达到V-measure 0.958；无监督方法也表现优异（0.943），仅需剔除1.14%的异常图像。模型对长尾分布具有鲁棒性，且通过过聚类可有效提取种内变异信息。

Conclusion: ViT基础模型（尤其是DINOv2/DINOv3）结合适当聚类策略可高效、准确地对未标记动物图像进行物种级自动分组，并揭示生态学上有意义的亚种结构，为生态学家提供了一套开源工具和方法选择指南。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

Abstract (中文翻译): 在生态学研究中，动物图像的人工标注仍然是一个重大瓶颈，限制了生物多样性监测工作的规模和效率。本研究探讨了最先进的视觉Transformer（ViT）基础模型是否能够直接将数千张未标注的动物图像聚类至物种级别。我们提出了一个全面的基准测试框架，评估了五种ViT模型结合五种降维技术和四种聚类算法（两种有监督、两种无监督）在60个物种（30种哺乳动物和30种鸟类）上的表现，每个测试使用每种物种200张经过验证的图像的随机子集。我们研究了聚类在物种级别何时成功、何时失败，以及物种内部的聚类是否能揭示具有生态学意义的模式，例如性别、年龄或表型变异。结果表明，使用DINOv3嵌入结合t-SNE和有监督层次聚类方法，可实现近乎完美的物种级聚类（V-measure：0.958）。无监督方法在无需先验物种知识的情况下也取得了具有竞争力的性能（0.943），仅将1.14%的图像作为异常值剔除并交由专家审核。我们进一步证明了该方法对真实场景中常见的物种长尾分布具有鲁棒性，并表明有意进行过聚类（over-clustering）能够可靠地提取种内变异信息，包括年龄阶段、两性异形和毛色差异。我们发布了一个开源的基准测试工具包，并为生态学家提供了针对其特定分类群和数据选择合适方法的建议。

</details>


### [10] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 本文提出了NH-Fair，一个统一的公平性评估基准，涵盖视觉模型和大型视觉语言模型，在标准化数据、指标和训练协议下系统评估多种去偏方法，并发现精心调优的经验风险最小化（ERM）基线往往优于许多复杂去偏方法，而复合数据增强策略在不牺牲效用的前提下持续提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性研究存在数据集异构、公平指标不一致、仅单独评估视觉或多模态模型、以及超参数调优不足等问题，导致不同去偏方法难以公平比较，因此亟需一个统一、可复现且考虑调优影响的公平性评估框架。

Method: 作者构建了NH-Fair基准，涵盖监督学习与零样本设定下的视觉模型和大型视觉语言模型（LVLMs），采用标准化的数据、公平性指标和训练流程；系统研究经验风险最小化（ERM）中的训练选择对效用与差异的影响，并对比多种去偏方法的效果。

Result: (1) ERM中某些训练选择对模型效用和群体差异有显著影响，据此可缩小超参搜索空间；(2) 多数去偏方法无法稳定超越调优后的ERM基线，而复合数据增强方法能一致提升公平性而不损失性能；(3) LVLM虽平均准确率更高，但仍存在子群差异，且模型规模带来的收益通常小于架构或训练协议改进带来的收益。

Conclusion: NH-Fair提供了一个可复现、考虑调优影响的公平性评估流程，强调在追求公平时不应忽视模型效用，并指出简单但精心调优的基线或数据增强策略可能是更实用的去偏方案。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

Abstract (中文翻译): 在真实世界数据上训练的机器学习模型常常继承并放大对某些社会群体的偏见，这引发了对其大规模部署的紧迫担忧。尽管已有大量偏见缓解方法被提出，但由于数据集异构、公平性指标不一致、仅孤立地评估视觉或多模态模型，以及超参数调优不足等问题，使得不同偏见缓解方法的有效性难以公平比较。我们提出了NH-Fair——一个“无害公平”（fairness without harm）的统一基准，涵盖视觉模型和大型视觉语言模型（LVLMs），在标准化的数据、指标和训练协议下进行评估，覆盖监督学习和零样本场景。我们的主要贡献包括：(1) 一项系统的经验风险最小化（ERM）调优研究，识别出对模型效用和群体差异具有显著影响的训练选择，从而为从业者提供基于实证的指导，以减少在实现高公平性与高准确性时所需的昂贵超参数搜索空间；(2) 发现许多去偏方法并不能稳定地优于经过良好调优的ERM基线，而一种复合数据增强方法则能持续提升公平性而不牺牲模型效用，展现出作为实用策略的潜力；(3) 分析表明，尽管LVLMs获得了更高的平均准确率，但仍存在子群体差异，且模型规模扩大带来的收益通常小于架构设计或训练协议改进所带来的收益。NH-Fair提供了一个可复现、关注调优影响的评估流程，用于开展严谨且关注潜在危害的公平性评估。

</details>
