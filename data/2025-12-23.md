<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes](https://arxiv.org/abs/2512.17939)
*Yuncheng Lu,Yucen Shi,Aobo Li,Zehao Li,Junying Li,Bo Wang,Tony Tae-Hyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种高能效的反无人机系统，结合帧基与事件驱动的目标跟踪，在40 nm工艺下实现极低功耗和98.2%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有反无人机系统在检测小型、高速无人机时存在能效低、鲁棒性不足的问题，亟需一种兼顾低功耗与高可靠性的解决方案。

Method: 系统采用基于游程编码的二值事件帧重建、区域提议生成，并根据目标尺寸与速度自适应切换帧模式与事件模式；引入快速目标跟踪单元，通过自适应阈值和轨迹分类提升高速目标跟踪鲁棒性；神经处理单元采用定制指令集与零跳过MAC架构，支持灰度块与轨迹推理。

Result: 芯片面积2 mm²，工作电压0.8 V，每像素每帧能耗96 pJ，每事件能耗61 pJ，在50–400米距离和5–80像素/秒速度范围内，在公开UAV数据集上达到98.2%识别准确率，神经计算冗余减少超97%。

Conclusion: 该系统在端到端能效方面达到当前反无人机系统的领先水平，兼具高精度与超低功耗。

Abstract: We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.

Abstract (中文翻译): 我们提出了一种高能效的反无人机系统，该系统融合了基于帧和事件驱动的目标跟踪方法，以实现对小型高速无人机的可靠检测。系统利用游程编码重建二值事件帧，生成区域提议，并根据目标的尺寸和速度自适应地在帧模式与事件模式之间切换。快速目标跟踪单元通过自适应阈值和基于轨迹的分类，提高了对高速目标的跟踪鲁棒性。神经处理单元采用定制指令集和零跳过乘累加（MAC）架构，同时支持灰度图像块和轨迹推理，将冗余的神经计算减少了97%以上。该芯片采用40 nm CMOS工艺实现，面积为2 mm²，在0.8 V电压下每像素每帧能耗为96 pJ，每个事件能耗为61 pJ，在50至400米探测距离和5至80像素/秒目标速度范围内，在公开无人机数据集上达到了98.2%的识别准确率。实验结果表明，该系统在端到端能效方面达到了当前反无人机系统的领先水平。

</details>


### [2] [NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction](https://arxiv.org/abs/2512.17943)
*Karthik Prabhakar*

Main category: cs.CV

TL;DR: NystagmusNet 是一个基于 AI 的系统，用于预测眼震患者在特定光照环境下的光敏感风险，并提供实时视觉调节建议。


<details>
  <summary>Details</summary>
Motivation: 眼震患者因环境亮度引发的不自主眼球运动而面临日常生活困难，现有辅助手段缺乏个性化预测能力。

Method: 采用双分支卷积神经网络，结合合成与增强数据集，根据环境亮度和眼球运动变异估算光敏感风险评分，并集成 SHAP 与 GradCAM 提高可解释性；同时配备基于规则的推荐引擎以提供自适应滤镜建议。

Result: 模型在合成数据上达到 75% 的验证准确率，并能通过可视化技术标示环境中的高风险区域。

Conclusion: NystagmusNet 为眼震患者的光敏感问题提供了可解释、个性化的预测与干预框架，未来可通过智能眼镜部署并引入强化学习优化推荐。

Abstract: Nystagmus patients with photosensitivity face significant daily challenges due to involuntary eye movements exacerbated by environmental brightness conditions. Current assistive solutions are limited to symptomatic treatments without predictive personalization. This paper proposes NystagmusNet, an AI-driven system that predicts high-risk visual environments and recommends real-time visual adaptations. Using a dual-branch convolutional neural network trained on synthetic and augmented datasets, the system estimates a photosensitivity risk score based on environmental brightness and eye movement variance. The model achieves 75% validation accuracy on synthetic data. Explainability techniques including SHAP and GradCAM are integrated to highlight environmental risk zones, improving clinical trust and model interpretability. The system includes a rule-based recommendation engine for adaptive filter suggestions. Future directions include deployment via smart glasses and reinforcement learning for personalized recommendations.

Abstract (中文翻译): 患有光敏感的眼震患者由于环境亮度条件加剧了不自主的眼球运动，日常生活中面临重大挑战。目前的辅助解决方案仅限于对症治疗，缺乏预测性和个性化。本文提出了 NystagmusNet——一种由人工智能驱动的系统，能够预测高风险视觉环境并提供实时视觉适应建议。该系统使用双分支卷积神经网络，在合成与增强数据集上进行训练，根据环境亮度和眼球运动变异估算光敏感风险评分。模型在合成数据上达到了 75% 的验证准确率。系统集成了包括 SHAP 和 GradCAM 在内的可解释性技术，以突出显示环境中的风险区域，从而提升临床可信度和模型可解释性。此外，系统还包含一个基于规则的推荐引擎，用于提供自适应滤镜建议。未来的研究方向包括通过智能眼镜部署该系统，以及利用强化学习实现个性化推荐。

</details>


### [3] [SuperFlow: Training Flow Matching Models with RL on the Fly](https://arxiv.org/abs/2512.17951)
*Kaijie Chen,Zhiyang Xu,Ying Shen,Zihao Lin,Yuguang Yao,Lifu Huang*

Main category: cs.CV

TL;DR: SuperFlow 是一种用于流模型的强化学习训练框架，通过方差感知采样动态调整每提示组大小，并计算与连续时间流动力学一致的逐步优势，从而提升训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的流模型训练存在两个问题：一是固定每提示组大小忽略了不同提示在采样重要性上的差异，导致采样效率低下；二是将轨迹级优势重复用作逐步估计，造成信用分配偏差。

Method: 提出 SuperFlow 框架，采用方差感知采样动态调整组大小，并设计符合连续时间流动力学的逐步优势计算方法。

Result: SuperFlow 在仅使用原训练步数 5.4%–56.3% 的情况下达到更优性能，训练时间减少 5.2%–16.7%；在多项文本到图像任务上，相较 SD3.5-M 提升 4.6%–47.2%，相较 Flow-GRPO 提升 1.7%–16.0%。

Conclusion: SuperFlow 有效解决了现有流模型强化学习训练中的采样效率和信用分配问题，在不改变模型架构的前提下显著提升了训练效率和生成效果。

Abstract: Recent progress in flow-based generative models and reinforcement learning (RL) has improved text-image alignment and visual quality. However, current RL training for flow models still has two main problems: (i) GRPO-style fixed per-prompt group sizes ignore variation in sampling importance across prompts, which leads to inefficient sampling and slower training; and (ii) trajectory-level advantages are reused as per-step estimates, which biases credit assignment along the flow. We propose SuperFlow, an RL training framework for flow-based models that adjusts group sizes with variance-aware sampling and computes step-level advantages in a way that is consistent with continuous-time flow dynamics. Empirically, SuperFlow reaches promising performance while using only 5.4% to 56.3% of the original training steps and reduces training time by 5.2% to 16.7% without any architectural changes. On standard text-to-image (T2I) tasks, including text rendering, compositional image generation, and human preference alignment, SuperFlow improves over SD3.5-M by 4.6% to 47.2%, and over Flow-GRPO by 1.7% to 16.0%.

Abstract (中文翻译): 近期基于流的生成模型和强化学习（RL）的进展改善了文本-图像对齐和视觉质量。然而，当前针对流模型的强化学习训练仍存在两个主要问题：(i) GRPO 风格的固定每提示组大小忽略了不同提示在采样重要性上的差异，导致采样效率低下和训练速度变慢；(ii) 将轨迹级优势重复用作逐步估计，导致沿流路径的信用分配出现偏差。我们提出了 SuperFlow——一种用于流模型的强化学习训练框架，该框架通过方差感知采样动态调整组大小，并以与连续时间流动力学一致的方式计算逐步优势。实验表明，SuperFlow 在仅使用原始训练步数 5.4% 至 56.3% 的情况下即能达到优异性能，并在不进行任何架构修改的前提下将训练时间减少 5.2% 至 16.7%。在包括文本渲染、组合图像生成和人类偏好对齐在内的标准文本到图像（T2I）任务上，SuperFlow 相较 SD3.5-M 提升了 4.6% 至 47.2%，相较 Flow-GRPO 提升了 1.7% 至 16.0%。

</details>


### [4] [Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition](https://arxiv.org/abs/2512.17953)
*Ellie Zhou,Jihoon Chung,Olga Russakovsky*

Main category: cs.CV

TL;DR: 本文系统分析了多种视频理解模型中的背景偏见问题，发现分类模型、图文对比预训练模型和视频大语言模型均倾向于依赖背景而非人体动作进行判断；作者提出对分类模型使用人体分割输入可降低3.78%的背景偏见，而通过提示词调优可使视频大语言模型的人体聚焦推理提升9.85%。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别模型常依赖背景线索而非人体运动与姿态进行预测，导致背景偏见问题，影响模型对人类动作本身的理解能力。

Method: 对分类模型、图文对比预训练模型和视频大语言模型（VLLM）进行背景偏见的系统分析；针对分类模型采用人体分割输入以缓解偏见；对VLLM探索手动与自动提示词调优策略。

Result: 所有被测模型均表现出显著的背景偏见；使用分割后的人体输入可将分类模型的背景偏见降低3.78%；通过优化提示词可使VLLM的人体聚焦推理能力提升9.85%。

Conclusion: 背景偏见广泛存在于当前主流视频理解模型中，但可通过输入处理和提示工程有效缓解，从而引导模型更关注人体动作本身。

Abstract: Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.

Abstract (中文翻译): 人类动作识别模型常常依赖背景线索而非人体运动和姿态来进行预测，这种行为被称为背景偏见。我们对分类模型、对比式图文预训练模型以及视频大语言模型（VLLM）中的背景偏见进行了系统性分析，发现它们都表现出强烈的依赖背景进行推理的倾向。接着，我们提出了针对分类模型的缓解策略，并证明引入分割后的人体输入能有效将背景偏见降低3.78%。最后，我们探索了针对VLLM的手动与自动提示词调优方法，表明通过精心设计提示词可将模型预测引导至更关注人体本身的推理，提升幅度达9.85%。

</details>


### [5] [SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries](https://arxiv.org/abs/2512.17954)
*Bin Wang,Fadi Dornaika*

Main category: cs.CV

TL;DR: 本文提出 SCS-SupCon 方法，通过引入基于 sigmoid 的成对对比损失和风格-内容解耦约束，有效缓解负样本稀释问题并实现自适应决策边界，在多个细粒度图像分类数据集上取得 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督对比学习方法（如基于 InfoNCE 损失的方法）在细粒度图像分类任务中受限于类间差异微弱、类内变化大，存在负样本稀释和缺乏自适应决策边界的问题，导致判别能力不足。

Method: 提出 Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon)：1）设计基于 sigmoid 的成对对比损失，引入可学习的温度与偏置参数以实现自适应决策边界，强调难负样本并缓解负样本稀释；2）加入显式的风格距离约束，解耦风格与内容表征。

Result: 在包括 CUB200-2011 和 Stanford Dogs 在内的六个基准数据集上，SCS-SupCon 在 CNN 与 Transformer 主干网络下均达到 SOTA。在 CIFAR-100（ResNet-50）上，五折交叉验证下 top-1 准确率比 SupCon 高约 3.9%，比 CS-SupCon 高约 1.7%；在细粒度数据集上优于 CS-SupCon 0.4–3.0 个百分点。Friedman 检验与 Nemenyi 事后分析验证了方法的稳定性和泛化能力。

Conclusion: SCS-SupCon 通过改进对比损失函数和引入风格-内容解耦机制，显著提升了细粒度图像分类的性能，具有良好的鲁棒性与泛化能力。

Abstract: Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.

Abstract (中文翻译): 图像分类受到微弱的类间差异和显著的类内变化的阻碍，这限制了现有对比学习方法的有效性。基于 InfoNCE 损失的监督对比方法存在负样本稀释问题，并且缺乏自适应决策边界，从而降低了细粒度识别任务中的判别能力。为解决这些问题，我们提出了基于 Sigmoid 的共性与风格监督对比学习方法（SCS-SupCon）。该框架引入了一种基于 Sigmoid 的成对对比损失，其中包含可学习的温度和偏置参数，以实现自适应决策边界。该设计强调难负样本，缓解负样本稀释，并更有效地利用监督信息。此外，一个显式的风格距离约束进一步解耦了风格与内容表征，从而实现更鲁棒的特征学习。在包括 CUB200-2011 和 Stanford Dogs 在内的六个基准数据集上的综合实验表明，SCS-SupCon 在 CNN 和 Transformer 主干网络上均取得了最先进的性能。在使用 ResNet-50 的 CIFAR-100 数据集上，五折交叉验证下 SCS-SupCon 的 top-1 准确率比 SupCon 提高了约 3.9 个百分点，比 CS-SupCon 提高了约 1.7 个百分点。在细粒度数据集上，其性能优于 CS-SupCon 0.4 至 3.0 个百分点。广泛的消融研究和统计分析进一步证实了所提框架的鲁棒性和泛化能力，Friedman 检验和 Nemenyi 事后评估验证了所观察到的性能提升的稳定性。

</details>


### [6] [A Modular Framework for Single-View 3D Reconstruction of Indoor Environments](https://arxiv.org/abs/2512.17955)
*Yuxiao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的模块化单视角室内场景3D重建框架，通过分步完成遮挡物体补全、房间布局修复、深度估计和实例对齐，显著提升了重建质量和视觉效果。


<details>
  <summary>Details</summary>
Motivation: 传统单视角室内3D重建方法难以处理复杂物体形状与遮挡问题，直接从不完整2D图像预测3D形状常导致重建质量受限。

Method: 将重建过程分为两步：首先利用基于扩散的技术预测房间背景和被遮挡物体的完整视图，再将其转换为3D；具体包括遮挡实例的非模态补全模块、专用于房间布局预测的图像修复模型、兼顾几何精度与细节表达的混合深度估计方法，以及融合2D/3D线索的视图空间对齐策略。

Result: 在3D-Front数据集上的大量实验表明，该方法在视觉质量和重建精度上均优于当前最先进的方法。

Conclusion: 所提出的模块化框架能有效从单张图像重建前景物体和房间背景，在室内设计、房地产和增强现实等领域具有广阔应用前景。

Abstract: We propose a modular framework for single-view indoor scene 3D reconstruction, where several core modules are powered by diffusion techniques. Traditional approaches for this task often struggle with the complex instance shapes and occlusions inherent in indoor environments. They frequently overshoot by attempting to predict 3D shapes directly from incomplete 2D images, which results in limited reconstruction quality. We aim to overcome this limitation by splitting the process into two steps: first, we employ diffusion-based techniques to predict the complete views of the room background and occluded indoor instances, then transform them into 3D. Our modular framework makes contributions to this field through the following components: an amodal completion module for restoring the full view of occluded instances, an inpainting model specifically trained to predict room layouts, a hybrid depth estimation technique that balances overall geometric accuracy with fine detail expressiveness, and a view-space alignment method that exploits both 2D and 3D cues to ensure precise placement of instances within the scene. This approach effectively reconstructs both foreground instances and the room background from a single image. Extensive experiments on the 3D-Front dataset demonstrate that our method outperforms current state-of-the-art (SOTA) approaches in terms of both visual quality and reconstruction accuracy. The framework holds promising potential for applications in interior design, real estate, and augmented reality.

Abstract (中文翻译): 我们提出了一种用于单视角室内场景3D重建的模块化框架，其中多个核心模块由扩散技术驱动。传统的此类任务方法通常难以应对室内环境中固有的复杂实例形状和遮挡问题，往往试图直接从不完整的2D图像预测3D形状，从而导致重建质量受限。为克服这一局限，我们将重建过程分为两个步骤：首先利用基于扩散的技术预测房间背景和被遮挡室内物体的完整视图，然后将其转换为3D形式。我们的模块化框架包含以下关键组件：用于恢复被遮挡物体完整视图的非模态补全模块、专门训练用于预测房间布局的图像修复模型、在整体几何精度与细节表现力之间取得平衡的混合深度估计技术，以及利用2D和3D线索确保场景中物体精确放置的视图空间对齐方法。该方法能够有效地从单张图像中重建前景物体和房间背景。在3D-Front数据集上的大量实验表明，我们的方法在视觉质量和重建精度方面均优于当前最先进的方法。该框架在室内设计、房地产和增强现实等应用领域展现出良好的潜力。

</details>


### [7] [Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization](https://arxiv.org/abs/2512.17987)
*Omar Faruq Shikdar,Fahad Ahammed,B. M. Shahria Alam,Golam Kibria,Tawhidur Rahman,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动化系统，用于识别七类茶树叶病害。通过构建包含5278张图像的新数据集，并结合DenseNet、Inception和EfficientNet模型以及注意力机制，所提出的集成模型达到了85.68%的准确率，并引入可解释AI以增强模型透明度。


<details>
  <summary>Details</summary>
Motivation: 茶是全球消费最广泛的饮品之一，茶产业对许多国家至关重要。然而，茶叶病害若未能及时控制，将导致农民遭受巨大经济损失。传统人工识别方法效率低且不可靠，因此亟需一种高效自动化的病害识别方案。

Method: 研究构建了一个包含5278张图像、涵盖七类病害的新数据集，并进行了预处理。采用三种预训练模型（DenseNet、Inception 和 EfficientNet），其中 EfficientNet 仅用于集成模型；同时引入两种注意力模块以提升性能，并使用可解释AI技术增强模型可解释性。

Result: 所提出的集成模型在茶树叶病害分类任务中取得了85.68%的最高准确率。

Conclusion: 该研究成功开发了一种高效的自动化茶树叶病害识别系统，具备良好的准确率与可解释性，有助于农民及时采取措施减少损失。

Abstract: Tea is among the most widely consumed drinks globally. Tea production is a key industry for many countries. One of the main challenges in tea harvesting is tea leaf diseases. If the spread of tea leaf diseases is not stopped in time, it can lead to massive economic losses for farmers. Therefore, it is crucial to identify tea leaf diseases as soon as possible. Manually identifying tea leaf disease is an ineffective and time-consuming method, without any guarantee of success. Automating this process will improve both the efficiency and the success rate of identifying tea leaf diseases. The purpose of this study is to create an automated system that can classify different kinds of tea leaf diseases, allowing farmers to take action to minimize the damage. A novel dataset was developed specifically for this study. The dataset contains 5278 images across seven classes. The dataset was pre-processed prior to training the model. We deployed three pretrained models: DenseNet, Inception, and EfficientNet. EfficientNet was used only in the ensemble model. We utilized two different attention modules to improve model performance. The ensemble model achieved the highest accuracy of 85.68%. Explainable AI was introduced for better model interpretability.

Abstract (中文翻译): 茶是全球消费最广泛的饮品之一，茶叶生产是许多国家的重要产业。茶叶采收过程中面临的主要挑战之一是茶树叶病害。如果不能及时遏制病害的传播，将给农民带来巨大的经济损失。因此，尽早识别茶树叶病害至关重要。人工识别方法效率低下、耗时且无法保证效果，而自动化识别则能显著提升效率和成功率。本研究旨在构建一个能够自动分类多种茶树叶病害的系统，帮助农民及时采取措施以减少损失。为此，研究专门构建了一个包含5278张图像、涵盖七类病害的新数据集，并在模型训练前对数据进行了预处理。研究部署了三种预训练模型：DenseNet、Inception 和 EfficientNet（后者仅用于集成模型），并引入两种注意力模块以提升模型性能。最终，集成模型取得了85.68%的最高准确率。此外，研究还引入了可解释人工智能（Explainable AI）以增强模型的可解释性。

</details>


### [8] [Name That Part: 3D Part Segmentation and Naming](https://arxiv.org/abs/2512.18003)
*Soumava Paul,Prakhar Kaushik,Ankit Vaidya,Anand Bhattad,Alan Yuille*

Main category: cs.CV

TL;DR: 本文提出ALIGN-Parts方法，将语义3D部件分割建模为集合对齐任务，通过融合几何、外观和语言模型生成的语义信息，实现一次性（one-shot）的3D部件分割与命名，并构建了统一的跨数据集部件本体。


<details>
  <summary>Details</summary>
Motivation: 现有3D部件分割数据集的标注定义不一致，限制了模型的鲁棒训练；以往方法要么产生无标签分解，要么仅检索单个部件而缺乏完整形状标注。

Method: 提出ALIGN-Parts方法，将部件命名视为集合对齐任务：将形状分解为隐式的“partlets”，并通过二分图匹配将其与部件描述对齐；融合3D部件场的几何线索、多视角视觉特征的外观信息以及语言模型生成的功能性语义描述；采用文本对齐损失使partlets与文本共享嵌入空间，支持开放词汇匹配。

Result: 该方法可实现一次性3D部件分割与命名，在多个下游任务中有效，包括作为可扩展的标注引擎；通过零样本匹配任意描述及置信度校准预测，结合人工验证，构建了涵盖PartNet、3DCoMPaT++和Find3D的统一本体（含1,794个唯一3D部件）；并发布了新数据集Tex-Parts；还提出了两项适用于命名3D部件分割的新评估指标。

Conclusion: ALIGN-Parts提供了一种高效、新颖的一次性3D部件分割与命名框架，支持开放词汇、零样本匹配和跨数据集对齐，为3D语义理解与标注提供了可扩展的解决方案。

Abstract: We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.

Abstract (中文翻译): 我们研究语义3D部件分割问题，即把物体分解为具有有意义名称的部件。尽管已有带部件标注的数据集，但其定义在不同数据集中不一致，限制了模型的鲁棒训练。以往方法要么生成无标签的分解结果，要么仅检索单个部件而缺乏完整形状的标注。我们提出ALIGN-Parts方法，将部件命名建模为直接的集合对齐任务。该方法将形状分解为“partlets”——一种隐式的3D部件表示，并通过二分图匹配将其与部件描述对齐。我们融合了来自3D部件场的几何线索、多视角视觉特征的外观信息，以及由语言模型生成的功能性语义描述。文本对齐损失确保partlets与文本共享同一嵌入空间，从而在数据充足的前提下支持理论上开放词汇的匹配设置。我们提出的高效且新颖的一次性3D部件分割与命名方法可应用于多种下游任务，包括作为可扩展的标注引擎。由于模型支持对任意描述的零样本匹配，以及对已知类别的置信度校准预测，结合人工验证，我们构建了一个统一的本体，对齐了PartNet、3DCoMPaT++和Find3D三个数据集，共包含1,794个唯一的3D部件。我们还展示了新构建的Tex-Parts数据集中的示例，并引入了两项适用于命名3D部件分割任务的新评估指标。

</details>


### [9] [Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models](https://arxiv.org/abs/2512.18004)
*Shubham Kumar Nigam,Parjanya Aditya Shukla,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CV

TL;DR: 本文比较了传统OCR+机器翻译两阶段方法与端到端视觉大语言模型在马拉地语手写法律文档翻译任务上的性能，旨在为资源稀缺环境提供高效、可部署的法律文本数字化解决方案。


<details>
  <summary>Details</summary>
Motivation: 印度地方法院和高等法院中存在大量马拉地语手写法律文件（如FIR、起诉书和证人陈述），亟需可扩展且准确的翻译系统进行数字化，以提升非母语人士和法律从业者对法律信息的获取能力。

Method: 对比传统OCR-MT两阶段流程与端到端视觉大语言模型（VLLM）在手写文本图像直接翻译任务中的表现，并在自建的马拉地语手写法律文档数据集上进行评估。

Result: 实验结果提供了关于两种方法在低资源场景下性能差异的可操作性见解，有助于构建鲁棒、可在边缘设备部署的解决方案。

Conclusion: 端到端视觉大语言模型在特定条件下具有替代传统OCR+MT流程的潜力，为低资源语言的手写法律文档自动化处理提供了可行路径。

Abstract: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.

Abstract (中文翻译): 手写文本识别（HTR）和机器翻译仍然是重大挑战，尤其对于马拉地语等低资源语言而言，这类语言缺乏大规模数字化语料库，且手写风格高度多样化。传统方法通常采用两阶段流水线：首先通过OCR系统从手写图像中提取文本，再利用机器翻译模型将其翻译为目标语言。本文探索并比较了传统OCR-MT流水线与试图将这两个阶段统一起来、直接以端到端方式翻译手写文本图像的视觉大语言模型（Vision Large Language Models）的性能。我们的研究动机源于迫切需要可扩展且准确的翻译系统，以实现印度地方法院和高等法院中诸如FIR（第一信息报告）、起诉书和证人陈述等法律记录的数字化。我们在一个精心整理的马拉地语手写法律文档数据集上评估了上述两种方法，目标是在低资源环境下实现高效的法律文档处理。我们的研究结果为构建鲁棒、可在边缘设备部署的解决方案提供了可操作的见解，从而提升非母语使用者和法律专业人士获取法律信息的能力。

</details>


### [10] [NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging](https://arxiv.org/abs/2512.18038)
*Fakrul Islam Tushar,Ehsan Samei,Cynthia Rudin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: NodMAISI 是一个面向肺结节的 CT 合成与增强框架，通过解剖约束和病灶感知增强，在多个公开数据集上显著提升了合成图像质量、小结节可检测性及下游恶性分类性能，尤其在临床数据稀缺时效果突出。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据虽日益丰富，但对肺癌筛查至关重要的异常发现（尤其是小肺结节）仍存在代表性不足和标注不一致的问题。

Method: 提出 NodMAISI 框架，整合：(i) 标准化标注流程（含器官掩膜与结节级标注），(ii) 基于 MAISI-v2 并引入 ControlNet 条件的 rectified-flow 生成器以保证解剖与病灶一致性，(iii) 病灶感知增强策略（可控缩小结节掩膜同时保留周围解剖结构）生成配对 CT 变体。

Result: 在六个公开测试集上，NodMAISI 相较 MAISI-v2 显著提升分布保真度（FID 更低）；使用 MONAI 结节检测器评估时，其合成图像的平均敏感性更高，尤其对亚厘米结节；在 LUNA25 上训练并在多个外部数据集上评估的恶性分类任务中，NodMAISI 增强在仅用 10–20% 临床数据时 AUC 提升 0.07–0.21，有效缓解数据稀缺问题。

Conclusion: NodMAISI 能有效生成高质量、解剖一致且病灶真实的肺部 CT 图像，在提升小结节检测与分类性能方面具有显著优势，尤其适用于标注数据有限的临床场景。

Abstract: Objective: Although medical imaging datasets are increasingly available, abnormal and annotation-intensive findings critical to lung cancer screening, particularly small pulmonary nodules, remain underrepresented and inconsistently curated. Methods: We introduce NodMAISI, an anatomically constrained, nodule-oriented CT synthesis and augmentation framework trained on a unified multi-source cohort (7,042 patients, 8,841 CTs, 14,444 nodules). The framework integrates: (i) a standardized curation and annotation pipeline linking each CT with organ masks and nodule-level annotations, (ii) a ControlNet-conditioned rectified-flow generator built on MAISI-v2's foundational blocks to enforce anatomy- and lesion-consistent synthesis, and (iii) lesion-aware augmentation that perturbs nodule masks (controlled shrinkage) while preserving surrounding anatomy to generate paired CT variants. Results: Across six public test datasets, NodMAISI improved distributional fidelity relative to MAISI-v2 (real-to-synthetic FID range 1.18 to 2.99 vs 1.69 to 5.21). In lesion detectability analysis using a MONAI nodule detector, NodMAISI substantially increased average sensitivity and more closely matched clinical scans (IMD-CT: 0.69 vs 0.39; DLCS24: 0.63 vs 0.20), with the largest gains for sub-centimeter nodules where MAISI-v2 frequently failed to reproduce the conditioned lesion. In downstream nodule-level malignancy classification trained on LUNA25 and externally evaluated on LUNA16, LNDbv4, and DLCS24, NodMAISI augmentation improved AUC by 0.07 to 0.11 at <=20% clinical data and by 0.12 to 0.21 at 10%, consistently narrowing the performance gap under data scarcity.

Abstract (中文翻译): 目的：尽管医学影像数据集日益普及，但对肺癌筛查至关重要的异常发现（尤其是小型肺结节）仍然代表性不足，且标注不一致。方法：我们提出了 NodMAISI——一种受解剖结构约束、面向结节的 CT 合成与增强框架，该框架基于一个统一的多源队列（7,042 名患者、8,841 次 CT 扫描、14,444 个结节）进行训练。该框架整合了：(i) 一套标准化的数据整理与标注流程，将每例 CT 与器官掩膜及结节级标注相关联；(ii) 一个基于 MAISI-v2 基础模块并采用 ControlNet 条件控制的 rectified-flow 生成器，以确保合成图像在解剖结构和病灶特征上的一致性；(iii) 一种病灶感知增强策略，在保留周围解剖结构的同时对结节掩膜进行扰动（可控收缩），从而生成配对的 CT 变体。结果：在六个公开测试数据集上，NodMAISI 相较于 MAISI-v2 显著提升了分布保真度（真实与合成图像间的 FID 范围为 1.18 至 2.99，而 MAISI-v2 为 1.69 至 5.21）。在使用 MONAI 结节检测器进行的病灶可检测性分析中，NodMAISI 显著提高了平均敏感性，并更接近临床扫描结果（IMD-CT：0.69 对比 0.39；DLCS24：0.63 对比 0.20），其中对亚厘米结节的提升最为显著，因为 MAISI-v2 经常无法复现所设定的病灶条件。在下游结节级别恶性分类任务中，以 LUNA25 训练并在 LUNA16、LNDbv4 和 DLCS24 上进行外部评估时，NodMAISI 增强在仅使用 ≤20% 临床数据时使 AUC 提高 0.07 至 0.11，在仅使用 10% 数据时提高 0.12 至 0.21，始终有效缩小了数据稀缺情况下的性能差距。

</details>
