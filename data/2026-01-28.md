<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dynamic Mask-Based Backdoor Attack Against Vision AI Models: A Case Study on Mushroom Detection](https://arxiv.org/abs/2601.18845)
*Zeineb Dridi,Jihen Bennaceur,Amine Ben Hassouna*

Main category: cs.CV

TL;DR: 本文提出了一种基于动态掩码的新型后门攻击方法，专门针对目标检测模型。通过数据集投毒并利用SAM模型生成动态触发器掩码，在保持干净样本高准确率的同时，实现了对中毒样本的高攻击成功率，超越了传统静态触发器方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在计算机视觉领域广泛应用的同时，也面临着各种对抗性攻击的威胁，尤其是后门攻击。现有研究多集中于静态触发器，缺乏对现实场景中（如数据外包）复杂攻击的探讨。因此，有必要设计一种更隐蔽、更有效的后门攻击方法，以揭示当前模型在实际应用中的脆弱性，并强调开发鲁棒防御措施的紧迫性。

Method: 该方法采用数据集投毒策略，利用强大的图像分割模型SAM为恶意触发器生成动态掩码，并将其嵌入到训练数据集中（以蘑菇检测数据集为例）。这种动态掩码使得触发器能够根据目标对象的位置和形状自适应地放置，从而实现一种新颖且隐蔽的后门攻击。

Result: 在YOLOv7目标检测模型上的大量实验表明，被攻击的模型在干净数据上仍能保持高准确率，同时在带有动态触发器的中毒样本上达到了很高的攻击成功率。该方法在效果上优于基于静态、固定模式的传统后门注入方法。

Conclusion: 本研究通过构建一个详细的、基于现实场景（如数据外包）的后门攻击案例，成功展示了一种利用动态掩码的新型高效攻击方法。结果凸显了深度学习模型，特别是目标检测模型，在面对此类不断演变的对抗性威胁时的脆弱性，迫切需要开发更强大的防御对策。

Abstract: Deep learning has revolutionized numerous tasks within the computer vision field, including image classification, image segmentation, and object detection. However, the increasing deployment of deep learning models has exposed them to various adversarial attacks, including backdoor attacks. This paper presents a novel dynamic mask-based backdoor attack method, specifically designed for object detection models. We exploit a dataset poisoning technique to embed a malicious trigger, rendering any models trained on this compromised dataset vulnerable to our backdoor attack. We particularly focus on a mushroom detection dataset to demonstrate the practical risks posed by such attacks on critical real-life domains. Our work also emphasizes the importance of creating a detailed backdoor attack scenario to illustrate the significant risks associated with the outsourcing practice. Our approach leverages SAM, a recent and powerful image segmentation AI model, to create masks for dynamic trigger placement, introducing a new and stealthy attack method. Through extensive experimentation, we show that our sophisticated attack scenario maintains high accuracy on clean data with the YOLOv7 object detection model while achieving high attack success rates on poisoned samples. Our approach surpasses traditional methods for backdoor injection, which are based on static and consistent patterns. Our findings underscore the urgent need for robust countermeasures to protect deep learning models from these evolving adversarial threats.

Abstract (中文翻译): 深度学习已经彻底改变了计算机视觉领域的众多任务，包括图像分类、图像分割和目标检测。然而，深度学习模型日益广泛的部署使其面临各种对抗性攻击，包括后门攻击。本文提出了一种新颖的基于动态掩码的后门攻击方法，专门针对目标检测模型。我们利用数据集投毒技术嵌入一个恶意触发器，使得任何在此被污染的数据集上训练的模型都容易受到我们的后门攻击。我们特别聚焦于一个蘑菇检测数据集，以展示此类攻击对关键现实领域所构成的实际风险。我们的工作还强调了构建详细后门攻击场景的重要性，以阐明与外包实践相关的重大风险。我们的方法利用了SAM（一种近期出现且功能强大的图像分割AI模型）来为动态触发器放置创建掩码，引入了一种新颖且隐蔽的攻击方法。通过大量实验，我们证明了我们精心设计的攻击场景在YOLOv7目标检测模型上对干净数据保持了高准确率，同时在中毒样本上实现了高攻击成功率。我们的方法超越了基于静态和一致模式的传统后门注入方法。我们的研究结果强调了迫切需要强有力的对策来保护深度学习模型免受这些不断演变的对抗性威胁。

</details>


### [2] [Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding](https://arxiv.org/abs/2601.18849)
*Yuhui Zhang,Hui Yu,Wei Liang,Sunjie Zhang*

Main category: cs.CV

TL;DR: 本文提出一种结合眨眼嵌入与哈希网格关键点编码的动态NeRF方法，显著提升说话人像嘴部动作的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有动态NeRF在生成高保真说话人像时，仍难以准确高效地捕捉嘴部运动细节。

Method: 利用面部特征作为条件特征，通过Dynamic Landmark Transformer将音频特征作为残差项融入模型，并采用眨眼嵌入和哈希网格关键点编码；整体使用神经辐射场建模人脸。

Result: 实验表明，该方法在嘴部动作真实感方面优于现有方法。

Conclusion: 所提方法有效提升了动态说话人像的嘴部细节表现力和整体逼真度。

Abstract: Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.

Abstract (中文翻译): 动态神经辐射场（NeRF）在生成高保真说话人像三维模型方面已取得显著成果。尽管渲染速度与生成质量已有大幅提升，但在准确且高效地捕捉说话人像嘴部运动方面仍存在挑战。为解决这一问题，本文提出一种基于眨眼嵌入与哈希网格关键点编码的自动方法，可显著增强说话人像的真实感。具体而言，我们将面部特征编码为条件特征，并通过动态关键点Transformer将音频特征作为残差项融入模型。此外，我们采用神经辐射场对整张人脸进行建模，从而实现逼真的人脸表示。实验评估验证了本方法相较于现有方法的优越性。

</details>


### [3] [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/abs/2601.18851)
*Wei Liang,Hui Yu,Derui Ding,Rachael E. Jack,Philippe G. Schyns*

Main category: cs.CV

TL;DR: 本文提出一种仅使用单张自拍视频即可生成高保真头部虚拟形象的方法，结合3DMM与StyleGAN，在保留皱纹、发丝等高频细节的同时实现高质量的头部重演。


<details>
  <summary>Details</summary>
Motivation: 现有方法在头部虚拟形象重建中存在两大问题：一是基于3DMM的方法难以实时捕捉包括非面部区域和背景在内的完整头部；二是基于GAN的方法虽能生成高质量重演结果，但难以还原皱纹、发丝等精细细节。此外，大多数方法依赖大量训练数据，缺乏仅用一段自拍视频实现重演的能力。

Method: 该方法结合3D Morphable Model（3DMM）与StyleGAN生成器，提出一个细节重建模型，在对抗训练中引入混合损失函数，用于前景重建与虚拟形象图像生成，以恢复高频细节。

Result: 在自重演和跨重演任务上的定性与定量评估表明，所提方法在纹理丰富度和细节还原方面优于现有方法，能够生成更逼真的头部虚拟形象。

Conclusion: 本研究通过融合3DMM与StyleGAN，并设计针对性的损失函数，成功实现了仅用单段自拍视频即可生成包含精细细节的高质量头部虚拟形象，为低数据依赖的虚拟人技术提供了有效解决方案。

Abstract: Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.

Abstract (中文翻译): 头部虚拟形象重演旨在从单目视频中创建可动画的个性化虚拟形象，是社交信号理解、游戏、人机交互和计算机视觉等应用的基础。近年来，基于3D可变形模型（3DMM）的面部重建方法在高保真人脸估计方面取得了显著成果。然而，一方面，这些方法难以实时捕捉包括非面部区域和背景在内的完整头部，而这是生成逼真高保真头部虚拟形象的关键；另一方面，近期利用生成对抗网络（GAN）从视频中生成头部虚拟形象的方法虽能实现高质量重演，但在再现皱纹和发丝纹理等细粒度头部细节方面仍存在局限。此外，现有方法通常依赖大量训练数据，很少关注仅使用一段简单的自拍视频即可实现虚拟形象重演。为解决上述挑战，本研究提出一种仅使用自拍视频即可实现细节丰富的头部虚拟形象重演的方法。该方法结合3DMM与基于StyleGAN的生成器，提出一个细节重建模型，在对抗训练过程中引入用于前景重建和虚拟形象图像生成的混合损失函数，以恢复高频细节。在自重演和跨重演任务上的定性与定量评估表明，所提方法相比现有方法能够生成纹理更丰富、细节更精细的头部虚拟形象。

</details>


### [4] [Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)](https://arxiv.org/abs/2601.18891)
*Ghazaleh Serati,Samuel Foucher,Jerome Theau*

Main category: cs.CV

TL;DR: 本文提出一种基于弱监督的斑块级预训练方法，提升HerdNet在复杂北极航拍图像中对北美驯鹿的检测鲁棒性，在有限标注数据下取得优于ImageNet初始化的性能。


<details>
  <summary>Details</summary>
Motivation: 近几十年来北极驯鹿数量下降，亟需可扩展且准确的监测方法以支持保护决策；然而自动检测面临背景复杂、目标稀疏、遮挡严重和尺度变化大等挑战。

Method: 提出一种弱监督斑块级预训练策略，利用“空”与“非空”粗粒度标签对HerdNet检测网络进行预训练，再迁移到精细检测任务中。

Result: 该方法在2017年多群测试集和2019年独立测试集上分别达到93.7%和92.6%的F1分数；相比ImageNet初始化，在正样本斑块和全图计数任务上均取得一致提升。

Conclusion: 在标注数据有限的情况下，基于粗标签的弱监督预训练能有效提升检测性能，接近甚至优于通用权重初始化，但仍受限于背景干扰和低密度遮挡导致的误检与漏检。

Abstract: Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.

Abstract (中文翻译): 近几十年来，整个北极地区的北美驯鹿数量持续下降，促使人们开发可扩展且准确的监测方法，以指导基于证据的保护行动和政策制定。然而，从航拍图像中进行人工判读既费时又容易出错，凸显了在不同场景下实现自动且可靠检测的必要性。但此类自动检测极具挑战性，原因包括严重的背景异质性、大量空旷区域（类别不平衡）、目标尺寸小或被遮挡，以及种群密度和尺度的巨大差异。为增强检测模型（HerdNet）对上述挑战的鲁棒性，本文提出一种基于检测网络架构的弱监督斑块级预训练方法。检测数据集涵盖分布在阿拉斯加的五个驯鹿群。通过学习该数据集中“空”与“非空”的粗粒度标签，该方法在HerdNet正式训练前获得初步的弱监督知识，从而提升检测性能，优于使用通用权重初始化的HerdNet。实验表明，基于斑块的预训练网络在多群落图像（2017年）及其独立年份（2019年）测试集上分别取得了93.7%和92.6%的F1分数，能够可靠地识别包含动物的区域，便于在大范围航拍图像上进行人工计数。将该预训练权重迁移至检测任务后，在正样本斑块（F1：92.6%/93.5% vs. 89.3%/88.6%）和全图计数（F1：95.5%/93.3% vs. 91.5%/90.4%）方面均持续优于ImageNet初始化权重。当前方法的主要局限在于动物状背景杂波引起的假阳性，以及低密度遮挡导致的假阴性。总体而言，在检测任务前利用粗标签进行预训练，即使在标注数据有限的情况下，也能依赖弱监督预训练权重，取得与通用权重初始化相当甚至更优的结果。

</details>


### [5] [RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection](https://arxiv.org/abs/2601.18900)
*Haim Zisman,Uri Shaham*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、基于统计的通用框架，通过融合多个检测器的统计量并计算p值，实现对AI生成图像的可解释性检测。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法往往缺乏形式上的可解释性，并依赖于对伪造内容的隐含假设，在分布偏移下鲁棒性受限。

Method: 结合多个无需训练的检测统计量，计算其p值，并利用经典统计集成方法聚合这些p值，以评估测试图像与真实图像分布的一致性。

Result: 所提框架具有通用性、灵活性和无需训练的特点，适用于多种动态变化场景下的鲁棒伪造图像检测。

Conclusion: 该方法提供了一种统计上严谨且可解释的概率评分机制，有效提升了伪造图像检测在分布变化下的鲁棒性。

Abstract: As generative models continue to evolve, detecting AI-generated images remains a critical challenge. While effective detection methods exist, they often lack formal interpretability and may rely on implicit assumptions about fake content, potentially limiting robustness to distributional shifts. In this work, we introduce a rigorous, statistically grounded framework for fake image detection that focuses on producing a probability score interpretable with respect to the real-image population. Our method leverages the strengths of multiple existing detectors by combining training-free statistics. We compute p-values over a range of test statistics and aggregate them using classical statistical ensembling to assess alignment with the unified real-image distribution. This framework is generic, flexible, and training-free, making it well-suited for robust fake image detection across diverse and evolving settings.

Abstract (中文翻译): 随着生成模型不断发展，检测AI生成图像仍然是一个关键挑战。尽管已有有效的检测方法，但它们通常缺乏形式上的可解释性，并可能依赖于对伪造内容的隐含假设，从而在分布偏移情况下的鲁棒性受限。在本研究中，我们提出了一种严格且基于统计的伪造图像检测框架，该框架聚焦于生成一个相对于真实图像总体具有可解释性的概率评分。我们的方法通过融合多个现有检测器的优势，结合无需训练的统计量，计算一系列检验统计量的p值，并利用经典统计集成方法将这些p值聚合，以评估测试图像与统一的真实图像分布的一致性。该框架具有通用性、灵活性且无需训练，非常适合在多样且不断演化的场景中进行鲁棒的伪造图像检测。

</details>


### [6] [On the Role of Depth in Surgical Vision Foundation Models: An Empirical Study of RGB-D Pre-training](https://arxiv.org/abs/2601.18929)
*John J. Han,Adam Schmidt,Muhammad Abdullah Jamal,Chinedu Nwoye,Anita Rau,Jie Ying Wu,Omid Mohareri*

Main category: cs.CV

TL;DR: 在手术视觉任务中，引入深度信息进行多模态预训练的ViT模型（如MultiMAE）显著优于仅使用RGB的单模态模型，且在仅用25%标注数据微调时即可超越全量数据训练的RGB模型，而推理阶段无需任何改动。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景理解方法主要依赖单模态RGB预训练，忽略了手术环境中固有的复杂3D几何结构；尽管通用视觉领域已有支持多模态或几何感知的架构，但深度信息在手术场景中的价值尚未充分探索。

Method: 对8种基于ViT的视觉基础模型（VFM）进行大规模实证研究，这些模型在预训练领域、学习目标和输入模态（RGB vs. RGB-D）上有所不同；使用140万张机器人手术图像及其对应深度图（由现成网络生成）进行预训练，并在8个手术数据集上评估冻结主干和端到端微调两种设置下的性能。

Result: 引入显式几何标记（如MultiMAE）的模型在所有任务（目标检测、分割、深度估计、姿态估计）上均显著优于单模态基线；几何感知预训练带来显著的数据效率提升，仅用25%标注数据微调的模型即可超越使用全量数据训练的RGB模型；且推理阶段无需深度输入或架构修改。

Conclusion: 多模态（RGB-D）预训练是构建更强大手术视觉系统的一条可行路径，能有效提升模型性能与数据效率，且易于部署。

Abstract: Vision foundation models (VFMs) have emerged as powerful tools for surgical scene understanding. However, current approaches predominantly rely on unimodal RGB pre-training, overlooking the complex 3D geometry inherent to surgical environments. Although several architectures support multimodal or geometry-aware inputs in general computer vision, the benefits of incorporating depth information in surgical settings remain underexplored. We conduct a large-scale empirical study comparing eight ViT-based VFMs that differ in pre-training domain, learning objective, and input modality (RGB vs. RGB-D). For pre-training, we use a curated dataset of 1.4 million robotic surgical images paired with depth maps generated from an off-the-shelf network. We evaluate these models under both frozen-backbone and end-to-end fine-tuning protocols across eight surgical datasets spanning object detection, segmentation, depth estimation, and pose estimation. Our experiments yield several consistent findings. Models incorporating explicit geometric tokenization, such as MultiMAE, substantially outperform unimodal baselines across all tasks. Notably, geometric-aware pre-training enables remarkable data efficiency: models fine-tuned on just 25% of labeled data consistently surpass RGB-only models trained on the full dataset. Importantly, these gains require no architectural or runtime changes at inference; depth is used only during pre-training, making adoption straightforward. These findings suggest that multimodal pre-training offers a viable path towards building more capable surgical vision systems.

Abstract (中文翻译): 视觉基础模型（VFMs）已成为手术场景理解的强大工具。然而，当前方法主要依赖单模态RGB预训练，忽视了手术环境中固有的复杂3D几何结构。尽管通用计算机视觉领域已有若干支持多模态或几何感知输入的架构，但在手术场景中引入深度信息的优势仍缺乏充分探索。我们开展了一项大规模实证研究，比较了八种基于ViT的视觉基础模型，这些模型在预训练领域、学习目标和输入模态（RGB vs. RGB-D）方面各不相同。预训练使用了一个精心整理的数据集，包含140万张机器人手术图像及其对应的深度图（由现成网络生成）。我们在八个手术数据集上评估了这些模型在冻结主干和端到端微调两种协议下的表现，涵盖目标检测、分割、深度估计和姿态估计等任务。实验得出若干一致结论：采用显式几何标记（如MultiMAE）的模型在所有任务上均显著优于单模态基线。值得注意的是，几何感知预训练带来了显著的数据效率：仅使用25%标注数据微调的模型，其性能始终优于使用完整数据集训练的纯RGB模型。重要的是，这些性能提升在推理阶段无需任何架构或运行时改动——深度信息仅用于预训练阶段，因此易于部署。这些发现表明，多模态预训练是构建更强大手术视觉系统的一条可行路径。

</details>


### [7] [Smart Split-Federated Learning over Noisy Channels for Embryo Image Segmentation](https://arxiv.org/abs/2601.18948)
*Zahra Hafezi Kafshgari,Ivan V. Bajic,Parvaneh Saeedi*

Main category: cs.CV

TL;DR: 本文提出一种智能平均策略，显著提升SplitFed学习在通信信道噪声下的鲁棒性，可在保持模型精度的同时容忍强两个数量级的噪声。


<details>
  <summary>Details</summary>
Motivation: SplitFed学习虽降低了客户端计算要求，但在通信过程中传输特征值、梯度和模型更新易受信道噪声影响，从而损害模型性能。因此，亟需提升其对通信噪声的鲁棒性。

Method: 提出一种智能平均策略，用于SplitFed学习框架中，以增强对通信信道噪声的容忍能力。

Result: 在胚胎图像分割模型上的实验表明，所提策略相比传统平均方法，能在保持最终模型精度的同时，容忍强两个数量级的通信噪声。

Conclusion: 智能平均策略有效提升了SplitFed学习在噪声通信环境下的稳定性与实用性，为资源受限设备上的联邦学习提供了更可靠的解决方案。

Abstract: Split-Federated (SplitFed) learning is an extension of federated learning that places minimal requirements on the clients computing infrastructure, since only a small portion of the overall model is deployed on the clients hardware. In SplitFed learning, feature values, gradient updates, and model updates are transferred across communication channels. In this paper, we study the effects of noise in the communication channels on the learning process and the quality of the final model. We propose a smart averaging strategy for SplitFed learning with the goal of improving resilience against channel noise. Experiments on a segmentation model for embryo images shows that the proposed smart averaging strategy is able to tolerate two orders of magnitude stronger noise in the communication channels compared to conventional averaging, while still maintaining the accuracy of the final model.

Abstract (中文翻译): 拆分联邦（SplitFed）学习是联邦学习的一种扩展形式，它对客户端计算基础设施的要求极低，因为整体模型中只有一小部分部署在客户端硬件上。在SplitFed学习中，特征值、梯度更新和模型更新通过通信信道进行传输。本文研究了通信信道中的噪声对学习过程及最终模型质量的影响，并提出了一种用于SplitFed学习的智能平均策略，旨在提高其对信道噪声的鲁棒性。在胚胎图像分割模型上的实验表明，与传统平均方法相比，所提出的智能平均策略能够在保持最终模型精度的同时，容忍强两个数量级的通信信道噪声。

</details>


### [8] [Pay Attention to Where You Look](https://arxiv.org/abs/2601.18970)
*Alex Beriand,JhihYang Wu,Daniel Brignac,Natnael Daba,Abhijit Mahalanobis*

Main category: cs.CV

TL;DR: 本文提出一种相机加权机制，通过根据源视图与目标视图的相关性动态调整其重要性，从而提升少样本新视角合成（NVS）的质量。


<details>
  <summary>Details</summary>
Motivation: 现有少样本新视角合成方法通常假设所有输入视图对目标视图同等重要，忽略了不同视图与目标的相关性差异，导致合成结果次优。

Method: 提出两种视图加权方法：一是基于欧氏距离和角度差等几何属性的确定性加权方案；二是基于交叉注意力的学习型加权方案。该机制可集成到多种NVS算法中，并支持进一步微调以提升性能。

Result: 实验表明，所提出的自适应视图加权机制能显著提升新视角合成的准确性和真实感。

Conclusion: 引入相机加权机制是一种有效且通用的改进策略，为提升少样本新视角合成性能提供了新方向。

Abstract: Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.
  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.

Abstract (中文翻译): 新视角合成（NVS）随着生成建模的发展取得了显著进展，能够生成逼真的图像。在仅有少量输入视图的少样本NVS场景中，现有方法通常假设所有输入视图相对于目标视图具有同等重要性，从而导致次优结果。为解决这一局限性，我们引入了一种相机加权机制，根据源视图与目标视图的相关性动态调整其重要性。我们提出了两种方法：一种是利用欧氏距离和角度差等几何属性的确定性加权方案，另一种是基于交叉注意力机制的学习型加权方案。此外，模型还可结合我们的相机加权机制进行进一步训练，以更准确地理解视图相关性并提升合成质量。该机制具有良好的适应性，可集成到多种NVS算法中，从而增强其合成高质量新视角的能力。实验结果表明，自适应视图加权显著提升了合成结果的准确性与真实感，为改进NVS提供了一个有前景的方向。

</details>


### [9] [FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction](https://arxiv.org/abs/2601.18993)
*Wei Cao,Hao Zhang,Fengrui Tian,Yulun Wu,Yingying Li,Shenlong Wang,Ning Yu,Yaoyao Liu*

Main category: cs.CV

TL;DR: 本文提出FreeOrbit4D，一种无需训练的框架，通过构建几何完整的4D代理来解决单目视频大角度视角重定向中的几何模糊和时序不一致问题。


<details>
  <summary>Details</summary>
Motivation: 单目视频仅提供动态3D场景的有限时空视图，在大角度相机重定向任务中导致严重的几何模糊和时序不一致，现有基于扩散的方法在此类情况下表现不佳。

Method: FreeOrbit4D通过解耦前景与背景重建：将单目视频反投影为静态背景和几何不完整的前景点云；利用以对象为中心的多视图扩散模型在规范对象空间中合成多视图图像并重建几何完整的前景点云；再通过密集像素同步的3D-3D对应关系将规范前景点云对齐到全局场景空间，形成几何完整的4D代理，并以此引导条件视频扩散模型生成目标视角视频。

Result: 实验表明，FreeOrbit4D在具有挑战性的大角度轨迹下能生成更忠实的重定向视频，且其4D代理可支持编辑传播和4D数据生成等应用。

Conclusion: FreeOrbit4D有效缓解了单目视频大角度重定向中的几何模糊问题，提供了一种无需训练的实用解决方案，并为下游应用开辟了新可能。

Abstract: Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.

Abstract (中文翻译): 相机重定向旨在从单段单目视频中，根据用户指定的相机轨迹重放动态场景。然而，大角度重定向本质上是病态问题：单目视频仅捕获动态3D场景的狭窄时空视图，对底层4D世界的观测高度不完整。关键挑战在于从这种有限输入中恢复出完整且一致的表示，确保几何与运动的一致性。尽管近期基于扩散的方法取得了令人印象深刻的结果，但在远离原始轨迹的大角度视角变化下往往失效，因缺乏视觉依据而导致严重的几何模糊和时序不一致。为此，我们提出了FreeOrbit4D——一种无需训练的有效框架，通过恢复一个几何完整的4D代理作为视频生成的结构依据来解决该几何模糊问题。我们通过解耦前景与背景重建来获得该代理：将单目视频反投影到统一的全局空间中，得到静态背景和几何不完整的前景点云；然后利用以对象为中心的多视图扩散模型，在规范对象空间中合成多视图图像并重建几何完整的前景点云。通过密集的像素同步3D–3D对应关系将规范前景点云对齐至全局场景空间，并将几何完整的4D代理投影到目标相机视角，从而为条件视频扩散模型提供几何支架。大量实验表明，FreeOrbit4D在具有挑战性的大角度轨迹下能生成更忠实的重定向视频，且我们的几何完整4D代理为进一步的实际应用（如编辑传播和4D数据生成）开辟了潜在途径。项目页面和代码将很快发布。

</details>


### [10] [Anatomically-aware conformal prediction for medical image segmentation with random walks](https://arxiv.org/abs/2601.18997)
*Mélanie Gaillochet,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: 本文提出了一种名为随机游走保形预测（RW-CP）的新方法，通过在保形预测中引入基于预训练视觉基础模型特征构建的图结构和随机游走扩散机制，增强医学图像分割中不确定性量化结果的空间一致性和解剖合理性，在保持严格边缘覆盖率的同时显著提升分割质量。


<details>
  <summary>Details</summary>
Motivation: 标准保形预测（CP）在医学图像分割中常忽略解剖上下文，导致预测集碎片化、空间不连贯和过度分割，限制了其临床实用性。因此，需要一种既能提供严格统计误差保证，又能生成解剖上合理且空间连贯的预测集的方法。

Method: 提出Random-Walk Conformal Prediction (RW-CP)框架，该框架是模型无关的，可附加于任何分割方法之上。它利用预训练视觉基础模型的特征构建k近邻图，并通过随机游走扩散来正则化非一致性分数，从而增强预测集的空间连贯性。

Result: 在多模态公开数据集上的评估表明，在允许的错误率α=0.1下，RW-CP相比标准CP基线方法，分割质量最高提升了35.4%，同时保持了严格的边缘覆盖率。

Conclusion: RW-CP成功地将解剖上下文信息融入保形预测中，有效解决了标准方法产生的空间不连贯问题，在保证统计严谨性的同时大幅提升了医学图像分割预测集的临床可用性。

Abstract: The reliable deployment of deep learning in medical imaging requires uncertainty quantification that provides rigorous error guarantees while remaining anatomically meaningful. Conformal prediction (CP) is a powerful distribution-free framework for constructing statistically valid prediction intervals. However, standard applications in segmentation often ignore anatomical context, resulting in fragmented, spatially incoherent, and over-segmented prediction sets that limit clinical utility. To bridge this gap, this paper proposes Random-Walk Conformal Prediction (RW-CP), a model-agnostic framework which can be added on top of any segmentation method. RW-CP enforces spatial coherence to generate anatomically valid sets. Our method constructs a k-nearest neighbour graph from pre-trained vision foundation model features and applies a random walk to diffuse uncertainty. The random walk diffusion regularizes the non-conformity scores, making the prediction sets less sensitive to the conformal calibration parameter $λ$, ensuring more stable and continuous anatomical boundaries. RW-CP maintains rigorous marginal coverage while significantly improving segmentation quality. Evaluations on multi-modal public datasets show improvements of up to $35.4\%$ compared to standard CP baselines, given an allowable error rate of $α=0.1$.

Abstract (中文翻译): 深度学习在医学影像中的可靠部署需要能够提供严格误差保证且具有解剖意义的不确定性量化方法。保形预测（CP）是一种强大的无分布假设框架，可用于构建统计上有效的预测区间。然而，其在分割任务中的标准应用往往忽略了解剖上下文，导致预测集碎片化、空间不连贯且过度分割，从而限制了临床实用性。为弥合这一差距，本文提出了随机游走保形预测（RW-CP），这是一种模型无关的框架，可附加于任何分割方法之上。RW-CP通过强制空间一致性来生成解剖上有效的预测集。该方法从预训练视觉基础模型的特征中构建k近邻图，并应用随机游走来扩散不确定性。这种随机游走扩散对非一致性分数进行了正则化，使预测集对保形校准参数λ的敏感性降低，从而确保更稳定、连续的解剖边界。RW-CP在保持严格边缘覆盖率的同时，显著提升了分割质量。在多模态公开数据集上的评估显示，在允许的错误率α=0.1下，其性能相比标准CP基线最高提升了35.4%。

</details>
