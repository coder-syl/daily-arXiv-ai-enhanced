{"id": "2601.06067", "pdf": "https://arxiv.org/pdf/2601.06067", "abs": "https://arxiv.org/abs/2601.06067", "authors": ["Chimdi Walter Ndubuisi", "Toni Kazic"], "title": "HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders", "categories": ["cs.CV"], "comment": "13 pages, 8 figures. Code available at https://github.com/ChimdiWalter/HyperTopo-Adapters", "summary": "Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyperTopo-Adapters\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53c2\u6570\u9ad8\u6548\u7684\u5934\u90e8\u6a21\u5757\uff0c\u7ed3\u5408\u53cc\u66f2+\u6b27\u51e0\u91cc\u5f97+\u7403\u9762\uff08H+E+S\uff09\u4e58\u79ef\u6d41\u5f62\u5d4c\u5165\u4e0e\u62d3\u6251\u5148\u9a8c\uff08\u6301\u4e45\u540c\u8c03\u8ddd\u79bb\u548c\u53ef\u5fae\u4ee3\u7406\u635f\u5931\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u53f6\u90e8\u75c5\u6591\u5206\u5272\u4e2d\u7684\u8fb9\u754c\u4e0e\u62d3\u6251\u7ed3\u6784\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301Dice/IoU\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u53ef\u590d\u73b0\u7684\u8bad\u7ec3/\u8bc4\u4f30\u5957\u4ef6\u3002", "motivation": "\u6807\u51c6\u50cf\u7d20\u7ea7\u635f\u5931\u5728\u6b27\u5f0f\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u53f6\u90e8\u75c5\u6591\u5206\u5272\u4e2d\u7684\u5c0f\u5408\u5e76\u3001\u5206\u88c2\u6216\u865a\u5047\u5b54\u6d1e\u7b49\u62d3\u6251\u654f\u611f\u9519\u8bef\u60e9\u7f5a\u4e0d\u8db3\uff0c\u800c\u8fd9\u4e9b\u9519\u8bef\u5728\u751f\u7269\u5b66\u4e0a\u53ef\u80fd\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u51e0\u4f55\u4e0e\u62d3\u6251\u5148\u9a8c\u4ee5\u63d0\u5347\u5206\u5272\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002", "method": "\u4f5c\u8005\u63d0\u51faHyperTopo-Adapters\uff0c\u4f5c\u4e3a\u51bb\u7ed3\u89c6\u89c9\u7f16\u7801\u5668\u4e4b\u4e0a\u7684\u8f7b\u91cf\u7ea7\u5934\u90e8\uff0c\u5c06\u7279\u5f81\u5d4c\u5165\u5230\u53cc\u66f2+\u6b27\u51e0\u91cc\u5f97+\u7403\u9762\uff08H+E+S\uff09\u4e58\u79ef\u6d41\u5f62\u4e2d\uff0c\u5e76\u5f15\u5165\u62d3\u6251\u5148\u9a8c\uff1a(i) \u7528\u6301\u4e45\u540c\u8c03\uff08PH\uff09\u8ddd\u79bb\u8fdb\u884c\u8bc4\u4f30\u4e0e\u6a21\u578b\u9009\u62e9\uff1b(ii) \u4f7f\u7528\u7ed3\u5408\u8f6f\u6b27\u62c9\u7279\u5f81\u5339\u914d\u4e0e\u5168\u53d8\u5dee\u6b63\u5219\u5316\u7684\u53ef\u5fae\u4ee3\u7406\u635f\u5931\u8fdb\u884c\u7a33\u5b9a\u8bad\u7ec3\u3002\u6b64\u5916\u8fd8\u5305\u62ec\u9488\u5bf9\u53cc\u66f2\u5bf9\u6bd4\u9879\u548c\u62d3\u6251\u5148\u9a8c\u7684warm-up\u7b56\u7565\u3001\u6837\u672c\u7ea7\u7ed3\u6784\u611f\u77e5\u6307\u6807\u8bc4\u4f30\uff0c\u4ee5\u53ca\u57fa\u4e8emin-PD within top-K Dice\u7684\u68c0\u67e5\u70b9\u9009\u62e9\u89c4\u5219\u3002", "result": "\u5728Kaggle\u53f6\u90e8\u75c5\u6591\u6570\u636e\u96c6\uff08N=2,940\uff09\u4e0a\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fb9\u754c\u548c\u62d3\u6251\u6307\u6807\u4e0a\u6301\u7eed\u63d0\u5347\uff08\u5982\u5c06\u0394\u03b2\u2081\u5b54\u6d1e\u8bef\u5dee\u964d\u4f4e9%\uff09\uff0c\u540c\u65f6Dice/IoU\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u4f5c\u8005\u8fd8\u8fdb\u884c\u4e86\u591a\u9879\u63a7\u5236\u6d88\u878d\u5b9e\u9a8c\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u7f16\u7801\u5668\u3001\u8f93\u5165\u5206\u8fa8\u7387\u3001PH\u6743\u91cd\u53ca\u90e8\u5206\u89e3\u51bb\u7b56\u7565\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u8bca\u65ad\u6027\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u51e0\u4f55\u4e0e\u62d3\u6251\u5148\u9a8c\u5bf9\u63d0\u5347\u5206\u5272\u7ed3\u6784\u4fdd\u771f\u5ea6\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u53ef\u590d\u73b0\u7684\u8bad\u7ec3/\u8bc4\u4f30\u5957\u4ef6\uff0c\u65e8\u5728\u63ed\u793a\u5931\u8d25\u6a21\u5f0f\u5e76\u6307\u5bfc\u672a\u6765\u66f4\u5f3a\u5927\u7684\u62d3\u6251\u4fdd\u6301\u67b6\u6784\u8bbe\u8ba1\u3002", "summary_cn": "\u53f6\u90e8\u75c5\u6591\u5206\u5272\u5bf9\u62d3\u6251\u7ed3\u6784\u654f\u611f\uff1a\u5fae\u5c0f\u7684\u5408\u5e76\u3001\u5206\u88c2\u6216\u865a\u5047\u5b54\u6d1e\u5728\u751f\u7269\u5b66\u4e0a\u53ef\u80fd\u662f\u751f\u5316\u901a\u8def\u7684\u91cd\u8981\u63cf\u8ff0\u7b26\uff0c\u4f46\u6807\u51c6\u7684\u50cf\u7d20\u7ea7\u635f\u5931\u5728\u6b27\u5f0f\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u6b64\u7c7b\u9519\u8bef\u60e9\u7f5a\u8f83\u5f31\u3002\u672c\u6587\u63a2\u7d22\u4e86HyperTopo-Adapters\u2014\u2014\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53c2\u6570\u9ad8\u6548\u7684\u5934\u90e8\u6a21\u5757\uff0c\u5b83\u5728\u51bb\u7ed3\u7684\u89c6\u89c9\u7f16\u7801\u5668\u4e4b\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u7279\u5f81\u5d4c\u5165\u5230\u53cc\u66f2+\u6b27\u51e0\u91cc\u5f97+\u7403\u9762\uff08H+E+S\uff09\u4e58\u79ef\u6d41\u5f62\u4e2d\uff0c\u4ee5\u4fc3\u8fdb\u5c42\u6b21\u5206\u79bb\uff08H\uff09\u3001\u5c40\u90e8\u7ebf\u6027\u7ec6\u8282\uff08E\uff09\u548c\u5168\u5c40\u95ed\u5408\u6027\uff08S\uff09\u3002\u62d3\u6251\u5148\u9a8c\u4ee5\u4e24\u79cd\u5f62\u5f0f\u8865\u5145Dice/BCE\u635f\u5931\uff1a(i) \u7528\u4e8e\u8bc4\u4f30\u4e0e\u6a21\u578b\u9009\u62e9\u7684\u6301\u4e45\u540c\u8c03\uff08PH\uff09\u8ddd\u79bb\uff1b(ii) \u4e00\u79cd\u53ef\u5fae\u4ee3\u7406\u635f\u5931\uff0c\u7ed3\u5408\u8f6f\u6b27\u62c9\u7279\u5f81\u5339\u914d\u4e0e\u5168\u53d8\u5dee\u6b63\u5219\u5316\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u3002\u672c\u6587\u8fd8\u5f15\u5165\u4e86\u9488\u5bf9\u53cc\u66f2\u5bf9\u6bd4\u9879\u548c\u62d3\u6251\u5148\u9a8c\u7684\u9884\u70ed\u7b56\u7565\u3001\u6837\u672c\u7ea7\u7ed3\u6784\u611f\u77e5\u6307\u6807\uff08Boundary-F1\u3001Betti\u8bef\u5dee\u3001PD\u8ddd\u79bb\uff09\u8bc4\u4f30\uff0c\u4ee5\u53ca\u57fa\u4e8etop-K Dice\u4e2d\u6700\u5c0fPD\u8ddd\u79bb\u7684\u68c0\u67e5\u70b9\u9009\u62e9\u89c4\u5219\u3002\u5728Kaggle\u53f6\u90e8\u75c5\u6591\u6570\u636e\u96c6\uff08N=2,940\uff09\u4e0a\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fb9\u754c\u548c\u62d3\u6251\u6307\u6807\u4e0a\u6301\u7eed\u63d0\u5347\uff08\u5c06\u0394\u03b2\u2081\u5b54\u6d1e\u8bef\u5dee\u964d\u4f4e9%\uff09\uff0c\u540c\u65f6Dice/IoU\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u672c\u7814\u7a76\u5728\u8bbe\u8ba1\u4e0a\u5177\u6709\u8bca\u65ad\u6027\uff1a\u62a5\u544a\u4e86\u591a\u9879\u53d7\u63a7\u6d88\u878d\u5b9e\u9a8c\uff08\u66f2\u7387\u5b66\u4e60\u3001\u6f5c\u5728\u7ef4\u5ea6\u3001\u5bf9\u6bd4\u6e29\u5ea6\u3001\u4ee3\u7406\u8bbe\u7f6e\uff09\uff0c\u5e76\u6b63\u5728\u8fdb\u884c\u4e0d\u540c\u7f16\u7801\u5668\uff08ResNet-50\u3001DeepLabV3\u3001DINOv2/v3\uff09\u3001\u8f93\u5165\u5206\u8fa8\u7387\u3001PH\u6743\u91cd\u53ca\u540e\u671f\u6a21\u5757\u90e8\u5206\u89e3\u51bb\u7684\u6d4b\u8bd5\u3002\u672c\u6587\u8d21\u732e\u4e86\u4e00\u4e2a\u5f00\u6e90\u3001\u53ef\u590d\u73b0\u7684\u8bad\u7ec3/\u8bc4\u4f30\u5957\u4ef6\uff08https://github.com/ChimdiWalter/HyperTopo-Adapters\uff09\uff0c\u7528\u4e8e\u5206\u79bb\u51e0\u4f55/\u62d3\u6251\u5148\u9a8c\u5e76\u63ed\u793a\u5931\u8d25\u6a21\u5f0f\uff0c\u4ee5\u6307\u5bfc\u66f4\u5f3a\u7684\u62d3\u6251\u4fdd\u6301\u67b6\u6784\u8bbe\u8ba1\u3002"}}
{"id": "2601.06078", "pdf": "https://arxiv.org/pdf/2601.06078", "abs": "https://arxiv.org/abs/2601.06078", "authors": ["Yin Wang", "Chunlin Gong", "Zhuozhen Xu", "Lehan Zhang", "Xiang Wu"], "title": "OptFormer: Optical Flow-Guided Attention and Phase Space Reconstruction for SST Forecasting", "categories": ["cs.CV", "physics.ao-ph"], "comment": "11 pages,4 figures, 5 tables", "summary": "Sea Surface Temperature (SST) prediction plays a vital role in climate modeling and disaster forecasting. However, it remains challenging due to its nonlinear spatiotemporal dynamics and extended prediction horizons. To address this, we propose OptFormer, a novel encoder-decoder model that integrates phase-space reconstruction with a motion-aware attention mechanism guided by optical flow. Unlike conventional attention, our approach leverages inter-frame motion cues to highlight relative changes in the spatial field, allowing the model to focus on dynamic regions and capture long-range temporal dependencies more effectively. Experiments on NOAA SST datasets across multiple spatial scales demonstrate that OptFormer achieves superior performance under a 1:1 training-to-prediction setting, significantly outperforming existing baselines in accuracy and robustness.", "AI": {"tldr": "\u63d0\u51faOptFormer\u6a21\u578b\uff0c\u7ed3\u5408\u76f8\u7a7a\u95f4\u91cd\u6784\u4e0e\u5149\u6d41\u5f15\u5bfc\u7684\u8fd0\u52a8\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6d77\u8868\u6e29\u5ea6\uff08SST\uff09\u9884\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6d77\u8868\u6e29\u5ea6\uff08SST\uff09\u9884\u6d4b\u5bf9\u6c14\u5019\u5efa\u6a21\u548c\u707e\u5bb3\u9884\u62a5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u975e\u7ebf\u6027\u65f6\u7a7a\u52a8\u6001\u7279\u6027\u53ca\u8f83\u957f\u7684\u9884\u6d4b\u65f6\u95f4\u8de8\u5ea6\uff0c\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578bOptFormer\uff0c\u878d\u5408\u76f8\u7a7a\u95f4\u91cd\u6784\u4e0e\u7531\u5149\u6d41\u5f15\u5bfc\u7684\u8fd0\u52a8\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u5e27\u95f4\u8fd0\u52a8\u7ebf\u7d22\u7a81\u51fa\u7a7a\u95f4\u573a\u4e2d\u7684\u76f8\u5bf9\u53d8\u5316\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u6355\u6349\u52a8\u6001\u533a\u57df\u548c\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u3002", "result": "\u5728\u591a\u5c3a\u5ea6NOAA SST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOptFormer\u57281:1\u8bad\u7ec3-\u9884\u6d4b\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "OptFormer\u901a\u8fc7\u5f15\u5165\u8fd0\u52a8\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86SST\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u65f6\u7a7a\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "summary_cn": "\u6d77\u8868\u6e29\u5ea6\uff08SST\uff09\u9884\u6d4b\u5728\u6c14\u5019\u5efa\u6a21\u548c\u707e\u5bb3\u9884\u62a5\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u975e\u7ebf\u6027\u7684\u65f6\u7a7a\u52a8\u6001\u7279\u6027\u4ee5\u53ca\u8f83\u957f\u7684\u9884\u6d4b\u65f6\u95f4\u8de8\u5ea6\uff0cSST\u9884\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86OptFormer\u2014\u2014\u4e00\u79cd\u65b0\u9896\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u76f8\u7a7a\u95f4\u91cd\u6784\u4e0e\u7531\u5149\u6d41\u5f15\u5bfc\u7684\u8fd0\u52a8\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u76f8\u7ed3\u5408\u3002\u4e0e\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5e27\u95f4\u8fd0\u52a8\u7ebf\u7d22\u6765\u7a81\u51fa\u7a7a\u95f4\u573a\u4e2d\u7684\u76f8\u5bf9\u53d8\u5316\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u805a\u7126\u4e8e\u52a8\u6001\u533a\u57df\uff0c\u5e76\u66f4\u6709\u6548\u5730\u6355\u6349\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u5728\u591a\u4e2a\u7a7a\u95f4\u5c3a\u5ea6\u7684NOAA SST\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOptFormer\u57281:1\u7684\u8bad\u7ec3-\u9884\u6d4b\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.06097", "pdf": "https://arxiv.org/pdf/2601.06097", "abs": "https://arxiv.org/abs/2601.06097", "authors": ["Aradhya Dixit", "Tianxi Liang"], "title": "Semantic Event Graphs for Long-Form Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 6 figures", "summary": "Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START/END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u4e8b\u4ef6\u56fe\uff08SEG\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7b26\u53f7\u5316\u89c6\u9891-\u8bed\u8a00\u63a5\u53e3\uff0c\u5c06\u539f\u59cb\u89c6\u9891\u5e27\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u65f6\u95f4\u4ea4\u4e92\u65e5\u5fd7\uff0c\u663e\u8457\u51cf\u5c11\u957f\u89c6\u9891\u95ee\u7b54\u6240\u9700\u7684token\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5c0f\u65f6\u7ea7\u957f\u89c6\u9891\u95ee\u7b54\u65f6\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u5bc6\u96c6\u7684\u89c6\u89c9\u5d4c\u5165\u4f1a\u8fc5\u901f\u8d85\u51fa\u5b9e\u9645\u53ef\u7528\u7684token\u548c\u8ba1\u7b97\u9884\u7b97\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u4e0b\u91c7\u6837\u5e27\u6216\u5411\u5927\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u8f93\u5165\u5bc6\u96c6\u89c6\u89c9\u5d4c\u5165\u6765\u6743\u8861\u65f6\u95f4\u8986\u76d6\u8303\u56f4\u4e0e\u6210\u672c\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u4e8b\u4ef6\u56fe\uff08SEG\uff09\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528YOLOv11\u68c0\u6d4b\u5e76\u8ddf\u8e2a\u7269\u4f53\uff1b\u7136\u540e\u5c06\u7269\u4f53\u95f4\u7684\u90bb\u8fd1\u6a21\u5f0f\u8f6c\u5316\u4e3aSTART/END\u5f62\u5f0f\u7684\u4eba-\u7269\u4ea4\u4e92\u4e8b\u4ef6\uff0c\u5e76\u7ec4\u7ec7\u6210\u65f6\u95f4\u573a\u666f\u56fe\uff08TSG\uff09\uff1b\u63a8\u7406\u65f6\uff0c\u901a\u8fc7\u67e5\u8be2\u611f\u77e5\u526a\u679d\u6a21\u5757\u8bc6\u522b\u951a\u70b9\u5b9e\u4f53\u548c\u8bcd\u6c47\u76f8\u5173\u4e8b\u4ef6\uff0c\u4ec5\u8fd4\u56de\u5c0f\u578b\u5b50\u56fe\uff0c\u5c06\u5176\u6587\u672c\u5316\u540e\u8f93\u5165Gemini 2.5 Flash\u751f\u6210\u7b54\u6848\u3002", "result": "\u5728\u4e94\u4e2aYouTube\u89c6\u9891\uff08\u6bcf\u4e2a\u542b300\u2013500\u6b21\u4ea4\u4e92\uff09\u548c120\u4e2a\u81ea\u52a8\u751f\u6210\u7684\u957f\u65f6\u7a0b\u95ee\u9898\u4e0a\uff0cSEG\u4ee5\u6bcf\u67e5\u8be2\u4ec53.47k tokens\u8fbe\u523065.0%\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u4f7f\u752840.39k tokens\u7684\u5b8c\u6574\u65e5\u5fd7\u57fa\u7ebf\uff0862.5%\uff09\uff0ctoken\u4f7f\u7528\u51cf\u5c1191.4%\uff1b\u800c\u4ec5\u4f7f\u7528\u6700\u540e30\u79d2\u89c6\u9891\u7684\u77ed\u4e0a\u4e0b\u6587\u57fa\u7ebf\u51c6\u786e\u7387\u9aa4\u964d\u81f32.5%\u3002", "conclusion": "\u7b26\u53f7\u5316\u65f6\u95f4\u56fe\u53ef\u4f5c\u4e3a\u73b0\u6210\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u5373\u63d2\u5373\u7528\u8bb0\u5fc6\u5c42\uff0c\u5728\u4fdd\u7559\u957f\u8ddd\u79bb\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u957f\u89c6\u9891\u95ee\u7b54\u7684token\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "summary_cn": "\u957f\u89c6\u9891\u95ee\u7b54\u5bf9\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4ecd\u5177\u6311\u6218\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5904\u7406\u5c0f\u65f6\u7ea7\u89c6\u9891\u65f6\u96be\u4ee5\u5728\u4e0d\u8d85\u51fa\u5b9e\u9645token\u548c\u8ba1\u7b97\u9884\u7b97\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u6709\u6548\u63a8\u7406\u3002\u73b0\u6709\u7cfb\u7edf\u901a\u5e38\u901a\u8fc7\u4e0b\u91c7\u6837\u5e27\u6216\u5c06\u5bc6\u96c6\u89c6\u89c9\u5d4c\u5165\u8f93\u5165\u5927\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u65f6\u95f4\u8986\u76d6\u8303\u56f4\u4e0e\u6210\u672c\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u4e49\u4e8b\u4ef6\u56fe\uff08Semantic Event Graphs, SEG\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u9891\u4e0e\u8bed\u8a00\u4e4b\u95f4\u7684\u7b26\u53f7\u5316\u63a5\u53e3\uff0c\u7528\u7d27\u51d1\u7684\u65f6\u95f4\u4ea4\u4e92\u65e5\u5fd7\u66ff\u4ee3\u539f\u59cb\u89c6\u9891\u5e27\u3002\u6211\u4eec\u7684\u6d41\u7a0b\u4f7f\u7528YOLOv11\u68c0\u6d4b\u5e76\u8ddf\u8e2a\u7269\u4f53\uff0c\u5c06\u90bb\u8fd1\u6a21\u5f0f\u8f6c\u6362\u4e3aSTART/END\u5f62\u5f0f\u7684\u4eba-\u7269\u4ea4\u4e92\u4e8b\u4ef6\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u65f6\u95f4\u573a\u666f\u56fe\uff08Temporal Scene Graph, TSG\uff09\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u4e00\u4e2a\u67e5\u8be2\u611f\u77e5\u526a\u679d\u6a21\u5757\u8bc6\u522b\u951a\u70b9\u5b9e\u4f53\u548c\u8bcd\u6c47\u76f8\u5173\u7684\u4e8b\u4ef6\uff0c\u4ec5\u8fd4\u56de\u4e00\u4e2a\u5c0f\u578b\u5b50\u56fe\uff0c\u8be5\u5b50\u56fe\u88ab\u6587\u672c\u5316\u540e\u4f20\u7ed9Gemini 2.5 Flash\u7528\u4e8e\u751f\u6210\u7b54\u6848\u3002\u5728\u4e94\u4e2aYouTube\u89c6\u9891\uff08\u6bcf\u4e2a\u5305\u542b300\u2013500\u6b21\u4ea4\u4e92\uff09\u548c120\u4e2a\u81ea\u52a8\u751f\u6210\u7684\u957f\u65f6\u7a0b\u95ee\u9898\u4e0a\uff0cSEG\u6bcf\u67e5\u8be2\u4ec5\u4f7f\u75283.47k\u4e2atoken\u5c31\u8fbe\u5230\u4e8665.0%\u7684\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u4f7f\u752840.39k\u4e2atoken\u7684\u5b8c\u6574\u65e5\u5fd7\u57fa\u7ebf\uff0862.5%\uff09\uff0c\u540c\u65f6\u5c06token\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8691.4%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4ec5\u9650\u4e8e\u6700\u540e30\u79d2\u89c6\u9891\u7684\u77ed\u4e0a\u4e0b\u6587\u57fa\u7ebf\u51c6\u786e\u7387\u9aa4\u964d\u81f32.5%\uff0c\u51f8\u663e\u4e86\u663e\u5f0f\u65f6\u95f4\u8bb0\u5fc6\u7684\u5fc5\u8981\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u7b26\u53f7\u5316\u65f6\u95f4\u56fe\u53ef\u4f5c\u4e3a\u73b0\u6210\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u4e00\u79cd\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u8bb0\u5fc6\u5c42\uff0c\u5728\u4fdd\u7559\u957f\u8ddd\u79bb\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4f7f\u957f\u89c6\u9891\u95ee\u7b54\u5728token\u548c\u6210\u672c\u65b9\u9762\u663e\u8457\u66f4\u9ad8\u6548\u3002\u4ee3\u7801\u3001\u65e5\u5fd7\u548c\u4e8b\u4ef6\u63d0\u53d6\u5de5\u5177\u5c06\u516c\u5f00\u53d1\u5e03\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2601.06122", "pdf": "https://arxiv.org/pdf/2601.06122", "abs": "https://arxiv.org/abs/2601.06122", "authors": ["Canming Xia", "Peixi Peng", "Guang Tan", "Zhan Su", "Haoran Xu", "Zhenxian Liu", "Luntong Li"], "title": "COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The paper was accepted by the Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Visual reinforcement learning (RL) suffers from poor sample efficiency due to high-dimensional observations in complex tasks. While existing works have shown that vision-language models (VLMs) can assist RL, they often focus on knowledge distillation from the VLM to RL, overlooking the potential of RL-generated interaction data to enhance the VLM. To address this, we propose COVR, a collaborative optimization framework that enables the mutual enhancement of the VLM and RL policies. Specifically, COVR fine-tunes the VLM with RL-generated data to enhance the semantic reasoning ability consistent with the target task, and uses the enhanced VLM to further guide policy learning via action priors. To improve fine-tuning efficiency, we introduce two key modules: (1) an Exploration-Driven Dynamic Filter module that preserves valuable exploration samples using adaptive thresholds based on the degree of exploration, and (2) a Return-Aware Adaptive Loss Weight module that improves the stability of training by quantifying the inconsistency of sampling actions via return signals of RL. We further design a progressive fine-tuning strategy to reduce resource consumption. Extensive experiments show that COVR achieves strong performance across various challenging visual control tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCOVR\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u534f\u540c\u4f18\u5316\uff0c\u5229\u7528RL\u751f\u6210\u7684\u6570\u636e\u5fae\u8c03VLM\u4ee5\u63d0\u5347\u5176\u4efb\u52a1\u76f8\u5173\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u7528\u589e\u5f3a\u540e\u7684VLM\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u6a21\u5757\u63d0\u9ad8\u5fae\u8c03\u6548\u7387\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u5fae\u8c03\u7b56\u7565\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u77e5\u8bc6\u84b8\u998f\u5230\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\uff0c\u5ffd\u7565\u4e86RL\u751f\u6210\u7684\u4ea4\u4e92\u6570\u636e\u5bf9\u63d0\u5347VLM\u672c\u8eab\u7684\u6f5c\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u65e8\u5728\u5efa\u7acb\u4e00\u4e2aVLM\u4e0eRL\u7b56\u7565\u76f8\u4e92\u589e\u5f3a\u7684\u534f\u4f5c\u673a\u5236\u3002", "method": "\u63d0\u51faCOVR\u534f\u540c\u4f18\u5316\u6846\u67b6\uff1a1\uff09\u4f7f\u7528RL\u751f\u6210\u7684\u6570\u636e\u5bf9VLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u66f4\u8d34\u5408\u76ee\u6807\u4efb\u52a1\uff1b2\uff09\u5229\u7528\u589e\u5f3a\u540e\u7684VLM\u901a\u8fc7\u52a8\u4f5c\u5148\u9a8c\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002\u4e3a\u63d0\u5347\u5fae\u8c03\u6548\u7387\uff0c\u8bbe\u8ba1\u4e86\u63a2\u7d22\u9a71\u52a8\u7684\u52a8\u6001\u8fc7\u6ee4\u6a21\u5757\u548c\u56de\u62a5\u611f\u77e5\u7684\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u6e10\u8fdb\u5f0f\u5fae\u8c03\u7b56\u7565\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "result": "\u5728\u591a\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eCOVR\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "COVR\u901a\u8fc7VLM\u4e0eRL\u7684\u53cc\u5411\u534f\u540c\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6574\u4f53\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5229\u7528RL\u4ea4\u4e92\u6570\u636e\u589e\u5f3aVLM\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "summary_cn": "\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7531\u4e8e\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9762\u4e34\u9ad8\u7ef4\u89c2\u6d4b\uff0c\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8868\u660e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u53ef\u8f85\u52a9RL\uff0c\u4f46\u8fd9\u4e9b\u5de5\u4f5c\u901a\u5e38\u805a\u7126\u4e8e\u5c06VLM\u7684\u77e5\u8bc6\u84b8\u998f\u5230RL\u4e2d\uff0c\u5ffd\u89c6\u4e86RL\u751f\u6210\u7684\u4ea4\u4e92\u6570\u636e\u5bf9\u589e\u5f3aVLM\u672c\u8eab\u7684\u6f5c\u529b\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86COVR\u2014\u2014\u4e00\u79cd\u4f7fVLM\u4e0eRL\u7b56\u7565\u76f8\u4e92\u589e\u5f3a\u7684\u534f\u540c\u4f18\u5316\u6846\u67b6\u3002\u5177\u4f53\u800c\u8a00\uff0cCOVR\u5229\u7528RL\u751f\u6210\u7684\u6570\u636e\u5bf9VLM\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u589e\u5f3a\u5176\u4e0e\u76ee\u6807\u4efb\u52a1\u4e00\u81f4\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5229\u7528\u589e\u5f3a\u540e\u7684VLM\u901a\u8fc7\u52a8\u4f5c\u5148\u9a8c\u8fdb\u4e00\u6b65\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002\u4e3a\u63d0\u9ad8\u5fae\u8c03\u6548\u7387\uff0c\u6211\u4eec\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\uff081\uff09\u63a2\u7d22\u9a71\u52a8\u7684\u52a8\u6001\u8fc7\u6ee4\u6a21\u5757\uff0c\u57fa\u4e8e\u63a2\u7d22\u7a0b\u5ea6\u7684\u81ea\u9002\u5e94\u9608\u503c\u4fdd\u7559\u6709\u4ef7\u503c\u7684\u63a2\u7d22\u6837\u672c\uff1b\uff082\uff09\u56de\u62a5\u611f\u77e5\u7684\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u6a21\u5757\uff0c\u901a\u8fc7RL\u7684\u56de\u62a5\u4fe1\u53f7\u91cf\u5316\u91c7\u6837\u52a8\u4f5c\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5fae\u8c03\u7b56\u7565\u4ee5\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCOVR\u5728\u591a\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2601.06138", "pdf": "https://arxiv.org/pdf/2601.06138", "abs": "https://arxiv.org/abs/2601.06138", "authors": ["Sao Mai Nguyen"], "title": "Low-Back Pain Physical Rehabilitation by Movement Analysis in Clinical Trial", "categories": ["cs.CV", "cs.HC", "cs.RO"], "comment": "ICMST, Tokyo University of Science; Taiwanese Society of Movement Science and Technology; Research institute for Science and Technology, Nov 2025, Tokyo, Japan", "summary": "To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we propose a medical dataset of clinical patients carrying out low back-pain rehabilitation exercises and benchmark on state of the art human movement analysis algorithms. This dataset is valuable because it includes rehabilitation motions in a clinical setting with patients in their rehabilitation program. This paper introduces the Keraal dataset, a clinically collected dataset to enable intelligent tutoring systems (ITS) for rehabilitation. It addresses four challenges in exercise monitoring: motion assessment, error recognition, spatial localization, temporal localization", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Keraal\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u6536\u96c6\u7684\u60a3\u8005\u8fdb\u884c\u8170\u75db\u5eb7\u590d\u8bad\u7ec3\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u652f\u6301\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\uff08ITS\uff09\u5728\u5eb7\u590d\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5bf9\u5f53\u524d\u4eba\u4f53\u52a8\u4f5c\u5206\u6790\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f00\u53d1\u548c\u8bc4\u4f30\u7528\u4e8e\u7269\u7406\u5eb7\u590d\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u9700\u8981\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e0b\u7684\u5eb7\u590d\u8fd0\u52a8\u6570\u636e\uff0c\u800c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u6b64\u7c7b\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u5e76\u53d1\u5e03Keraal\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e34\u5e8a\u60a3\u8005\u6267\u884c\u8170\u75db\u5eb7\u590d\u8bad\u7ec3\u7684\u52a8\u4f5c\uff0c\u5e76\u5229\u7528\u5f53\u524d\u5148\u8fdb\u7684\u4eba\u4f53\u52a8\u4f5c\u5206\u6790\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8be5\u6570\u636e\u96c6\u6709\u6548\u652f\u6301\u5bf9\u5eb7\u590d\u8bad\u7ec3\u4e2d\u56db\u4e2a\u5173\u952e\u6311\u6218\u7684\u5904\u7406\uff1a\u52a8\u4f5c\u8bc4\u4f30\u3001\u9519\u8bef\u8bc6\u522b\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u65f6\u95f4\u5b9a\u4f4d\u3002", "conclusion": "Keraal\u6570\u636e\u96c6\u586b\u8865\u4e86\u4e34\u5e8a\u5eb7\u590d\u573a\u666f\u4e0b\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u7814\u7a76\u7684\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u5eb7\u590d\u8fd0\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "summary_cn": "\u4e3a\u4e86\u652f\u6301\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u5728\u7269\u7406\u5eb7\u590d\u4e2d\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e34\u5e8a\u60a3\u8005\u6267\u884c\u8170\u75db\u5eb7\u590d\u8bad\u7ec3\u52a8\u4f5c\u7684\u533b\u5b66\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u5f53\u524d\u5148\u8fdb\u7684\u4eba\u4f53\u52a8\u4f5c\u5206\u6790\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u8be5\u6570\u636e\u96c6\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86\u5904\u4e8e\u5eb7\u590d\u8ba1\u5212\u4e2d\u7684\u60a3\u8005\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5eb7\u590d\u52a8\u4f5c\u3002\u672c\u6587\u4ecb\u7ecd\u4e86Keraal\u6570\u636e\u96c6\u2014\u2014\u4e00\u4e2a\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u91c7\u96c6\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8\u5eb7\u590d\u9886\u57df\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\uff08ITS\uff09\u53d1\u5c55\u3002\u8be5\u6570\u636e\u96c6\u9488\u5bf9\u8fd0\u52a8\u76d1\u6d4b\u4e2d\u7684\u56db\u5927\u6311\u6218\uff1a\u52a8\u4f5c\u8bc4\u4f30\u3001\u9519\u8bef\u8bc6\u522b\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u65f6\u95f4\u5b9a\u4f4d\u3002"}}
{"id": "2601.06163", "pdf": "https://arxiv.org/pdf/2601.06163", "abs": "https://arxiv.org/abs/2601.06163", "authors": ["Kaiyuan Deng", "Bo Hui", "Gen Li", "Jie Ji", "Minghai Qin", "Geng Yuan", "Xiaolong Ma"], "title": "Forget-It-All: Multi-Concept Machine Unlearning via Concept-Aware Neuron Masking", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The widespread adoption of text-to-image (T2I) diffusion models has raised concerns about their potential to generate copyrighted, inappropriate, or sensitive imagery learned from massive training corpora. As a practical solution, machine unlearning aims to selectively erase unwanted concepts from a pre-trained model without retraining from scratch. While most existing methods are effective for single-concept unlearning, they often struggle in real-world scenarios that require removing multiple concepts, since extending them to this setting is both non-trivial and problematic, causing significant challenges in unlearning effectiveness, generation quality, and sensitivity to hyperparameters and datasets. In this paper, we take a unique perspective on multi-concept unlearning by leveraging model sparsity and propose the Forget It All (FIA) framework. FIA first introduces Contrastive Concept Saliency to quantify each weight connection's contribution to a target concept. It then identifies Concept-Sensitive Neurons by combining temporal and spatial information, ensuring that only neurons consistently responsive to the target concept are selected. Finally, FIA constructs masks from the identified neurons and fuses them into a unified multi-concept mask, where Concept-Agnostic Neurons that broadly support general content generation are preserved while concept-specific neurons are pruned to remove the targets. FIA is training-free and requires only minimal hyperparameter tuning for new tasks, thereby promoting a plug-and-play paradigm. Extensive experiments across three distinct unlearning tasks demonstrate that FIA achieves more reliable multi-concept unlearning, improving forgetting effectiveness while maintaining semantic fidelity and image quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Forget It All (FIA) \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u9ad8\u6548\u3001\u53ef\u9760\u5730\u540c\u65f6\u9057\u5fd8\u591a\u4e2a\u6982\u5ff5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6a21\u578b\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6982\u5ff5\u663e\u8457\u6027\u8bc6\u522b\u4e0e\u76ee\u6807\u6982\u5ff5\u76f8\u5173\u7684\u6743\u91cd\u8fde\u63a5\uff0c\u5e76\u7ed3\u5408\u65f6\u7a7a\u4fe1\u606f\u5b9a\u4f4d\u201c\u6982\u5ff5\u654f\u611f\u795e\u7ecf\u5143\u201d\uff0c\u8fdb\u800c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u6982\u5ff5\u63a9\u7801\u8fdb\u884c\u526a\u679d\u3002FIA \u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4ec5\u9700\u6781\u5c11\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u5373\u53ef\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u6709\u6548\u79fb\u9664\u591a\u4e2a\u654f\u611f\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6982\u5ff5\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u80fd\u751f\u6210\u53d7\u7248\u6743\u4fdd\u62a4\u3001\u4e0d\u5f53\u6216\u654f\u611f\u5185\u5bb9\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u5355\u4e00\u6982\u5ff5\uff0c\u5728\u5904\u7406\u9700\u8981\u540c\u65f6\u9057\u5fd8\u591a\u4e2a\u6982\u5ff5\u7684\u73b0\u5b9e\u573a\u666f\u65f6\u9762\u4e34\u6548\u679c\u4e0d\u4f73\u3001\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u4ee5\u53ca\u5bf9\u8d85\u53c2\u6570\u548c\u6570\u636e\u96c6\u654f\u611f\u7b49\u6311\u6218\u3002", "method": "\u4f5c\u8005\u63d0\u51fa Forget It All (FIA) \u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6a21\u578b\u7a00\u758f\u6027\uff0c\u9996\u5148\u4f7f\u7528\u201c\u5bf9\u6bd4\u6982\u5ff5\u663e\u8457\u6027\u201d\u91cf\u5316\u6bcf\u4e2a\u6743\u91cd\u8fde\u63a5\u5bf9\u76ee\u6807\u6982\u5ff5\u7684\u8d21\u732e\uff1b\u7136\u540e\u7ed3\u5408\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u606f\u8bc6\u522b\u51fa\u201c\u6982\u5ff5\u654f\u611f\u795e\u7ecf\u5143\u201d\uff1b\u6700\u540e\uff0c\u4ece\u8fd9\u4e9b\u795e\u7ecf\u5143\u6784\u5efa\u63a9\u7801\u5e76\u878d\u5408\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6982\u5ff5\u63a9\u7801\uff0c\u5728\u4fdd\u7559\u652f\u6301\u901a\u7528\u5185\u5bb9\u751f\u6210\u7684\u201c\u6982\u5ff5\u65e0\u5173\u795e\u7ecf\u5143\u201d\u7684\u540c\u65f6\uff0c\u526a\u679d\u6389\u4e0e\u7279\u5b9a\u6982\u5ff5\u76f8\u5173\u7684\u795e\u7ecf\u5143\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u9057\u5fd8\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFIA \u80fd\u591f\u66f4\u53ef\u9760\u5730\u5b9e\u73b0\u591a\u6982\u5ff5\u9057\u5fd8\uff0c\u5728\u63d0\u9ad8\u9057\u5fd8\u6548\u679c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u8d28\u91cf\u3002", "conclusion": "FIA \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5373\u63d2\u5373\u7528\u7684\u591a\u6982\u5ff5\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u4e14\u5bf9\u8d85\u53c2\u6570\u4e0d\u654f\u611f\uff0c\u4e3a\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b89\u5168\u90e8\u7f72\u5927\u578b\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "summary_cn": "\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u7684\u5e7f\u6cdb\u91c7\u7528\u5f15\u53d1\u4e86\u4eba\u4eec\u7684\u62c5\u5fe7\uff0c\u5373\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u4f1a\u4ece\u5e9e\u5927\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u751f\u6210\u53d7\u7248\u6743\u4fdd\u62a4\u3001\u4e0d\u9002\u5f53\u6216\u654f\u611f\u7684\u56fe\u50cf\u3002\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u673a\u5668\u9057\u5fd8\u65e8\u5728\u6709\u9009\u62e9\u5730\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u64e6\u9664\u4e0d\u9700\u8981\u7684\u6982\u5ff5\uff0c\u800c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u91cd\u65b0\u8bad\u7ec3\u3002\u5c3d\u7ba1\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u6982\u5ff5\u9057\u5fd8\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u4f46\u5728\u9700\u8981\u79fb\u9664\u591a\u4e2a\u6982\u5ff5\u7684\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u5b83\u4eec\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u591a\u6982\u5ff5\u573a\u666f\u65e2\u975e\u6613\u4e8b\u53c8\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u5728\u9057\u5fd8\u6548\u679c\u3001\u751f\u6210\u8d28\u91cf\u548c\u5bf9\u8d85\u53c2\u6570\u53ca\u6570\u636e\u96c6\u7684\u654f\u611f\u6027\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ece\u6a21\u578b\u7a00\u758f\u6027\u7684\u72ec\u7279\u89c6\u89d2\u51fa\u53d1\uff0c\u63d0\u51fa\u4e86\u201c\u5168\u90e8\u9057\u5fd8\u201d\uff08Forget It All, FIA\uff09\u6846\u67b6\u3002FIA \u9996\u5148\u5f15\u5165\u201c\u5bf9\u6bd4\u6982\u5ff5\u663e\u8457\u6027\u201d\u6765\u91cf\u5316\u6bcf\u4e2a\u6743\u91cd\u8fde\u63a5\u5bf9\u76ee\u6807\u6982\u5ff5\u7684\u8d21\u732e\u3002\u7136\u540e\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u548c\u7a7a\u95f4\u4fe1\u606f\u6765\u8bc6\u522b\u201c\u6982\u5ff5\u654f\u611f\u795e\u7ecf\u5143\u201d\uff0c\u786e\u4fdd\u53ea\u9009\u62e9\u90a3\u4e9b\u5bf9\u76ee\u6807\u6982\u5ff5\u6301\u7eed\u54cd\u5e94\u7684\u795e\u7ecf\u5143\u3002\u6700\u540e\uff0cFIA \u4ece\u5df2\u8bc6\u522b\u7684\u795e\u7ecf\u5143\u6784\u5efa\u63a9\u7801\uff0c\u5e76\u5c06\u5176\u878d\u5408\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6982\u5ff5\u63a9\u7801\uff0c\u5728\u5176\u4e2d\u4fdd\u7559\u4e86\u5e7f\u6cdb\u652f\u6301\u901a\u7528\u5185\u5bb9\u751f\u6210\u7684\u201c\u6982\u5ff5\u65e0\u5173\u795e\u7ecf\u5143\u201d\uff0c\u540c\u65f6\u526a\u679d\u6389\u4e0e\u7279\u5b9a\u6982\u5ff5\u76f8\u5173\u7684\u795e\u7ecf\u5143\u4ee5\u79fb\u9664\u76ee\u6807\u3002FIA \u65e0\u9700\u8bad\u7ec3\uff0c\u4e14\u5bf9\u4e8e\u65b0\u4efb\u52a1\u4ec5\u9700\u6781\u5c11\u7684\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8303\u5f0f\u3002\u5728\u4e09\u4e2a\u4e0d\u540c\u9057\u5fd8\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFIA \u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u591a\u6982\u5ff5\u9057\u5fd8\uff0c\u5728\u63d0\u9ad8\u9057\u5fd8\u6548\u679c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2601.06165", "pdf": "https://arxiv.org/pdf/2601.06165", "abs": "https://arxiv.org/abs/2601.06165", "authors": ["Dasol Choi", "Guijin Son", "Hanwool Lee", "Minhyuk Kim", "Hyunwoo Ko", "Teabin Lim", "Ahn Eungyeol", "Jungwhan Kim", "Seunghyeok Hong", "Youngsook Song"], "title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.", "AI": {"tldr": "\u73b0\u5b9e\u7528\u6237\u89c6\u89c9\u67e5\u8be2\u5e38\u542b\u7cca\u4e0d\u6e05\uff0c\u800c\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u57fa\u51c6\u591a\u4f7f\u7528\u7ed3\u6784\u6e05\u6670\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faHAERAE-Vision\u6570\u636e\u96c6\uff0c\u5305\u542b653\u4e2a\u771f\u5b9e\u97e9\u8bed\u793e\u533a\u89c6\u89c9\u95ee\u9898\u53ca\u5176\u660e\u786e\u6539\u5199\u7248\u672c\uff0c\u51711,306\u4e2a\u67e5\u8be2\u53d8\u4f53\u3002\u8bc4\u4f3039\u4e2aVLM\u53d1\u73b0\uff0c\u5373\u4f7f\u9876\u5c16\u6a21\u578b\u5728\u539f\u59cb\u6a21\u7cca\u67e5\u8be2\u4e0a\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e50%\uff1b\u4ec5\u5c06\u67e5\u8be2\u660e\u786e\u5316\u5373\u53ef\u63d0\u53478\u201322\u4e2a\u767e\u5206\u70b9\uff0c\u5c0f\u6a21\u578b\u53d7\u76ca\u6700\u5927\u3002\u5373\u4f7f\u7ed3\u5408\u7f51\u7edc\u641c\u7d22\uff0c\u6a21\u7cca\u67e5\u8be2\u8868\u73b0\u4ecd\u4e0d\u5982\u65e0\u641c\u7d22\u7684\u660e\u786e\u67e5\u8be2\uff0c\u8868\u660e\u68c0\u7d22\u65e0\u6cd5\u5f25\u8865\u4fe1\u606f\u7f3a\u5931\u3002\u7814\u7a76\u63ed\u793aVLM\u6027\u80fd\u74f6\u9888\u4e3b\u8981\u6e90\u4e8e\u81ea\u7136\u67e5\u8be2\u7684\u6a21\u7cca\u6027\uff0c\u800c\u975e\u6a21\u578b\u80fd\u529b\u672c\u8eab\uff0c\u51f8\u663e\u5f53\u524d\u8bc4\u6d4b\u4e0e\u5b9e\u9645\u5e94\u7528\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u4e2d\u7684\u95ee\u9898\u901a\u5e38\u7ed3\u6784\u826f\u597d\u3001\u63d0\u793a\u660e\u786e\uff0c\u4f46\u771f\u5b9e\u7528\u6237\u7684\u67e5\u8be2\u5f80\u5f80\u975e\u6b63\u5f0f\u4e14\u4fe1\u606f\u4e0d\u8db3\uff0c\u4f9d\u8d56\u56fe\u50cf\u4f20\u9012\u4e0a\u4e0b\u6587\u3002\u8fd9\u79cd\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5374\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u573a\u666f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6784\u5efa\u66f4\u8d34\u8fd1\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u4ee5\u8861\u91cf\u548c\u63d0\u5347\u6a21\u578b\u5728\u6a21\u7cca\u67e5\u8be2\u4e0b\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u4ece\u97e9\u56fd\u5728\u7ebf\u793e\u533a\u6536\u96c6\u4e8686,000\u4e2a\u5019\u9009\u89c6\u89c9\u95ee\u9898\uff0c\u7ecf\u8fc7\u4e25\u683c\u7b5b\u9009\u4fdd\u7559653\u4e2a\u771f\u5b9e\u7528\u6237\u67e5\u8be2\uff08\u7559\u5b58\u73870.76%\uff09\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u539f\u59cb\u6a21\u7cca\u67e5\u8be2\u63d0\u4f9b\u4e00\u4e2a\u4eba\u5de5\u660e\u786e\u6539\u5199\u7248\u672c\uff0c\u6784\u6210HAERAE-Vision\u57fa\u51c6\uff0c\u603b\u8ba11,306\u4e2a\u67e5\u8be2\u53d8\u4f53\u3002\u968f\u540e\u5728\u8be5\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e8639\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecGPT-5\u3001Gemini 2.5 Pro\u7b49\uff09\uff0c\u6bd4\u8f83\u5176\u5728\u539f\u59cb\u6a21\u7cca\u67e5\u8be2\u4e0e\u660e\u786e\u6539\u5199\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7a76\u7ed3\u5408\u7f51\u7edc\u641c\u7d22\u662f\u5426\u80fd\u5f25\u8865\u6a21\u7cca\u6027\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684VLM\u5728\u539f\u59cb\u6a21\u7cca\u67e5\u8be2\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e50%\uff1b\u4ec5\u901a\u8fc7\u5c06\u67e5\u8be2\u660e\u786e\u5316\uff0c\u6a21\u578b\u6027\u80fd\u53ef\u63d0\u53478\u81f322\u4e2a\u767e\u5206\u70b9\uff0c\u5176\u4e2d\u8f83\u5c0f\u6a21\u578b\u63d0\u5347\u66f4\u4e3a\u663e\u8457\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5f15\u5165\u7f51\u7edc\u641c\u7d22\uff0c\u6a21\u7cca\u67e5\u8be2\u7684\u8868\u73b0\u4ecd\u4e0d\u5982\u672a\u4f7f\u7528\u641c\u7d22\u7684\u660e\u786e\u67e5\u8be2\uff0c\u8bf4\u660e\u73b0\u6709\u68c0\u7d22\u6280\u672f\u65e0\u6cd5\u6709\u6548\u8865\u507f\u7528\u6237\u672a\u660e\u8bf4\u7684\u4fe1\u606f\u3002", "conclusion": "VLM\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u5e76\u975e\u6a21\u578b\u672c\u8eab\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u662f\u6e90\u4e8e\u7528\u6237\u81ea\u7136\u67e5\u8be2\u7684\u6a21\u7cca\u6027\u548c\u4fe1\u606f\u7f3a\u5931\u3002\u5f53\u524d\u4e3b\u6d41\u57fa\u51c6\u672a\u80fd\u53cd\u6620\u8fd9\u4e00\u5173\u952e\u95ee\u9898\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0e\u5b9e\u9645\u90e8\u7f72\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u672a\u6765\u7814\u7a76\u5e94\u66f4\u5173\u6ce8\u5bf9\u6a21\u7cca\u67e5\u8be2\u7684\u7406\u89e3\u4e0e\u8865\u5168\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002", "summary_cn": "\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u4e3b\u8981\u5305\u542b\u7ed3\u6784\u826f\u597d\u3001\u63d0\u793a\u6e05\u6670\u660e\u786e\u7684\u95ee\u9898\u3002\u7136\u800c\uff0c\u771f\u5b9e\u7528\u6237\u7684\u67e5\u8be2\u5f80\u5f80\u662f\u975e\u6b63\u5f0f\u4e14\u4fe1\u606f\u4e0d\u8db3\u7684\uff0c\u7528\u6237\u81ea\u7136\u5730\u7701\u7565\u5927\u91cf\u4fe1\u606f\uff0c\u4f9d\u8d56\u56fe\u50cf\u6765\u4f20\u8fbe\u4e0a\u4e0b\u6587\u3002\u6211\u4eec\u63d0\u51fa\u4e86HAERAE-Vision\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b653\u4e2a\u6765\u81ea\u97e9\u56fd\u5728\u7ebf\u793e\u533a\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u95ee\u9898\u7684\u57fa\u51c6\uff08\u4ece8.6\u4e07\u4e2a\u5019\u9009\u95ee\u9898\u4e2d\u7b5b\u9009\u51fa\uff0c\u7559\u5b58\u7387\u4ec5\u4e3a0.76%\uff09\uff0c\u6bcf\u4e2a\u95ee\u9898\u5747\u914d\u6709\u4e00\u4e2a\u660e\u786e\u7684\u6539\u5199\u7248\u672c\uff0c\u603b\u5171\u5f62\u62101,306\u4e2a\u67e5\u8be2\u53d8\u4f53\u3002\u5728\u5bf939\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u8bc4\u4f30\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5982GPT-5\u3001Gemini 2.5 Pro\uff09\u5728\u539f\u59cb\u67e5\u8be2\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e50%\u3002\u5173\u952e\u7684\u662f\uff0c\u4ec5\u901a\u8fc7\u5c06\u67e5\u8be2\u660e\u786e\u5316\uff0c\u6a21\u578b\u6027\u80fd\u5c31\u80fd\u63d0\u53478\u523022\u4e2a\u767e\u5206\u70b9\uff0c\u5176\u4e2d\u8f83\u5c0f\u7684\u6a21\u578b\u53d7\u76ca\u6700\u4e3a\u660e\u663e\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u5373\u4f7f\u7ed3\u5408\u7f51\u7edc\u641c\u7d22\uff0c\u4fe1\u606f\u4e0d\u8db3\u7684\u67e5\u8be2\u8868\u73b0\u4ecd\u4e0d\u5982\u4e0d\u4f7f\u7528\u641c\u7d22\u7684\u660e\u786e\u67e5\u8be2\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u7684\u68c0\u7d22\u6280\u672f\u65e0\u6cd5\u5f25\u8865\u7528\u6237\u672a\u8a00\u660e\u7684\u4fe1\u606f\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cVLM\u9762\u4e34\u7684\u5f88\u5927\u4e00\u90e8\u5206\u56f0\u96be\u6e90\u4e8e\u81ea\u7136\u67e5\u8be2\u7684\u4fe1\u606f\u4e0d\u8db3\uff0c\u800c\u975e\u6a21\u578b\u672c\u8eab\u7684\u80fd\u529b\u9650\u5236\uff0c\u8fd9\u7a81\u663e\u4e86\u5f53\u524d\u57fa\u51c6\u8bc4\u4f30\u4e0e\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u7684\u5173\u952e\u5dee\u8ddd\u3002"}}
{"id": "2601.06166", "pdf": "https://arxiv.org/pdf/2601.06166", "abs": "https://arxiv.org/abs/2601.06166", "authors": ["Di Xu", "Hengjie Liu", "Yang Yang", "Mary Feng", "Jin Ning", "Xin Miao", "Jessica E. Scholey", "Alexandra E. Hotca-cho", "William C. Chen", "Michael Ohliger", "Martina Descovich", "Huiming Dong", "Wensha Yang", "Ke Sheng"], "title": "B-FIRE: Binning-Free Diffusion Implicit Neural Representation for Hyper-Accelerated Motion-Resolved MRI", "categories": ["cs.CV"], "comment": null, "summary": "Accelerated dynamic volumetric magnetic resonance imaging (4DMRI) is essential for applications relying on motion resolution. Existing 4DMRI produces acceptable artifacts of averaged breathing phases, which can blur and misrepresent instantaneous dynamic information. Recovery of such information requires a new paradigm to reconstruct extremely undersampled non-Cartesian k-space data. We propose B-FIRE, a binning-free diffusion implicit neural representation framework for hyper-accelerated MR reconstruction capable of reflecting instantaneous 3D abdominal anatomy. B-FIRE employs a CNN-INR encoder-decoder backbone optimized using diffusion with a comprehensive loss that enforces image-domain fidelity and frequency-aware constraints. Motion binned image pairs were used as training references, while inference was performed on binning-free undersampled data. Experiments were conducted on a T1-weighted StarVIBE liver MRI cohort, with accelerations ranging from 8 spokes per frame (RV8) to RV1. B-FIRE was compared against direct NuFFT, GRASP-CS, and an unrolled CNN method. Reconstruction fidelity, motion trajectory consistency, and inference latency were evaluated.", "AI": {"tldr": "B-FIRE\u662f\u4e00\u79cd\u65e0\u9700\u547c\u5438\u76f8\u4f4d\u5206\u7bb1\u7684\u6269\u6563\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8d85\u52a0\u901f4D MRI\u91cd\u5efa\uff0c\u80fd\u51c6\u786e\u6062\u590d\u77ac\u65f6\u8179\u90e83D\u89e3\u5256\u7ed3\u6784\u3002", "motivation": "\u73b0\u67094DMRI\u5728\u5904\u7406\u547c\u5438\u8fd0\u52a8\u65f6\u4f1a\u4ea7\u751f\u5e73\u5747\u76f8\u4f4d\u4f2a\u5f71\uff0c\u6a21\u7cca\u5e76\u9519\u8bef\u8868\u8fbe\u77ac\u65f6\u52a8\u6001\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u6765\u91cd\u5efa\u6781\u5ea6\u6b20\u91c7\u6837\u7684\u975e\u7b1b\u5361\u5c14k\u7a7a\u95f4\u6570\u636e\u3002", "method": "\u63d0\u51faB-FIRE\u6846\u67b6\uff0c\u91c7\u7528CNN-INR\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u7ed3\u5408\u6269\u6563\u4f18\u5316\u548c\u5305\u542b\u56fe\u50cf\u57df\u4fdd\u771f\u5ea6\u4e0e\u9891\u7387\u611f\u77e5\u7ea6\u675f\u7684\u7efc\u5408\u635f\u5931\u51fd\u6570\uff1b\u8bad\u7ec3\u4f7f\u7528\u5206\u7bb1\u540e\u7684\u56fe\u50cf\u5bf9\uff0c\u63a8\u7406\u5219\u76f4\u63a5\u5904\u7406\u65e0\u5206\u7bb1\u7684\u6b20\u91c7\u6837\u6570\u636e\u3002", "result": "\u5728T1\u52a0\u6743StarVIBE\u809d\u810fMRI\u6570\u636e\u96c6\u4e0a\uff08\u52a0\u901f\u500d\u6570\u4eceRV8\u5230RV1\uff09\uff0cB-FIRE\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u8fd0\u52a8\u8f68\u8ff9\u4e00\u81f4\u6027\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8eNuFFT\u3001GRASP-CS\u548c\u5c55\u5f00\u5f0fCNN\u65b9\u6cd5\u3002", "conclusion": "B-FIRE\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u9ad8\u52a0\u901f\u6bd4\u4e0b\u7684\u9ad8\u8d28\u91cf4D MRI\u91cd\u5efa\uff0c\u51c6\u786e\u53cd\u6620\u77ac\u65f6\u52a8\u6001\u89e3\u5256\u4fe1\u606f\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "summary_cn": "\u52a0\u901f\u52a8\u6001\u5bb9\u79ef\u78c1\u5171\u632f\u6210\u50cf\uff084DMRI\uff09\u5bf9\u4e8e\u4f9d\u8d56\u8fd0\u52a8\u5206\u8fa8\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u76844DMRI\u5728\u547c\u5438\u76f8\u4f4d\u5e73\u5747\u8fc7\u7a0b\u4e2d\u4f1a\u4ea7\u751f\u53ef\u63a5\u53d7\u4f46\u6709\u5bb3\u7684\u4f2a\u5f71\uff0c\u4ece\u800c\u6a21\u7cca\u5e76\u9519\u8bef\u8868\u8fbe\u77ac\u65f6\u52a8\u6001\u4fe1\u606f\u3002\u8981\u6062\u590d\u6b64\u7c7b\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u8303\u5f0f\u6765\u91cd\u5efa\u6781\u5ea6\u6b20\u91c7\u6837\u7684\u975e\u7b1b\u5361\u5c14k\u7a7a\u95f4\u6570\u636e\u3002\u6211\u4eec\u63d0\u51fa\u4e86B-FIRE\u2014\u2014\u4e00\u79cd\u65e0\u9700\u5206\u7bb1\u7684\u6269\u6563\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u52a0\u901f\u78c1\u5171\u632f\u91cd\u5efa\uff0c\u80fd\u591f\u53cd\u6620\u77ac\u65f6\u4e09\u7ef4\u8179\u90e8\u89e3\u5256\u7ed3\u6784\u3002B-FIRE\u91c7\u7528CNN-INR\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4e3b\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u4e86\u5f3a\u5236\u56fe\u50cf\u57df\u4fdd\u771f\u5ea6\u548c\u9891\u7387\u611f\u77e5\u7ea6\u675f\u7684\u7efc\u5408\u635f\u5931\u51fd\u6570\u3002\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u7ecf\u8fc7\u8fd0\u52a8\u5206\u7bb1\u7684\u56fe\u50cf\u5bf9\u4f5c\u4e3a\u53c2\u8003\uff0c\u800c\u63a8\u7406\u9636\u6bb5\u5219\u76f4\u63a5\u5904\u7406\u65e0\u5206\u7bb1\u7684\u6b20\u91c7\u6837\u6570\u636e\u3002\u5b9e\u9a8c\u5728T1\u52a0\u6743StarVIBE\u809d\u810fMRI\u961f\u5217\u4e0a\u8fdb\u884c\uff0c\u52a0\u901f\u500d\u6570\u4ece\u6bcf\u5e278\u6761\u5c04\u7ebf\uff08RV8\uff09\u5230RV1\u4e0d\u7b49\u3002B-FIRE\u4e0e\u76f4\u63a5NuFFT\u3001GRASP-CS\u4ee5\u53ca\u4e00\u79cd\u5c55\u5f00\u5f0fCNN\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u8fd0\u52a8\u8f68\u8ff9\u4e00\u81f4\u6027\u53ca\u63a8\u7406\u5ef6\u8fdf\u7b49\u65b9\u9762\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002"}}
{"id": "2601.06168", "pdf": "https://arxiv.org/pdf/2601.06168", "abs": "https://arxiv.org/abs/2601.06168", "authors": ["Jyotiraditya Gupta"], "title": "Analyzing the Structure of Handwritten Digits: A Comparative Study of PCA, Factor Analysis, and UMAP", "categories": ["cs.CV"], "comment": "15 pages, 12 figures", "summary": "Handwritten digit images lie in a high-dimensional pixel space but exhibit strong geometric and statistical structure. This paper investigates the latent organization of handwritten digits in the MNIST dataset using three complementary dimensionality reduction techniques: Principal Component Analysis (PCA), Factor Analysis (FA), and Uniform Manifold Approximation and Projection (UMAP). Rather than focusing on classification accuracy, we study how each method characterizes intrinsic dimensionality, shared variation, and nonlinear geometry. PCA reveals dominant global variance directions and enables high-fidelity reconstructions using a small number of components. FA decomposes digits into interpretable latent handwriting primitives corresponding to strokes, loops, and symmetry. UMAP uncovers nonlinear manifolds that reflect smooth stylistic transitions between digit classes. Together, these results demonstrate that handwritten digits occupy a structured low-dimensional manifold and that different statistical frameworks expose complementary aspects of this structure.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PCA\u3001\u56e0\u5b50\u5206\u6790\u548cUMAP\u4e09\u79cd\u964d\u7ef4\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86MNIST\u624b\u5199\u6570\u5b57\u5728\u9ad8\u7ef4\u50cf\u7d20\u7a7a\u95f4\u4e2d\u5b9e\u9645\u5b58\u5728\u4e8e\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4f4e\u7ef4\u6d41\u5f62\u4e0a\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5982\u4f55\u4ece\u4e0d\u540c\u89d2\u5ea6\u523b\u753b\u5176\u5185\u5728\u7ed3\u6784\u3002", "motivation": "\u624b\u5199\u6570\u5b57\u56fe\u50cf\u867d\u5904\u4e8e\u9ad8\u7ef4\u50cf\u7d20\u7a7a\u95f4\uff0c\u4f46\u5177\u6709\u660e\u663e\u7684\u51e0\u4f55\u4e0e\u7edf\u8ba1\u7ed3\u6784\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u5176\u6f5c\u5728\u7684\u4f4e\u7ef4\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u800c\u975e\u8ffd\u6c42\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u4e92\u8865\u7684\u964d\u7ef4\u6280\u672f\uff1a\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3001\u56e0\u5b50\u5206\u6790\uff08FA\uff09\u548c\u5747\u5300\u6d41\u5f62\u8fd1\u4f3c\u4e0e\u6295\u5f71\uff08UMAP\uff09\uff0c\u5206\u522b\u4ece\u5168\u5c40\u65b9\u5dee\u3001\u53ef\u89e3\u91ca\u6f5c\u53d8\u91cf\u548c\u975e\u7ebf\u6027\u6d41\u5f62\u89d2\u5ea6\u5206\u6790MNIST\u6570\u636e\u96c6\u3002", "result": "PCA\u80fd\u7528\u5c11\u91cf\u4e3b\u6210\u5206\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\uff1bFA\u63d0\u53d6\u51fa\u5bf9\u5e94\u7b14\u753b\u3001\u73af\u8def\u548c\u5bf9\u79f0\u6027\u7684\u53ef\u89e3\u91ca\u6f5c\u53d8\u91cf\uff1bUMAP\u63ed\u793a\u4e86\u6570\u5b57\u7c7b\u522b\u95f4\u5e73\u6ed1\u98ce\u683c\u8fc7\u6e21\u7684\u975e\u7ebf\u6027\u6d41\u5f62\u3002", "conclusion": "\u624b\u5199\u6570\u5b57\u5360\u636e\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4f4e\u7ef4\u6d41\u5f62\uff0c\u4e0d\u540c\u7edf\u8ba1\u6846\u67b6\u80fd\u63ed\u793a\u8be5\u7ed3\u6784\u7684\u4e0d\u540c\u4e92\u8865\u65b9\u9762\u3002", "summary_cn": "\u624b\u5199\u6570\u5b57\u56fe\u50cf\u4f4d\u4e8e\u9ad8\u7ef4\u50cf\u7d20\u7a7a\u95f4\u4e2d\uff0c\u4f46\u5c55\u73b0\u51fa\u5f3a\u70c8\u7684\u51e0\u4f55\u4e0e\u7edf\u8ba1\u7ed3\u6784\u3002\u672c\u6587\u5229\u7528\u4e09\u79cd\u4e92\u8865\u7684\u964d\u7ef4\u6280\u672f\u2014\u2014\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u3001\u56e0\u5b50\u5206\u6790\uff08FA\uff09\u548c\u5747\u5300\u6d41\u5f62\u8fd1\u4f3c\u4e0e\u6295\u5f71\uff08UMAP\uff09\u2014\u2014\u7814\u7a76\u4e86MNIST\u6570\u636e\u96c6\u4e2d\u624b\u5199\u6570\u5b57\u7684\u6f5c\u5728\u7ec4\u7ec7\u65b9\u5f0f\u3002\u7814\u7a76\u91cd\u70b9\u5e76\u975e\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u800c\u662f\u8003\u5bdf\u6bcf\u79cd\u65b9\u6cd5\u5982\u4f55\u523b\u753b\u6570\u636e\u7684\u672c\u5f81\u7ef4\u5ea6\u3001\u5171\u4eab\u53d8\u5f02\u6027\u548c\u975e\u7ebf\u6027\u51e0\u4f55\u7279\u6027\u3002PCA\u63ed\u793a\u4e86\u4e3b\u5bfc\u7684\u5168\u5c40\u65b9\u5dee\u65b9\u5411\uff0c\u5e76\u80fd\u4f7f\u7528\u5c11\u91cf\u6210\u5206\u5b9e\u73b0\u9ad8\u4fdd\u771f\u91cd\u5efa\uff1bFA\u5c06\u6570\u5b57\u5206\u89e3\u4e3a\u5bf9\u5e94\u4e8e\u7b14\u753b\u3001\u73af\u8def\u548c\u5bf9\u79f0\u6027\u7684\u53ef\u89e3\u91ca\u6f5c\u53d8\u91cf\uff1bUMAP\u5219\u63ed\u793a\u4e86\u53cd\u6620\u6570\u5b57\u7c7b\u522b\u4e4b\u95f4\u5e73\u6ed1\u98ce\u683c\u8fc7\u6e21\u7684\u975e\u7ebf\u6027\u6d41\u5f62\u3002\u7efc\u5408\u7ed3\u679c\u8868\u660e\uff0c\u624b\u5199\u6570\u5b57\u5b9e\u9645\u4e0a\u5360\u636e\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4f4e\u7ef4\u6d41\u5f62\uff0c\u800c\u4e0d\u540c\u7684\u7edf\u8ba1\u6846\u67b6\u80fd\u591f\u63ed\u793a\u8fd9\u4e00\u7ed3\u6784\u7684\u4e92\u8865\u65b9\u9762\u3002"}}
{"id": "2601.06169", "pdf": "https://arxiv.org/pdf/2601.06169", "abs": "https://arxiv.org/abs/2601.06169", "authors": ["Zhiyong Ma", "Zhenpeng Li", "Yuanjie Shi", "Zhengping Li", "Jiahao Chen", "Qingyuan Chuai"], "title": "Think Bright, Diffuse Nice: Enhancing T2I-ICL via Inductive-Bias Hint Instruction and Query Contrastive Decoding", "categories": ["cs.CV"], "comment": "Submitted to ACL 2026", "summary": "Text-to-Image In-Context Learning (T2I-ICL) enables customized image synthesis via interleaved text-image examples but faces two mutually reinforcing bottlenecks, compliance failure and prior-dominated hallucination, that form a vicious cycle degrading generation quality. Existing methods rely on tailored training, which limits flexibility and raises deployment costs. To address these challenges effectively, we propose TBDN, a training-free framework integrating two complementary closed-loop mechanisms: Hint Instruction (HI) and Query Contrastive Decoding (QCD). HI injects task-aware inductive bias via lightweight prompt engineering to anchor models on contextual mapping rules, thereby mitigating compliance failure. QCD adjusts the decoding distributions of language models by contrasting full-input and query-omitted distributions, suppressing prior-dominated hallucination. TBDN achieves State-of-the-Art performance on CoBSAT and Text-to-Image Fast Mini-ImageNet, with robust generalization across model backbones, prompt designs, and hyperparameters. It also maintains promising performance in concept preservation and prompt following on Dreambench++. By breaking the two bottlenecks, TBDN establishes a simple yet effective framework for efficient and reliable T2I-ICL.", "AI": {"tldr": "TBDN \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08T2I-ICL\uff09\u6846\u67b6\uff0c\u901a\u8fc7 Hint Instruction \u548c Query Contrastive Decoding \u4e24\u79cd\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5408\u89c4\u5931\u8d25\u548c\u5148\u9a8c\u4e3b\u5bfc\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230 SOTA \u6027\u80fd\u3002", "motivation": "\u73b0\u6709 T2I-ICL \u65b9\u6cd5\u9762\u4e34\u5408\u89c4\u5931\u8d25\u4e0e\u5148\u9a8c\u4e3b\u5bfc\u5e7b\u89c9\u76f8\u4e92\u52a0\u5267\u7684\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u7279\u5b9a\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u5e76\u589e\u52a0\u90e8\u7f72\u6210\u672c\u3002", "method": "\u63d0\u51fa TBDN \u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7684\u95ed\u73af\u673a\u5236\uff1aHint Instruction\uff08HI\uff09\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63d0\u793a\u5de5\u7a0b\u6ce8\u5165\u4efb\u52a1\u611f\u77e5\u5f52\u7eb3\u504f\u7f6e\uff0c\u7f13\u89e3\u5408\u89c4\u5931\u8d25\uff1bQuery Contrastive Decoding\uff08QCD\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b8c\u6574\u8f93\u5165\u4e0e\u7701\u7565\u67e5\u8be2\u7684\u5206\u5e03\uff0c\u6291\u5236\u5148\u9a8c\u4e3b\u5bfc\u5e7b\u89c9\u3002", "result": "TBDN \u5728 CoBSAT \u548c Text-to-Image Fast Mini-ImageNet \u4e0a\u8fbe\u5230 SOTA\uff0c\u5bf9\u6a21\u578b\u4e3b\u5e72\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u8d85\u53c2\u6570\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728 Dreambench++ \u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6982\u5ff5\u4fdd\u7559\u548c\u63d0\u793a\u8ddf\u968f\u80fd\u529b\u3002", "conclusion": "TBDN \u901a\u8fc7\u6253\u7834\u4e24\u5927\u74f6\u9888\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3 T2I-ICL \u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u4e14\u7075\u6d3b\u7684\u5b9a\u5236\u56fe\u50cf\u751f\u6210\u3002", "summary_cn": "\u6587\u672c\u5230\u56fe\u50cf\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08T2I-ICL\uff09\u901a\u8fc7\u4ea4\u9519\u7684\u6587\u672c-\u56fe\u50cf\u793a\u4f8b\u5b9e\u73b0\u5b9a\u5236\u5316\u56fe\u50cf\u5408\u6210\uff0c\u4f46\u9762\u4e34\u5408\u89c4\u5931\u8d25\u4e0e\u5148\u9a8c\u4e3b\u5bfc\u5e7b\u89c9\u8fd9\u4e24\u4e2a\u76f8\u4e92\u52a0\u5267\u7684\u74f6\u9888\uff0c\u5f62\u6210\u6076\u6027\u5faa\u73af\uff0c\u964d\u4f4e\u751f\u6210\u8d28\u91cf\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e13\u95e8\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u5e76\u63d0\u9ad8\u4e86\u90e8\u7f72\u6210\u672c\u3002\u4e3a\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TBDN\u2014\u2014\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u6574\u5408\u4e86\u4e24\u4e2a\u4e92\u8865\u7684\u95ed\u73af\u673a\u5236\uff1a\u63d0\u793a\u6307\u4ee4\uff08Hint Instruction, HI\uff09\u548c\u67e5\u8be2\u5bf9\u6bd4\u89e3\u7801\uff08Query Contrastive Decoding, QCD\uff09\u3002HI \u901a\u8fc7\u8f7b\u91cf\u7ea7\u63d0\u793a\u5de5\u7a0b\u6ce8\u5165\u4efb\u52a1\u611f\u77e5\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f7f\u6a21\u578b\u805a\u7126\u4e8e\u4e0a\u4e0b\u6587\u6620\u5c04\u89c4\u5219\uff0c\u4ece\u800c\u7f13\u89e3\u5408\u89c4\u5931\u8d25\uff1bQCD \u901a\u8fc7\u5bf9\u6bd4\u5b8c\u6574\u8f93\u5165\u4e0e\u7701\u7565\u67e5\u8be2\u6761\u4ef6\u4e0b\u7684\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u5206\u5e03\uff0c\u6291\u5236\u5148\u9a8c\u4e3b\u5bfc\u7684\u5e7b\u89c9\u3002TBDN \u5728 CoBSAT \u548c Text-to-Image Fast Mini-ImageNet \u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u4e3b\u5e72\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5728 Dreambench++ \u4e0a\uff0c\u5b83\u5728\u6982\u5ff5\u4fdd\u7559\u548c\u63d0\u793a\u8ddf\u968f\u65b9\u9762\u4e5f\u4fdd\u6301\u4e86\u4f18\u5f02\u8868\u73b0\u3002\u901a\u8fc7\u6253\u7834\u4e0a\u8ff0\u4e24\u5927\u74f6\u9888\uff0cTBDN \u5efa\u7acb\u4e86\u4e00\u4e2a\u7b80\u6d01\u800c\u9ad8\u6548\u7684 T2I-ICL \u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u5b9a\u5236\u56fe\u50cf\u751f\u6210\u3002"}}
