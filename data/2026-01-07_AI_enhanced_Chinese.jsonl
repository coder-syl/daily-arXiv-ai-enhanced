{"id": "2601.02392", "pdf": "https://arxiv.org/pdf/2601.02392", "abs": "https://arxiv.org/abs/2601.02392", "authors": ["Mo Chen"], "title": "Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, in Chinese language, 2 figures", "summary": "Coronary calcification creates blooming artifacts in Computed Tomography Angiography (CTA), severely hampering the diagnosis of lumen stenosis. While Deep Convolutional Neural Networks (DCNNs) like Dense-Unet have shown promise in removing these artifacts via inpainting, they often require large labeled datasets which are scarce in the medical domain. Inspired by recent advancements in Masked Autoencoders (MAE) for 3D point clouds, we propose \\textbf{Dense-MAE}, a novel self-supervised learning framework for volumetric medical data. We introduce a pre-training strategy that randomly masks 3D patches of the vessel lumen and trains the Dense-Unet to reconstruct the missing geometry. This forces the encoder to learn high-level latent features of arterial topology without human annotation. Experimental results on clinical CTA datasets demonstrate that initializing the Calcium Removal network with our MAE-based weights significantly improves inpainting accuracy and stenosis estimation compared to training from scratch, specifically in few-shot scenarios.", "AI": {"tldr": "\u63d0\u51faDense-MAE\uff0c\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u51a0\u72b6\u52a8\u8109\u9499\u5316\u4f2a\u5f71\u53bb\u9664\u548c\u7ba1\u8154\u72ed\u7a84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u51a0\u72b6\u52a8\u8109\u9499\u5316\u5728CTA\u4e2d\u4ea7\u751fblooming\u4f2a\u5f71\uff0c\u4e25\u91cd\u5f71\u54cd\u7ba1\u8154\u72ed\u7a84\u8bca\u65ad\uff1b\u73b0\u6709\u57fa\u4e8eDCNN\uff08\u5982Dense-Unet\uff09\u7684\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u533b\u5b66\u9886\u57df\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u53d73D\u70b9\u4e91Masked Autoencoder\uff08MAE\uff09\u542f\u53d1\uff0c\u63d0\u51faDense-MAE\uff1a\u901a\u8fc7\u968f\u673a\u63a9\u7801\u8840\u7ba1\u7ba1\u8154\u76843D\u5757\uff0c\u5e76\u8bad\u7ec3Dense-Unet\u91cd\u5efa\u7f3a\u5931\u51e0\u4f55\u7ed3\u6784\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u4e34\u5e8aCTA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528MAE\u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u9499\u5316\u53bb\u9664\u7f51\u7edc\uff0c\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u4ece\u96f6\u8bad\u7ec3\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u4fee\u590d\u7cbe\u5ea6\u548c\u72ed\u7a84\u7a0b\u5ea6\u4f30\u8ba1\u6548\u679c\u3002", "conclusion": "Dense-MAE\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6709\u6548\u7f13\u89e3\u533b\u5b66\u56fe\u50cf\u4e2d\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u51a0\u72b6\u52a8\u8109\u9499\u5316\u4f2a\u5f71\u53bb\u9664\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002", "summary_cn": "\u51a0\u72b6\u52a8\u8109\u9499\u5316\u5728\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u8840\u7ba1\u9020\u5f71\uff08CTA\uff09\u4e2d\u4f1a\u4ea7\u751fblooming\u4f2a\u5f71\uff0c\u4e25\u91cd\u963b\u788d\u7ba1\u8154\u72ed\u7a84\u7684\u8bca\u65ad\u3002\u5c3d\u7ba1Dense-Unet\u7b49\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08DCNN\uff09\u5df2\u901a\u8fc7\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u53bb\u9664\u6b64\u7c7b\u4f2a\u5f71\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u8fd9\u7c7b\u6570\u636e\u5728\u533b\u5b66\u9886\u57df\u5341\u5206\u7a00\u7f3a\u3002\u53d7\u8fd1\u671f3D\u70b9\u4e91\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Dense-MAE\u2014\u2014\u4e00\u79cd\u9762\u5411\u4f53\u7d20\u533b\u5b66\u6570\u636e\u7684\u65b0\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3\u7b56\u7565\uff1a\u968f\u673a\u63a9\u7801\u8840\u7ba1\u7ba1\u8154\u76843D\u56fe\u50cf\u5757\uff0c\u5e76\u8bad\u7ec3Dense-Unet\u91cd\u5efa\u7f3a\u5931\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4ece\u800c\u8feb\u4f7f\u7f16\u7801\u5668\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u52a8\u8109\u62d3\u6251\u7684\u9ad8\u5c42\u6f5c\u5728\u7279\u5f81\u3002\u5728\u4e34\u5e8aCTA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u76f8\u6bd4\uff0c\u4f7f\u7528\u6211\u4eec\u57fa\u4e8eMAE\u7684\u6743\u91cd\u521d\u59cb\u5316\u9499\u5316\u53bb\u9664\u7f51\u7edc\u80fd\u663e\u8457\u63d0\u5347\u56fe\u50cf\u4fee\u590d\u7cbe\u5ea6\u548c\u72ed\u7a84\u7a0b\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2601.02409", "pdf": "https://arxiv.org/pdf/2601.02409", "abs": "https://arxiv.org/abs/2601.02409", "authors": ["Longwei Wang", "Ifrat Ikhtear Uddin", "KC Santosh"], "title": "Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted for publication in IEEE Journal of Biomedical and Health Informatics, 2025", "summary": "Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\\%, 76\\%, and 62\\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\\% accuracy with only 680 samples versus 57\\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEGxFSL\u548cxGAL\u53cc\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u5bb6\u5f15\u5bfc\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u5c11\u6837\u672c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u540c\u65f6\u63d0\u5347\u6027\u80fd\u4e0e\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e24\u5927\u6311\u6218\uff0c\u963b\u788d\u4e86\u4e34\u5e8aAI\u90e8\u7f72\u3002\u73b0\u6709\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u9884\u6d4b\u900f\u660e\u5ea6\uff0c\u800c\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u6240\u9009\u6837\u672c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6846\u67b6\uff1a1\uff09EGxFSL\uff0c\u5229\u7528\u653e\u5c04\u79d1\u533b\u751f\u5b9a\u4e49\u7684\u5174\u8da3\u533a\u57df\u4f5c\u4e3a\u7a7a\u95f4\u76d1\u7763\uff0c\u901a\u8fc7\u57fa\u4e8eGrad-CAM\u7684Dice\u635f\u5931\u4e0e\u539f\u578b\u5206\u7c7b\u8054\u5408\u4f18\u5316\uff1b2\uff09xGAL\uff0c\u8fed\u4ee3\u9009\u62e9\u6837\u672c\u65f6\u517c\u987e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u6ce8\u610f\u529b\u9519\u4f4d\uff0c\u5f62\u6210\u53ef\u89e3\u91ca\u6027\u5f15\u5bfc\u8bad\u7ec3\u4e0e\u91c7\u6837\u7684\u95ed\u73af\u3002", "result": "\u5728BraTS\u3001VinDr-CXR\u548cSIIM-COVID-19\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523092%\u300176%\u548c62%\u7684\u51c6\u786e\u7387\uff0c\u5747\u4f18\u4e8e\u975e\u5f15\u5bfc\u57fa\u7ebf\u3002\u5728\u6781\u4f4e\u6570\u636e\u91cf\u4e0b\uff08\u4ec5680\u6837\u672c\uff09\uff0cxGAL\u8fbe76%\u51c6\u786e\u7387\uff0c\u8fdc\u8d85\u968f\u673a\u91c7\u6837\u768457%\u3002Grad-CAM\u53ef\u89c6\u5316\u663e\u793a\u6a21\u578b\u805a\u7126\u4e8e\u8bca\u65ad\u76f8\u5173\u533a\u57df\uff0c\u5e76\u5728\u4e73\u817a\u8d85\u58f0\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5c06\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u53ef\u89e3\u91ca\u6027\u673a\u5236\u878d\u5408\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5c11\u6837\u672c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u4e34\u5e8aAI\u90e8\u7f72\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002", "summary_cn": "\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u4e24\u8005\u90fd\u963b\u788d\u4e86\u4e34\u5e8aAI\u7684\u90e8\u7f72\u3002\u5c11\u6837\u672c\u5b66\u4e60\uff08FSL\uff09\u867d\u80fd\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u4f46\u5176\u9884\u6d4b\u7f3a\u4e4f\u900f\u660e\u5ea6\uff1b\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u65b9\u6cd5\u867d\u80fd\u4f18\u5316\u6570\u636e\u83b7\u53d6\uff0c\u5374\u5ffd\u89c6\u4e86\u6240\u83b7\u53d6\u6837\u672c\u7684\u53ef\u89e3\u91ca\u6027\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u6846\u67b6\u89e3\u51b3\u65b9\u6848\uff1a\u4e13\u5bb6\u5f15\u5bfc\u7684\u53ef\u89e3\u91ca\u5c11\u6837\u672c\u5b66\u4e60\uff08EGxFSL\uff09\u548c\u53ef\u89e3\u91ca\u6027\u5f15\u5bfc\u7684\u4e3b\u52a8\u5b66\u4e60\uff08xGAL\uff09\u3002EGxFSL\u901a\u8fc7\u57fa\u4e8eGrad-CAM\u7684Dice\u635f\u5931\uff0c\u5c06\u653e\u5c04\u79d1\u533b\u751f\u5b9a\u4e49\u7684\u5174\u8da3\u533a\u57df\u4f5c\u4e3a\u7a7a\u95f4\u76d1\u7763\u4fe1\u53f7\uff0c\u4e0e\u539f\u578b\u5206\u7c7b\u8054\u5408\u4f18\u5316\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5c11\u6837\u672c\u5b66\u4e60\u3002xGAL\u5219\u5728\u8fed\u4ee3\u6837\u672c\u83b7\u53d6\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8003\u8651\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e0e\u6ce8\u610f\u529b\u9519\u4f4d\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u6027\u534f\u540c\u6307\u5bfc\u8bad\u7ec3\u4e0e\u6837\u672c\u9009\u62e9\u7684\u95ed\u73af\u6846\u67b6\u3002\u5728BraTS\uff08MRI\uff09\u3001VinDr-CXR\uff08\u80f8\u90e8X\u5149\uff09\u548cSIIM-COVID-19\uff08\u80f8\u90e8X\u5149\uff09\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5206\u522b\u53d6\u5f97\u4e8692%\u300176%\u548c62%\u7684\u51c6\u786e\u7387\uff0c\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u6301\u7eed\u4f18\u4e8e\u975e\u5f15\u5bfc\u57fa\u7ebf\u3002\u5728\u6781\u7aef\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\uff0cxGAL\u4ec5\u7528680\u4e2a\u6837\u672c\u5c31\u8fbe\u5230\u4e8676%\u7684\u51c6\u786e\u7387\uff0c\u800c\u968f\u673a\u91c7\u6837\u4ec5\u4e3a57%\u3002Grad-CAM\u53ef\u89c6\u5316\u8868\u660e\uff0c\u5f15\u5bfc\u6a21\u578b\u805a\u7126\u4e8e\u5177\u6709\u8bca\u65ad\u610f\u4e49\u7684\u533a\u57df\uff0c\u5e76\u5728\u4e73\u817a\u8d85\u58f0\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u8de8\u6a21\u6001\u9002\u7528\u6027\u3002"}}
{"id": "2601.02414", "pdf": "https://arxiv.org/pdf/2601.02414", "abs": "https://arxiv.org/abs/2601.02414", "authors": ["Jichao Zhu", "Jun Yu"], "title": "MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5MIAR\uff0c\u901a\u8fc7\u6a21\u6001\u95f4\u4ea4\u4e92\u751f\u6210\u7279\u5f81token\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u5f52\u4e00\u5316\u7b56\u7565\u5bf9\u9f50\u6a21\u6001\uff0c\u5728CMU-MOSI\u548cCMU-MOSEI\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u5728\u6a21\u6001\u878d\u5408\u65f6\u672a\u80fd\u5145\u5206\u5904\u7406\u6a21\u6001\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4e5f\u672a\u8003\u8651\u5404\u6a21\u6001\u5bf9\u4efb\u52a1\u8d21\u732e\u7684\u4e0d\u5747\u8861\u6027\uff0c\u4e14\u5728\u4e0d\u540c\u6587\u672c\u6a21\u578b\u7279\u5f81\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51faModality Interaction and Alignment Representation\uff08MIAR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u4ea4\u4e92\u751f\u6210\u4ee3\u8868\u5168\u5c40\u4fe1\u606f\u7684\u7279\u5f81token\uff0c\u5e76\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u5f52\u4e00\u5316\u7b56\u7565\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\u3002", "result": "\u5728CMU-MOSI\u548cCMU-MOSEI\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMIAR\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u3002", "conclusion": "MIAR\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u5206\u5e03\u5dee\u5f02\u4e0e\u8d21\u732e\u4e0d\u5747\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "summary_cn": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\uff08MER\uff09\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u97f3\u9891\u4e09\u79cd\u6a21\u6001\u611f\u77e5\u4eba\u7c7b\u60c5\u611f\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u6001\u878d\u5408\uff0c\u5374\u672a\u80fd\u5145\u5206\u5904\u7406\u6a21\u6001\u4e4b\u95f4\u663e\u8457\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4e5f\u672a\u8003\u8651\u5404\u6a21\u6001\u5bf9\u4efb\u52a1\u7684\u4e0d\u540c\u8d21\u732e\uff0c\u540c\u65f6\u5728\u9762\u5bf9\u591a\u6837\u5316\u7684\u6587\u672c\u6a21\u578b\u7279\u5f81\u65f6\u7f3a\u4e4f\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6a21\u6001\u4ea4\u4e92\u4e0e\u5bf9\u9f50\u8868\u793a\u201d\uff08Modality Interaction and Alignment Representation, MIAR\uff09\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u7f51\u7edc\u901a\u8fc7\u8de8\u6a21\u6001\u7684\u7279\u5f81\u4ea4\u4e92\u751f\u6210\u7279\u5f81token\uff0c\u4ee5\u8868\u793a\u67d0\u4e00\u6a21\u6001\u4ece\u5176\u4ed6\u6a21\u6001\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u5168\u5c40\u8868\u5f81\uff1b\u8fd9\u56db\u4e2atoken\u5206\u522b\u4ee3\u8868\u6bcf\u4e2a\u6a21\u6001\u5982\u4f55\u4ece\u5176\u4ed6\u6a21\u6001\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u5168\u5c40\u8868\u793a\u3002MIAR\u8fd8\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u5f52\u4e00\u5316\u7b56\u7565\u6765\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\u3002\u6211\u4eec\u5728CMU-MOSI\u548cCMU-MOSEI\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eMIAR\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u3002"}}
{"id": "2601.02415", "pdf": "https://arxiv.org/pdf/2601.02415", "abs": "https://arxiv.org/abs/2601.02415", "authors": ["Wangyuan Zhu", "Jun Yu"], "title": "Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u901a\u9053\u7279\u5f81\u63d0\u53d6\u4e0e\u5bf9\u79f0\u4e92\u4fc3\uff08SMP\uff09\u8de8\u6a21\u6001\u878d\u5408\u673a\u5236\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u6001\u5185\u8868\u793a\u5e76\u4fc3\u8fdb\u6a21\u6001\u95f4\u4fe1\u606f\u4ea4\u4e92\uff0c\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u4e0a\u4e0d\u591f\u4e30\u5bcc\uff0c\u4e14\u5728\u8de8\u6a21\u6001\u878d\u5408\u65f6\u5ffd\u89c6\u4e86\u6a21\u6001\u95f4\u7279\u5f81\u5dee\u5f02\uff0c\u5bfc\u81f4\u878d\u5408\u6548\u679c\u53d7\u9650\u3002", "method": "\u9996\u5148\u5728\u89c6\u89c9\u548c\u542c\u89c9\u6a21\u6001\u4e2d\u5206\u522b\u91c7\u7528\u53cc\u901a\u9053\u7279\u5f81\u63d0\u53d6\u4ee5\u589e\u5f3a\u6a21\u6001\u5185\u8868\u793a\uff1b\u7136\u540e\u63d0\u51fa\u5bf9\u79f0\u4e92\u4fc3\uff08SMP\uff09\u8de8\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u79f0\u4ea4\u53c9\u6ce8\u610f\u529b\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4fc3\u8fdb\u6a21\u6001\u95f4\u6709\u7528\u4fe1\u606f\u4ea4\u6362\uff1b\u6700\u540e\u878d\u5408\u6a21\u6001\u5185\u4e0e\u6a21\u6001\u95f4\u7279\u5f81\uff0c\u517c\u987e\u4e92\u8865\u6027\u4e0e\u5dee\u5f02\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u66f4\u5168\u9762\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6709\u6548\u7684\u8de8\u6a21\u6001\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "summary_cn": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u662f\u4eba\u673a\u4ea4\u4e92\u4e0e\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7684\u5173\u952e\u6280\u672f\u3002\u51c6\u786e\u8bc6\u522b\u4eba\u7c7b\u60c5\u611f\u72b6\u6001\u5bf9\u4e8e\u5b9e\u73b0\u4eba\u4e0e\u673a\u5668\u4e4b\u95f4\u7684\u987a\u7545\u6c9f\u901a\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7814\u7a76\u5df2\u53d6\u5f97\u4e00\u5b9a\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002\u7b2c\u4e00\u4e2a\u6311\u6218\u662f\u5355\u6a21\u6001\u6570\u636e\u6240\u63d0\u53d6\u7684\u7279\u5f81\u6709\u9650\u4e14\u4e0d\u591f\u4e30\u5bcc\uff1b\u5176\u6b21\uff0c\u5927\u591a\u6570\u7814\u7a76\u4ec5\u5173\u6ce8\u6a21\u6001\u95f4\u7279\u5f81\u4fe1\u606f\u7684\u4e00\u81f4\u6027\uff0c\u5ffd\u7565\u4e86\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u7279\u5f81\u878d\u5408\u4e0d\u5145\u5206\u3002\u672c\u6587\u9996\u5148\u63d0\u53d6\u591a\u901a\u9053\u7279\u5f81\u4ee5\u83b7\u5f97\u66f4\u5168\u9762\u7684\u7279\u5f81\u4fe1\u606f\uff0c\u5728\u89c6\u89c9\u548c\u542c\u89c9\u6a21\u6001\u4e2d\u5747\u91c7\u7528\u53cc\u901a\u9053\u7279\u5f81\u4ee5\u589e\u5f3a\u6a21\u6001\u5185\u7279\u5f81\u8868\u793a\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e00\u79cd\u5bf9\u79f0\u4e92\u4fc3\uff08Symmetric Mutual Promotion, SMP\uff09\u7684\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u5bf9\u79f0\u4ea4\u53c9\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff1a\u4ea4\u53c9\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u7528\u4e8e\u6355\u6349\u5176\u4ed6\u6a21\u6001\u4e2d\u7684\u6709\u7528\u4fe1\u606f\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5219\u7528\u4e8e\u5efa\u6a21\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u4fc3\u8fdb\u6a21\u6001\u95f4\u6709\u7528\u4fe1\u606f\u7684\u4ea4\u6362\uff0c\u52a0\u5f3a\u6a21\u6001\u95f4\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u6a21\u6001\u5185\u7279\u5f81\u4e0e\u8de8\u6a21\u6001\u878d\u5408\u540e\u7684\u7279\u5f81\u8fdb\u884c\u6574\u5408\uff0c\u5728\u5145\u5206\u5229\u7528\u6a21\u6001\u95f4\u7279\u5f81\u4fe1\u606f\u4e92\u8865\u6027\u7684\u540c\u65f6\uff0c\u4e5f\u8003\u8651\u4e86\u7279\u5f81\u4fe1\u606f\u7684\u5dee\u5f02\u6027\u3002\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.02422", "pdf": "https://arxiv.org/pdf/2601.02422", "abs": "https://arxiv.org/abs/2601.02422", "authors": ["Wenting Lu", "Didi Zhu", "Tao Shen", "Donglin Zhu", "Ayong Ye", "Chao Wu"], "title": "Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.", "AI": {"tldr": "\u63d0\u51faCoCoT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u591a\u533a\u57df\u5b9a\u4f4d\u548c\u5173\u7cfb\u611f\u77e5\u63a8\u7406\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709Chain-of-Thought\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u573a\u666f\u4e2d\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4e00\u7c97\u7c92\u5ea6\u56fe\u50cf\u533a\u57df\uff0c\u4ee5\u53ca\u63a8\u7406\u6b65\u9aa4\u95f4\u8bed\u4e49\u5272\u88c2\u3002", "method": "\u63d0\u51faCoCoT\uff08\u534f\u4f5c\u5f0f\u8de8\u6a21\u6001\u601d\u7ef4\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u5173\u952e\u6280\u672f\uff1a1\uff09\u52a8\u6001\u591a\u533a\u57df\u5b9a\u4f4d\uff0c\u6839\u636e\u95ee\u9898\u81ea\u9002\u5e94\u68c0\u6d4b\u6700\u76f8\u5173\u56fe\u50cf\u533a\u57df\uff1b2\uff09\u5173\u7cfb\u611f\u77e5\u63a8\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u5bf9\u9f50\u89c6\u89c9\u7ebf\u7d22\u5b9e\u73b0\u591a\u533a\u57df\u534f\u4f5c\uff0c\u6784\u5efa\u8fde\u8d2f\u903b\u8f91\u63a8\u7406\u94fe\u3002\u540c\u65f6\u6784\u5efa\u4e86\u542b74,691\u4e2a\u6837\u672c\u7684CoCoT-70K\u6570\u636e\u96c6\u3002", "result": "\u5728\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoCoT\u5728LLaVA-1.5\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534715.4%\uff0c\u5728Qwen2-VL\u4e0a\u63d0\u53474.0%\u3002", "conclusion": "CoCoT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u7684\u533a\u57df\u9009\u62e9\u4e0e\u8bed\u4e49\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "summary_cn": "\u591a\u6a21\u6001\u63a8\u7406\u9700\u8981\u89c6\u89c9\u4e0e\u8bed\u8a00\u7ebf\u7d22\u7684\u65e0\u7f1d\u878d\u5408\uff0c\u7136\u800c\u73b0\u6709\u7684\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u573a\u666f\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\uff081\uff09\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4e00\u7684\u7c97\u7c92\u5ea6\u56fe\u50cf\u533a\u57df\uff1b\uff082\uff09\u8fde\u7eed\u63a8\u7406\u6b65\u9aa4\u4e4b\u95f4\u5b58\u5728\u8bed\u4e49\u788e\u7247\u5316\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CoCoT\uff08\u534f\u4f5c\u5f0f\u8de8\u6a21\u6001\u601d\u7ef4\uff09\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u5305\u542b\u4e24\u9879\u521b\u65b0\uff1aa) \u52a8\u6001\u591a\u533a\u57df\u5b9a\u4f4d\uff0c\u53ef\u6839\u636e\u95ee\u9898\u81ea\u9002\u5e94\u5730\u68c0\u6d4b\u6700\u76f8\u5173\u7684\u56fe\u50cf\u533a\u57df\uff1bb) \u5173\u7cfb\u611f\u77e5\u63a8\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u5bf9\u9f50\u89c6\u89c9\u7ebf\u7d22\uff0c\u5b9e\u73b0\u591a\u533a\u57df\u534f\u4f5c\uff0c\u4ece\u800c\u5f62\u6210\u8fde\u8d2f\u4e14\u903b\u8f91\u6e05\u6670\u7684\u63a8\u7406\u94fe\u3002\u57fa\u4e8e\u8be5\u65b9\u6cd5\uff0c\u6211\u4eec\u6784\u5efa\u4e86CoCoT-70K\u6570\u636e\u96c6\uff0c\u5305\u542b74,691\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u5747\u5e26\u6709\u591a\u4e2a\u56fe\u50cf\u533a\u57df\u6807\u6ce8\u548c\u7ed3\u6784\u5316\u7684\u63a8\u7406\u94fe\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCoCoT\u663e\u8457\u589e\u5f3a\u4e86\u590d\u6742\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5728\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5206\u522b\u5728LLaVA-1.5\u548cQwen2-VL\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574715.4%\u548c4.0%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002\u76f8\u5173\u6570\u636e\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/deer-echo/CoCoT\u3002"}}
{"id": "2601.02427", "pdf": "https://arxiv.org/pdf/2601.02427", "abs": "https://arxiv.org/abs/2601.02427", "authors": ["Lo\u00efc Magne", "Anas Awadalla", "Guanzhi Wang", "Yinzhen Xu", "Joshua Belofsky", "Fengyuan Hu", "Joohwan Kim", "Ludwig Schmidt", "Georgia Gkioxari", "Jan Kautz", "Yisong Yue", "Yejin Choi", "Yuke Zhu", "Linxi \"Jim\" Fan"], "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "16 pages, 7 figures", "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.", "AI": {"tldr": "NitroGen \u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\u7684\u89c6\u89c9-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\uff0c\u57fa\u4e8e 4 \u4e07\u5c0f\u65f6\u30011000 \u591a\u6b3e\u6e38\u620f\u7684\u89c6\u9891\u8bad\u7ec3\u800c\u6210\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u6d4b\u73af\u5883\u548c\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u73b0\u6709\u6e38\u620f\u667a\u80fd\u4f53\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u6e38\u620f\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u8de8\u6e38\u620f\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u63a8\u52a8\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7814\u7a76\uff0c\u4f5c\u8005\u6784\u5efa\u5927\u89c4\u6a21\u591a\u6e38\u620f\u89c6\u9891-\u52a8\u4f5c\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u7edf\u4e00\u6a21\u578b\u4ee5\u5b9e\u73b0\u8de8\u6e38\u620f\u8fc1\u79fb\u3002", "method": "NitroGen \u6a21\u578b\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a1\uff09\u4ece\u516c\u5f00\u6e38\u620f\u89c6\u9891\u81ea\u52a8\u63d0\u53d6\u52a8\u4f5c\u6784\u5efa\u7684\u5927\u89c4\u6a21\u89c6\u9891-\u52a8\u4f5c\u6570\u636e\u96c6\uff1b2\uff09\u53ef\u8bc4\u4f30\u8de8\u6e38\u620f\u6cdb\u5316\u80fd\u529b\u7684\u591a\u6e38\u620f\u57fa\u51c6\u73af\u5883\uff1b3\uff09\u4f7f\u7528\u5927\u89c4\u6a21\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u7684\u7edf\u4e00\u89c6\u89c9-\u52a8\u4f5c\u6a21\u578b\u3002", "result": "NitroGen \u5728\u591a\u79cd\u6e38\u620f\u7c7b\u578b\uff08\u59823D\u52a8\u4f5c\u6e38\u620f\u6218\u6597\u30012D\u5e73\u53f0\u9ad8\u7cbe\u5ea6\u63a7\u5236\u3001\u7a0b\u5e8f\u751f\u6210\u4e16\u754c\u63a2\u7d22\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u672a\u89c1\u6e38\u620f\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u6700\u9ad8\u63d0\u534752%\uff08\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\uff09\u3002", "conclusion": "NitroGen \u5c55\u793a\u4e86\u5927\u89c4\u6a21\u89c6\u9891-\u52a8\u4f5c\u9884\u8bad\u7ec3\u5728\u6784\u5efa\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5176\u5f00\u6e90\u8d44\u6e90\u5c06\u4fc3\u8fdb\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7684\u7814\u7a76\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86 NitroGen\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\u7684\u89c6\u89c9-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\uff0c\u57fa\u4e8e\u8d85\u8fc7 1,000 \u6b3e\u6e38\u620f\u5171\u8ba1 40,000 \u5c0f\u65f6\u7684\u6e38\u620f\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\u3002\u6211\u4eec\u878d\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a1\uff09\u901a\u8fc7\u81ea\u52a8\u4ece\u516c\u5f00\u6e38\u620f\u89c6\u9891\u4e2d\u63d0\u53d6\u73a9\u5bb6\u52a8\u4f5c\u6784\u5efa\u7684\u4e92\u8054\u7f51\u89c4\u6a21\u89c6\u9891-\u52a8\u4f5c\u6570\u636e\u96c6\uff1b2\uff09\u53ef\u8861\u91cf\u8de8\u6e38\u620f\u6cdb\u5316\u80fd\u529b\u7684\u591a\u6e38\u620f\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\uff1b3\uff09\u91c7\u7528\u5927\u89c4\u6a21\u884c\u4e3a\u514b\u9686\u8bad\u7ec3\u7684\u7edf\u4e00\u89c6\u89c9-\u52a8\u4f5c\u6a21\u578b\u3002NitroGen \u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u5305\u62ec 3D \u52a8\u4f5c\u6e38\u620f\u4e2d\u7684\u6218\u6597\u573a\u666f\u30012D \u5e73\u53f0\u6e38\u620f\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u63a7\u5236\u4ee5\u53ca\u7a0b\u5e8f\u751f\u6210\u4e16\u754c\u4e2d\u7684\u63a2\u7d22\u4efb\u52a1\u3002\u8be5\u6a21\u578b\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u672a\u89c1\u8fc7\u7684\u6e38\u620f\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u76f8\u8f83\u4ece\u96f6\u8bad\u7ec3\u7684\u6a21\u578b\u6700\u9ad8\u63d0\u5347 52%\u3002\u6211\u4eec\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u3001\u8bc4\u6d4b\u5957\u4ef6\u548c\u6a21\u578b\u6743\u91cd\uff0c\u4ee5\u63a8\u52a8\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7684\u7814\u7a76\u3002"}}
{"id": "2601.02437", "pdf": "https://arxiv.org/pdf/2601.02437", "abs": "https://arxiv.org/abs/2601.02437", "authors": ["Zhibo Wang", "Zuoyuan Zhang", "Xiaoyi Pang", "Qile Zhang", "Xuanyi Hao", "Shuguo Zhuo", "Peng Sun"], "title": "TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision Transformers (ViTs) have demonstrated strong performance across a wide range of vision tasks, yet their substantial computational and memory demands hinder efficient deployment on resource-constrained mobile and edge devices. Pruning has emerged as a promising direction for reducing ViT complexity. However, existing approaches either (i) produce a single pruned model shared across all devices, ignoring device heterogeneity, or (ii) rely on fine-tuning with device-local data, which is often infeasible due to limited on-device resources and strict privacy constraints. As a result, current methods fall short of enabling task-customized ViT pruning in privacy-preserving mobile computing settings. This paper introduces TAP-ViTs, a novel task-adaptive pruning framework that generates device-specific pruned ViT models without requiring access to any raw local data. Specifically, to infer device-level task characteristics under privacy constraints, we propose a Gaussian Mixture Model (GMM)-based metric dataset construction mechanism. Each device fits a lightweight GMM to approximate its private data distribution and uploads only the GMM parameters. Using these parameters, the cloud selects distribution-consistent samples from public data to construct a task-representative metric dataset for each device. Based on this proxy dataset, we further develop a dual-granularity importance evaluation-based pruning strategy that jointly measures composite neuron importance and adaptive layer importance, enabling fine-grained, task-aware pruning tailored to each device's computational budget. Extensive experiments across multiple ViT backbones and datasets demonstrate that TAP-ViTs consistently outperforms state-of-the-art pruning methods under comparable compression ratios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTAP-ViTs\uff0c\u4e00\u79cd\u65e0\u9700\u8bbf\u95ee\u8bbe\u5907\u672c\u5730\u539f\u59cb\u6570\u636e\u5373\u53ef\u4e3a\u4e0d\u540c\u8bbe\u5907\u751f\u6210\u4efb\u52a1\u81ea\u9002\u5e94\u526a\u679dViT\u6a21\u578b\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u6709ViT\u526a\u679d\u65b9\u6cd5\u8981\u4e48\u4e3a\u6240\u6709\u8bbe\u5907\u751f\u6210\u5355\u4e00\u6a21\u578b\u800c\u5ffd\u7565\u8bbe\u5907\u5f02\u6784\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u8bbe\u5907\u672c\u5730\u6570\u636e\u5fae\u8c03\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u548c\u9690\u79c1\u7ea6\u675f\u4e0b\u4e0d\u53ef\u884c\uff0c\u96be\u4ee5\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u4efb\u52a1\u5b9a\u5236\u5316\u526a\u679d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u5ea6\u91cf\u6570\u636e\u96c6\u6784\u5efa\u673a\u5236\uff1a\u5404\u8bbe\u5907\u7528\u8f7b\u91cfGMM\u62df\u5408\u79c1\u6709\u6570\u636e\u5206\u5e03\u5e76\u4e0a\u4f20\u53c2\u6570\uff1b\u4e91\u7aef\u5229\u7528\u8fd9\u4e9b\u53c2\u6570\u4ece\u516c\u5f00\u6570\u636e\u4e2d\u9009\u53d6\u5206\u5e03\u4e00\u81f4\u6837\u672c\u6784\u5efa\u8bbe\u5907\u4e13\u5c5e\u4ee3\u7406\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u53cc\u7c92\u5ea6\u91cd\u8981\u6027\u8bc4\u4f30\u526a\u679d\u7b56\u7565\uff0c\u8054\u5408\u8861\u91cf\u590d\u5408\u795e\u7ecf\u5143\u91cd\u8981\u6027\u548c\u81ea\u9002\u5e94\u5c42\u91cd\u8981\u6027\u3002", "result": "\u5728\u591a\u4e2aViT\u4e3b\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTAP-ViTs\u5728\u76f8\u540c\u538b\u7f29\u6bd4\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "TAP-ViTs\u6709\u6548\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u79fb\u52a8\u8ba1\u7b97\u573a\u666f\u4e2dViT\u6a21\u578b\u7684\u4efb\u52a1\u5b9a\u5236\u5316\u526a\u679d\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u8bbe\u5907\u7279\u5b9a\u7684\u6a21\u578b\u538b\u7f29\u3002", "summary_cn": "\u89c6\u89c9Transformer\uff08ViTs\uff09\u5728\u5404\u7c7b\u89c6\u89c9\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u3002\u526a\u679d\u5df2\u6210\u4e3a\u964d\u4f4eViT\u590d\u6742\u5ea6\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\uff08i\uff09\u4e3a\u6240\u6709\u8bbe\u5907\u751f\u6210\u4e00\u4e2a\u5171\u4eab\u7684\u526a\u679d\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u8bbe\u5907\u5f02\u6784\u6027\uff0c\u8981\u4e48\uff08ii\uff09\u4f9d\u8d56\u4f7f\u7528\u8bbe\u5907\u672c\u5730\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u800c\u8fd9\u901a\u5e38\u7531\u4e8e\u8bbe\u5907\u8d44\u6e90\u6709\u9650\u548c\u4e25\u683c\u7684\u9690\u79c1\u9650\u5236\u800c\u4e0d\u53ef\u884c\u3002\u56e0\u6b64\uff0c\u5f53\u524d\u65b9\u6cd5\u5c1a\u65e0\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u79fb\u52a8\u8ba1\u7b97\u73af\u5883\u4e2d\u5b9e\u73b0\u4efb\u52a1\u5b9a\u5236\u5316\u7684ViT\u526a\u679d\u3002\u672c\u6587\u63d0\u51fa\u4e86TAP-ViTs\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u526a\u679d\u6846\u67b6\uff0c\u53ef\u5728\u4e0d\u8bbf\u95ee\u4efb\u4f55\u539f\u59cb\u672c\u5730\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u8bbe\u5907\u7279\u5b9a\u7684\u526a\u679dViT\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4e3a\u4e86\u5728\u9690\u79c1\u7ea6\u675f\u4e0b\u63a8\u65ad\u8bbe\u5907\u7ea7\u4efb\u52a1\u7279\u5f81\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u5ea6\u91cf\u6570\u636e\u96c6\u6784\u5efa\u673a\u5236\u3002\u6bcf\u4e2a\u8bbe\u5907\u62df\u5408\u4e00\u4e2a\u8f7b\u91cf\u7ea7GMM\u4ee5\u8fd1\u4f3c\u5176\u79c1\u6709\u6570\u636e\u5206\u5e03\uff0c\u5e76\u4ec5\u4e0a\u4f20GMM\u53c2\u6570\u3002\u4e91\u7aef\u5229\u7528\u8fd9\u4e9b\u53c2\u6570\u4ece\u516c\u5f00\u6570\u636e\u4e2d\u9009\u62e9\u5206\u5e03\u4e00\u81f4\u7684\u6837\u672c\uff0c\u4e3a\u6bcf\u4e2a\u8bbe\u5907\u6784\u5efa\u4e00\u4e2a\u4efb\u52a1\u4ee3\u8868\u6027\u7684\u5ea6\u91cf\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u8be5\u4ee3\u7406\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u4e00\u79cd\u53cc\u7c92\u5ea6\u91cd\u8981\u6027\u8bc4\u4f30\u526a\u679d\u7b56\u7565\uff0c\u8054\u5408\u8861\u91cf\u590d\u5408\u795e\u7ecf\u5143\u91cd\u8981\u6027\u548c\u81ea\u9002\u5e94\u5c42\u91cd\u8981\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u9488\u5bf9\u6bcf\u4e2a\u8bbe\u5907\u8ba1\u7b97\u9884\u7b97\u7684\u7ec6\u7c92\u5ea6\u3001\u4efb\u52a1\u611f\u77e5\u526a\u679d\u3002\u5728\u591a\u4e2aViT\u4e3b\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTAP-ViTs\u5728\u53ef\u6bd4\u538b\u7f29\u6bd4\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u526a\u679d\u65b9\u6cd5\u3002"}}
{"id": "2601.02441", "pdf": "https://arxiv.org/pdf/2601.02441", "abs": "https://arxiv.org/abs/2601.02441", "authors": ["Yuan Li", "Shin'ya Nishida"], "title": "Understanding Pure Textual Reasoning for Blind Image Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": "Code available at https://anonymous.4open.science/r/Bridging-Image-Text-Gap-for-BIQA-CF5B/. This work is under review", "summary": "Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.", "AI": {"tldr": "\u672c\u6587\u4ece\u4fe1\u606f\u6d41\u89d2\u5ea6\u63a2\u7a76\u6587\u672c\u5728\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08BIQA\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e09\u79cd\u8303\u5f0f\uff08Chain-of-Thought\u3001Self-Consistency\u3001Autoencoder\uff09\u53d1\u73b0\u4ec5\u7528\u6587\u672c\u9884\u6d4b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5176\u4e2dSelf-Consistency\u6700\u6709\u6548\u7f29\u5c0f\u56fe\u50cf\u4e0e\u6587\u672c\u6761\u4ef6\u9884\u6d4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u6587\u672c\u4fe1\u606f\u5982\u4f55\u4fc3\u8fdbBIQA\u4e2d\u7684\u8d28\u91cf\u9884\u6d4b\uff0c\u4ee5\u53ca\u6587\u672c\u80fd\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u8868\u5f81\u4e0e\u8bc4\u5206\u76f8\u5173\u7684\u56fe\u50cf\u5185\u5bb9\u3002", "method": "\u4ece\u4fe1\u606f\u6d41\u89c6\u89d2\u51fa\u53d1\uff0c\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e09\u79cd\u5b66\u4e60\u56fe\u50cf-\u6587\u672c-\u8bc4\u5206\u5173\u7cfb\u7684\u8303\u5f0f\uff1aChain-of-Thought\u3001Self-Consistency \u548c Autoencoder\uff0c\u5e76\u4e0e\u73b0\u6709BIQA\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u4ec5\u4f7f\u7528\u6587\u672c\u4fe1\u606f\u65f6\u73b0\u6709\u6a21\u578b\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff1bChain-of-Thought\u5bf9BIQA\u63d0\u5347\u6709\u9650\uff1bSelf-Consistency\u663e\u8457\u7f29\u5c0f\u56fe\u50cf\u4e0e\u6587\u672c\u6761\u4ef6\u9884\u6d4b\u4e4b\u95f4\u7684PLCC/SRCC\u5dee\u8ddd\u81f30.02/0.03\uff1bAutoencoder\u8303\u5f0f\u6548\u679c\u8f83\u5f31\u4f46\u63d0\u793a\u4e86\u4f18\u5316\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6587\u672c\u63a8\u7406\u5728BIQA\u4e2d\u7684\u5c40\u9650\u4e0e\u6f5c\u529b\uff0c\u4e3a\u6539\u8fdbBIQA\u53ca\u9ad8\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6587\u672c\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "summary_cn": "\u6700\u8fd1\uff0c\u6587\u672c\u63a8\u7406\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08BIQA\uff09\u4e2d\u3002\u7136\u800c\uff0c\u6587\u672c\u4fe1\u606f\u5982\u4f55\u4fc3\u8fdb\u8d28\u91cf\u9884\u6d4b\uff0c\u4ee5\u53ca\u6587\u672c\u80fd\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u8868\u5f81\u4e0e\u8bc4\u5206\u76f8\u5173\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u76ee\u524d\u4ecd\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u4ece\u4fe1\u606f\u6d41\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u901a\u8fc7\u5bf9\u6bd4\u73b0\u6709BIQA\u6a21\u578b\u4e0e\u4e09\u79cd\u65e8\u5728\u5b66\u4e60\u56fe\u50cf-\u6587\u672c-\u8bc4\u5206\u5173\u7cfb\u7684\u8303\u5f0f\uff08\u601d\u7ef4\u94fe Chain-of-Thought\u3001\u81ea\u4e00\u81f4\u6027 Self-Consistency \u548c\u81ea\u7f16\u7801\u5668 Autoencoder\uff09\u6765\u56de\u7b54\u8fd9\u4e9b\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4ec5\u4f7f\u7528\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u65f6\uff0c\u73b0\u6709\u6a21\u578b\u7684\u8bc4\u5206\u9884\u6d4b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u5176\u4e2d\uff0c\u601d\u7ef4\u94fe\u8303\u5f0f\u5bf9BIQA\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u800c\u81ea\u4e00\u81f4\u6027\u8303\u5f0f\u663e\u8457\u7f29\u5c0f\u4e86\u57fa\u4e8e\u56fe\u50cf\u548c\u57fa\u4e8e\u6587\u672c\u7684\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5c06PLCC/SRCC\u5dee\u5f02\u7f29\u5c0f\u81f30.02/0.03\u3002\u81ea\u7f16\u7801\u5668\u7c7b\u8303\u5f0f\u5728\u5f25\u5408\u56fe\u50cf-\u6587\u672c\u5dee\u8ddd\u65b9\u9762\u6548\u679c\u8f83\u5f31\uff0c\u4f46\u63ed\u793a\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u65b9\u5411\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6539\u8fdbBIQA\u53ca\u9ad8\u5c42\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6587\u672c\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2601.02443", "pdf": "https://arxiv.org/pdf/2601.02443", "abs": "https://arxiv.org/abs/2601.02443", "authors": ["Li Wang", "Xi Chen", "XiangWen Deng", "HuaHui Yi", "ZeKun Jiang", "Kang Li", "Jian Li"], "title": "Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u819d\u9aa8\u5173\u8282\u708eX\u5149\u7247\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5176\u4f5c\u4e3a\u5206\u7c7b\u5668\u7684\u6548\u679c\u4e0d\u5982\u5355\u72ec\u4f18\u5316\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff1b\u7814\u7a76\u53d1\u73b0\u6570\u636e\u5e73\u8861\u4e0e\u8d28\u91cf\u6bd4\u6570\u636e\u89c4\u6a21\u66f4\u91cd\u8981\uff0c\u5efa\u8bae\u5728\u7279\u5b9a\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u5148\u4f18\u5316\u89c6\u89c9\u7f16\u7801\u5668\u5e76\u8c28\u614e\u6784\u5efa\u6570\u636e\u96c6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u533b\u5b66\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u75be\u75c5\u7279\u5f02\u6027\u5206\u7c7b\uff08\u5982\u819d\u9aa8\u5173\u8282\u708eX\u5149\u7247\u5206\u7ea7\uff09\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\uff0c\u4e14\u8be5\u4efb\u52a1\u5728\u73b0\u6709\u533b\u5b66MLLM\u57fa\u51c6\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u800c\u5168\u7403\u67093\u20134\u4ebf\u4eba\u53d7\u6b64\u75c5\u5f71\u54cd\uff0c\u4e9f\u9700\u53ef\u9760\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u5728\u819d\u9aa8\u5173\u8282\u708eX\u5149\u7247\u5206\u7c7b\u4efb\u52a1\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540cMLLM\u67b6\u6784\uff0c\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u522b\u8c03\u6574\u89c6\u89c9\u7f16\u7801\u5668\u3001\u8fde\u63a5\u6a21\u5757\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5e76\u5728\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\u4e0b\u8861\u91cf\u5404\u7ec4\u4ef6\u5bf9\u8bca\u65ad\u51c6\u786e\u7387\u7684\u8d21\u732e\uff1b\u540c\u65f6\u6bd4\u8f83\u4e86\u5c0f\u89c4\u6a21\u5e73\u8861\u6570\u636e\u96c6\uff08500\u5f20\u56fe\u50cf\uff09\u4e0e\u5927\u89c4\u6a21\u4e0d\u5e73\u8861\u6570\u636e\u96c6\uff085,778\u5f20\u56fe\u50cf\uff09\u5728LoRA\u5fae\u8c03\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4ec5\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5373\u53ef\u8d85\u8d8a\u5b8c\u6574MLLM\u6d41\u7a0b\uff1b\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\u76f8\u6bd4\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u672a\u5e26\u6765\u663e\u8457\u63d0\u5347\uff1b\u4f7f\u7528\u5c0f\u800c\u5e73\u8861\u7684\u6570\u636e\u96c6\u8fdb\u884cLoRA\u5fae\u8c03\u6548\u679c\u4f18\u4e8e\u4f7f\u7528\u5927\u4f46\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u5bf9\u4e8e\u9700\u8981\u9ad8\u786e\u5b9a\u6027\u7684\u7279\u5b9a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0cMLLM\u67b6\u6784\u5e76\u4e0d\u7406\u60f3\uff1bLLM\u66f4\u9002\u5408\u4f5c\u4e3a\u89e3\u91ca\u5668\u548c\u62a5\u544a\u751f\u6210\u5668\uff0c\u800c\u975e\u4e3b\u8981\u5206\u7c7b\u5668\uff1b\u5e94\u4f18\u5148\u4f18\u5316\u89c6\u89c9\u7f16\u7801\u5668\u5e76\u6ce8\u91cd\u9ad8\u8d28\u91cf\u3001\u7c7b\u522b\u5e73\u8861\u7684\u6570\u636e\u96c6\u6784\u5efa\u3002", "summary_cn": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u548c\u62a5\u544a\u751f\u6210\u65b9\u9762\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u751f\u6210\u4e0e\u89e3\u91ca\u80fd\u529b\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u8fc1\u79fb\u5230\u75be\u75c5\u7279\u5f02\u6027\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002\u6211\u4eec\u5728\u819d\u9aa8\u5173\u8282\u708e\uff08OA\uff09X\u5149\u7247\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u591a\u79cdMLLM\u67b6\u6784\u2014\u2014\u8be5\u4efb\u52a1\u5728\u73b0\u6709\u533b\u5b66MLLM\u57fa\u51c6\u4e2d\u4ecd\u7f3a\u4e4f\u5145\u5206\u4ee3\u8868\uff0c\u5c3d\u7ba1\u5168\u7403\u7ea6\u67093\u4ebf\u81f34\u4ebf\u4eba\u53d7\u819d\u9aa8\u5173\u8282\u708e\u5f71\u54cd\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u6d88\u878d\u5b9e\u9a8c\uff0c\u6211\u4eec\u5206\u522b\u64cd\u63a7\u89c6\u89c9\u7f16\u7801\u5668\u3001\u8fde\u63a5\u6a21\u5757\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5e76\u5728\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\u4e0b\u6d4b\u91cf\u5404\u7ec4\u4ef6\u5bf9\u8bca\u65ad\u51c6\u786e\u7387\u7684\u8d21\u732e\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u4ec5\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u5373\u53ef\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u8d85\u8d8a\u5b8c\u6574\u7684MLLM\u6d41\u7a0b\uff1b\u800c\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\u76f8\u6bd4\u57fa\u4e8e\u63d0\u793a\u7684\u5f15\u5bfc\u5e76\u672a\u5e26\u6765\u6709\u610f\u4e49\u7684\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5728\u4e00\u4e2a\u5c0f\u578b\u4f46\u7c7b\u522b\u5e73\u8861\u7684\u6570\u636e\u96c6\uff08500\u5f20\u56fe\u50cf\uff09\u4e0a\u8fdb\u884cLoRA\u5fae\u8c03\uff0c\u5176\u6548\u679c\u4f18\u4e8e\u5728\u4e00\u4e2a\u66f4\u5927\u4f46\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\uff085,778\u5f20\u56fe\u50cf\uff09\u4e0a\u8bad\u7ec3\uff0c\u8fd9\u8868\u660e\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\uff0c\u6570\u636e\u7684\u5e73\u8861\u6027\u4e0e\u8d28\u91cf\u53ef\u80fd\u6bd4\u539f\u59cb\u6570\u636e\u89c4\u6a21\u66f4\u4e3a\u91cd\u8981\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5bf9\u4e8e\u9886\u57df\u7279\u5b9a\u7684\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u9002\u5408\u4f5c\u4e3a\u89e3\u91ca\u5668\u548c\u62a5\u544a\u751f\u6210\u5668\uff0c\u800c\u975e\u4e3b\u8981\u5206\u7c7b\u5668\u3002\u56e0\u6b64\uff0cMLLM\u67b6\u6784\u4f3c\u4e4e\u4e0d\u592a\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u786e\u5b9a\u6027\u7684\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u5206\u7c7b\u4efb\u52a1\u3002\u6211\u4eec\u5efa\u8bae\u5728\u5f00\u53d1\u4e34\u5e8a\u53ef\u7528\u7cfb\u7edf\u65f6\uff0c\u4f18\u5148\u4f18\u5316\u89c6\u89c9\u7f16\u7801\u5668\u5e76\u7cbe\u5fc3\u6784\u5efa\u6570\u636e\u96c6\u3002"}}
{"id": "2601.02445", "pdf": "https://arxiv.org/pdf/2601.02445", "abs": "https://arxiv.org/abs/2601.02445", "authors": ["Parashjyoti Borah", "Sanghamitra Sarkar", "Ranjan Phukan"], "title": "A Spatio-Temporal Deep Learning Approach For High-Resolution Gridded Monsoon Prediction", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 3 figures, 2 Tables, to be submitted to \"IEEE Transactions on Geoscience and Remote Sensing\"", "summary": "The Indian Summer Monsoon (ISM) is a critical climate phenomenon, fundamentally impacting the agriculture, economy, and water security of over a billion people. Traditional long-range forecasting, whether statistical or dynamical, has predominantly focused on predicting a single, spatially-averaged seasonal value, lacking the spatial detail essential for regional-level resource management. To address this gap, we introduce a novel deep learning framework that reframes gridded monsoon prediction as a spatio-temporal computer vision task. We treat multi-variable, pre-monsoon atmospheric and oceanic fields as a sequence of multi-channel images, effectively creating a video-like input tensor. Using 85 years of ERA5 reanalysis data for predictors and IMD rainfall data for targets, we employ a Convolutional Neural Network (CNN)-based architecture to learn the complex mapping from the five-month pre-monsoon period (January-May) to a high-resolution gridded rainfall pattern for the subsequent monsoon season. Our framework successfully produces distinct forecasts for each of the four monsoon months (June-September) as well as the total seasonal average, demonstrating its utility for both intra-seasonal and seasonal outlooks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u5370\u5ea6\u590f\u5b63\u98ce\uff08ISM\uff09\u7684\u7f51\u683c\u5316\u9884\u6d4b\u8f6c\u5316\u4e3a\u65f6\u7a7a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5229\u7528CNN\u67b6\u6784\u4ece1-5\u6708\u7684\u5927\u6c14\u548c\u6d77\u6d0b\u573a\u6570\u636e\u4e2d\u9884\u6d4b6-9\u6708\u9ad8\u5206\u8fa8\u7387\u9010\u6708\u53ca\u5b63\u8282\u603b\u964d\u96e8\u91cf\u3002", "motivation": "\u4f20\u7edf\u957f\u671f\u9884\u62a5\u65b9\u6cd5\u901a\u5e38\u4ec5\u9884\u6d4b\u4e00\u4e2a\u7a7a\u95f4\u5e73\u5747\u7684\u5b63\u98ce\u5b63\u8282\u503c\uff0c\u7f3a\u4e4f\u5bf9\u533a\u57df\u8d44\u6e90\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u7684\u7a7a\u95f4\u7ec6\u8282\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\uff0c\u4f5c\u8005\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u5316\u9884\u6d4b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06\u591a\u53d8\u91cf\u3001\u5b63\u98ce\u524d\u7684\u5927\u6c14\u548c\u6d77\u6d0b\u573a\u89c6\u4e3a\u591a\u901a\u9053\u56fe\u50cf\u5e8f\u5217\uff0c\u6784\u5efa\u7c7b\u4f3c\u89c6\u9891\u7684\u8f93\u5165\u5f20\u91cf\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u67b6\u6784\uff0c\u4ece85\u5e74ERA5\u518d\u5206\u6790\u6570\u636e\uff08\u9884\u6d4b\u56e0\u5b50\uff09\u548cIMD\u964d\u96e8\u6570\u636e\uff08\u76ee\u6807\uff09\u4e2d\u5b66\u4e60\u4ece1-5\u6708\u5230\u540e\u7eed\u5b63\u98ce\u5b63\u8282\u9ad8\u5206\u8fa8\u7387\u964d\u96e8\u7684\u7a7a\u95f4\u6620\u5c04\u5173\u7cfb\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u751f\u6210\u4e86\u5b63\u98ce\u56db\u4e2a\u6708\uff086-9\u6708\uff09\u5404\u81ea\u4ee5\u53ca\u6574\u4e2a\u5b63\u8282\u5e73\u5747\u964d\u96e8\u91cf\u7684\u72ec\u7acb\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\uff0c\u8bc1\u660e\u5176\u5728\u5b63\u8282\u5185\u548c\u5b63\u8282\u5c3a\u5ea6\u5c55\u671b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u5b63\u98ce\u9884\u6d4b\u95ee\u9898\u91cd\u6784\u4e3a\u65f6\u7a7a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\uff0c\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u4f20\u7edf\u65b9\u6cd5\u6240\u7f3a\u4e4f\u7684\u7cbe\u7ec6\u7a7a\u95f4\u4fe1\u606f\uff0c\u5bf9\u533a\u57df\u5c42\u9762\u7684\u6c34\u8d44\u6e90\u548c\u519c\u4e1a\u7ba1\u7406\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "summary_cn": "\u5370\u5ea6\u590f\u5b63\u98ce\uff08ISM\uff09\u662f\u4e00\u79cd\u5173\u952e\u7684\u6c14\u5019\u73b0\u8c61\uff0c\u5bf9\u8d85\u8fc7\u5341\u4ebf\u4eba\u53e3\u7684\u519c\u4e1a\u3001\u7ecf\u6d4e\u548c\u6c34\u5b89\u5168\u6709\u7740\u6839\u672c\u6027\u5f71\u54cd\u3002\u4f20\u7edf\u7684\u957f\u671f\u9884\u62a5\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u7edf\u8ba1\u8fd8\u662f\u52a8\u529b\u65b9\u6cd5\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u9884\u6d4b\u4e00\u4e2a\u5355\u4e00\u7684\u7a7a\u95f4\u5e73\u5747\u5b63\u8282\u503c\uff0c\u7f3a\u4e4f\u533a\u57df\u5c42\u9762\u8d44\u6e90\u7ba1\u7406\u6240\u5fc5\u9700\u7684\u7a7a\u95f4\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u7f51\u683c\u5316\u7684\u5b63\u98ce\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u65f6\u7a7a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002\u6211\u4eec\u5c06\u591a\u53d8\u91cf\u7684\u5b63\u98ce\u524d\u671f\u5927\u6c14\u548c\u6d77\u6d0b\u573a\u89c6\u4e3a\u4e00\u4e2a\u591a\u901a\u9053\u56fe\u50cf\u5e8f\u5217\uff0c\u6709\u6548\u5730\u521b\u5efa\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u89c6\u9891\u7684\u8f93\u5165\u5f20\u91cf\u3002\u5229\u752885\u5e74\u7684ERA5\u518d\u5206\u6790\u6570\u636e\u4f5c\u4e3a\u9884\u6d4b\u56e0\u5b50\uff0cIMD\u964d\u96e8\u6570\u636e\u4f5c\u4e3a\u76ee\u6807\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u67b6\u6784\uff0c\u5b66\u4e60\u4ece\u4e94\u4e2a\u6708\u7684\u5b63\u98ce\u524d\u671f\uff081\u6708\u81f35\u6708\uff09\u5230\u968f\u540e\u5b63\u98ce\u5b63\u8282\u9ad8\u5206\u8fa8\u7387\u7f51\u683c\u5316\u964d\u96e8\u6a21\u5f0f\u4e4b\u95f4\u7684\u590d\u6742\u6620\u5c04\u5173\u7cfb\u3002\u6211\u4eec\u7684\u6846\u67b6\u6210\u529f\u5730\u4e3a\u5b63\u98ce\u56db\u4e2a\u6708\uff086\u6708\u81f39\u6708\uff09\u4e2d\u7684\u6bcf\u4e00\u4e2a\u6708\u4ee5\u53ca\u6574\u4e2a\u5b63\u8282\u7684\u5e73\u5747\u964d\u96e8\u91cf\u751f\u6210\u4e86\u72ec\u7acb\u7684\u9884\u6d4b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b63\u8282\u5185\u548c\u5b63\u8282\u5c55\u671b\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002"}}
