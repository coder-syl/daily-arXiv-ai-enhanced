<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: 本文研究了科学图像合成方法，提出ImgCoder框架以提升结构准确性，并构建SciGenBench评估基准，发现基于像素的模型存在系统性缺陷，而高质量合成图像可有效提升多模态模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型生成的图像虽视觉上合理，但常缺乏科学正确性，导致视觉与逻辑不一致，限制了其在下游推理任务中的应用。因此亟需探索能生成科学严谨图像的方法。

Method: 作者系统比较了直接像素生成与程序化合成两种范式，提出ImgCoder——一个遵循“理解-规划-编码”流程的逻辑驱动框架，并构建SciGenBench基准，从信息效用和逻辑有效性两方面评估图像的科学正确性。

Result: 评估揭示了像素级模型的系统性失败模式，凸显了表达力与精度之间的根本权衡；同时发现，在经严格验证的合成科学图像上微调大语言多模态模型（LMMs）可带来稳定的推理性能提升。

Conclusion: 高保真度的科学图像合成是释放大规模多模态推理能力的有效路径，其潜力类似于文本领域中合成数据的作用。

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

Abstract (中文翻译): 尽管合成数据已被证明在文本领域中能有效提升科学推理能力，但多模态推理仍受限于难以合成具有科学严谨性的图像。现有的文生图（T2I）模型通常生成视觉上合理但科学上错误的图像，导致持续存在的视觉-逻辑偏差，限制了其在下游推理任务中的价值。受新一代T2I模型最新进展的启发，我们对科学图像合成在生成范式、评估方法和下游应用方面进行了系统性研究。我们分析了基于像素的直接生成与程序化合成两种方法，并提出了ImgCoder——一种逻辑驱动的框架，遵循明确的“理解-规划-编码”工作流程，以提升结构精度。为严格评估科学正确性，我们引入了SciGenBench，该基准根据信息效用和逻辑有效性对生成图像进行评估。我们的评估揭示了像素级模型的系统性失效模式，并突显了表达能力与精度之间的根本权衡。最后，我们表明，在经过严格验证的合成科学图像上微调大型多模态模型（LMMs）可带来一致的推理能力提升，其潜在的扩展趋势与文本领域类似，从而验证了高保真科学图像合成是解锁大规模多模态推理能力的可行路径。

</details>


### [2] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: 本文提出一种双增强框架，通过结合空间流形扩展和语义病灶注入，显著提升医学图像分割模型在有限标注数据下的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割的性能越来越依赖于数据利用效率而非原始数据量，尤其对于脑膜瘤等复杂病理，需要从有限的高质量标注中充分挖掘潜在信息。

Method: 提出双增强框架：1）利用隐式神经表示（INR）建模连续速度场，并通过对积分形变场进行线性混合，在形变空间内插值生成解剖学上合理的结构变异；2）引入Sim2Real病灶注入模块，将病灶纹理移植到健康解剖背景中，构建高保真模拟域。

Result: 在混合数据集上的实验表明，该框架显著提升了nnU-Net和U-Mamba等先进模型的数据效率和鲁棒性。

Conclusion: 所提方法为在标注预算有限的情况下实现高性能医学图像分析提供了一种有效策略。

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

Abstract (中文翻译): 医学图像分割的性能日益取决于数据利用效率，而不仅仅是原始数据的数量。对于脑膜瘤等复杂病理的精确分割，要求模型充分利用有限高质量标注中的潜在信息。为最大化现有数据集的价值，我们提出了一种新颖的双增强框架，协同整合了空间流形扩展与语义病灶注入。具体而言，我们利用隐式神经表示（INR）对连续速度场进行建模。与以往方法不同，我们在积分形变场上进行线性混合，通过在形变空间内插值，高效生成解剖学上合理的结构变异，从而从小规模锚点集中广泛探索结构多样性。此外，我们引入了一个Sim2Real病灶注入模块，该模块通过将病灶纹理移植到健康解剖背景中，构建高保真模拟域，有效弥合了合成增强与真实病理之间的差距。在混合数据集上的综合实验表明，我们的框架显著提升了包括nnU-Net和U-Mamba在内的先进模型的数据效率和鲁棒性，为在有限标注预算下实现高性能医学图像分析提供了一种强有力的策略。

</details>


### [3] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: 本文提出了一种基于外周血涂片图像分析的自动化红细胞（RBC）分类方法，用于区分正常、细长及其他变形红细胞，以辅助镰状细胞贫血的诊断。


<details>
  <summary>Details</summary>
Motivation: 传统显微镜下观察红细胞形态依赖人工操作，耗时且主观性强，易导致高误判率，因此需要一种自动、客观且准确的红细胞形态分析方法。

Method: 采用Chan-Vese主动轮廓模型对红细胞进行图像分割，并利用圆形形状因子（CSF）和椭圆形状因子（ESF）进行形状分析；对聚集遮挡的红细胞，通过椭圆拟合调整以识别盘状和细长形红细胞。

Result: 该方法在正常和细长红细胞上的F值分别达到0.97和0.95，多项多分类性能指标优于现有先进方法，适用于镰状细胞贫血的临床诊断支持。

Conclusion: 所提方法具有高准确性和临床适用性，可有效辅助镰状细胞贫血的诊断与治疗决策。

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

Abstract (中文翻译): 红细胞（RBC）变形是多种疾病的后果，包括镰状细胞贫血，该病会导致反复发作的疼痛和严重的明显贫血。对这类患者的监测通常需要在显微镜下观察外周血样本，这一过程耗时且需由专业人员操作，而由于对孤立红细胞的观察具有主观性，错误率较高。本文提出了一种基于外周血涂片图像分析的自动化红细胞差异计数方法。该方法利用Chan-Vese主动轮廓模型对图像中的目标进行分割，然后使用基本形状分析描述符——圆形形状因子（CSF）和椭圆形状因子（ESF）——将红细胞（又称红血球）分类为正常、细长或其他变形类型。为分析在样本制备过程中部分被遮挡成簇的细胞，还进行了椭圆拟合调整，以支持对盘状和细长形红细胞的分析。研究所用的患者血样图像由古巴圣地亚哥“Dr. Juan Bruno Zayas”综合医院血液专科实验室专家采集。实验结果表明，与当前一些先进方法相比，所提方法在镰状细胞贫血诊断方面表现更优，其优越性体现在较高的F值（正常细胞为0.97，细长细胞为0.95）及多项整体多分类性能指标上。该方法的结果适用于镰状细胞贫血的临床治疗与诊断支持。

</details>


### [4] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: 本文提出新基准 AMVICC，系统评估多模态大语言模型（MLLMs）和图像生成模型（IGMs）在视觉推理任务中的失败模式，发现两类模型在基础视觉概念理解上存在共性与特异性缺陷。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型快速发展，但在理解或生成基本视觉概念（如物体朝向、数量、空间关系）方面仍存在明显不足，缺乏对跨模态视觉推理能力的系统评估方法。

Method: 基于 MMVP 基准问题，构建包含显式与隐式提示的新型跨模态评估基准 AMVICC，在九类视觉推理任务中测试 11 个 MLLMs 和 3 个 IGMs 的表现。

Result: MLLMs 和 IGMs 在多种视觉推理任务中表现出共享及各自特有的失败模式；IGMs 尤其在显式提示下难以精确控制细粒度视觉属性。

Conclusion: 该研究为评估现有视觉语言模型的结构化视觉推理能力提供了有效框架，并为未来跨模态对齐与统一建模指明了改进方向。

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

Abstract (中文翻译): 我们通过构建一个新颖的基准，系统地比较了多模态大语言模型（MLLMs）和图像生成模型（IGMs）在图像到文本和文本到图像任务中的失败模式，从而实现对视觉理解能力的跨模态评估。尽管机器学习领域发展迅速，但视觉语言模型（VLMs）在理解或生成物体朝向、数量或空间关系等基本视觉概念方面仍然存在不足，这凸显了其在基础视觉推理方面的差距。通过将 MMVP 基准问题改编为显式和隐式提示，我们创建了名为 AMVICC 的新基准，用于刻画不同模态下的失败模式。在对 11 个 MLLMs 和 3 个 IGMs 进行九类视觉推理任务测试后，结果表明，失败模式通常在模型和模态之间共享，但也存在模型特异性和模态特异性的失败，这可能归因于多种因素。IGMs 在响应提示时始终难以操控特定的视觉组件，尤其在显式提示下表现更差，表明其对细粒度视觉属性的控制能力较弱。我们的发现最直接适用于评估现有最先进的模型在结构化视觉推理任务上的表现。本研究为未来的跨模态对齐研究奠定了基础，提供了一个框架来探究生成与理解失败是否源于共同的局限性，从而指导未来统一视觉-语言建模的改进。

</details>


### [5] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: 本文提出一种结合深度特征提取与传统机器学习分类器的混合视觉方法，用于自动分类建筑垃圾，在自建数据集上达到99.5%准确率，优于端到端深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 建筑行业产生大量废弃物，有效分类对可持续管理和资源回收至关重要；现有方法在实际部署中存在鲁棒性或复杂性问题，亟需高效、实用的自动分类方案。

Method: 收集包含1800张高质量图像的UAE实地建筑垃圾数据集（四类材料）；使用预训练Xception网络提取深度特征，并系统评估SVM、kNN、Bagged Trees、LDA和逻辑回归等经典机器学习分类器的性能。

Result: 基于Xception特征的混合方法（如Linear SVM、kNN、Bagged Trees）在测试中达到最高99.5%的准确率和macro-F1分数，优于更复杂的端到端深度学习模型。

Conclusion: 该混合方法在保持高精度的同时具备良好的部署可行性，适用于现场建筑垃圾识别，并为未来与机器人及自动化系统集成提供基础。

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

Abstract (中文翻译): 建筑业产生了大量废弃物，因此有效的分类对于可持续的废物管理和资源回收至关重要。本研究提出了一种混合视觉管道，将深度特征提取与经典机器学习（ML）分类器相结合，用于自动分类建筑与拆除（C&D）废弃物。研究从阿联酋的实际建筑工地收集了一个包含1800张均衡、高质量图像的新数据集，涵盖四种材料类别：陶瓷/瓷砖、混凝土、垃圾/废料和木材，充分反映了多样化的现实场景。利用预训练的Xception网络提取深度特征，并系统评估了包括支持向量机（SVM）、k近邻（kNN）、Bagged Trees、线性判别分析（LDA）和逻辑回归在内的多种机器学习分类器。结果表明，采用Xception特征与简单分类器（如线性SVM、kNN和Bagged Trees）的混合管道实现了最先进的性能，准确率和macro-F1分数最高可达99.5%，优于更复杂或端到端的深度学习方法。该分析突显了此方法在鲁棒性和现场部署方面的优势，并为未来与机器人及现场自动化系统的集成提供了可行路径。

</details>


### [6] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: 本文提出了MANGO，一个包含42,703对图像-掩码的大规模全球红树林数据集，覆盖124个国家，旨在支持基于深度学习的红树林监测。


<details>
  <summary>Details</summary>
Motivation: 现有红树林数据集存在不足，如仅提供年度地图、缺乏单日期图像-掩码对、地域覆盖有限或未公开，阻碍了深度学习在红树林检测中的应用。

Method: 利用2020年所有可用的Sentinel-2影像，结合年度红树林掩码，通过一种基于目标检测的方法，利用像素级坐标参考选取最优单日期观测，构建图像-掩码对；并基于国家不重叠划分对多种语义分割模型进行基准测试。

Result: 构建了覆盖124个国家、包含42,703个标注样本的MANGO数据集，并提供了多种分割模型的基准性能，为全球红树林监测奠定基础。

Conclusion: MANGO数据集填补了现有红树林遥感数据的空白，支持可扩展、可靠的全球红树林深度学习监测研究。

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

Abstract (中文翻译): 红树林对于缓解气候变化至关重要，需要可靠的监测手段以实现有效保护。尽管深度学习已成为红树林检测的有力工具，但其进展受到现有数据集局限性的制约。特别是，许多资源仅提供年度地图产品，缺乏经过整理的单日期图像-掩码对，局限于特定区域而非全球覆盖，或未向公众开放。为应对这些挑战，我们提出了MANGO——一个大规模全球数据集，包含来自124个国家的42,703对标注图像-掩码。为构建该数据集，我们在2020年红树林区域检索所有可用的Sentinel-2影像，并选择与红树林年度掩码最匹配的最佳单日期观测。该选择过程采用了一种基于目标检测的方法，利用像素级坐标参考，以确保图像-掩码配对的自适应性和代表性。我们还在国家不重叠划分下对多种语义分割架构进行了基准测试，为可扩展且可靠的全球红树林监测奠定了基础。

</details>


### [7] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: 本文提出了一种用于转录15-16世纪拉丁文历史文献的流水线方法，通过结合版面分析与OCR模型，在保留特殊字符和符号的同时高效实现全文数字化。


<details>
  <summary>Details</summary>
Motivation: 15至16世纪用拉丁文书写的歷史文献包含具有特定含义的特殊字符和符号，转录时需保留这些特征以维持文本的原始风格与意义，这对传统OCR方法构成挑战。

Method: 在现有文本行识别方法基础上，引入版面分析模型：先用版面分析模型从历史文档图像中提取文本行，再交由OCR模型生成完整数字化页面。

Result: 该流水线在多个数据集上表现良好，能高效处理手写、印刷及多语言文本，验证了所用掩码自编码器的有效性。

Conclusion: 所提出的结合版面分析与OCR的流水线方法能有效保留历史文献中的特殊字符与格式，实现高质量的数字化转录。

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

Abstract (中文翻译): 对15和16世纪用拉丁文书写的歷史文献进行转录具有特殊挑战，因为必须保留具有特定含义的字符和特殊符号，以确保历史文本保持其原始风格和意义。本研究提出了一种用于转录历史文献并保留这些特殊特征的流水线方法。我们建议在现有文本行识别方法的基础上，加入一个版面分析模型。具体而言，我们使用版面分析模型对历史文本图像进行分析以提取文本行，然后由OCR模型处理这些文本行，生成完全数字化的页面。我们展示了该流水线能够简化页面处理流程并产生高效的结果。我们在多个数据集上评估了该方法，并证明掩码自编码器能够有效处理包括手写、印刷和多语言在内的多种文本类型。

</details>


### [8] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: 本文提出一种融合Leap Motion与RGB摄像头数据的多模态方法，用于阿拉伯手语识别，在自建18词数据集上达到78%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯手语识别系统多依赖单一传感器（如Leap Motion或RGB摄像头），难以准确捕捉复杂手部姿态和三维动作，限制了识别性能。

Method: 构建双通道子网络：一为带Dropout和L2正则化的自定义密集神经网络处理Leap Motion数据，另一为经数据增强微调的VGG16模型处理RGB图像；两路特征拼接后通过全连接层和SoftMax分类。

Result: 在包含18个阿拉伯手语词汇的自建数据集上，系统正确识别13个，整体准确率达78%。

Conclusion: 多模态融合在阿拉伯手语识别中具有初步可行性，但需进一步优化模型并扩展数据集以提升性能。

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

Abstract (中文翻译): 阿拉伯手语（ArSL）是聋人及听力障碍群体的重要交流方式。然而，现有识别系统因依赖单一传感器（如Leap Motion或RGB摄像头）而面临显著挑战，难以充分追踪复杂的手部朝向和精确识别三维手部动作。本研究旨在探索结合Leap Motion与RGB摄像头数据的多模态方法在ArSL识别中的可行性。系统架构包含两个并行子网络：一个针对Leap Motion数据的自定义密集神经网络（引入Dropout和L2正则化），以及一个基于微调VGG16模型并辅以数据增强技术的图像子网络。两种模态的特征表示在融合模型中拼接，并通过全连接层处理，最终利用SoftMax激活函数进行分类，以分析手势的空间与时间特征。系统在包含18个ArSL词汇的自建数据集上进行评估，成功识别其中13个，整体准确率达到78%。该结果初步验证了多模态融合在手语识别中的潜力，并指出了进一步优化模型与扩充数据集的方向。

</details>


### [9] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种解耦的成员-子空间注意力机制（DMSA），通过解耦MCR2目标中“成员矩阵”与“子空间矩阵U”的功能关系，实现了更快的编码压缩率和更高的ImageNet-1K准确率，同时提升了计算效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MCR2的白盒Transformer中，“成员矩阵”与“子空间矩阵U”紧密耦合，导致在错误token投影下产生冗余编码，影响模型效率与性能。

Method: 作者解耦MCR2目标中成员矩阵与子空间的功能关系，从输入直接学习成员矩阵，并从全空间S导出稀疏子空间；通过对优化后的MCR2目标进行梯度展开，推导出可解释的稀疏线性注意力算子DMSA。

Result: 在ImageNet-1K上，仅将ToST中的注意力模块替换为DMSA（即DMST）即可提升1.08%-1.45%的top-1准确率，并实现更快的编码压缩率；相比标准Transformer，DMST具有更高的计算效率和可解释性。

Conclusion: 所提出的DMSA有效解决了MCR2中成员与子空间耦合带来的冗余问题，在保持可解释性的同时显著提升了模型性能与效率。

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

Abstract (中文翻译): 基于结构化表征学习的最大编码率缩减（MCR2）驱动的白盒Transformer统一了可解释性与效率，为视觉建模提供了可靠的白盒解决方案。然而，在现有设计中，MCR2中的“成员矩阵”与“子空间矩阵U”之间存在紧密耦合，导致在错误的token投影下产生冗余编码。为此，我们解耦了MCR2目标中“成员矩阵”与“子空间U”之间的功能关系，并从优化目标的展开梯度下降中推导出一种可解释的稀疏线性注意力算子。具体而言，我们提出直接从输入中学习成员矩阵，随后从全空间S中导出稀疏子空间。因此，对优化后的MCR2目标进行梯度展开，得到了一种可解释的稀疏线性注意力算子：解耦成员-子空间注意力（DMSA）。在视觉任务上的实验结果表明，仅将Token统计Transformer（ToST）中的注意力模块替换为DMSA（我们称之为DMST），不仅实现了更快的编码缩减率，而且在ImageNet-1K数据集上的top-1准确率比ToST高出1.08%–1.45%。与标准Transformer架构相比，DMST展现出显著更高的计算效率和可解释性。

</details>


### [10] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 提出一种基于语义分割的深度学习方法，从高噪声透射电镜（TEM）图像中准确估计原子柱的三维深度信息。


<details>
  <summary>Details</summary>
Motivation: 透射电镜（TEM）图像常受严重噪声干扰，难以直接提取原子级三维结构信息，因此需要一种鲁棒且准确的方法来从噪声图像中恢复深度信息。

Method: 将深度估计问题转化为语义分割任务，利用合成噪声污染的模拟数据训练深度卷积神经网络，生成像素级深度分割图。

Result: 在CeO2纳米颗粒的模拟和真实TEM图像上验证了该方法，结果表明其深度估计准确、经过校准且对噪声具有鲁棒性。

Conclusion: 该方法有效解决了从高噪声TEM图像中提取原子级三维信息的难题，为材料科学中的三维结构分析提供了新工具。

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

Abstract (中文翻译): 我们提出了一种新颖的方法，用于从受显著噪声影响的透射电子显微镜（TEM）图像中提取原子级三维信息。该方法将深度估计问题转化为语义分割任务，并通过使用添加了合成噪声的模拟数据训练深度卷积神经网络，生成像素级的深度分割图。所提出的方法被应用于从模拟图像和真实TEM数据中估计CeO2纳米颗粒中原子柱的深度。实验结果表明，所得深度估计结果准确、经过校准，并且对噪声具有良好的鲁棒性。

</details>
