<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation](https://arxiv.org/abs/2512.21402)
*Arnav Gupta,Gurekas Singh Sahney,Hardik Rathi,Abhishek Chandwani,Ishaan Gupta,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉语言模型（VLM）的数据驱动框架，用于评估短视频内容的观众参与度，通过提取无监督音视频特征、聚类为可解释因子，并训练回归模型预测教育娱乐类短视频的参与度，相比传统指标更具可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估方法（如VideoScore-2）虽能衡量视觉与语义保真度，但无法揭示具体音视频属性如何影响真实观众参与度，因此需要更贴近人类感知且具备多模态推理能力的评估体系。

Method: 利用视觉语言模型（VLM）从短视频中提取无监督的音视频特征，将这些特征聚类为可解释的因素，并基于此训练一个回归模型来预测观众参与度；研究使用了自建的YouTube Shorts数据集进行系统分析。

Result: 实验表明，该方法预测的参与度与实际观众行为高度相关，且相比SSIM、FID等传统指标，所提出的轻量级特征评估器在可解释性和可扩展性方面表现更优。

Conclusion: 通过结合多模态特征重要性与以人类为中心的参与信号，该方法推动了稳健且可解释的视频理解评估的发展。

Abstract: Evaluating short-form video content requires moving beyond surface-level quality metrics toward human-aligned, multimodal reasoning. While existing frameworks like VideoScore-2 assess visual and semantic fidelity, they do not capture how specific audiovisual attributes drive real audience engagement. In this work, we propose a data-driven evaluation framework that uses Vision-Language Models (VLMs) to extract unsupervised audiovisual features, clusters them into interpretable factors, and trains a regression-based evaluator to predict engagement on short-form edutainment videos. Our curated YouTube Shorts dataset enables systematic analysis of how VLM-derived features relate to human engagement behavior. Experiments show strong correlations between predicted and actual engagement, demonstrating that our lightweight, feature-based evaluator provides interpretable and scalable assessments compared to traditional metrics (e.g., SSIM, FID). By grounding evaluation in both multimodal feature importance and human-centered engagement signals, our approach advances toward robust and explainable video understanding.

Abstract (中文翻译): 评估短视频内容需要超越表面质量指标，转向与人类对齐的多模态推理。尽管现有的框架（如VideoScore-2）能够评估视觉和语义保真度，但它们未能捕捉特定音视频属性如何驱动真实观众的参与行为。在本研究中，我们提出了一种数据驱动的评估框架，该框架利用视觉语言模型（VLM）提取无监督的音视频特征，将其聚类为可解释的因素，并训练一个基于回归的评估器，用于预测教育娱乐类短视频的观众参与度。我们构建的YouTube Shorts数据集使得系统分析VLM提取的特征与人类参与行为之间的关系成为可能。实验结果表明，预测参与度与实际参与度之间具有强相关性，证明我们提出的轻量级、基于特征的评估器相比传统指标（如SSIM、FID）提供了更具可解释性和可扩展性的评估方式。通过将评估建立在多模态特征重要性与以人类为中心的参与信号基础之上，我们的方法朝着实现稳健且可解释的视频理解迈出了重要一步。

</details>


### [2] [A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding](https://arxiv.org/abs/2512.21414)
*Christina Liu,Alan Q. Wang,Joy Hsu,Jiajun Wu,Ehsan Adeli*

Main category: cs.CV

TL;DR: 本文提出了一种用于医学图像理解的新型工具使用框架——工具瓶颈框架（TBF），通过学习一个工具瓶颈模型（TBM）来融合由视觉语言模型（VLM）选择的工具输出，而非依赖文本组合。该方法在组织病理学和皮肤科任务中表现优于或媲美现有深度学习模型、VLM及先进工具使用框架，尤其在数据有限场景下效果显著，并提升了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型（VLM）的工具使用框架通常依赖文本（如代码或自然语言中的函数调用）来组合多个工具的输出，但在医学图像理解任务中效果不佳，因为医学图像中的关键信息常以空间局部特征形式存在，难以仅通过文本有效融合。

Method: 作者提出了工具瓶颈框架（TBF），首先利用现成的医学VLM从工具箱中选择与临床相关的特征提取工具；然后引入一个可学习的工具瓶颈模型（TBM），通过神经网络对所选工具的输出进行计算与融合，最终生成预测结果。该方法避免了文本组合的局限性，并支持任意VLM工具选择下的预测。

Result: 在组织病理学和皮肤科任务上的实验表明，TBF在性能上达到或超过现有的深度学习分类器、VLM以及最先进的工具使用框架，尤其在数据稀缺的情况下提升明显。同时，该框架提供了更具可解释性和临床依据的预测。

Conclusion: 工具瓶颈框架（TBF）通过神经网络融合VLM选择的工具输出，有效解决了医学图像理解中空间局部特征难以文本组合的问题，在多个医学图像任务中展现出优越性能和良好可解释性，尤其适用于数据有限的场景。

Abstract: Recent tool-use frameworks powered by vision-language models (VLMs) improve image understanding by grounding model predictions with specialized tools. Broadly, these frameworks leverage VLMs and a pre-specified toolbox to decompose the prediction task into multiple tool calls (often deep learning models) which are composed to make a prediction. The dominant approach to composing tools is using text, via function calls embedded in VLM-generated code or natural language. However, these methods often perform poorly on medical image understanding, where salient information is encoded as spatially-localized features that are difficult to compose or fuse via text alone. To address this, we propose a tool-use framework for medical image understanding called the Tool Bottleneck Framework (TBF), which composes VLM-selected tools using a learned Tool Bottleneck Model (TBM). For a given image and task, TBF leverages an off-the-shelf medical VLM to select tools from a toolbox that each extract clinically-relevant features. Instead of text-based composition, these tools are composed by the TBM, which computes and fuses the tool outputs using a neural network before outputting the final prediction. We propose a simple and effective strategy for TBMs to make predictions with any arbitrary VLM tool selection. Overall, our framework not only improves tool-use in medical imaging contexts, but also yields more interpretable, clinically-grounded predictors. We evaluate TBF on tasks in histopathology and dermatology and find that these advantages enable our framework to perform on par with or better than deep learning-based classifiers, VLMs, and state-of-the-art tool-use frameworks, with particular gains in data-limited regimes. Our code is available at https://github.com/christinaliu2020/tool-bottleneck-framework.

Abstract (中文翻译): 近期基于视觉语言模型（VLM）的工具使用框架通过将模型预测与专用工具相结合，提升了图像理解能力。这类框架通常利用VLM和预定义的工具箱，将预测任务分解为多次工具调用（通常是深度学习模型），再将这些工具的输出组合以形成最终预测。目前主流的工具组合方式是通过文本实现的，例如在VLM生成的代码或自然语言中嵌入函数调用。然而，这些方法在医学图像理解任务中表现不佳，因为医学图像中的关键信息通常以空间局部特征的形式编码，仅靠文本难以有效组合或融合。为解决这一问题，我们提出了一种面向医学图像理解的工具使用框架——工具瓶颈框架（Tool Bottleneck Framework, TBF），它通过一个可学习的工具瓶颈模型（Tool Bottleneck Model, TBM）来组合由VLM选定的工具。对于给定图像和任务，TBF利用现成的医学VLM从工具箱中选择能提取临床相关特征的工具；不同于基于文本的组合方式，这些工具的输出由TBM通过神经网络进行计算与融合，最终输出预测结果。我们还提出了一种简单而有效的策略，使TBM能够对任意VLM工具选择进行预测。总体而言，该框架不仅改善了医学影像场景下的工具使用效果，还生成了更具可解释性且临床依据充分的预测器。我们在组织病理学和皮肤科任务上评估了TBF，结果表明其性能达到甚至优于基于深度学习的分类器、VLM以及当前最先进的工具使用框架，尤其在数据有限的情况下优势更为明显。代码已开源：https://github.com/christinaliu2020/tool-bottleneck-framework。

</details>


### [3] [Scalable Deep Subspace Clustering Network](https://arxiv.org/abs/2512.21434)
*Nairouz Mrabah,Mohamed Bouguessa,Sihem Sami*

Main category: cs.CV

TL;DR: SDSNet 是一种可扩展的深度子空间聚类网络，通过锚点近似、联合优化和因子化谱聚类，将计算复杂度从 O(n³) 降至 O(n)，在保持聚类性能的同时大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类方法因需构建完整的 n×n 相似度矩阵并进行谱分解，导致 O(n³) 的计算复杂度，难以扩展；即使深度学习方法改进了特征提取，仍受限于全对相似度计算的瓶颈。

Method: 提出 SDSNet 框架，采用（1）基于锚点的近似避免完整亲和矩阵，（2）联合优化自编码器重建与自表达目标，（3）在因子化表示上直接进行谱聚类，并结合卷积自编码器与子空间保持约束。

Result: 实验表明，SDSNet 在聚类质量上与当前最优方法相当，同时显著提升了计算效率。

Conclusion: SDSNet 成功解决了子空间聚类中的可扩展性问题，在保持高聚类性能的同时实现了线性时间复杂度。

Abstract: Subspace clustering methods face inherent scalability limits due to the $O(n^3)$ cost (with $n$ denoting the number of data samples) of constructing full $n\times n$ affinities and performing spectral decomposition. While deep learning-based approaches improve feature extraction, they maintain this computational bottleneck through exhaustive pairwise similarity computations. We propose SDSNet (Scalable Deep Subspace Network), a deep subspace clustering framework that achieves $\mathcal{O}(n)$ complexity through (1) landmark-based approximation, avoiding full affinity matrices, (2) joint optimization of auto-encoder reconstruction with self-expression objectives, and (3) direct spectral clustering on factorized representations. The framework combines convolutional auto-encoders with subspace-preserving constraints. Experimental results demonstrate that SDSNet achieves comparable clustering quality to state-of-the-art methods with significantly improved computational efficiency.

Abstract (中文翻译): 子空间聚类方法由于构建完整的 $n\times n$ 亲和矩阵及执行谱分解所需 $O(n^3)$ 的计算代价（其中 $n$ 表示数据样本数量），存在固有的可扩展性限制。尽管基于深度学习的方法改善了特征提取，但它们仍通过穷举式的成对相似度计算维持了这一计算瓶颈。我们提出了 SDSNet（可扩展深度子空间网络），这是一种深度子空间聚类框架，通过以下方式实现 $\mathcal{O}(n)$ 的复杂度：（1）基于锚点的近似方法，避免构建完整的亲和矩阵；（2）联合优化自编码器重建损失与自表达目标；（3）在因子化表示上直接进行谱聚类。该框架结合了卷积自编码器与子空间保持约束。实验结果表明，SDSNet 在聚类质量上与当前最先进的方法相当，同时显著提升了计算效率。

</details>


### [4] [Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism](https://arxiv.org/abs/2512.21452)
*Haotian Lv,Yuhui Zhang,Jiangbo Dai,Hanli Wu,Jiaji Wang,Dawei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于DCGAN数据增强和新型多模态链与全局注意力网络（MCGA-Net）的框架，用于提升探地雷达（GPR）图像中道路地下缺陷的自动检测精度与鲁棒性，在复杂背景下实现了高准确率和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统探地雷达（GPR）图像解释高度依赖专家主观判断，效率低且易出错，亟需一种自动化、高精度且鲁棒的缺陷检测方法。

Method: 该研究提出了一个综合框架：(1) 使用DCGAN进行数据增强，生成高保真GPR图像以缓解数据稀缺问题；(2) 提出MCGA-Net网络，结合多模态链特征融合（MCFF）和全局注意力机制（GAM）；(3) 利用MS COCO迁移学习微调主干网络以加速收敛并提升泛化能力。

Result: MCGA-Net在实验中达到Precision 92.8%、Recall 92.5%、mAP@50 95.9%，在高斯噪声、弱信号和小目标检测任务中表现优于其他模型，展现出良好鲁棒性。

Conclusion: 该工作为基于GPR的地下缺陷自动检测建立了一个兼顾计算效率与高精度的新范式，适用于复杂地下环境。

Abstract: Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.

Abstract (中文翻译): 探地雷达（GPR）已成为评估道路地下缺陷的一种关键无损检测工具。然而，传统的GPR图像解释严重依赖主观经验，导致效率低下和准确性不足。本研究提出了一套综合框架以解决上述问题：（1）基于DCGAN的数据增强策略可合成高保真度的GPR图像，在复杂背景下保留缺陷形态，缓解数据稀缺问题；（2）提出一种新颖的多模态链与全局注意力网络（MCGA-Net），通过多模态链特征融合（MCFF）实现多层次多尺度缺陷表征，并利用全局注意力机制（GAM）增强上下文感知特征；（3）采用MS COCO迁移学习对主干网络进行微调，加快收敛速度并提升泛化能力。消融实验与对比实验验证了该框架的有效性。MCGA-Net在Precision（92.8%）、Recall（92.5%）和mAP@50（95.9%）等指标上表现优异，在高斯噪声、弱信号及小目标检测任务中仍保持稳健性，优于其他模型。本研究为基于GPR的自动化缺陷检测建立了一种在复杂地下环境中兼顾计算效率与高精度的新范式。

</details>


### [5] [CCAD: Compressed Global Feature Conditioned Anomaly Detection](https://arxiv.org/abs/2512.21459)
*Xiao Jin,Liang Diao,Qixin Xiao,Yifan Hu,Ziqi Zhang,Yuchen Liu,Haisong Gu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CCAD的新方法，结合了重建和无监督表示两种范式的优势，通过将全局特征作为重建模型的新模态条件，并引入自适应压缩机制，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法中，无监督表示方法在域偏移下难以提取鲁棒特征，而基于重建的方法则因约束不足导致训练效率低、性能下降。为解决这些问题，作者提出了新方法CCAD。

Method: 提出Compressed Global Feature Conditioned Anomaly Detection（CCAD）方法，将全局特征作为重建模型的条件，并设计自适应压缩机制以提升泛化能力和训练效率。

Result: 实验表明，CCAD在AUC指标上持续优于当前最先进的方法，并且收敛更快；同时发布了重新整理和标注的DAGM 2007数据集。

Conclusion: CCAD有效结合了重建与无监督表示方法的优点，在异常检测任务中表现出更优的性能和效率。

Abstract: Anomaly detection holds considerable industrial significance, especially in scenarios with limited anomalous data. Currently, reconstruction-based and unsupervised representation-based approaches are the primary focus. However, unsupervised representation-based methods struggle to extract robust features under domain shift, whereas reconstruction-based methods often suffer from low training efficiency and performance degradation due to insufficient constraints. To address these challenges, we propose a novel method named Compressed Global Feature Conditioned Anomaly Detection (CCAD). CCAD synergizes the strengths of both paradigms by adapting global features as a new modality condition for the reconstruction model. Furthermore, we design an adaptive compression mechanism to enhance both generalization and training efficiency. Extensive experiments demonstrate that CCAD consistently outperforms state-of-the-art methods in terms of AUC while achieving faster convergence. In addition, we contribute a reorganized and re-annotated version of the DAGM 2007 dataset with new annotations to further validate our method's effectiveness. The code for reproducing main results is available at https://github.com/chloeqxq/CCAD.

Abstract (中文翻译): 异常检测在工业领域具有重要意义，尤其是在异常数据有限的场景中。目前，基于重建和基于无监督表示的方法是研究的重点。然而，无监督表示方法在域偏移情况下难以提取鲁棒特征，而基于重建的方法常因约束不足而导致训练效率低下和性能下降。为应对这些挑战，我们提出了一种名为“压缩全局特征条件异常检测”（Compressed Global Feature Conditioned Anomaly Detection, CCAD）的新方法。CCAD通过将全局特征作为重建模型的一种新模态条件，融合了上述两类方法的优势。此外，我们设计了一种自适应压缩机制，以增强模型的泛化能力和训练效率。大量实验表明，CCAD在AUC指标上始终优于当前最先进的方法，并实现了更快的收敛速度。此外，我们还提供了重新整理和重新标注的DAGM 2007数据集，包含新的标注，以进一步验证所提方法的有效性。复现主要结果的代码已公开于 https://github.com/chloeqxq/CCAD。

</details>


### [6] [IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset](https://arxiv.org/abs/2512.21472)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 本文发布了ISIC MultiAnnot++，一个大规模公开的多标注者皮肤病变分割数据集，包含17,684个分割掩码，覆盖14,967张皮肤镜图像，其中2,394张图像具有2–5个不同标注，并附带标注者技能水平和工具等元数据，是目前最大的公开SLS数据集。


<details>
  <summary>Details</summary>
Motivation: 多标注者医学图像分割研究缺乏大规模、公开可用且带有标注者信息的皮肤镜图像分割数据集，限制了相关方法的发展与评估。

Method: 构建并发布名为ISIC MultiAnnot++的新数据集，整合来自ISIC Archive的皮肤镜图像及其多个专家标注的分割掩码，并提供标注者技能等级、所用工具等元数据；同时提供数据集特性分析、精选数据划分及共识分割掩码。

Result: 该数据集包含17,684个分割掩码，覆盖14,967张皮肤镜图像，其中2,394张图像具有2–5个独立标注，成为当前最大规模的公开多标注者皮肤病变分割数据集，并支持标注偏好建模与元数据分析等研究。

Conclusion: ISIC MultiAnnot++填补了多标注者皮肤镜图像分割数据集的空白，为医学图像分割中涉及标注者差异性的研究提供了重要资源。

Abstract: Multi-annotator medical image segmentation is an important research problem, but requires annotated datasets that are expensive to collect. Dermoscopic skin lesion imaging allows human experts and AI systems to observe morphological structures otherwise not discernable from regular clinical photographs. However, currently there are no large-scale publicly available multi-annotator skin lesion segmentation (SLS) datasets with annotator-labels for dermoscopic skin lesion imaging. We introduce ISIC MultiAnnot++, a large public multi-annotator skin lesion segmentation dataset for images from the ISIC Archive. The final dataset contains 17,684 segmentation masks spanning 14,967 dermoscopic images, where 2,394 dermoscopic images have 2-5 segmentations per image, making it the largest publicly available SLS dataset. Further, metadata about the segmentation, including the annotators' skill level and segmentation tool, is included, enabling research on topics such as annotator-specific preference modeling for segmentation and annotator metadata analysis. We provide an analysis on the characteristics of this dataset, curated data partitions, and consensus segmentation masks.

Abstract (中文翻译): 多标注者医学图像分割是一个重要的研究问题，但需要昂贵的标注数据集。皮肤镜皮肤病变成像使人类专家和人工智能系统能够观察到常规临床照片中无法分辨的形态结构。然而，目前尚无大规模公开可用的、带有标注者标签的皮肤镜皮肤病变分割（SLS）数据集。我们提出了ISIC MultiAnnot++，这是一个面向ISIC档案库图像的大规模公开多标注者皮肤病变分割数据集。最终数据集包含17,684个分割掩码，涵盖14,967张皮肤镜图像，其中2,394张图像每张具有2至5个分割标注，使其成为目前最大的公开SLS数据集。此外，数据集还包含关于分割的元数据，如标注者的技能水平和所使用的分割工具，从而支持诸如针对特定标注者偏好的分割建模和标注者元数据分析等研究。我们还提供了对该数据集特性的分析、精选的数据划分以及共识分割掩码。

</details>


### [7] [GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification](https://arxiv.org/abs/2512.21476)
*Suncheng Xiang,Xiaoyang Wang,Junjie Jiang,Hejia Wang,Dahong Qian*

Main category: cs.CV

TL;DR: 本文提出了一种门控渐进融合网络（Gated Progressive Fusion Network），通过多层级特征的门控全连接融合策略，提升结肠镜息肉重识别（ReID）任务中对小目标的识别性能，在标准基准上优于现有单模态方法。


<details>
  <summary>Details</summary>
Motivation: 在结肠镜息肉重识别任务中，由于高阶特征分辨率粗糙，难以保留小息肉的细节信息，导致识别效果不佳。

Method: 提出一种名为门控渐进融合网络的新架构，利用门控机制以全连接方式选择性融合多层级特征，并引入逐层细化的语义信息融合策略。

Result: 在标准基准上的实验表明，所提出的多模态设置结合专门的融合策略，显著优于当前最先进的单模态ReID模型。

Conclusion: 该方法有效提升了息肉重识别的准确性，尤其适用于细节信息关键的小目标场景，对结直肠癌的计算机辅助诊疗具有重要意义。

Abstract: Colonoscopic Polyp Re-Identification aims to match the same polyp from a large gallery with images from different views taken using different cameras, which plays an important role in the prevention and treatment of colorectal cancer in computer-aided diagnosis. However, the coarse resolution of high-level features of a specific polyp often leads to inferior results for small objects where detailed information is important. To address this challenge, we propose a novel architecture, named Gated Progressive Fusion network, to selectively fuse features from multiple levels using gates in a fully connected way for polyp ReID. On the basis of it, a gated progressive fusion strategy is introduced to achieve layer-wise refinement of semantic information through multi-level feature interactions. Experiments on standard benchmarks show the benefits of the multimodal setting over state-of-the-art unimodal ReID models, especially when combined with the specialized multimodal fusion strategy.

Abstract (中文翻译): 结肠镜息肉重识别旨在从大量图库中匹配出由不同摄像头、不同视角拍摄的同一息肉图像，在计算机辅助诊断中对结直肠癌的预防与治疗具有重要作用。然而，特定息肉的高层特征分辨率较粗，往往在细节信息至关重要的小目标上表现不佳。为解决这一挑战，我们提出了一种名为门控渐进融合网络（Gated Progressive Fusion Network）的新架构，通过门控机制以全连接方式选择性地融合多层级特征。在此基础上，引入一种门控渐进融合策略，通过多层级特征交互实现语义信息的逐层精细化。在标准基准上的实验表明，该多模态设置相比当前最先进的单模态ReID模型具有明显优势，尤其是在结合专门设计的多模态融合策略时。

</details>


### [8] [Generative Multi-Focus Image Fusion](https://arxiv.org/abs/2512.21495)
*Xinzhe Xie,Buyu Guo,Bolin Li,Shuangyan He,Yanzhen Gu,Qingyan Jiang,Peiliang Li*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段生成式多聚焦图像融合框架GMFF：第一阶段使用StackMFF V4进行确定性融合，第二阶段通过IFControlNet利用潜在扩散模型进行生成式修复，以恢复缺失焦平面内容、消除边缘伪影并提升细节，从而在复杂场景中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设每个空间位置至少有一个输入图像处于聚焦状态，且在复杂真实场景中常因聚焦估计不准或硬选择操作而产生边缘伪影。为克服这些局限，作者提出新的生成式融合框架。

Method: 提出GMFF框架，包含两个级联阶段：1）使用StackMFF V4进行确定性融合，整合可用焦平面信息生成初始融合图像；2）通过IFControlNet利用潜在扩散模型的生成能力，重建缺失焦平面内容、恢复细节并消除边缘伪影。

Result: 大量实验表明，GMFF在多聚焦图像融合任务上达到当前最优性能，在包含复杂多焦内容的实际应用中展现出显著潜力。

Conclusion: 所提出的GMFF框架有效解决了传统方法在焦平面缺失和边缘伪影方面的不足，具备良好的实用价值，并已开源实现代码。

Abstract: Multi-focus image fusion aims to generate an all-in-focus image from a sequence of partially focused input images. Existing fusion algorithms generally assume that, for every spatial location in the scene, there is at least one input image in which that location is in focus. Furthermore, current fusion models often suffer from edge artifacts caused by uncertain focus estimation or hard-selection operations in complex real-world scenarios. To address these limitations, we propose a generative multi-focus image fusion framework, termed GMFF, which operates in two sequential stages. In the first stage, deterministic fusion is implemented using StackMFF V4, the latest version of the StackMFF series, and integrates the available focal plane information to produce an initial fused image. The second stage, generative restoration, is realized through IFControlNet, which leverages the generative capabilities of latent diffusion models to reconstruct content from missing focal planes, restore fine details, and eliminate edge artifacts. Each stage is independently developed and functions seamlessly in a cascaded manner. Extensive experiments demonstrate that GMFF achieves state-of-the-art fusion performance and exhibits significant potential for practical applications, particularly in scenarios involving complex multi-focal content. The implementation is publicly available at https://github.com/Xinzhe99/StackMFF-Series.

Abstract (中文翻译): 多聚焦图像融合旨在从一系列部分聚焦的输入图像中生成一张全聚焦图像。现有的融合算法通常假设场景中的每个空间位置至少在一个输入图像中是聚焦的。此外，当前的融合模型在复杂的现实场景中常常由于聚焦估计不确定或硬选择操作而产生边缘伪影。为了解决这些问题，我们提出了一种称为GMFF的生成式多聚焦图像融合框架，该框架分为两个连续阶段。第一阶段采用StackMFF系列的最新版本StackMFF V4进行确定性融合，并整合可用的焦平面信息以生成初始融合图像。第二阶段通过IFControlNet实现生成式修复，利用潜在扩散模型的生成能力来重建缺失焦平面的内容、恢复精细细节并消除边缘伪影。每个阶段独立开发，并以级联方式无缝运行。大量实验表明，GMFF实现了最先进的融合性能，在涉及复杂多焦内容的场景中展现出显著的实际应用潜力。相关代码已在https://github.com/Xinzhe99/StackMFF-Series公开。

</details>


### [9] [SVBench: Evaluation of Video Generation Models on Social Reasoning](https://arxiv.org/abs/2512.21507)
*Wenshuo Peng,Gongxuan Wang,Tianmeng Yang,Chuanhao Li,Xiaojie Xu,Hui He,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 本文提出首个用于评估视频生成模型社会推理能力的基准，发现当前模型虽在视觉逼真度上表现良好，但在意图识别、信念推理等社会认知任务上存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型虽然在视觉真实感、运动保真度和文本-视频对齐方面取得显著进展，但无法生成具有社会连贯性的行为，缺乏对人类意图、信念、情绪和社会规范的理解能力。

Method: 作者基于发展与社会心理学构建包含30个经典社会认知范式的基准，涵盖7个核心维度；并开发了一个无需训练的基于智能体的评估流程，包括提炼推理机制、合成多样化场景、通过线索批判实现概念中立与难度控制，并利用高性能视觉语言模型（VLM）从五个可解释维度评估生成视频。

Result: 对七个最先进视频生成系统的首次大规模评估显示，这些模型在表面合理性上表现良好，但在意图识别、信念推理、共同注意和亲社会推断等方面存在显著不足。

Conclusion: 现有视频生成模型尚未具备真正理解或再现人类社会行为的能力，亟需将社会认知机制整合进生成模型以提升其社会连贯性。

Abstract: Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.

Abstract (中文翻译): 近期的文本到视频生成模型在视觉真实感、运动保真度以及文本与视频对齐方面取得了显著进展，但它们在生成具有社会连贯性的行为方面仍存在根本性局限。与人类能够从简短的视觉线索中轻松推断意图、信念、情绪和社会规范不同，当前模型往往仅渲染字面场景，而未能捕捉其背后的因果或心理逻辑。为系统评估这一差距，我们提出了首个用于视频生成中社会推理能力的基准。该基准基于发展心理学与社会心理学的研究成果，将30个经典的社会认知范式组织为七个核心维度，包括心理状态推断、目标导向行为、共同注意、社会协调、亲社会行为、社会规范以及多智能体策略。为实现这些范式的可操作化，我们开发了一套完全无需训练的基于智能体的评估流程：（i）提炼每个实验的推理机制，（ii）合成多样化的可生成视频场景，（iii）通过基于线索的批判确保概念中立性和难度控制，（iv）利用高容量视觉语言模型（VLM）从五个可解释的社会推理维度对生成视频进行评估。利用该框架，我们对七个最先进的视频生成系统进行了首次大规模研究。结果表明，尽管现代模型在表层合理性方面表现出色，但在意图识别、信念推理、共同注意和亲社会推断方面系统性失败。

</details>


### [10] [Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification](https://arxiv.org/abs/2512.21508)
*Md Ashik Khan,Md Nahid Siddique*

Main category: cs.CV

TL;DR: 在胸部X光多模态分析中，参数高效训练（PET）方法（如LoRA、Adapter等）在仅使用2.5%可训练参数的情况下，显著优于全微调（AUROC 0.892–0.908 vs. 0.770），并在外部数据集上验证了可扩展性；但其校准性能较差，需后处理校正。


<details>
  <summary>Details</summary>
Motivation: 多模态胸部X光分析通常依赖对大型视觉-语言模型进行全参数微调，计算成本高昂。作者旨在探索参数高效的训练策略，在大幅减少可训练参数的同时维持甚至提升模型性能，并评估其在真实临床场景中的可行性。

Method: 在Indiana大学胸部X光数据集上，采用多种参数高效训练（PET）方法（包括冻结编码器、BitFit、LoRA和Adapter），固定可训练参数预算（2.37M，占总参数2.51%）；为避免数据泄露，从文本输入中删除病理学术语但保留临床上下文；同时在CheXpert数据集上进行外部验证，并与参数预算匹配的纯视觉模型对比。

Result: 所有PET方法在内部测试集上AUROC达0.892–0.908，显著优于使用94.3M参数的全微调（0.770）；在CheXpert上均取得>0.69 AUROC（<9%可训练参数），其中Adapter最佳（0.7214）；参数预算相同时，纯视觉模型（0.653 AUROC）略优于多模态模型（0.641 AUROC）；但PET方法校准误差较高（ECE 0.29–0.34 vs. 0.049）。

Conclusion: 冻结编码器的参数高效训练策略能在大幅降低计算成本的同时实现更优的分类性能，但其校准性能较差，需通过后处理校准才能满足临床部署要求；性能提升主要源于参数分配效率，而非跨模态协同作用。

Abstract: Multimodal chest X-Ray analysis often fine-tunes large vision-language models, which is computationally costly. We study parameter-efficient training (PET) strategies, including frozen encoders, BitFit, LoRA, and adapters for multi-label classification on the Indiana University Chest X-Ray dataset (3,851 image-report pairs; 579 test samples). To mitigate data leakage, we redact pathology terms from reports used as text inputs while retaining clinical context. Under a fixed parameter budget (2.37M parameters, 2.51% of total), all PET variants achieve AUROC between 0.892 and 0.908, outperforming full fine-tuning (0.770 AUROC), which uses 94.3M trainable parameters, a 40x reduction. External validation on CheXpert (224,316 images, 58x larger) confirms scalability: all PET methods achieve >0.69 AUROC with <9% trainable parameters, with Adapter achieving best performance (0.7214 AUROC). Budget-matched comparisons reveal that vision-only models (0.653 AUROC, 1.06M parameters) outperform budget-matched multimodal models (0.641 AUROC, 1.06M parameters), indicating improvements arise primarily from parameter allocation rather than cross-modal synergy. While PET methods show degraded calibration (ECE: 0.29-0.34) compared to simpler models (ECE: 0.049), this represents a tractable limitation addressable through post-hoc calibration methods. These findings demonstrate that frozen encoder strategies provide superior discrimination at substantially reduced computational cost, though calibration correction is essential for clinical deployment.

Abstract (中文翻译): 多模态胸部X光分析通常需要对大型视觉-语言模型进行微调，计算成本高昂。本文研究了多种参数高效训练（PET）策略，包括冻结编码器、BitFit、LoRA和适配器（Adapter），在印第安纳大学胸部X光数据集（3,851个图像-报告对，含579个测试样本）上进行多标签分类任务。为避免数据泄露，我们在用作文本输入的报告中删除了病理学术语，同时保留了临床上下文信息。在固定参数预算（237万参数，占模型总参数的2.51%）下，所有PET变体的AUROC介于0.892至0.908之间，显著优于使用9430万可训练参数的全微调方法（AUROC为0.770），实现了约40倍的参数减少。在规模大58倍的CheXpert数据集（224,316张图像）上的外部验证进一步证实了该方法的可扩展性：所有PET方法在可训练参数少于9%的情况下均达到超过0.69的AUROC，其中Adapter表现最佳（AUROC为0.7214）。在相同参数预算下（106万参数），纯视觉模型（AUROC 0.653）略优于多模态模型（AUROC 0.641），表明性能提升主要源于更高效的参数分配，而非跨模态协同效应。尽管PET方法的校准性能较差（ECE为0.29–0.34），相比简单模型（ECE为0.049）存在明显退化，但这一局限可通过后处理校准方法加以解决。研究结果表明，冻结编码器的策略能在显著降低计算成本的同时提供更优的判别能力，但临床部署前必须进行校准修正。

</details>
