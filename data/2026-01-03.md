<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments](https://arxiv.org/abs/2512.23786)
*Ankan Aich,Yangming Lee*

Main category: cs.CV

TL;DR: 本文提出一种结合Depth Anything V2高保真合成先验与动态向量低秩适配（DV-LORA）的方法，在内窥镜手术场景中实现更鲁棒的单目深度估计，尤其在高反光和透明结构区域表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自监督单目深度估计方法在手术内窥镜环境中因依赖含噪声的真实伪标签，常在细小器械和透明表面上出现边界崩溃问题，难以满足机器人手术对精度的要求。

Method: 利用Depth Anything V2架构提供的高保真合成先验，并通过动态向量低秩适配（DV-LORA）高效地将其迁移到医疗领域，以最小参数开销弥合合成与真实数据之间的差距。

Result: 在SCARED数据集上引入物理分层评估协议，新方法达到98.1%的准确率（<1.25），相对平方误差降低超17%，显著优于现有基线。

Conclusion: 所提方法在高反光等恶劣手术光照条件下展现出卓越的鲁棒性，为机器人手术中的单目深度估计设立了新的技术标杆。

Abstract: Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.

Abstract (中文翻译): 精确的单目深度估计（MDE）对于机器人手术至关重要，但在充满反光和液体的内窥镜环境中仍然十分脆弱。现有的自监督方法通常依赖于使用含噪声的真实世界伪标签训练的基础模型，在细小手术器械和透明表面上常常出现边界崩溃问题。本文通过利用Depth Anything V2架构所提供的高保真合成先验来解决这一问题，该先验能够天然捕捉细小结构的精确几何细节。我们采用动态向量低秩适配（DV-LORA）方法，高效地将这些先验知识迁移到医学领域，在最小化参数开销的同时弥合了合成域与真实域之间的差距。此外，我们在SCARED数据集上引入了一种物理分层的评估协议，以严格量化在高反光区域的性能表现——这些区域往往被整体指标所掩盖。我们的方法建立了新的最先进水平，在准确率（<1.25）上达到98.1%，并将平方相对误差降低了超过17%，相较于现有基线方法展现出在恶劣手术光照条件下的卓越鲁棒性。

</details>


### [2] [Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments](https://arxiv.org/abs/2512.23819)
*Surya Rayala,Marcos Quinones-Grueiro,Naveeduddin Mohammed,Ashwin T S,Benjamin Goldberg,Randall Spain,Paige Lawton,Gautam Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的自动评估系统，用于城市作战训练中的“进入并清房”（ECR）演练，通过计算机视觉技术从训练视频中提取人体姿态、视线和轨迹等信息，构建任务特定指标以量化认知、心理运动和团队协作能力，并整合到扩展的认知任务分析（CTA）框架中生成综合评分，支持在GIFT等平台中进行可操作的复盘反馈。


<details>
  <summary>Details</summary>
Motivation: 传统军事训练评估依赖昂贵侵入式传感器或主观人工观察，难以客观、规模化地评价士兵在合成训练环境（STE）中执行复杂任务（如ECR）时的认知、心理运动及团队协作能力。

Method: 利用计算机视觉模型从普通训练视频中提取2D骨架、视线向量和运动轨迹，构建任务特定指标衡量心理运动流畅性、态势感知与团队协调，并将其整合进加权的扩展认知任务分析（CTA）层级结构中，生成整体表现评分。

Result: 在真实ECR演练案例中验证了该方法，成功生成了可操作的、领域特定的个体与团队表现指标，并可通过Gamemaster和GIFT平台的交互式仪表板支持行动后复盘（AAR）。

Conclusion: 该视频分析方法为合成训练环境提供了无需额外硬件的可扩展评估手段，尽管存在跟踪精度、真值验证等局限，未来将拓展至3D视频分析以进一步提升评估能力。

Abstract: Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.

Abstract (中文翻译): 有效的城市作战训练需要通过在逼真且受控的环境中反复练习来培养态势感知能力和肌肉记忆。其中一项关键训练科目是“进入并清房”（Enter and Clear the Room, ECR），要求士兵进行威胁评估、协同配合并控制密闭空间。军队采用合成训练环境（Synthetic Training Environments, STEs）提供可扩展、受控的场景以支持重复演练。然而，实现自动化的表现评估仍具挑战性，尤其是在力求客观评价认知能力、心理运动技能和团队协作能力方面。传统方法通常依赖昂贵且侵入式的传感器或主观的人工观察，限制了评估的可扩展性和准确性。本文提出了一种基于视频的评估流程，能够直接从训练视频中提取表现分析数据，而无需额外硬件。通过利用计算机视觉模型，该系统可提取2D人体骨架、视线向量和运动轨迹。基于这些数据，我们开发了任务特定的指标，用于衡量心理运动流畅性、态势感知和团队协调能力。这些指标被整合进一个扩展的认知任务分析（Cognitive Task Analysis, CTA）层级结构中，通过加权组合生成团队协作和认知能力的整体表现评分。我们通过一个真实世界ECR演练的案例研究展示了该方法的有效性，提供了可操作的、领域特定的个体与团队表现指标。我们还探讨了如何通过Gamemaster和通用智能辅导框架（Generalized Intelligent Framework for Tutoring, GIFT）中的交互式仪表板，利用这些洞察支持行动后复盘（After Action Reviews），提供直观易懂的反馈。最后，我们讨论了本方法的局限性，包括目标跟踪困难、真值验证问题以及方法的普适性，并指出未来工作将扩展至3D视频数据分析，并利用视频分析技术实现合成训练环境中可扩展的评估。

</details>


### [3] [Pretraining Frame Preservation in Autoregressive Video Memory Compression](https://arxiv.org/abs/2512.23851)
*Lvmin Zhang,Shengqu Cai,Muyang Li,Chong Zeng,Beijia Lu,Anyi Rao,Song Han,Gordon Wetzstein,Maneesh Agrawala*

Main category: cs.CV

TL;DR: 提出PFP神经网络结构，用于将长视频压缩为短上下文，保留任意时间位置的高频率细节，并可作为自回归视频模型的记忆编码器。


<details>
  <summary>Details</summary>
Motivation: 现有视频压缩方法在保留高频率细节和实现低上下文成本之间存在挑战，尤其在需要长期记忆的自回归视频生成任务中。

Method: 设计PFP神经网络结构，通过显式的预训练目标将20秒视频压缩至约5k长度的上下文，并支持任意帧的高质量重建；预训练模型可直接微调为自回归视频模型的记忆编码器。

Result: PFP能有效压缩视频并在低上下文成本下保持帧的感知质量；作为记忆编码器时，在长期历史建模中实现较低的保真度损失。

Conclusion: PFP提供了一种高效压缩长视频并保留细节的方法，适用于需要长期记忆的视频生成任务，并揭示了不同神经架构设计之间的权衡。

Abstract: We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.

Abstract (中文翻译): 我们提出了PFP，一种用于将长视频压缩为短上下文的神经网络结构，其显式的预训练目标旨在保留任意时间位置单帧的高频细节。基础模型可将一段20秒的视频压缩为长度约5k的上下文，从中可随机检索出感知外观得以保留的帧。此类预训练模型可直接微调为自回归视频模型的记忆编码器，在上下文开销较低的情况下实现长期历史记忆，并保持相对较低的保真度损失。我们通过消融实验评估该框架，并讨论了不同神经架构设计之间的权衡。

</details>


### [4] [Lifelong Domain Adaptive 3D Human Pose Estimation](https://arxiv.org/abs/2512.23860)
*Qucheng Peng,Hongfei Xue,Pu Wang,Chen Chen*

Main category: cs.CV

TL;DR: 本文提出了一种面向3D人体姿态估计（3D HPE）的终身域自适应新任务，并设计了一个结合3D姿态生成器、2D姿态判别器和3D姿态估计器的GAN框架，以应对目标域非平稳性和灾难性遗忘问题，在多个数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计方法依赖于受控环境中采集的标注3D数据，难以泛化到多样化的野外场景；而当前的域自适应方法（如通用DA和无源DA）忽略了目标姿态数据集的非平稳性问题。

Method: 提出终身域自适应3D HPE任务，并构建一个新型GAN框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器；其中3D姿态生成器融合姿态感知、时序感知和域感知知识，以增强当前域适应能力并缓解对先前域的灾难性遗忘。

Result: 在多个域自适应3D HPE数据集上的大量实验表明，所提方法性能优越。

Conclusion: 该工作首次将终身域自适应引入3D人体姿态估计任务，有效解决了目标域非平稳性和知识保留难题，为实际应用中的持续学习提供了新思路。

Abstract: 3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.

Abstract (中文翻译): 3D人体姿态估计（3D HPE）在人员重识别、动作识别到虚拟现实等多种应用中至关重要。然而，现有方法依赖于在受控环境中采集的标注3D数据，这使其难以泛化到多样化的野外场景。当前针对3D HPE的域自适应（DA）范式（如通用DA和无源DA）忽视了目标姿态数据集的非平稳性问题。为解决这些挑战，我们提出了一项名为“终身域自适应3D HPE”的新任务。据我们所知，这是首次将终身域自适应引入3D HPE任务。在此设定下，姿态估计器首先在源域上预训练，随后依次适应到不同的目标域；且在适应当前目标域时，无法访问源域及所有先前的目标域。该任务需同时克服对当前域姿态的适应挑战，并保留来自先前域的知识，尤其是应对灾难性遗忘问题。为此，我们提出了一种创新的生成对抗网络（GAN）框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器，有效缓解域偏移并实现原始与增强姿态的对齐。此外，我们构建了一种新颖的3D姿态生成器范式，融合姿态感知、时序感知和域感知知识，以增强当前域的适应能力并减轻对先前域的灾难性遗忘。大量实验在多个域自适应3D HPE数据集上验证了所提方法的优越性能。

</details>


### [5] [MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework](https://arxiv.org/abs/2512.23894)
*Krithika Iyer,Austin Tapp,Athelia Paulli,Gabrielle Dickerson,Syed Muhammad Anwar,Natasha Lepore,Marius George Linguraru*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的方法，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），并实现颅骨分割与颅缝概率热图生成，从而在无辐射条件下完成对颅缝的精确评估。


<details>
  <summary>Details</summary>
Motivation: 量化儿童颅骨发育和颅缝骨化对于诊断和治疗生长相关头颅疾病至关重要。尽管CT常用于评估颅骨和颅缝畸形，但其电离辐射对无明显异常的儿童存在禁忌；而MRI虽无辐射且软组织对比度高，却无法清晰显示颅缝、估算骨密度或评估颅顶生长。

Method: 研究开发了一个深度学习驱动的流程，利用变分自编码器将0.2–2岁儿童的T1加权MRI转换为合成CT（sCT），并在此基础上进行详细的颅骨分割、生成颅缝概率热图，并从中直接提取颅缝分割结果。

Result: 在内部儿科数据集上，合成CT与真实CT相比达到99%的结构相似性和1.01的Frechet inception距离；七块颅骨的平均Dice系数为85%，颅缝Dice系数达80%；通过双单侧检验（TOST）确认了sCT与真实CT在颅骨和颅缝分割上的等效性（p < 0.05）。

Conclusion: 这是首个能在MRI衍生的合成CT上实现颅缝分割的儿科颅骨CT合成框架，克服了MRI难以显示骨和颅缝的局限，通过高质量合成CT弥合了无创颅骨评估的关键空白。

Abstract: Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.

Abstract (中文翻译): 量化规范化的儿童颅骨发育和颅缝骨化对于诊断和治疗生长相关的头颅疾病至关重要。计算机断层扫描（CT）广泛用于评估颅骨和颅缝畸形，但其电离辐射对于无明显异常的儿童属于禁忌。磁共振成像（MRI）提供无辐射扫描且具有优异的软组织对比度，但与CT不同，MRI无法清晰显示颅缝、估算颅骨密度或评估颅顶生长。本研究提出了一种基于深度学习的流程，将0.2至2岁儿童的T1加权MRI转化为合成CT（sCT），并在此基础上预测详细的颅骨分割、生成颅缝概率热图，并从热图中直接获得颅缝分割结果。利用我们内部的儿科数据，合成CT相对于真实CT达到了99%的结构相似性和1.01的Frechet inception距离。七块颅骨的平均Dice系数为85%，颅缝Dice系数达到80%。通过双单侧检验（TOST，p < 0.05）确认了合成CT与真实CT在颅骨和颅缝分割上的等效性。据我们所知，这是首个能够在MRI衍生的合成CT上实现颅缝分割的儿科颅骨CT合成框架，尽管MRI对骨和颅缝的显示能力有限。通过结合鲁棒的、领域特定的变分自编码器，该方法能从常规儿科MRI中生成视觉上难以区分的颅骨合成CT，填补了无创颅骨评估中的关键空白。

</details>


### [6] [Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale](https://arxiv.org/abs/2512.23903)
*Charith Wickrema,Eliza Mace,Hunter Brown,Heidys Cabrera,Nick Krall,Matthew O'Neill,Shivangi Sarkar,Lowell Weissman,Eric Hughes,Guido Zarrella*

Main category: cs.CV

TL;DR: 该论文研究了在遥感领域使用超大规模高分辨率光电（EO）数据训练基础模型的缩放行为，发现即使在当前规模下，性能仍受数据限制而非模型参数限制，为未来遥感基础模型的数据收集、算力预算和优化策略提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前多模态机器学习（如生成式AI）依赖于针对非文本模态的专用编码器，而在遥感等高价值领域，缺乏像自然图像那样成熟的缩放规律来指导模型、算力与数据规模的协同扩展。

Method: 利用超过一千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练更大规模的视觉Transformer（ViT）骨干网络，并在拍字节级规模下观察其成功与失败模式。

Result: 实验表明，即使在如此大规模下，模型性能仍处于数据受限状态，而非模型参数受限；同时分析了跨遥感模态的领域差距问题。

Conclusion: 研究结果为遥感基础模型未来的数据采集策略、计算资源分配和训练调度提供了实践依据，有助于推动前沿遥感基础模型的发展。

Abstract: We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.

Abstract (中文翻译): 我们探索了人工智能的缩放行为，以建立在高分辨率光电（EO）数据集上训练基础模型的实用技术，这些数据集的规模比当前最先进的水平高出数个数量级。现代多模态机器学习（ML）应用，例如用于图像描述、搜索和推理的生成式人工智能（GenAI）系统，依赖于针对非文本模态的鲁棒且领域专用的编码器。在互联网规模数据丰富的自然图像领域，已有成熟的缩放规律可用于优化模型容量、训练算力和数据集规模的联合缩放。然而，在遥感（RS）等高价值领域，这些关系远未被充分理解。我们利用超过一千万亿像素的商业卫星EO数据和MITRE联邦AI沙箱，逐步训练更大规模的视觉Transformer（ViT）骨干网络，报告在拍字节级规模下观察到的成功与失败模式，并分析其对弥合其他遥感模态间领域差距的启示。我们观察到，即使在此规模下，模型性能仍符合数据受限状态，而非模型参数受限状态。这些实践洞见旨在为数据收集策略、算力预算和优化调度提供指导，以推动前沿遥感基础模型的未来发展。

</details>


### [7] [Learning to learn skill assessment for fetal ultrasound scanning](https://arxiv.org/abs/2512.23920)
*Yipei Wang,Qianye Yang,Lior Drukker,Aris T. Papageorghiou,Yipeng Hu,J. Alison Noble*

Main category: cs.CV

TL;DR: 本文提出一种无需人工标注技能等级的双层优化框架，通过评估胎儿超声图像上临床任务的完成质量来自动量化操作者的超声技能水平。


<details>
  <summary>Details</summary>
Motivation: 传统超声技能评估依赖专家主观监督，耗时且缺乏客观性；现有自动化方法多采用监督学习，并局限于预设的技能影响因素，难以全面反映真实技能水平。

Method: 提出一种新颖的双层优化框架，包含临床任务预测器和技能预测器，二者联合优化，通过在获取的胎儿超声图像上任务执行效果来评估技能，无需人工预定义技能评分。

Result: 在真实胎儿头部扫描超声视频数据上验证了该方法的有效性，结果表明该框架能通过优化后的任务表现有效预测超声操作技能。

Conclusion: 所提框架能够以任务性能作为技能指标，实现对胎儿超声技能的客观、自动化评估，具有实际应用潜力。

Abstract: Traditionally, ultrasound skill assessment has relied on expert supervision and feedback, a process known for its subjectivity and time-intensive nature. Previous works on quantitative and automated skill assessment have predominantly employed supervised learning methods, often limiting the analysis to predetermined or assumed factors considered influential in determining skill levels. In this work, we propose a novel bi-level optimisation framework that assesses fetal ultrasound skills by how well a task is performed on the acquired fetal ultrasound images, without using manually predefined skill ratings. The framework consists of a clinical task predictor and a skill predictor, which are optimised jointly by refining the two networks simultaneously. We validate the proposed method on real-world clinical ultrasound videos of scanning the fetal head. The results demonstrate the feasibility of predicting ultrasound skills by the proposed framework, which quantifies optimised task performance as a skill indicator.

Abstract (中文翻译): 传统上，超声技能评估依赖于专家的监督与反馈，这一过程具有主观性强且耗时的特点。以往关于定量和自动化技能评估的研究主要采用监督学习方法，通常将分析局限于预先设定或假定的影响技能水平的因素。在本研究中，我们提出了一种新颖的双层优化框架，通过评估在所获取的胎儿超声图像上任务完成的质量来评估胎儿超声技能，而无需使用人工预定义的技能评分。该框架包含一个临床任务预测器和一个技能预测器，通过同时优化两个网络进行联合训练。我们在真实临床胎儿头部扫描超声视频上验证了所提出的方法。结果表明，该框架可通过量化优化后的任务表现作为技能指标，有效预测超声技能水平。

</details>


### [8] [MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation](https://arxiv.org/abs/2512.23936)
*Yulong Zou,Bo Liu,Cun-Jing Zheng,Yuan-ming Geng,Siyue Li,Qiankun Zuo,Shuihua Wang,Yudong Zhang,Jin Hong*

Main category: cs.CV

TL;DR: 本文提出了一种名为MGML的新框架，用于在多模态MRI数据缺失情况下提升脑肿瘤分割性能，包含元参数自适应模态融合和一致性正则化模块，在BraTS2020/2023上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床中多模态MRI数据常不完整，难以充分利用多模态信息进行病灶分割，因此如何最大化利用不完整的多模态信息成为关键挑战。

Method: 提出元引导多模态学习（MGML）框架，包括元参数化自适应模态融合（Meta-AMF）和一致性正则化模块；Meta-AMF根据可用模态生成自适应软标签监督信号，促进多模态融合；一致性正则化提升分割性能并增强模型鲁棒性与泛化能力，且无需修改原模型结构，可端到端训练。

Result: 在BraTS2020和BraTS2023数据集上实验表明，该方法优于多个SOTA方法；在BraTS2020的15种缺失模态组合平均Dice分数上，WT、TC、ET分别达到87.55、79.36和62.67。

Conclusion: 所提MGML框架能有效应对多模态MRI缺失问题，显著提升脑肿瘤分割性能，具有良好的通用性和实用性，并已开源代码。

Abstract: Leveraging multimodal information from Magnetic Resonance Imaging (MRI) plays a vital role in lesion segmentation, especially for brain tumors. However, in clinical practice, multimodal MRI data are often incomplete, making it challenging to fully utilize the available information. Therefore, maximizing the utilization of this incomplete multimodal information presents a crucial research challenge. We present a novel meta-guided multi-modal learning (MGML) framework that comprises two components: meta-parameterized adaptive modality fusion and consistency regularization module. The meta-parameterized adaptive modality fusion (Meta-AMF) enables the model to effectively integrate information from multiple modalities under varying input conditions. By generating adaptive soft-label supervision signals based on the available modalities, Meta-AMF explicitly promotes more coherent multimodal fusion. In addition, the consistency regularization module enhances segmentation performance and implicitly reinforces the robustness and generalization of the overall framework. Notably, our approach does not alter the original model architecture and can be conveniently integrated into the training pipeline for end-to-end model optimization. We conducted extensive experiments on the public BraTS2020 and BraTS2023 datasets. Compared to multiple state-of-the-art methods from previous years, our method achieved superior performance. On BraTS2020, for the average Dice scores across fifteen missing modality combinations, building upon the baseline, our method obtained scores of 87.55, 79.36, and 62.67 for the whole tumor (WT), the tumor core (TC), and the enhancing tumor (ET), respectively. We have made our source code publicly available at https://github.com/worldlikerr/MGML.

Abstract (中文翻译): 利用磁共振成像（MRI）的多模态信息在病灶分割（尤其是脑肿瘤分割）中起着至关重要的作用。然而在临床实践中，多模态MRI数据常常不完整，使得难以充分利用已有信息。因此，如何最大化利用这种不完整的多模态信息成为一个关键的研究挑战。我们提出了一种新颖的元引导多模态学习（MGML）框架，该框架包含两个组成部分：元参数化自适应模态融合模块和一致性正则化模块。元参数化自适应模态融合（Meta-AMF）使模型能够在不同输入条件下有效整合多模态信息，通过基于可用模态生成自适应的软标签监督信号，显式促进更一致的多模态融合。此外，一致性正则化模块提升了分割性能，并隐式增强了整体框架的鲁棒性与泛化能力。值得注意的是，我们的方法无需更改原始模型架构，可便捷地集成到端到端模型优化的训练流程中。我们在公开的BraTS2020和BraTS2023数据集上进行了大量实验，结果表明，与近年来多种先进方法相比，我们的方法取得了更优的性能。在BraTS2020数据集上，针对十五种缺失模态组合的平均Dice分数，以基线模型为基础，我们的方法在全肿瘤（WT）、肿瘤核心（TC）和增强肿瘤（ET）上分别达到了87.55、79.36和62.67。我们已在https://github.com/worldlikerr/MGML公开了源代码。

</details>


### [9] [Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation](https://arxiv.org/abs/2512.23938)
*Hualin Ye,Bingxi Liu,Jixiang Du,Yu Qin,Ziyi Chen,Hong Zhang*

Main category: cs.CV

TL;DR: 本文提出一种新型跨视角地理定位（CVGL）系统，通过DINOv2骨干网络、多尺度通道重分配模块和基于MoE的聚合模块，在减少参数量的同时取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 跨视角地理定位面临视角差异大导致特征聚合与对齐困难的问题，现有方法难以有效处理这种异构输入。

Method: 1）采用DINOv2骨干网络结合卷积适配器微调；2）设计多尺度通道重分配模块以增强空间表示的多样性与稳定性；3）在特征聚合中引入混合专家（MoE）路由机制，动态选择专家子空间处理键值对。

Result: 在University-1652和SUES-200数据集上的实验表明，该方法以更少的训练参数实现了具有竞争力的性能。

Conclusion: 所提方法有效缓解了跨视角差异带来的挑战，在保持模型轻量化的同时提升了地理定位精度。

Abstract: Cross-view geo-localisation (CVGL) aims to estimate the geographic location of a query image by matching it with images from a large-scale database. However, the significant view-point discrepancies present considerable challenges for effective feature aggregation and alignment. To address these challenges, we propose a novel CVGL system that incorporates three key improvements. Firstly, we leverage the DINOv2 backbone with a convolution adapter fine-tuning to enhance model adaptability to cross-view variations. Secondly, we propose a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. Finally, we propose an improved aggregation module that integrates a Mixture-of-Experts (MoE) routing into the feature aggregation process. Specifically, the module dynamically selects expert subspaces for the keys and values in a cross-attention framework, enabling adaptive processing of heterogeneous input domains. Extensive experiments on the University-1652 and SUES-200 datasets demonstrate that our method achieves competitive performance with fewer trained parameters.

Abstract (中文翻译): 跨视角地理定位（CVGL）旨在通过将查询图像与大规模数据库中的图像进行匹配，来估计其地理位置。然而，显著的视角差异给有效的特征聚合与对齐带来了巨大挑战。为应对这些挑战，我们提出了一种新颖的CVGL系统，包含三项关键改进：首先，利用DINOv2骨干网络并结合卷积适配器进行微调，以增强模型对跨视角变化的适应能力；其次，提出一个多尺度通道重分配模块，以增强空间表示的多样性和稳定性；最后，提出一种改进的聚合模块，将混合专家（MoE）路由机制融入特征聚合过程，具体而言，该模块在交叉注意力框架中动态为键（keys）和值（values）选择专家子空间，从而实现对异构输入域的自适应处理。在University-1652和SUES-200数据集上的大量实验表明，我们的方法在使用更少训练参数的情况下取得了具有竞争力的性能。

</details>


### [10] [Kinematic-Based Assessment of Surgical Actions in Microanastomosis](https://arxiv.org/abs/2512.23942)
*Yan Meng,Daniel Donoho,Marcelle Altshuler,Omar Arnaout*

Main category: cs.CV

TL;DR: 本文提出了一种基于AI的自动化框架，用于显微吻合术中的动作分割与技能评估，可在边缘计算设备上高效运行，实现了92.4%的动作分割准确率和85.5%的技能分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统显微外科手术评估依赖专家人工评分，存在主观性强、评分者间差异大、耗时长等问题，亟需客观、可扩展的自动化评估方法。

Method: 该系统包含三个模块：(1) 基于YOLO和DeepSORT的器械尖端追踪与定位；(2) 利用自相似矩阵进行动作边界检测与无监督聚类的动作分割模块；(3) 用于评估手术手势熟练度的有监督分类模块。

Result: 在58个专家评分的显微吻合视频数据集上验证，系统达到92.4%的帧级动作分割准确率和85.5%的整体技能分类准确率。

Conclusion: 所提方法有望为显微外科教学提供客观、实时的反馈，推动标准化、数据驱动的培训方案，并提升高风险手术环境中的能力评估水平。

Abstract: Proficiency in microanastomosis is a critical surgical skill in neurosurgery, where the ability to precisely manipulate fine instruments is crucial to successful outcomes. These procedures require sustained attention, coordinated hand movements, and highly refined motor skills, underscoring the need for objective and systematic methods to evaluate and enhance microsurgical training. Conventional assessment approaches typically rely on expert raters supervising the procedures or reviewing surgical videos, which is an inherently subjective process prone to inter-rater variability, inconsistency, and significant time investment. These limitations highlight the necessity for automated and scalable solutions. To address this challenge, we introduce a novel AI-driven framework for automated action segmentation and performance assessment in microanastomosis procedures, designed to operate efficiently on edge computing platforms. The proposed system comprises three main components: (1) an object tip tracking and localization module based on YOLO and DeepSORT; (2) an action segmentation module leveraging self-similarity matrix for action boundary detection and unsupervised clustering; and (3) a supervised classification module designed to evaluate surgical gesture proficiency. Experimental validation on a dataset of 58 expert-rated microanastomosis videos demonstrates the effectiveness of our approach, achieving a frame-level action segmentation accuracy of 92.4% and an overall skill classification accuracy of 85.5% in replicating expert evaluations. These findings demonstrate the potential of the proposed method to provide objective, real-time feedback in microsurgical education, thereby enabling more standardized, data-driven training protocols and advancing competency assessment in high-stakes surgical environments.

Abstract (中文翻译): 显微吻合术的熟练程度是神经外科中一项关键的手术技能，精准操控精细器械的能力对成功手术至关重要。这类操作需要持续专注、协调的手部动作以及高度精细的运动技能，因此亟需客观且系统化的方法来评估并提升显微外科培训效果。传统的评估方法通常依赖专家现场监督或回看手术视频进行评分，这一过程本质上具有主观性，易受评分者间差异、不一致性和大量时间投入的影响。这些局限性凸显了对自动化、可扩展解决方案的需求。为应对这一挑战，我们提出了一种新颖的AI驱动框架，用于显微吻合术中的自动化动作分割与表现评估，该框架专为在边缘计算平台上高效运行而设计。所提出的系统包含三个主要组成部分：（1）基于YOLO和DeepSORT的器械尖端追踪与定位模块；（2）利用自相似矩阵进行动作边界检测与无监督聚类的动作分割模块；（3）用于评估手术手势熟练度的有监督分类模块。在包含58个经专家评分的显微吻合视频的数据集上的实验验证表明，我们的方法具有良好的有效性，实现了92.4%的帧级动作分割准确率和85.5%的整体技能分类准确率。这些结果表明，所提出的方法有望在显微外科教育中提供客观、实时的反馈，从而实现更标准化、数据驱动的培训流程，并推动高风险手术环境中能力评估的发展。

</details>
