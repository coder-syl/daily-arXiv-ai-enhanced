{"id": "2601.17027", "pdf": "https://arxiv.org/pdf/2601.17027", "abs": "https://arxiv.org/abs/2601.17027", "authors": ["Honglin Lin", "Chonghan Qin", "Zheng Liu", "Qizhi Pei", "Yu Li", "Zhanping Zhong", "Xin Gao", "Yanfeng Wang", "Conghui He", "Lijun Wu"], "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\uff0c\u63d0\u51faImgCoder\u6846\u67b6\u4ee5\u63d0\u5347\u7ed3\u6784\u51c6\u786e\u6027\uff0c\u5e76\u6784\u5efaSciGenBench\u8bc4\u4f30\u57fa\u51c6\uff0c\u53d1\u73b0\u57fa\u4e8e\u50cf\u7d20\u7684\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u800c\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u53ef\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u751f\u56fe\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u867d\u89c6\u89c9\u4e0a\u5408\u7406\uff0c\u4f46\u5e38\u7f3a\u4e4f\u79d1\u5b66\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u89c6\u89c9\u4e0e\u903b\u8f91\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\u4e9f\u9700\u63a2\u7d22\u80fd\u751f\u6210\u79d1\u5b66\u4e25\u8c28\u56fe\u50cf\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u6bd4\u8f83\u4e86\u76f4\u63a5\u50cf\u7d20\u751f\u6210\u4e0e\u7a0b\u5e8f\u5316\u5408\u6210\u4e24\u79cd\u8303\u5f0f\uff0c\u63d0\u51faImgCoder\u2014\u2014\u4e00\u4e2a\u9075\u5faa\u201c\u7406\u89e3-\u89c4\u5212-\u7f16\u7801\u201d\u6d41\u7a0b\u7684\u903b\u8f91\u9a71\u52a8\u6846\u67b6\uff0c\u5e76\u6784\u5efaSciGenBench\u57fa\u51c6\uff0c\u4ece\u4fe1\u606f\u6548\u7528\u548c\u903b\u8f91\u6709\u6548\u6027\u4e24\u65b9\u9762\u8bc4\u4f30\u56fe\u50cf\u7684\u79d1\u5b66\u6b63\u786e\u6027\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u50cf\u7d20\u7ea7\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u51f8\u663e\u4e86\u8868\u8fbe\u529b\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6839\u672c\u6743\u8861\uff1b\u540c\u65f6\u53d1\u73b0\uff0c\u5728\u7ecf\u4e25\u683c\u9a8c\u8bc1\u7684\u5408\u6210\u79d1\u5b66\u56fe\u50cf\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u53ef\u5e26\u6765\u7a33\u5b9a\u7684\u63a8\u7406\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u5ea6\u7684\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u662f\u91ca\u653e\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u8def\u5f84\uff0c\u5176\u6f5c\u529b\u7c7b\u4f3c\u4e8e\u6587\u672c\u9886\u57df\u4e2d\u5408\u6210\u6570\u636e\u7684\u4f5c\u7528\u3002", "summary_cn": "\u5c3d\u7ba1\u5408\u6210\u6570\u636e\u5df2\u88ab\u8bc1\u660e\u5728\u6587\u672c\u9886\u57df\u4e2d\u80fd\u6709\u6548\u63d0\u5347\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u591a\u6a21\u6001\u63a8\u7406\u4ecd\u53d7\u9650\u4e8e\u96be\u4ee5\u5408\u6210\u5177\u6709\u79d1\u5b66\u4e25\u8c28\u6027\u7684\u56fe\u50cf\u3002\u73b0\u6709\u7684\u6587\u751f\u56fe\uff08T2I\uff09\u6a21\u578b\u901a\u5e38\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u79d1\u5b66\u4e0a\u9519\u8bef\u7684\u56fe\u50cf\uff0c\u5bfc\u81f4\u6301\u7eed\u5b58\u5728\u7684\u89c6\u89c9-\u903b\u8f91\u504f\u5dee\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c\u3002\u53d7\u65b0\u4e00\u4ee3T2I\u6a21\u578b\u6700\u65b0\u8fdb\u5c55\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5bf9\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u5728\u751f\u6210\u8303\u5f0f\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u4e0b\u6e38\u5e94\u7528\u65b9\u9762\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u6211\u4eec\u5206\u6790\u4e86\u57fa\u4e8e\u50cf\u7d20\u7684\u76f4\u63a5\u751f\u6210\u4e0e\u7a0b\u5e8f\u5316\u5408\u6210\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86ImgCoder\u2014\u2014\u4e00\u79cd\u903b\u8f91\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u9075\u5faa\u660e\u786e\u7684\u201c\u7406\u89e3-\u89c4\u5212-\u7f16\u7801\u201d\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u63d0\u5347\u7ed3\u6784\u7cbe\u5ea6\u3002\u4e3a\u4e25\u683c\u8bc4\u4f30\u79d1\u5b66\u6b63\u786e\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86SciGenBench\uff0c\u8be5\u57fa\u51c6\u6839\u636e\u4fe1\u606f\u6548\u7528\u548c\u903b\u8f91\u6709\u6548\u6027\u5bf9\u751f\u6210\u56fe\u50cf\u8fdb\u884c\u8bc4\u4f30\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u50cf\u7d20\u7ea7\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u7a81\u663e\u4e86\u8868\u8fbe\u80fd\u529b\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6839\u672c\u6743\u8861\u3002\u6700\u540e\uff0c\u6211\u4eec\u8868\u660e\uff0c\u5728\u7ecf\u8fc7\u4e25\u683c\u9a8c\u8bc1\u7684\u5408\u6210\u79d1\u5b66\u56fe\u50cf\u4e0a\u5fae\u8c03\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u53ef\u5e26\u6765\u4e00\u81f4\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\uff0c\u5176\u6f5c\u5728\u7684\u6269\u5c55\u8d8b\u52bf\u4e0e\u6587\u672c\u9886\u57df\u7c7b\u4f3c\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u9ad8\u4fdd\u771f\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u662f\u89e3\u9501\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.17031", "pdf": "https://arxiv.org/pdf/2601.17031", "abs": "https://arxiv.org/abs/2601.17031", "authors": ["Yunhao Xu", "Fuquan Zong", "Yexuan Xing", "Chulong Zhang", "Guang Yang", "Shilong Yang", "Xiaokun Liang", "Juan Yu"], "title": "Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53cc\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u6d41\u5f62\u6269\u5c55\u548c\u8bed\u4e49\u75c5\u7076\u6ce8\u5165\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u6570\u636e\u5229\u7528\u6548\u7387\u800c\u975e\u539f\u59cb\u6570\u636e\u91cf\uff0c\u5c24\u5176\u5bf9\u4e8e\u8111\u819c\u7624\u7b49\u590d\u6742\u75c5\u7406\uff0c\u9700\u8981\u4ece\u6709\u9650\u7684\u9ad8\u8d28\u91cf\u6807\u6ce8\u4e2d\u5145\u5206\u6316\u6398\u6f5c\u5728\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u53cc\u589e\u5f3a\u6846\u67b6\uff1a1\uff09\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5efa\u6a21\u8fde\u7eed\u901f\u5ea6\u573a\uff0c\u5e76\u901a\u8fc7\u5bf9\u79ef\u5206\u5f62\u53d8\u573a\u8fdb\u884c\u7ebf\u6027\u6df7\u5408\uff0c\u5728\u5f62\u53d8\u7a7a\u95f4\u5185\u63d2\u503c\u751f\u6210\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u7684\u7ed3\u6784\u53d8\u5f02\uff1b2\uff09\u5f15\u5165Sim2Real\u75c5\u7076\u6ce8\u5165\u6a21\u5757\uff0c\u5c06\u75c5\u7076\u7eb9\u7406\u79fb\u690d\u5230\u5065\u5eb7\u89e3\u5256\u80cc\u666f\u4e2d\uff0c\u6784\u5efa\u9ad8\u4fdd\u771f\u6a21\u62df\u57df\u3002", "result": "\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86nnU-Net\u548cU-Mamba\u7b49\u5148\u8fdb\u6a21\u578b\u7684\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e3a\u5728\u6807\u6ce8\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7b56\u7565\u3002", "summary_cn": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u65e5\u76ca\u53d6\u51b3\u4e8e\u6570\u636e\u5229\u7528\u6548\u7387\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u539f\u59cb\u6570\u636e\u7684\u6570\u91cf\u3002\u5bf9\u4e8e\u8111\u819c\u7624\u7b49\u590d\u6742\u75c5\u7406\u7684\u7cbe\u786e\u5206\u5272\uff0c\u8981\u6c42\u6a21\u578b\u5145\u5206\u5229\u7528\u6709\u9650\u9ad8\u8d28\u91cf\u6807\u6ce8\u4e2d\u7684\u6f5c\u5728\u4fe1\u606f\u3002\u4e3a\u6700\u5927\u5316\u73b0\u6709\u6570\u636e\u96c6\u7684\u4ef7\u503c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u589e\u5f3a\u6846\u67b6\uff0c\u534f\u540c\u6574\u5408\u4e86\u7a7a\u95f4\u6d41\u5f62\u6269\u5c55\u4e0e\u8bed\u4e49\u75c5\u7076\u6ce8\u5165\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5bf9\u8fde\u7eed\u901f\u5ea6\u573a\u8fdb\u884c\u5efa\u6a21\u3002\u4e0e\u4ee5\u5f80\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u5728\u79ef\u5206\u5f62\u53d8\u573a\u4e0a\u8fdb\u884c\u7ebf\u6027\u6df7\u5408\uff0c\u901a\u8fc7\u5728\u5f62\u53d8\u7a7a\u95f4\u5185\u63d2\u503c\uff0c\u9ad8\u6548\u751f\u6210\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u7684\u7ed3\u6784\u53d8\u5f02\uff0c\u4ece\u800c\u4ece\u5c0f\u89c4\u6a21\u951a\u70b9\u96c6\u4e2d\u5e7f\u6cdb\u63a2\u7d22\u7ed3\u6784\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2aSim2Real\u75c5\u7076\u6ce8\u5165\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u901a\u8fc7\u5c06\u75c5\u7076\u7eb9\u7406\u79fb\u690d\u5230\u5065\u5eb7\u89e3\u5256\u80cc\u666f\u4e2d\uff0c\u6784\u5efa\u9ad8\u4fdd\u771f\u6a21\u62df\u57df\uff0c\u6709\u6548\u5f25\u5408\u4e86\u5408\u6210\u589e\u5f3a\u4e0e\u771f\u5b9e\u75c5\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5305\u62ecnnU-Net\u548cU-Mamba\u5728\u5185\u7684\u5148\u8fdb\u6a21\u578b\u7684\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u6709\u529b\u7684\u7b56\u7565\u3002"}}
{"id": "2601.17032", "pdf": "https://arxiv.org/pdf/2601.17032", "abs": "https://arxiv.org/abs/2601.17032", "authors": ["Wilkie Delgado-Font", "Miriela Escobedo-Nicot", "Manuel Gonz\u00e1lez-Hidalgo", "Silena Herold-Garcia", "Antoni Jaume-i-Cap\u00f3", "Arnau Mir"], "title": "Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u5206\u6790\u7684\u81ea\u52a8\u5316\u7ea2\u7ec6\u80de\uff08RBC\uff09\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u533a\u5206\u6b63\u5e38\u3001\u7ec6\u957f\u53ca\u5176\u4ed6\u53d8\u5f62\u7ea2\u7ec6\u80de\uff0c\u4ee5\u8f85\u52a9\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u8bca\u65ad\u3002", "motivation": "\u4f20\u7edf\u663e\u5fae\u955c\u4e0b\u89c2\u5bdf\u7ea2\u7ec6\u80de\u5f62\u6001\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u8017\u65f6\u4e14\u4e3b\u89c2\u6027\u5f3a\uff0c\u6613\u5bfc\u81f4\u9ad8\u8bef\u5224\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u3001\u5ba2\u89c2\u4e14\u51c6\u786e\u7684\u7ea2\u7ec6\u80de\u5f62\u6001\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Chan-Vese\u4e3b\u52a8\u8f6e\u5ed3\u6a21\u578b\u5bf9\u7ea2\u7ec6\u80de\u8fdb\u884c\u56fe\u50cf\u5206\u5272\uff0c\u5e76\u5229\u7528\u5706\u5f62\u5f62\u72b6\u56e0\u5b50\uff08CSF\uff09\u548c\u692d\u5706\u5f62\u72b6\u56e0\u5b50\uff08ESF\uff09\u8fdb\u884c\u5f62\u72b6\u5206\u6790\uff1b\u5bf9\u805a\u96c6\u906e\u6321\u7684\u7ea2\u7ec6\u80de\uff0c\u901a\u8fc7\u692d\u5706\u62df\u5408\u8c03\u6574\u4ee5\u8bc6\u522b\u76d8\u72b6\u548c\u7ec6\u957f\u5f62\u7ea2\u7ec6\u80de\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6b63\u5e38\u548c\u7ec6\u957f\u7ea2\u7ec6\u80de\u4e0a\u7684F\u503c\u5206\u522b\u8fbe\u52300.97\u548c0.95\uff0c\u591a\u9879\u591a\u5206\u7c7b\u6027\u80fd\u6307\u6807\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u4e34\u5e8a\u8bca\u65ad\u652f\u6301\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u53ef\u6709\u6548\u8f85\u52a9\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u8bca\u65ad\u4e0e\u6cbb\u7597\u51b3\u7b56\u3002", "summary_cn": "\u7ea2\u7ec6\u80de\uff08RBC\uff09\u53d8\u5f62\u662f\u591a\u79cd\u75be\u75c5\u7684\u540e\u679c\uff0c\u5305\u62ec\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\uff0c\u8be5\u75c5\u4f1a\u5bfc\u81f4\u53cd\u590d\u53d1\u4f5c\u7684\u75bc\u75db\u548c\u4e25\u91cd\u7684\u660e\u663e\u8d2b\u8840\u3002\u5bf9\u8fd9\u7c7b\u60a3\u8005\u7684\u76d1\u6d4b\u901a\u5e38\u9700\u8981\u5728\u663e\u5fae\u955c\u4e0b\u89c2\u5bdf\u5916\u5468\u8840\u6837\u672c\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u7531\u4e13\u4e1a\u4eba\u5458\u64cd\u4f5c\uff0c\u800c\u7531\u4e8e\u5bf9\u5b64\u7acb\u7ea2\u7ec6\u80de\u7684\u89c2\u5bdf\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u9519\u8bef\u7387\u8f83\u9ad8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u5206\u6790\u7684\u81ea\u52a8\u5316\u7ea2\u7ec6\u80de\u5dee\u5f02\u8ba1\u6570\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528Chan-Vese\u4e3b\u52a8\u8f6e\u5ed3\u6a21\u578b\u5bf9\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u8fdb\u884c\u5206\u5272\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u672c\u5f62\u72b6\u5206\u6790\u63cf\u8ff0\u7b26\u2014\u2014\u5706\u5f62\u5f62\u72b6\u56e0\u5b50\uff08CSF\uff09\u548c\u692d\u5706\u5f62\u72b6\u56e0\u5b50\uff08ESF\uff09\u2014\u2014\u5c06\u7ea2\u7ec6\u80de\uff08\u53c8\u79f0\u7ea2\u8840\u7403\uff09\u5206\u7c7b\u4e3a\u6b63\u5e38\u3001\u7ec6\u957f\u6216\u5176\u4ed6\u53d8\u5f62\u7c7b\u578b\u3002\u4e3a\u5206\u6790\u5728\u6837\u672c\u5236\u5907\u8fc7\u7a0b\u4e2d\u90e8\u5206\u88ab\u906e\u6321\u6210\u7c07\u7684\u7ec6\u80de\uff0c\u8fd8\u8fdb\u884c\u4e86\u692d\u5706\u62df\u5408\u8c03\u6574\uff0c\u4ee5\u652f\u6301\u5bf9\u76d8\u72b6\u548c\u7ec6\u957f\u5f62\u7ea2\u7ec6\u80de\u7684\u5206\u6790\u3002\u7814\u7a76\u6240\u7528\u7684\u60a3\u8005\u8840\u6837\u56fe\u50cf\u7531\u53e4\u5df4\u5723\u5730\u4e9a\u54e5\u201cDr. Juan Bruno Zayas\u201d\u7efc\u5408\u533b\u9662\u8840\u6db2\u4e13\u79d1\u5b9e\u9a8c\u5ba4\u4e13\u5bb6\u91c7\u96c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f53\u524d\u4e00\u4e9b\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5176\u4f18\u8d8a\u6027\u4f53\u73b0\u5728\u8f83\u9ad8\u7684F\u503c\uff08\u6b63\u5e38\u7ec6\u80de\u4e3a0.97\uff0c\u7ec6\u957f\u7ec6\u80de\u4e3a0.95\uff09\u53ca\u591a\u9879\u6574\u4f53\u591a\u5206\u7c7b\u6027\u80fd\u6307\u6807\u4e0a\u3002\u8be5\u65b9\u6cd5\u7684\u7ed3\u679c\u9002\u7528\u4e8e\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u4e34\u5e8a\u6cbb\u7597\u4e0e\u8bca\u65ad\u652f\u6301\u3002"}}
{"id": "2601.17037", "pdf": "https://arxiv.org/pdf/2601.17037", "abs": "https://arxiv.org/abs/2601.17037", "authors": ["Aahana Basappa", "Pranay Goel", "Anusri Karra", "Anish Karra", "Asa Gilmore", "Kevin Zhu"], "title": "AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs", "categories": ["cs.CV", "cs.AI"], "comment": "Comments: 13 pages, 4 figures. Presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: NeurIPS 2025 VLM4RWD. Authors Aahana Basappa and Pranay Goel contributed equally to this work. Code: https://github.com/AahanaB24/AMVICC, Data: https://doi.org/10.5281/zenodo.17646068", "summary": "We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \\textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u57fa\u51c6 AMVICC\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08IGMs\uff09\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u53d1\u73b0\u4e24\u7c7b\u6a21\u578b\u5728\u57fa\u7840\u89c6\u89c9\u6982\u5ff5\u7406\u89e3\u4e0a\u5b58\u5728\u5171\u6027\u4e0e\u7279\u5f02\u6027\u7f3a\u9677\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5728\u7406\u89e3\u6216\u751f\u6210\u57fa\u672c\u89c6\u89c9\u6982\u5ff5\uff08\u5982\u7269\u4f53\u671d\u5411\u3001\u6570\u91cf\u3001\u7a7a\u95f4\u5173\u7cfb\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9\u8de8\u6a21\u6001\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e MMVP \u57fa\u51c6\u95ee\u9898\uff0c\u6784\u5efa\u5305\u542b\u663e\u5f0f\u4e0e\u9690\u5f0f\u63d0\u793a\u7684\u65b0\u578b\u8de8\u6a21\u6001\u8bc4\u4f30\u57fa\u51c6 AMVICC\uff0c\u5728\u4e5d\u7c7b\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u6d4b\u8bd5 11 \u4e2a MLLMs \u548c 3 \u4e2a IGMs \u7684\u8868\u73b0\u3002", "result": "MLLMs \u548c IGMs \u5728\u591a\u79cd\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5171\u4eab\u53ca\u5404\u81ea\u7279\u6709\u7684\u5931\u8d25\u6a21\u5f0f\uff1bIGMs \u5c24\u5176\u5728\u663e\u5f0f\u63d0\u793a\u4e0b\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0e\u7edf\u4e00\u5efa\u6a21\u6307\u660e\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "summary_cn": "\u6211\u4eec\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\uff0c\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff08IGMs\uff09\u5728\u56fe\u50cf\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u7684\u8de8\u6a21\u6001\u8bc4\u4f30\u3002\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7406\u89e3\u6216\u751f\u6210\u7269\u4f53\u671d\u5411\u3001\u6570\u91cf\u6216\u7a7a\u95f4\u5173\u7cfb\u7b49\u57fa\u672c\u89c6\u89c9\u6982\u5ff5\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u51f8\u663e\u4e86\u5176\u5728\u57fa\u7840\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u7684\u5dee\u8ddd\u3002\u901a\u8fc7\u5c06 MMVP \u57fa\u51c6\u95ee\u9898\u6539\u7f16\u4e3a\u663e\u5f0f\u548c\u9690\u5f0f\u63d0\u793a\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u540d\u4e3a AMVICC \u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u523b\u753b\u4e0d\u540c\u6a21\u6001\u4e0b\u7684\u5931\u8d25\u6a21\u5f0f\u3002\u5728\u5bf9 11 \u4e2a MLLMs \u548c 3 \u4e2a IGMs \u8fdb\u884c\u4e5d\u7c7b\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u6d4b\u8bd5\u540e\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5931\u8d25\u6a21\u5f0f\u901a\u5e38\u5728\u6a21\u578b\u548c\u6a21\u6001\u4e4b\u95f4\u5171\u4eab\uff0c\u4f46\u4e5f\u5b58\u5728\u6a21\u578b\u7279\u5f02\u6027\u548c\u6a21\u6001\u7279\u5f02\u6027\u7684\u5931\u8d25\uff0c\u8fd9\u53ef\u80fd\u5f52\u56e0\u4e8e\u591a\u79cd\u56e0\u7d20\u3002IGMs \u5728\u54cd\u5e94\u63d0\u793a\u65f6\u59cb\u7ec8\u96be\u4ee5\u64cd\u63a7\u7279\u5b9a\u7684\u89c6\u89c9\u7ec4\u4ef6\uff0c\u5c24\u5176\u5728\u663e\u5f0f\u63d0\u793a\u4e0b\u8868\u73b0\u66f4\u5dee\uff0c\u8868\u660e\u5176\u5bf9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u7684\u63a7\u5236\u80fd\u529b\u8f83\u5f31\u3002\u6211\u4eec\u7684\u53d1\u73b0\u6700\u76f4\u63a5\u9002\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u7ed3\u6784\u5316\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u672c\u7814\u7a76\u4e3a\u672a\u6765\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u63a2\u7a76\u751f\u6210\u4e0e\u7406\u89e3\u5931\u8d25\u662f\u5426\u6e90\u4e8e\u5171\u540c\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u6307\u5bfc\u672a\u6765\u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u5efa\u6a21\u7684\u6539\u8fdb\u3002"}}
{"id": "2601.17038", "pdf": "https://arxiv.org/pdf/2601.17038", "abs": "https://arxiv.org/abs/2601.17038", "authors": ["Obai Alashram", "Nejad Alagha", "Mahmoud AlKakuri", "Zeeshan Swaveel", "Abigail Copiaco"], "title": "Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u6df7\u5408\u89c6\u89c9\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5206\u7c7b\u5efa\u7b51\u5783\u573e\uff0c\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.5%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u4ea7\u751f\u5927\u91cf\u5e9f\u5f03\u7269\uff0c\u6709\u6548\u5206\u7c7b\u5bf9\u53ef\u6301\u7eed\u7ba1\u7406\u548c\u8d44\u6e90\u56de\u6536\u81f3\u5173\u91cd\u8981\uff1b\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u6216\u590d\u6742\u6027\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u81ea\u52a8\u5206\u7c7b\u65b9\u6848\u3002", "method": "\u6536\u96c6\u5305\u542b1800\u5f20\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684UAE\u5b9e\u5730\u5efa\u7b51\u5783\u573e\u6570\u636e\u96c6\uff08\u56db\u7c7b\u6750\u6599\uff09\uff1b\u4f7f\u7528\u9884\u8bad\u7ec3Xception\u7f51\u7edc\u63d0\u53d6\u6df1\u5ea6\u7279\u5f81\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30SVM\u3001kNN\u3001Bagged Trees\u3001LDA\u548c\u903b\u8f91\u56de\u5f52\u7b49\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "result": "\u57fa\u4e8eXception\u7279\u5f81\u7684\u6df7\u5408\u65b9\u6cd5\uff08\u5982Linear SVM\u3001kNN\u3001Bagged Trees\uff09\u5728\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u9ad899.5%\u7684\u51c6\u786e\u7387\u548cmacro-F1\u5206\u6570\uff0c\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5177\u5907\u826f\u597d\u7684\u90e8\u7f72\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u573a\u5efa\u7b51\u5783\u573e\u8bc6\u522b\uff0c\u5e76\u4e3a\u672a\u6765\u4e0e\u673a\u5668\u4eba\u53ca\u81ea\u52a8\u5316\u7cfb\u7edf\u96c6\u6210\u63d0\u4f9b\u57fa\u7840\u3002", "summary_cn": "\u5efa\u7b51\u4e1a\u4ea7\u751f\u4e86\u5927\u91cf\u5e9f\u5f03\u7269\uff0c\u56e0\u6b64\u6709\u6548\u7684\u5206\u7c7b\u5bf9\u4e8e\u53ef\u6301\u7eed\u7684\u5e9f\u7269\u7ba1\u7406\u548c\u8d44\u6e90\u56de\u6536\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u89c6\u89c9\u7ba1\u9053\uff0c\u5c06\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5206\u7c7b\u5668\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u81ea\u52a8\u5206\u7c7b\u5efa\u7b51\u4e0e\u62c6\u9664\uff08C&D\uff09\u5e9f\u5f03\u7269\u3002\u7814\u7a76\u4ece\u963f\u8054\u914b\u7684\u5b9e\u9645\u5efa\u7b51\u5de5\u5730\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b1800\u5f20\u5747\u8861\u3001\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u56db\u79cd\u6750\u6599\u7c7b\u522b\uff1a\u9676\u74f7/\u74f7\u7816\u3001\u6df7\u51dd\u571f\u3001\u5783\u573e/\u5e9f\u6599\u548c\u6728\u6750\uff0c\u5145\u5206\u53cd\u6620\u4e86\u591a\u6837\u5316\u7684\u73b0\u5b9e\u573a\u666f\u3002\u5229\u7528\u9884\u8bad\u7ec3\u7684Xception\u7f51\u7edc\u63d0\u53d6\u6df1\u5ea6\u7279\u5f81\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5305\u62ec\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u3001k\u8fd1\u90bb\uff08kNN\uff09\u3001Bagged Trees\u3001\u7ebf\u6027\u5224\u522b\u5206\u6790\uff08LDA\uff09\u548c\u903b\u8f91\u56de\u5f52\u5728\u5185\u7684\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3002\u7ed3\u679c\u8868\u660e\uff0c\u91c7\u7528Xception\u7279\u5f81\u4e0e\u7b80\u5355\u5206\u7c7b\u5668\uff08\u5982\u7ebf\u6027SVM\u3001kNN\u548cBagged Trees\uff09\u7684\u6df7\u5408\u7ba1\u9053\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u548cmacro-F1\u5206\u6570\u6700\u9ad8\u53ef\u8fbe99.5%\uff0c\u4f18\u4e8e\u66f4\u590d\u6742\u6216\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u8be5\u5206\u6790\u7a81\u663e\u4e86\u6b64\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u73b0\u573a\u90e8\u7f72\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u672a\u6765\u4e0e\u673a\u5668\u4eba\u53ca\u73b0\u573a\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2601.17039", "pdf": "https://arxiv.org/pdf/2601.17039", "abs": "https://arxiv.org/abs/2601.17039", "authors": ["Junhyuk Heo", "Beomkyu Choi", "Hyunjin Shin", "Darongsae Kwon"], "title": "MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MANGO\uff0c\u4e00\u4e2a\u5305\u542b42,703\u5bf9\u56fe\u50cf-\u63a9\u7801\u7684\u5927\u89c4\u6a21\u5168\u7403\u7ea2\u6811\u6797\u6570\u636e\u96c6\uff0c\u8986\u76d6124\u4e2a\u56fd\u5bb6\uff0c\u65e8\u5728\u652f\u6301\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ea2\u6811\u6797\u76d1\u6d4b\u3002", "motivation": "\u73b0\u6709\u7ea2\u6811\u6797\u6570\u636e\u96c6\u5b58\u5728\u4e0d\u8db3\uff0c\u5982\u4ec5\u63d0\u4f9b\u5e74\u5ea6\u5730\u56fe\u3001\u7f3a\u4e4f\u5355\u65e5\u671f\u56fe\u50cf-\u63a9\u7801\u5bf9\u3001\u5730\u57df\u8986\u76d6\u6709\u9650\u6216\u672a\u516c\u5f00\uff0c\u963b\u788d\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u7ea2\u6811\u6797\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u75282020\u5e74\u6240\u6709\u53ef\u7528\u7684Sentinel-2\u5f71\u50cf\uff0c\u7ed3\u5408\u5e74\u5ea6\u7ea2\u6811\u6797\u63a9\u7801\uff0c\u901a\u8fc7\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u50cf\u7d20\u7ea7\u5750\u6807\u53c2\u8003\u9009\u53d6\u6700\u4f18\u5355\u65e5\u671f\u89c2\u6d4b\uff0c\u6784\u5efa\u56fe\u50cf-\u63a9\u7801\u5bf9\uff1b\u5e76\u57fa\u4e8e\u56fd\u5bb6\u4e0d\u91cd\u53e0\u5212\u5206\u5bf9\u591a\u79cd\u8bed\u4e49\u5206\u5272\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6784\u5efa\u4e86\u8986\u76d6124\u4e2a\u56fd\u5bb6\u3001\u5305\u542b42,703\u4e2a\u6807\u6ce8\u6837\u672c\u7684MANGO\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u79cd\u5206\u5272\u6a21\u578b\u7684\u57fa\u51c6\u6027\u80fd\uff0c\u4e3a\u5168\u7403\u7ea2\u6811\u6797\u76d1\u6d4b\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "MANGO\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u7ea2\u6811\u6797\u9065\u611f\u6570\u636e\u7684\u7a7a\u767d\uff0c\u652f\u6301\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u5168\u7403\u7ea2\u6811\u6797\u6df1\u5ea6\u5b66\u4e60\u76d1\u6d4b\u7814\u7a76\u3002", "summary_cn": "\u7ea2\u6811\u6797\u5bf9\u4e8e\u7f13\u89e3\u6c14\u5019\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u53ef\u9760\u7684\u76d1\u6d4b\u624b\u6bb5\u4ee5\u5b9e\u73b0\u6709\u6548\u4fdd\u62a4\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5df2\u6210\u4e3a\u7ea2\u6811\u6797\u68c0\u6d4b\u7684\u6709\u529b\u5de5\u5177\uff0c\u4f46\u5176\u8fdb\u5c55\u53d7\u5230\u73b0\u6709\u6570\u636e\u96c6\u5c40\u9650\u6027\u7684\u5236\u7ea6\u3002\u7279\u522b\u662f\uff0c\u8bb8\u591a\u8d44\u6e90\u4ec5\u63d0\u4f9b\u5e74\u5ea6\u5730\u56fe\u4ea7\u54c1\uff0c\u7f3a\u4e4f\u7ecf\u8fc7\u6574\u7406\u7684\u5355\u65e5\u671f\u56fe\u50cf-\u63a9\u7801\u5bf9\uff0c\u5c40\u9650\u4e8e\u7279\u5b9a\u533a\u57df\u800c\u975e\u5168\u7403\u8986\u76d6\uff0c\u6216\u672a\u5411\u516c\u4f17\u5f00\u653e\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MANGO\u2014\u2014\u4e00\u4e2a\u5927\u89c4\u6a21\u5168\u7403\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea124\u4e2a\u56fd\u5bb6\u768442,703\u5bf9\u6807\u6ce8\u56fe\u50cf-\u63a9\u7801\u3002\u4e3a\u6784\u5efa\u8be5\u6570\u636e\u96c6\uff0c\u6211\u4eec\u57282020\u5e74\u7ea2\u6811\u6797\u533a\u57df\u68c0\u7d22\u6240\u6709\u53ef\u7528\u7684Sentinel-2\u5f71\u50cf\uff0c\u5e76\u9009\u62e9\u4e0e\u7ea2\u6811\u6797\u5e74\u5ea6\u63a9\u7801\u6700\u5339\u914d\u7684\u6700\u4f73\u5355\u65e5\u671f\u89c2\u6d4b\u3002\u8be5\u9009\u62e9\u8fc7\u7a0b\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u50cf\u7d20\u7ea7\u5750\u6807\u53c2\u8003\uff0c\u4ee5\u786e\u4fdd\u56fe\u50cf-\u63a9\u7801\u914d\u5bf9\u7684\u81ea\u9002\u5e94\u6027\u548c\u4ee3\u8868\u6027\u3002\u6211\u4eec\u8fd8\u5728\u56fd\u5bb6\u4e0d\u91cd\u53e0\u5212\u5206\u4e0b\u5bf9\u591a\u79cd\u8bed\u4e49\u5206\u5272\u67b6\u6784\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u5168\u7403\u7ea2\u6811\u6797\u76d1\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.17040", "pdf": "https://arxiv.org/pdf/2601.17040", "abs": "https://arxiv.org/abs/2601.17040", "authors": ["H Neji", "J Nogueras-Iso", "J Lacasta", "M\u00c1 Latre", "FJ Garc\u00eda-Marco"], "title": "FP-THD: Full page transcription of historical documents", "categories": ["cs.CV", "cs.AI"], "comment": "Figure 1: FP-THD architecture Overview: Layout Analysis and Masked Auto-encoder with Vision Trans- former", "summary": "The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8f6c\u5f5515-16\u4e16\u7eaa\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7248\u9762\u5206\u6790\u4e0eOCR\u6a21\u578b\uff0c\u5728\u4fdd\u7559\u7279\u6b8a\u5b57\u7b26\u548c\u7b26\u53f7\u7684\u540c\u65f6\u9ad8\u6548\u5b9e\u73b0\u5168\u6587\u6570\u5b57\u5316\u3002", "motivation": "15\u81f316\u4e16\u7eaa\u7528\u62c9\u4e01\u6587\u4e66\u5199\u7684\u6b77\u53f2\u6587\u732e\u5305\u542b\u5177\u6709\u7279\u5b9a\u542b\u4e49\u7684\u7279\u6b8a\u5b57\u7b26\u548c\u7b26\u53f7\uff0c\u8f6c\u5f55\u65f6\u9700\u4fdd\u7559\u8fd9\u4e9b\u7279\u5f81\u4ee5\u7ef4\u6301\u6587\u672c\u7684\u539f\u59cb\u98ce\u683c\u4e0e\u610f\u4e49\uff0c\u8fd9\u5bf9\u4f20\u7edfOCR\u65b9\u6cd5\u6784\u6210\u6311\u6218\u3002", "method": "\u5728\u73b0\u6709\u6587\u672c\u884c\u8bc6\u522b\u65b9\u6cd5\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u7248\u9762\u5206\u6790\u6a21\u578b\uff1a\u5148\u7528\u7248\u9762\u5206\u6790\u6a21\u578b\u4ece\u5386\u53f2\u6587\u6863\u56fe\u50cf\u4e2d\u63d0\u53d6\u6587\u672c\u884c\uff0c\u518d\u4ea4\u7531OCR\u6a21\u578b\u751f\u6210\u5b8c\u6574\u6570\u5b57\u5316\u9875\u9762\u3002", "result": "\u8be5\u6d41\u6c34\u7ebf\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u80fd\u9ad8\u6548\u5904\u7406\u624b\u5199\u3001\u5370\u5237\u53ca\u591a\u8bed\u8a00\u6587\u672c\uff0c\u9a8c\u8bc1\u4e86\u6240\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408\u7248\u9762\u5206\u6790\u4e0eOCR\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u5386\u53f2\u6587\u732e\u4e2d\u7684\u7279\u6b8a\u5b57\u7b26\u4e0e\u683c\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6570\u5b57\u5316\u8f6c\u5f55\u3002", "summary_cn": "\u5bf915\u548c16\u4e16\u7eaa\u7528\u62c9\u4e01\u6587\u4e66\u5199\u7684\u6b77\u53f2\u6587\u732e\u8fdb\u884c\u8f6c\u5f55\u5177\u6709\u7279\u6b8a\u6311\u6218\uff0c\u56e0\u4e3a\u5fc5\u987b\u4fdd\u7559\u5177\u6709\u7279\u5b9a\u542b\u4e49\u7684\u5b57\u7b26\u548c\u7279\u6b8a\u7b26\u53f7\uff0c\u4ee5\u786e\u4fdd\u5386\u53f2\u6587\u672c\u4fdd\u6301\u5176\u539f\u59cb\u98ce\u683c\u548c\u610f\u4e49\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8f6c\u5f55\u5386\u53f2\u6587\u732e\u5e76\u4fdd\u7559\u8fd9\u4e9b\u7279\u6b8a\u7279\u5f81\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\u3002\u6211\u4eec\u5efa\u8bae\u5728\u73b0\u6709\u6587\u672c\u884c\u8bc6\u522b\u65b9\u6cd5\u7684\u57fa\u7840\u4e0a\uff0c\u52a0\u5165\u4e00\u4e2a\u7248\u9762\u5206\u6790\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u4f7f\u7528\u7248\u9762\u5206\u6790\u6a21\u578b\u5bf9\u5386\u53f2\u6587\u672c\u56fe\u50cf\u8fdb\u884c\u5206\u6790\u4ee5\u63d0\u53d6\u6587\u672c\u884c\uff0c\u7136\u540e\u7531OCR\u6a21\u578b\u5904\u7406\u8fd9\u4e9b\u6587\u672c\u884c\uff0c\u751f\u6210\u5b8c\u5168\u6570\u5b57\u5316\u7684\u9875\u9762\u3002\u6211\u4eec\u5c55\u793a\u4e86\u8be5\u6d41\u6c34\u7ebf\u80fd\u591f\u7b80\u5316\u9875\u9762\u5904\u7406\u6d41\u7a0b\u5e76\u4ea7\u751f\u9ad8\u6548\u7684\u7ed3\u679c\u3002\u6211\u4eec\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u63a9\u7801\u81ea\u7f16\u7801\u5668\u80fd\u591f\u6709\u6548\u5904\u7406\u5305\u62ec\u624b\u5199\u3001\u5370\u5237\u548c\u591a\u8bed\u8a00\u5728\u5185\u7684\u591a\u79cd\u6587\u672c\u7c7b\u578b\u3002"}}
{"id": "2601.17041", "pdf": "https://arxiv.org/pdf/2601.17041", "abs": "https://arxiv.org/abs/2601.17041", "authors": ["Ghadeer Alanazi", "Abir Benabid"], "title": "Arabic Sign Language Recognition using Multimodal Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u878d\u5408Leap Motion\u4e0eRGB\u6444\u50cf\u5934\u6570\u636e\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7528\u4e8e\u963f\u62c9\u4f2f\u624b\u8bed\u8bc6\u522b\uff0c\u5728\u81ea\u5efa18\u8bcd\u6570\u636e\u96c6\u4e0a\u8fbe\u523078%\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u963f\u62c9\u4f2f\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\u591a\u4f9d\u8d56\u5355\u4e00\u4f20\u611f\u5668\uff08\u5982Leap Motion\u6216RGB\u6444\u50cf\u5934\uff09\uff0c\u96be\u4ee5\u51c6\u786e\u6355\u6349\u590d\u6742\u624b\u90e8\u59ff\u6001\u548c\u4e09\u7ef4\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u8bc6\u522b\u6027\u80fd\u3002", "method": "\u6784\u5efa\u53cc\u901a\u9053\u5b50\u7f51\u7edc\uff1a\u4e00\u4e3a\u5e26Dropout\u548cL2\u6b63\u5219\u5316\u7684\u81ea\u5b9a\u4e49\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\u5904\u7406Leap Motion\u6570\u636e\uff0c\u53e6\u4e00\u4e3a\u7ecf\u6570\u636e\u589e\u5f3a\u5fae\u8c03\u7684VGG16\u6a21\u578b\u5904\u7406RGB\u56fe\u50cf\uff1b\u4e24\u8def\u7279\u5f81\u62fc\u63a5\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u548cSoftMax\u5206\u7c7b\u3002", "result": "\u5728\u5305\u542b18\u4e2a\u963f\u62c9\u4f2f\u624b\u8bed\u8bcd\u6c47\u7684\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u6b63\u786e\u8bc6\u522b13\u4e2a\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8fbe78%\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u5728\u963f\u62c9\u4f2f\u624b\u8bed\u8bc6\u522b\u4e2d\u5177\u6709\u521d\u6b65\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u5e76\u6269\u5c55\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "summary_cn": "\u963f\u62c9\u4f2f\u624b\u8bed\uff08ArSL\uff09\u662f\u804b\u4eba\u53ca\u542c\u529b\u969c\u788d\u7fa4\u4f53\u7684\u91cd\u8981\u4ea4\u6d41\u65b9\u5f0f\u3002\u7136\u800c\uff0c\u73b0\u6709\u8bc6\u522b\u7cfb\u7edf\u56e0\u4f9d\u8d56\u5355\u4e00\u4f20\u611f\u5668\uff08\u5982Leap Motion\u6216RGB\u6444\u50cf\u5934\uff09\u800c\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u96be\u4ee5\u5145\u5206\u8ffd\u8e2a\u590d\u6742\u7684\u624b\u90e8\u671d\u5411\u548c\u7cbe\u786e\u8bc6\u522b\u4e09\u7ef4\u624b\u90e8\u52a8\u4f5c\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7ed3\u5408Leap Motion\u4e0eRGB\u6444\u50cf\u5934\u6570\u636e\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5728ArSL\u8bc6\u522b\u4e2d\u7684\u53ef\u884c\u6027\u3002\u7cfb\u7edf\u67b6\u6784\u5305\u542b\u4e24\u4e2a\u5e76\u884c\u5b50\u7f51\u7edc\uff1a\u4e00\u4e2a\u9488\u5bf9Leap Motion\u6570\u636e\u7684\u81ea\u5b9a\u4e49\u5bc6\u96c6\u795e\u7ecf\u7f51\u7edc\uff08\u5f15\u5165Dropout\u548cL2\u6b63\u5219\u5316\uff09\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u5fae\u8c03VGG16\u6a21\u578b\u5e76\u8f85\u4ee5\u6570\u636e\u589e\u5f3a\u6280\u672f\u7684\u56fe\u50cf\u5b50\u7f51\u7edc\u3002\u4e24\u79cd\u6a21\u6001\u7684\u7279\u5f81\u8868\u793a\u5728\u878d\u5408\u6a21\u578b\u4e2d\u62fc\u63a5\uff0c\u5e76\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u5904\u7406\uff0c\u6700\u7ec8\u5229\u7528SoftMax\u6fc0\u6d3b\u51fd\u6570\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u5206\u6790\u624b\u52bf\u7684\u7a7a\u95f4\u4e0e\u65f6\u95f4\u7279\u5f81\u3002\u7cfb\u7edf\u5728\u5305\u542b18\u4e2aArSL\u8bcd\u6c47\u7684\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6210\u529f\u8bc6\u522b\u5176\u4e2d13\u4e2a\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u523078%\u3002\u8be5\u7ed3\u679c\u521d\u6b65\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u878d\u5408\u5728\u624b\u8bed\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u4e0e\u6269\u5145\u6570\u636e\u96c6\u7684\u65b9\u5411\u3002"}}
{"id": "2601.17042", "pdf": "https://arxiv.org/pdf/2601.17042", "abs": "https://arxiv.org/abs/2601.17042", "authors": ["Tianyuan Liu", "Libin Hou", "Linyuan Wang", "Bin Yan"], "title": "Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages with 6 figures", "summary": "Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between \"membership matrix\" and \"subspace matrix U\" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the \"membership matrix\" and \"subspaces U\" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u6210\u5458-\u5b50\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff08DMSA\uff09\uff0c\u901a\u8fc7\u89e3\u8026MCR2\u76ee\u6807\u4e2d\u201c\u6210\u5458\u77e9\u9635\u201d\u4e0e\u201c\u5b50\u7a7a\u95f4\u77e9\u9635U\u201d\u7684\u529f\u80fd\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u7f16\u7801\u538b\u7f29\u7387\u548c\u66f4\u9ad8\u7684ImageNet-1K\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMCR2\u7684\u767d\u76d2Transformer\u4e2d\uff0c\u201c\u6210\u5458\u77e9\u9635\u201d\u4e0e\u201c\u5b50\u7a7a\u95f4\u77e9\u9635U\u201d\u7d27\u5bc6\u8026\u5408\uff0c\u5bfc\u81f4\u5728\u9519\u8beftoken\u6295\u5f71\u4e0b\u4ea7\u751f\u5197\u4f59\u7f16\u7801\uff0c\u5f71\u54cd\u6a21\u578b\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "\u4f5c\u8005\u89e3\u8026MCR2\u76ee\u6807\u4e2d\u6210\u5458\u77e9\u9635\u4e0e\u5b50\u7a7a\u95f4\u7684\u529f\u80fd\u5173\u7cfb\uff0c\u4ece\u8f93\u5165\u76f4\u63a5\u5b66\u4e60\u6210\u5458\u77e9\u9635\uff0c\u5e76\u4ece\u5168\u7a7a\u95f4S\u5bfc\u51fa\u7a00\u758f\u5b50\u7a7a\u95f4\uff1b\u901a\u8fc7\u5bf9\u4f18\u5316\u540e\u7684MCR2\u76ee\u6807\u8fdb\u884c\u68af\u5ea6\u5c55\u5f00\uff0c\u63a8\u5bfc\u51fa\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50DMSA\u3002", "result": "\u5728ImageNet-1K\u4e0a\uff0c\u4ec5\u5c06ToST\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5757\u66ff\u6362\u4e3aDMSA\uff08\u5373DMST\uff09\u5373\u53ef\u63d0\u53471.08%-1.45%\u7684top-1\u51c6\u786e\u7387\uff0c\u5e76\u5b9e\u73b0\u66f4\u5feb\u7684\u7f16\u7801\u538b\u7f29\u7387\uff1b\u76f8\u6bd4\u6807\u51c6Transformer\uff0cDMST\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DMSA\u6709\u6548\u89e3\u51b3\u4e86MCR2\u4e2d\u6210\u5458\u4e0e\u5b50\u7a7a\u95f4\u8026\u5408\u5e26\u6765\u7684\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u6548\u7387\u3002", "summary_cn": "\u57fa\u4e8e\u7ed3\u6784\u5316\u8868\u5f81\u5b66\u4e60\u7684\u6700\u5927\u7f16\u7801\u7387\u7f29\u51cf\uff08MCR2\uff09\u9a71\u52a8\u7684\u767d\u76d2Transformer\u7edf\u4e00\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7387\uff0c\u4e3a\u89c6\u89c9\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u767d\u76d2\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5728\u73b0\u6709\u8bbe\u8ba1\u4e2d\uff0cMCR2\u4e2d\u7684\u201c\u6210\u5458\u77e9\u9635\u201d\u4e0e\u201c\u5b50\u7a7a\u95f4\u77e9\u9635U\u201d\u4e4b\u95f4\u5b58\u5728\u7d27\u5bc6\u8026\u5408\uff0c\u5bfc\u81f4\u5728\u9519\u8bef\u7684token\u6295\u5f71\u4e0b\u4ea7\u751f\u5197\u4f59\u7f16\u7801\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u89e3\u8026\u4e86MCR2\u76ee\u6807\u4e2d\u201c\u6210\u5458\u77e9\u9635\u201d\u4e0e\u201c\u5b50\u7a7a\u95f4U\u201d\u4e4b\u95f4\u7684\u529f\u80fd\u5173\u7cfb\uff0c\u5e76\u4ece\u4f18\u5316\u76ee\u6807\u7684\u5c55\u5f00\u68af\u5ea6\u4e0b\u964d\u4e2d\u63a8\u5bfc\u51fa\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u76f4\u63a5\u4ece\u8f93\u5165\u4e2d\u5b66\u4e60\u6210\u5458\u77e9\u9635\uff0c\u968f\u540e\u4ece\u5168\u7a7a\u95f4S\u4e2d\u5bfc\u51fa\u7a00\u758f\u5b50\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u5bf9\u4f18\u5316\u540e\u7684MCR2\u76ee\u6807\u8fdb\u884c\u68af\u5ea6\u5c55\u5f00\uff0c\u5f97\u5230\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\uff1a\u89e3\u8026\u6210\u5458-\u5b50\u7a7a\u95f4\u6ce8\u610f\u529b\uff08DMSA\uff09\u3002\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u5c06Token\u7edf\u8ba1Transformer\uff08ToST\uff09\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5757\u66ff\u6362\u4e3aDMSA\uff08\u6211\u4eec\u79f0\u4e4b\u4e3aDMST\uff09\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u7f16\u7801\u7f29\u51cf\u7387\uff0c\u800c\u4e14\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\u7684top-1\u51c6\u786e\u7387\u6bd4ToST\u9ad8\u51fa1.08%\u20131.45%\u3002\u4e0e\u6807\u51c6Transformer\u67b6\u6784\u76f8\u6bd4\uff0cDMST\u5c55\u73b0\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.17046", "pdf": "https://arxiv.org/pdf/2601.17046", "abs": "https://arxiv.org/abs/2601.17046", "authors": ["Matan Leibovich", "Mai Tan", "Adria Marcos-Morales", "Sreyas Mohan", "Peter A. Crozier", "Carlos Fernandez-Granda"], "title": "Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u9ad8\u566a\u58f0\u900f\u5c04\u7535\u955c\uff08TEM\uff09\u56fe\u50cf\u4e2d\u51c6\u786e\u4f30\u8ba1\u539f\u5b50\u67f1\u7684\u4e09\u7ef4\u6df1\u5ea6\u4fe1\u606f\u3002", "motivation": "\u900f\u5c04\u7535\u955c\uff08TEM\uff09\u56fe\u50cf\u5e38\u53d7\u4e25\u91cd\u566a\u58f0\u5e72\u6270\uff0c\u96be\u4ee5\u76f4\u63a5\u63d0\u53d6\u539f\u5b50\u7ea7\u4e09\u7ef4\u7ed3\u6784\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u4ece\u566a\u58f0\u56fe\u50cf\u4e2d\u6062\u590d\u6df1\u5ea6\u4fe1\u606f\u3002", "method": "\u5c06\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u5229\u7528\u5408\u6210\u566a\u58f0\u6c61\u67d3\u7684\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u751f\u6210\u50cf\u7d20\u7ea7\u6df1\u5ea6\u5206\u5272\u56fe\u3002", "result": "\u5728CeO2\u7eb3\u7c73\u9897\u7c92\u7684\u6a21\u62df\u548c\u771f\u5b9eTEM\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\u5176\u6df1\u5ea6\u4f30\u8ba1\u51c6\u786e\u3001\u7ecf\u8fc7\u6821\u51c6\u4e14\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u9ad8\u566a\u58f0TEM\u56fe\u50cf\u4e2d\u63d0\u53d6\u539f\u5b50\u7ea7\u4e09\u7ef4\u4fe1\u606f\u7684\u96be\u9898\uff0c\u4e3a\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u4e09\u7ef4\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "summary_cn": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u53d7\u663e\u8457\u566a\u58f0\u5f71\u54cd\u7684\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff08TEM\uff09\u56fe\u50cf\u4e2d\u63d0\u53d6\u539f\u5b50\u7ea7\u4e09\u7ef4\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u5c06\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u6dfb\u52a0\u4e86\u5408\u6210\u566a\u58f0\u7684\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u751f\u6210\u50cf\u7d20\u7ea7\u7684\u6df1\u5ea6\u5206\u5272\u56fe\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u88ab\u5e94\u7528\u4e8e\u4ece\u6a21\u62df\u56fe\u50cf\u548c\u771f\u5b9eTEM\u6570\u636e\u4e2d\u4f30\u8ba1CeO2\u7eb3\u7c73\u9897\u7c92\u4e2d\u539f\u5b50\u67f1\u7684\u6df1\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u5f97\u6df1\u5ea6\u4f30\u8ba1\u7ed3\u679c\u51c6\u786e\u3001\u7ecf\u8fc7\u6821\u51c6\uff0c\u5e76\u4e14\u5bf9\u566a\u58f0\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
