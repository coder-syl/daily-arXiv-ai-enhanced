<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration](https://arxiv.org/abs/2601.07845)
*Shree Charran R,Rahul Kumar Dubey*

Main category: cs.CV

TL;DR: 本文提出一种基于边缘AI的实时路侧感知节点，用于多类交通违规检测与安全事件分发，具备高精度、低功耗和无需人工校准的特点，并支持V2X通信以增强协同感知与主动道路安全管理。


<details>
  <summary>Details</summary>
Motivation: 印度等新兴经济体机动车快速增长导致执法资源严重不足（2023年超1100万起违规，警力密度仅为每4000辆车1名警员），传统人工监控与开罚单方式无法应对如此规模，亟需自主、协作且节能的边缘AI感知基础设施。

Method: 系统在NVIDIA Jetson Nano上部署，集成YOLOv8 Nano进行高精度多目标检测、DeepSORT实现时序一致的车辆跟踪，以及符合MoRTH AIS 159和ISO 7591标准的规则引导OCR后处理引擎识别劣化或多语言车牌；通过TensorRT FP16量化优化，在9.6W功耗下实现28–30 FPS推理。

Result: 系统在五类违规（闯红灯、斑马线违规、逆行、非法掉头、超速）检测中达到97.7%的违规检测准确率和84.9%的OCR识别精度，无需人工ROI校准；相比YOLOv4 Tiny、PP-YOLOE-S和NanoDetPlus，mAP提升10.7%，单位功耗精度提升1.4倍。

Conclusion: 所提出的路侧边缘AI节点不仅能高效执行多类交通违规自动执法，还能通过V2X协议向联网车辆和智能交通系统发布CAM/DENM类型的安全事件，有效增强IEEE智能车辆生态中的协同感知与主动道路安全管理能力。

Abstract: Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.

Abstract (中文翻译): 印度等新兴经济体的快速机动化造成了严重的执法不对称问题：2023年记录的交通违规超过1100万起，而人力警力密度仅为每4000辆车约1名警员。传统的监控手段和人工开罚单方式无法应对如此庞大的规模，因此亟需一种自主、协作且节能的边缘AI感知基础设施。本文提出了一种实时路侧感知节点，用于在车联网与智能车辆生态系统中进行多类别交通违规分析与安全事件分发。该节点集成了YOLOv8 Nano以实现高精度的多目标检测，采用DeepSORT进行时序一致的车辆跟踪，并配备一个规则引导的OCR后处理引擎，能够识别符合MoRTH AIS 159和ISO 7591视觉对比度标准的劣化或多语言车牌。系统部署于配备128核Maxwell GPU的NVIDIA Jetson Nano平台，并通过TensorRT FP16量化进行优化，在9.6瓦功耗下可持续实现28至30帧每秒的推理速度。在无需人工设定感兴趣区域（ROI）的情况下，系统在五类违规行为（闯红灯、斑马线违规、逆行、非法掉头和超速）检测中达到了97.7%的违规检测准确率和84.9%的OCR识别精度。与YOLOv4 Tiny、PP-YOLOE-S和NanoDetPlus相比，该系统平均精度均值（mAP）提升了10.7%，单位功耗下的精度提高了1.4倍。此外，该节点还通过V2X协议向联网车辆和智能交通系统后端发布标准化的CAM和DENM类型安全事件，表明路侧边缘AI分析可有效增强IEEE智能车辆生态系统中的协同感知与主动道路安全管理能力。

</details>


### [2] [An Empirical Study on Knowledge Transfer under Domain and Label Shifts in 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.07855)
*Subeen Lee,Siyeong Lee,Namil Kim,Jaesik Choi*

Main category: cs.CV

TL;DR: 本文提出了ROAD基准，用于评估LiDAR点云分类模型在同时存在域偏移和标签演变（类别拆分、扩展、插入）下的持续学习与迁移能力，并通过大规模数据集分析现有方法的局限性，为鲁棒3D感知提供强基线。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云感知中的持续学习与迁移学习研究远落后于2D视觉，尤其缺乏对同时发生域偏移和标签偏移场景的系统评估。为填补这一空白，作者构建了一个更贴近现实应用场景的综合评测基准。

Method: 提出ROBust Autonomous driving under Dataset shifts (ROAD) 基准，整合Waymo、NuScenes、Argoverse2等大规模LiDAR数据集，明确建模域偏移及三类标签演变（类别拆分、扩展、插入），并评估零样本迁移、线性探针和持续学习（CL）方法，分析骨干网络、训练目标和CL策略的影响。

Result: 实验揭示了现有方法在面对真实世界中同时发生的域和标签偏移时表现不佳，同时建立了多个强基线结果，为后续研究提供参考。

Conclusion: ROAD基准有效揭示了当前3D感知模型在动态环境中的脆弱性，强调了开发能同时处理域偏移和标签演化的鲁棒方法的重要性，为未来研究指明方向。

Abstract: For 3D perception systems to be practical in real-world applications -- from autonomous driving to embodied AI -- models must adapt to continuously evolving object definitions and sensor domains. Yet, research on continual and transfer learning in 3D point cloud perception remains underexplored compared to 2D vision -- particularly under simultaneous domain and label shifts. To address this gap, we propose the RObust Autonomous driving under Dataset shifts (ROAD) benchmark, a comprehensive evaluation suite for LiDAR-based object classification that explicitly accounts for domain shifts as well as three key forms of label evolution: class split, class expansion, and class insertion. Using large-scale datasets (Waymo, NuScenes, Argoverse2), we evaluate zero-shot transfer, linear probe, and CL, and analyze the impact of backbone architectures, training objectives, and CL methods. Our findings reveal limitations of existing approaches under realistic shifts and establish strong baselines for future research in robust 3D perception.

Abstract (中文翻译): 要使3D感知系统在现实世界应用（从自动驾驶到具身智能）中具备实用性，模型必须能够适应不断演化的物体定义和传感器域。然而，相较于2D视觉，针对3D点云感知的持续学习与迁移学习研究仍显不足，尤其是在同时存在域偏移和标签偏移的情况下。为填补这一空白，我们提出了“数据集偏移下的鲁棒自动驾驶”（ROAD）基准，这是一个面向LiDAR点云目标分类的综合性评估套件，明确考虑了域偏移以及三类关键的标签演化形式：类别拆分、类别扩展和类别插入。我们利用大规模数据集（Waymo、NuScenes、Argoverse2）评估了零样本迁移、线性探针和持续学习方法，并分析了骨干网络架构、训练目标和持续学习策略的影响。我们的研究结果揭示了现有方法在面对真实偏移时的局限性，并为未来鲁棒3D感知研究建立了强有力的基线。

</details>


### [3] [Moonworks Lunara Aesthetic Dataset](https://arxiv.org/abs/2601.07941)
*Yan Wang,M M Sayeef Abdullah,Partho Hassan,Sabit Hassan*

Main category: cs.CV

TL;DR: 本文提出了Lunara美学数据集，该数据集由Moonworks Lunara模型生成，涵盖中东、北欧、东亚和南亚等地区的艺术风格以及素描、油画等通用类别，具有高质量美学评分、丰富风格多样性，并附带人工优化的提示词与结构化标注，以Apache 2.0许可证发布，支持学术与商业用途。


<details>
  <summary>Details</summary>
Motivation: 现有大规模网络来源数据集往往注重广度而忽视精度，缺乏对美学质量、风格多样性和许可透明度的关注。为填补这一空白，作者构建了一个专注于高美学价值和风格多样性的图像数据集。

Method: 使用Moonworks Lunara模型生成图像，涵盖多种地域性与通用艺术风格；每张图像均配有经人工优化的提示词及描述显著对象、属性、关系和风格线索的结构化标注。

Result: 所构建的Lunara美学数据集在美学评分上显著优于现有的美学导向和通用目的数据集，成为首个在此方面表现突出的数据集。

Conclusion: Lunara美学数据集通过强调美学质量、风格多样性和许可透明度，为相关研究和应用提供了高质量、可自由使用的资源。

Abstract: The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.

Abstract (中文翻译): 该数据集涵盖了多种艺术风格，包括来自中东、北欧、东亚和南亚等地区的本土美学风格，以及素描和油画等通用类别。所有图像均由Moonworks Lunara模型生成，并经过精心设计以体现独特且高质量的美学风格，从而构建出首个在美学评分上显著超越现有美学导向数据集及通用数据集的数据集。每张图像均配有经人工优化的提示词和结构化标注，共同描述其中的显著物体、属性、关系及风格线索。与强调广度而非精度的大规模网络来源数据集不同，Lunara美学数据集优先考虑美学质量、风格多样性和许可透明度，并以Apache 2.0许可证发布，以支持学术研究及无限制的商业用途。

</details>


### [4] [LWMSCNN-SE: A Lightweight Multi-Scale Network for Efficient Maize Disease Classification on Edge Devices](https://arxiv.org/abs/2601.07957)
*Fikadu Weloday,Jianmei Su*

Main category: cs.CV

TL;DR: 提出了一种轻量级CNN模型LWMSCNN-SE，用于玉米病害分类，在保持低计算成本的同时达到96.63%的准确率，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统病害检测模型在智能手机和无人机等资源受限环境中部署困难，因其计算开销高，难以满足实时性和效率需求。

Method: 提出LWMSCNN-SE模型，结合多尺度特征提取、深度可分离卷积和SE注意力机制，构建轻量级卷积神经网络。

Result: 模型在仅241,348个参数和0.666 GFLOPs下实现了96.63%的分类准确率，适合田间实时应用。

Conclusion: 该方法有效平衡了准确率与计算效率，展示了在精准农业系统中边缘设备上高效诊断玉米病害的潜力。

Abstract: Maize disease classification plays a vital role in mitigating yield losses and ensuring food security. However, the deployment of traditional disease detection models in resource-constrained environments, such as those using smartphones and drones, faces challenges due to high computational costs. To address these challenges, we propose LWMSCNN-SE, a lightweight convolutional neural network (CNN) that integrates multi-scale feature extraction, depthwise separable convolutions, and squeeze-and-Excitation (SE) attention mechanisms. This novel combination enables the model to achieve 96.63% classification accuracy with only 241,348 parameters and 0.666 GFLOPs, making it suitable for real-time deployment in field applications. Our approach addresses the accuracy--efficiency trade-off by delivering high accuracy while maintaining low computational costs, demonstrating its potential for efficient maize disease diagnosis on edge devices in precision farming systems.

Abstract (中文翻译): 玉米病害分类在减少产量损失和保障粮食安全方面起着至关重要的作用。然而，传统的病害检测模型由于计算成本高，在智能手机和无人机等资源受限环境中的部署面临挑战。为应对这些挑战，我们提出了LWMSCNN-SE——一种轻量级卷积神经网络（CNN），该网络融合了多尺度特征提取、深度可分离卷积以及压缩激励（Squeeze-and-Excitation, SE）注意力机制。这种新颖的组合使模型在仅有241,348个参数和0.666 GFLOPs的情况下达到了96.63%的分类准确率，非常适合在田间应用中进行实时部署。我们的方法通过在保持低计算成本的同时实现高准确率，有效解决了准确率与效率之间的权衡问题，展示了其在精准农业系统中边缘设备上高效诊断玉米病害的潜力。

</details>


### [5] [3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing](https://arxiv.org/abs/2601.07963)
*Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 本文提出3DGS-Drag，一种基于点的3D编辑框架，通过拖拽操作实现对真实3D场景的高效、直观编辑，结合3D高斯泼溅变形引导和扩散模型内容引导，在几何相关编辑任务中达到SOTA效果，单卡RTX 4090上仅需10-20分钟。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景编辑在实现直观拖拽操作方面仍面临挑战，尤其在几何相关内容编辑上，现有基于形变和基于2D编辑的方法存在局限性。

Method: 提出3DGS-Drag框架，结合两种关键技术：利用3D高斯泼溅进行一致几何修改的形变引导，以及用于内容修正与视觉质量提升的扩散引导，并采用渐进式编辑策略支持大幅度拖拽操作。

Result: 该方法支持运动变化、形状调整、图像修复和内容扩展等多种编辑类型，在多个场景中表现优异，在几何相关3D内容编辑任务中达到最先进水平，且编辑效率高（单张RTX 4090 GPU耗时10-20分钟）。

Conclusion: 3DGS-Drag有效弥合了现有3D编辑方法在几何内容编辑上的不足，实现了高效、直观且高质量的真实3D场景拖拽编辑。

Abstract: The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.

Abstract (中文翻译): 生成模型的进步逐步释放了3D内容创作的变革潜力。近期，具有几何变化的直观拖拽编辑在2D编辑中受到广泛关注，但在3D场景中仍具挑战性。本文提出3DGS-Drag——一种基于点的3D编辑框架，可对真实3D场景进行高效、直观的拖拽操作。我们的方法弥合了基于形变和基于2D编辑的3D编辑方法之间的差距，解决了它们在几何相关内容编辑方面的局限性。我们利用两项关键创新：一是利用3D高斯泼溅进行一致几何修改的形变引导，二是用于内容修正和视觉质量增强的扩散引导。此外，渐进式编辑策略进一步支持大幅度的3D拖拽编辑。本方法支持多种编辑类型，包括运动变化、形状调整、图像修复和内容扩展。实验结果表明，3DGS-Drag在各种场景中均表现出色，在几何相关的3D内容编辑任务中达到了最先进的性能。值得注意的是，该编辑过程高效，在单张RTX 4090 GPU上仅需10至20分钟。

</details>


### [6] [Sesame Plant Segmentation Dataset: A YOLO Formatted Annotated Dataset](https://arxiv.org/abs/2601.07970)
*Sunusi Ibrahim Muhammad,Ismail Ismail Tijjani,Saadatu Yusuf Jumare,Fatima Isah Jibrin*

Main category: cs.CV

TL;DR: 本文提出了芝麻植株分割数据集（Sesame Plant Segmentation Dataset），一个开源的像素级标注图像数据集，专为支持农业AI模型开发而设计。该数据集包含292张在尼日利亚农田采集的芝麻早期生长阶段图像，采用YOLO兼容的分割格式，并利用SAM v2模型在农民监督下进行标注。YOLOv8评估结果显示其在检测和分割任务上均表现优异，填补了尼日利亚芝麻农业视觉数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对芝麻作物、特别是适用于真实农田环境的高质量像素级标注农业视觉数据集，限制了AI技术在芝麻种植监测、产量估算等精准农业应用中的发展。作者旨在通过构建一个专门针对芝麻早期生长阶段的开源分割数据集来弥补这一空白。

Method: 研究团队在尼日利亚卡齐纳州的农田中，使用高分辨率手机相机采集了292张芝麻植株图像。这些图像被划分为训练集、验证集和测试集，并采用Segment Anything Model (SAM) v2在农民的专业监督下进行像素级分割标注，最终形成YOLO兼容的格式。随后，使用Ultralytics YOLOv8框架对该数据集进行模型训练与评估。

Result: 在YOLOv8框架下的评估表明，模型在该数据集上取得了优异性能。在目标检测任务中，召回率和精确率均为79%，mAP@0.50为84%，mAP@0.50:0.95为58%。在分割任务中，召回率为82%，精确率为77%，mAP@0.50为84%，mAP@0.50:0.95为52%。

Conclusion: 该芝麻植株分割数据集是尼日利亚首个专注于芝麻的农业视觉数据集，其像素级标注特性为芝麻植株的精确识别与分析提供了有力支持，对推动芝麻相关的植物监测、产量预估及农业研究等应用具有重要价值。

Abstract: This paper presents the Sesame Plant Segmentation Dataset, an open source annotated image dataset designed to support the development of artificial intelligence models for agricultural applications, with a specific focus on sesame plants. The dataset comprises 206 training images, 43 validation images, and 43 test images in YOLO compatible segmentation format, capturing sesame plants at early growth stages under varying environmental conditions. Data were collected using a high resolution mobile camera from farms in Jirdede, Daura Local Government Area, Katsina State, Nigeria, and annotated using the Segment Anything Model version 2 with farmer supervision. Unlike conventional bounding box datasets, this dataset employs pixel level segmentation to enable more precise detection and analysis of sesame plants in real world farm settings. Model evaluation using the Ultralytics YOLOv8 framework demonstrated strong performance for both detection and segmentation tasks. For bounding box detection, the model achieved a recall of 79 percent, precision of 79 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 58 percent. For segmentation, it achieved a recall of 82 percent, precision of 77 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 52 percent. The dataset represents a novel contribution to sesame focused agricultural vision datasets in Nigeria and supports applications such as plant monitoring, yield estimation, and agricultural research.

Abstract (中文翻译): 本文介绍了芝麻植株分割数据集（Sesame Plant Segmentation Dataset），这是一个开源的带注释图像数据集，旨在支持人工智能模型在农业领域的应用，特别聚焦于芝麻植株。该数据集包含206张训练图像、43张验证图像和43张测试图像，采用YOLO兼容的分割格式，捕捉了在不同环境条件下处于早期生长阶段的芝麻植株。数据采集自尼日利亚卡齐纳州道拉地方政府区的吉尔代德农场，使用高分辨率手机相机拍摄，并在农民监督下利用Segment Anything Model（SAM）第二版进行标注。与传统的边界框数据集不同，该数据集采用像素级分割，以实现对真实农田环境中芝麻植株更精确的检测与分析。使用Ultralytics YOLOv8框架进行的模型评估显示，该数据集在检测和分割任务上均表现出色：在边界框检测方面，模型达到了79%的召回率、79%的精确率、84%的mAP@0.50以及58%的mAP@0.50:0.95；在分割方面，达到了82%的召回率、77%的精确率、84%的mAP@0.50以及52%的mAP@0.50:0.95。该数据集是尼日利亚首个专注于芝麻的农业视觉数据集，为植物监测、产量估算和农业研究等应用提供了新的支持。

</details>


### [7] [An Efficient Additive Kolmogorov-Arnold Transformer for Point-Level Maize Localization in Unmanned Aerial Vehicle Imagery](https://arxiv.org/abs/2601.07975)
*Fei Li,Lang Qiao,Jiahao Fan,Yijia Xu,Shawn M. Kaeppler,Zhou Zhang*

Main category: cs.CV

TL;DR: 本文提出Additive Kolmogorov-Arnold Transformer（AKT）模型，用于解决高分辨率无人机影像中玉米植株点级定位的难题。通过引入基于Pade Kolmogorov-Arnold网络（PKAN）的模块和PKAN Additive Attention机制，在提升小目标特征表达能力的同时降低计算复杂度。实验表明，AKT在F1-score上优于现有方法4.2%，并显著减少计算量、提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率无人机摄影测量在精准农业中至关重要，但点级玉米定位仍面临三大挑战：（1）目标像素占比极小（通常低于0.1%）；（2）超高分辨率图像（>3000×4000像素）上使用传统二次注意力机制计算开销巨大；（3）农业场景特有的稀疏分布与环境变化难以被通用视觉模型有效处理。

Method: 作者提出Additive Kolmogorov-Arnold Transformer（AKT）：用Pade Kolmogorov-Arnold Network（PKAN）模块替代传统多层感知机以增强小目标特征提取能力，并设计PKAN Additive Attention（PAA）机制以低计算代价建模多尺度空间依赖。同时构建了包含1,928张高分辨率无人机图像和约50.1万点标注的Point-based Maize Localization（PML）数据集。

Result: AKT在PML数据集上达到62.8%的平均F1-score，比当前最优方法高4.2%；FLOPs降低12.6%，推理吞吐量提升20.7%。在下游任务中，株数计数的平均绝对误差为7.1，株间距估计的均方根误差为1.95–1.97 cm。

Conclusion: 将Kolmogorov-Arnold表示理论与高效注意力机制相结合，为高分辨率农业遥感提供了一个有效框架，尤其适用于小目标密集定位任务。

Abstract: High-resolution UAV photogrammetry has become a key technology for precision agriculture, enabling centimeter-level crop monitoring and point-level plant localization. However, point-level maize localization in UAV imagery remains challenging due to (1) extremely small object-to-pixel ratios, typically less than 0.1%, (2) prohibitive computational costs of quadratic attention on ultra-high-resolution images larger than 3000 x 4000 pixels, and (3) agricultural scene-specific complexities such as sparse object distribution and environmental variability that are poorly handled by general-purpose vision models.
  To address these challenges, we propose the Additive Kolmogorov-Arnold Transformer (AKT), which replaces conventional multilayer perceptrons with Pade Kolmogorov-Arnold Network (PKAN) modules to enhance functional expressivity for small-object feature extraction, and introduces PKAN Additive Attention (PAA) to model multiscale spatial dependencies with reduced computational complexity. In addition, we present the Point-based Maize Localization (PML) dataset, consisting of 1,928 high-resolution UAV images with approximately 501,000 point annotations collected under real field conditions.
  Extensive experiments show that AKT achieves an average F1-score of 62.8%, outperforming state-of-the-art methods by 4.2%, while reducing FLOPs by 12.6% and improving inference throughput by 20.7%. For downstream tasks, AKT attains a mean absolute error of 7.1 in stand counting and a root mean square error of 1.95-1.97 cm in interplant spacing estimation. These results demonstrate that integrating Kolmogorov-Arnold representation theory with efficient attention mechanisms offers an effective framework for high-resolution agricultural remote sensing.

Abstract (中文翻译): 高分辨率无人机摄影测量已成为精准农业的关键技术，可实现厘米级作物监测和点级植株定位。然而，由于（1）目标像素占比极小（通常低于0.1%），（2）在超过3000×4000像素的超高分辨率图像上使用传统二次注意力机制带来巨大的计算开销，以及（3）农业场景中稀疏目标分布和环境变化等复杂因素难以被通用视觉模型有效处理，点级玉米定位仍然具有挑战性。为应对这些挑战，本文提出Additive Kolmogorov-Arnold Transformer（AKT）模型，该模型采用Pade Kolmogorov-Arnold网络（PKAN）模块替代传统多层感知机，以增强对小目标的特征表达能力，并引入PKAN Additive Attention（PAA）机制，在降低计算复杂度的同时建模多尺度空间依赖关系。此外，本文还构建了Point-based Maize Localization（PML）数据集，包含1,928张高分辨率无人机图像及约50.1万个在真实田间条件下采集的点标注。大量实验表明，AKT模型平均F1-score达到62.8%，比当前最先进的方法高出4.2%，同时FLOPs降低12.6%，推理吞吐量提升20.7%。在下游任务中，AKT在株数计数上的平均绝对误差为7.1，在株间距估计上的均方根误差为1.95–1.97厘米。这些结果表明，将Kolmogorov-Arnold表示理论与高效注意力机制相结合，为高分辨率农业遥感提供了一种有效的解决方案。

</details>


### [8] [Likelihood ratio for a binary Bayesian classifier under a noise-exclusion model](https://arxiv.org/abs/2601.07982)
*Howard C. Gifford*

Main category: cs.CV

TL;DR: 提出了一种新的统计理想观察者模型，通过设置最小可提取图像特征的阈值来进行整体视觉搜索（或概览）处理，从而减少自由参数数量并简化系统。


<details>
  <summary>Details</summary>
Motivation: 现有视觉搜索模型通常参数繁多、复杂度高，难以在医学成像、计算机视觉和安防等领域高效应用。因此需要一种更简洁、更具实用性的理想观察者模型。

Method: 构建一种新型统计理想观察者模型，该模型通过对最小可提取图像特征设定阈值，实现整体视觉（gist）处理，并有效减少模型自由参数数量。

Result: 该模型成功简化了系统结构，并适用于多个领域，包括医学图像感知、计算机视觉、性能基准测试、特征选择与评估，以及国防/安全中的目标检测识别和传感器评估。

Conclusion: 所提出的理想观察者模型提供了一个通用且高效的框架，可用于优化成像系统、算法评估及多种视觉任务中的特征分析。

Abstract: We develop a new statistical ideal observer model that performs holistic visual search (or gist) processing in part by placing thresholds on minimum extractable image features. In this model, the ideal observer reduces the number of free parameters thereby shrinking down the system. The applications of this novel framework is in medical image perception (for optimizing imaging systems and algorithms), computer vision, benchmarking performance and enabling feature selection/evaluations. Other applications are in target detection and recognition in defense/security as well as evaluating sensors and detectors.

Abstract (中文翻译): 我们开发了一种新的统计理想观察者模型，该模型通过设定最小可提取图像特征的阈值，部分地实现整体视觉搜索（或称“概览”）处理。在此模型中，理想观察者减少了自由参数的数量，从而简化了整个系统。该新框架的应用包括医学图像感知（用于优化成像系统和算法）、计算机视觉、性能基准测试以及特征选择与评估。其他应用还包括国防与安全领域的目标检测与识别，以及传感器和探测器的评估。

</details>


### [9] [Predicting Region of Interest in Human Visual Search Based on Statistical Texture and Gabor Features](https://arxiv.org/abs/2601.07998)
*Hongwei Lin,Diego Andrade,Mini Das,Howard C. Gifford*

Main category: cs.CV

TL;DR: 该研究提出两种融合Gabor与GLCM特征的管道，用于预测人类在未知目标位置视觉搜索任务中的早期注视区域，并在数字乳腺断层合成图像上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 理解人类视觉搜索行为对建模注意力分配机制至关重要，尤其是在目标位置未知的搜索任务中；现有方法需更好地整合结构与纹理信息以提升预测准确性。

Method: 提出两种特征融合管道，结合Gabor特征与灰度共生矩阵（GLCM）纹理特征，用于缩小人类可能注视区域，并在模拟数字乳腺断层合成图像上进行评估。

Result: 所提方法预测的注视候选区域与基于阈值的模型观察者定性一致；GLCM均值与Gabor特征响应高度相关；眼动数据表明预测区域与人类早期注视行为一致。

Conclusion: 结合结构与纹理特征有助于更准确地建模人类视觉搜索行为，为构建感知驱动的观察者模型提供支持。

Abstract: Understanding human visual search behavior is a fundamental problem in vision science and computer vision, with direct implications for modeling how observers allocate attention in location-unknown search tasks. In this study, we investigate the relationship between Gabor-based features and gray-level co-occurrence matrix (GLCM) based texture features in modeling early-stage visual search behavior. Two feature-combination pipelines are proposed to integrate Gabor and GLCM features for narrowing the region of possible human fixations. The pipelines are evaluated using simulated digital breast tomosynthesis images. Results show qualitative agreement among fixation candidates predicted by the proposed pipelines and a threshold-based model observer. A strong correlation is observed between GLCM mean and Gabor feature responses, indicating that these features encode related image information despite their different formulations. Eye-tracking data from human observers further suggest consistency between predicted fixation regions and early-stage gaze behavior. These findings highlight the value of combining structural and texture-based features for modeling visual search and support the development of perceptually informed observer models.

Abstract (中文翻译): 理解人类视觉搜索行为是视觉科学与计算机视觉中的一个基本问题，对于建模观察者在目标位置未知的搜索任务中如何分配注意力具有直接意义。本研究探讨了基于Gabor的特征与基于灰度共生矩阵（GLCM）的纹理特征在建模早期视觉搜索行为中的关系。我们提出了两种特征融合管道，将Gabor特征与GLCM特征相结合，以缩小人类可能注视的区域。这些管道在模拟的数字乳腺断层合成图像上进行了评估。结果表明，所提出的管道预测的注视候选区域与基于阈值的模型观察者之间存在定性一致性。同时观察到GLCM均值与Gabor特征响应之间存在强相关性，表明尽管二者形式不同，但编码了相关的图像信息。来自人类观察者的眼动数据进一步表明，预测的注视区域与早期注视行为具有一致性。这些发现突显了结合结构与纹理特征在建模视觉搜索中的价值，并支持开发感知驱动的观察者模型。

</details>


### [10] [CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation](https://arxiv.org/abs/2601.08010)
*Chaoyu Li,Deeparghya Dutta Barua,Fei Tao,Pooyan Fazli*

Main category: cs.CV

TL;DR: 本文提出两种方法（CASHEW 和 CASHEW-RL）提升视觉语言模型多步推理的稳定性：前者通过推理时聚合多个轨迹并用视觉验证过滤幻觉，后者通过新提出的GSPO算法训练模型实现内部自聚合，在13个基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多步推理中存在不稳定性，对同一输入多次采样常产生不同推理路径和不一致预测，影响可靠性。

Method: 提出CASHEW框架，在推理时迭代聚合多个候选推理轨迹，并通过显式视觉验证剔除幻觉步骤；同时提出CASHEW-RL，利用Group Sequence Policy Optimization (GSPO)训练模型，使其在单次前向中内化聚合行为，奖励机制鼓励基于最少但充分视觉证据得出正确答案，并根据任务难度自适应分配推理资源。

Result: 在13个图像理解、视频理解和视频推理基准上取得显著提升，例如ScienceQA提升+23.6个百分点，EgoSchema提升+8.1个百分点。

Conclusion: 所提方法有效提升了视觉语言模型多步推理的稳定性和准确性，尤其在复杂任务上表现突出，验证了测试时缩放思想在多模态推理中的有效性。

Abstract: Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.

Abstract (中文翻译): 视觉语言模型在广泛的多模态理解与推理任务中表现出色，但其多步推理过程仍不稳定。对同一输入进行重复采样常常产生不同的推理路径和不一致的最终预测。为解决这一问题，我们提出了两种受测试时缩放（test-time scaling）启发的互补方法：（1）CASHEW，一种推理时框架，通过迭代聚合多个候选推理轨迹生成更高质量的推理链，并引入显式视觉验证机制来过滤幻觉步骤，使推理扎根于视觉证据；（2）CASHEW-RL，一种学习型变体，将上述聚合行为内化到单一模型中。CASHEW-RL采用群体序列策略优化（Group Sequence Policy Optimization, GSPO）进行训练，其复合奖励机制鼓励模型基于最少但充分的视觉证据得出正确答案，并能根据任务难度自适应地分配推理资源。该训练目标使模型在推理阶段具备强大的自聚合能力。在13个图像理解、视频理解和视频推理基准上的大量实验表明，该方法显著提升了性能，例如在ScienceQA上提升高达23.6个百分点，在EgoSchema上提升8.1个百分点。

</details>
