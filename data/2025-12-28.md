<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

TL;DR: 本文提出 VL4Gaze，首个大规模用于评估和训练视觉语言模型（VLM）理解人类凝视的基准数据集，包含489K自动生成的问答对，涵盖四个互补任务。实验表明，现有VLM在缺乏针对性监督时难以有效理解凝视语义与定位，而基于VL4Gaze的训练可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLM）虽在多种视觉任务中表现出色，但尚未系统研究其对人类凝视（gaze）的理解能力。由于缺乏专门的基准数据集，尚不清楚凝视理解能否通过通用视觉-语言预训练自然涌现。

Method: 作者构建了 VL4Gaze 基准，包含124K图像和489K自动生成的问答对，将凝视理解建模为统一的视觉问答（VQA）问题，并设计四个互补任务：凝视对象描述、凝视方向描述、凝视点定位和模糊问题识别。在上下文学习和微调设置下评估多个开源与商用VLM。

Result: 实验显示，即使大规模VLM在无任务特定监督的情况下也难以可靠地推断凝视语义和空间位置；而在 VL4Gaze 上训练后，所有任务性能均显著且一致提升。

Conclusion: 针对凝视理解的多任务监督对提升VLM在此类任务上的能力至关重要，通用预训练不足以自动获得该能力。作者将公开数据集与代码以推动后续研究。

Abstract: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

Abstract (中文翻译): 人类凝视为理解视觉场景中的注意力、意图和社会互动提供了关键线索，然而当前的视觉语言模型（VLM）在凝视理解方面仍鲜有探索。尽管近期的VLM在多种视觉任务中展现出强大的场景级推理能力，但尚无系统性评估或训练其凝视理解能力的基准，因此尚不清楚凝视理解是否能从通用视觉-语言预训练中自然涌现。为填补这一空白，我们提出了 VL4Gaze——首个大规模基准，旨在探究、评估并释放VLM在凝视理解方面的潜力。VL4Gaze 包含124K张图像上的48.9万条自动生成的问答对，并通过四项互补任务将凝视理解形式化为统一的视觉问答（VQA）问题：（1）凝视对象描述，（2）凝视方向描述，（3）凝视点定位，以及（4）模糊问题识别。我们在上下文学习和微调两种设置下全面评估了多个商用和开源VLM。结果表明，即使大规模VLM在缺乏任务特定监督的情况下也难以可靠地推断凝视语义和空间定位；相比之下，在 VL4Gaze 上进行训练可在所有任务上带来显著且一致的性能提升，凸显了针对性多任务监督对于发展VLM凝视理解能力的重要性。我们将发布该数据集和代码，以支持该方向的进一步研究与开发。

</details>


### [2] [TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection](https://arxiv.org/abs/2512.20746)
*Tony Tran,Bin Hu*

Main category: cs.CV

TL;DR: 本文提出一种硬件感知的神经架构搜索框架，在TACO数据集上为TinyML设备高效搜索出一系列名为TrashDets的轻量级垃圾检测器，在精度、参数量和能效方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在资源极度受限的TinyML边缘和物联网设备上实现高效的垃圾检测，需要在严格约束下设计兼顾精度与效率的模型，而现有方法难以同时满足这些要求。

Method: 提出一个迭代式硬件感知神经架构搜索框架：构建Once-for-All风格的ResDets超网络，并通过交替优化骨干网络与检测头/颈部的进化搜索策略，结合种群传递机制和精度预测器以降低搜索成本并提升稳定性。

Result: 在TACO五类子集上，最强模型TrashDet-l达到19.5 mAP50，参数仅30.5M，比先前工作最高提升3.6 mAP50且参数更少；整个TrashDet系列覆盖1.2M–30.5M参数范围。在MAX78002微控制器上，TrashDet-ResNet和TrashDet-MBNet分别在能效和精度上大幅超越基线，最高降低88%能耗、78%延迟和53%平均功耗。

Conclusion: 所提框架成功生成了一族适用于不同TinyML部署预算的高性能、低功耗垃圾检测器，在精度、延迟和能耗方面均显著优于现有TinyML检测器，验证了硬件感知神经架构搜索在边缘环境中的有效性。

Abstract: This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.

Abstract (中文翻译): 本文针对TACO数据集上的垃圾检测任务，在严格的TinyML约束下，提出了一种面向边缘和物联网设备的迭代式硬件感知神经架构搜索框架。该方法构建了一个Once-for-All风格的ResDets超网络，并采用交替优化骨干网络与检测头/颈部的迭代进化搜索策略，辅以种群传递机制和精度预测器，以降低搜索成本并提高稳定性。该框架生成了一系列可直接部署的检测器，称为TrashDets。在包含纸张、塑料、瓶子、易拉罐和烟头的五类TACO子集上，性能最强的变体TrashDet-l以30.5M参数实现了19.5 mAP50的精度，相比先前的检测器最高提升3.6 mAP50，同时使用更少的参数。TrashDet系列模型参数量覆盖1.2M至30.5M，mAP50在11.4至19.5之间，为不同TinyML部署预算提供了可扩展的选择。在MAX78002微控制器上使用TrashNet数据集时，两个专用变体TrashDet-ResNet和TrashDet-MBNet共同优于ai87-fpndetector基线：TrashDet-ResNet单次推理能耗为7525微焦，延迟26.7毫秒，帧率达37.45 FPS；TrashDet-MBNet则将mAP50提升了10.2%。两者相较现有TinyML检测器，最高可减少88%的能耗、78%的延迟和53%的平均功耗。

</details>


### [3] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

TL;DR: 本文提出了OccuFly，首个基于相机的真实世界空中语义场景补全（SSC）基准数据集，并设计了一种无需LiDAR的数据生成框架，利用传统3D重建技术自动将2D标注提升至3D点云，显著减少人工标注成本。该数据集涵盖多种季节、高度和场景类型，为无人机在高空视角下的3D感知研究提供了重要资源。


<details>
  <summary>Details</summary>
Motivation: 现有语义场景补全（SSC）研究主要集中在地面场景（如自动驾驶），而空中场景（如自主飞行）尚未充分探索；同时，主流依赖LiDAR传感器的SSC方法难以适用于受法规、重量与能耗限制的无人机，且高空视角下LiDAR点云稀疏，因此亟需一种基于相机模态的空中SSC解决方案。

Method: 作者构建了OccuFly数据集，包含不同季节、高度（50m/40m/30m）及城乡工业等多样化场景，并提出一种无LiDAR的数据生成框架：通过传统3D重建方法将部分带标注的2D图像掩码自动提升到重建的3D点云中，实现高效3D语义标注。

Result: OccuFly提供22个语义类别，格式兼容现有研究，并在该数据集上对当前先进方法进行了基准测试，揭示了高空视角下SSC任务的独特挑战。

Conclusion: OccuFly是首个基于相机的真实空中SSC基准，其提出的无LiDAR标注框架有效降低了3D标注成本，为推动无人机平台上的整体3D场景理解提供了重要基础。

Abstract: Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

Abstract (中文翻译): 语义场景补全（SSC）对于移动机器人中的三维感知至关重要，因为它能够通过联合估计密集体素占用率和每个体素的语义信息来实现对场景的整体理解。尽管SSC在自动驾驶等地面领域已被广泛研究，但在自主飞行等空中场景中仍鲜有探索，从而限制了下游应用的发展。此外，激光雷达（LiDAR）传感器是目前SSC数据生成的主要模态，但由于飞行法规、质量和能耗限制，以及从高视角获取的LiDAR点云稀疏性，这对大多数无人飞行器（UAV）构成了挑战。为解决这些问题，我们提出了OccuFly——首个基于相机的真实世界空中SSC基准数据集，该数据集在春季、夏季、秋季和冬季分别于50米、40米和30米的高度采集，涵盖城市、工业和乡村场景，提供22个语义类别，且数据格式遵循既定规范以方便与现有研究无缝集成。尤为重要的是，我们提出了一种基于相机模态的无LiDAR数据生成框架，该模态在现代UAV上普遍存在。通过利用传统的三维重建方法，我们的框架可将部分带注释的二维掩码自动提升至重建的点云中，从而大幅减少人工三维标注的工作量。最后，我们在OccuFly上对当前最先进的方法进行了基准测试，并突出了高视角下特有的挑战，构建了一个全面的视觉基准，用于整体空中三维场景理解。

</details>


### [4] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: NullBUS 是一种新型多模态混合监督框架，可在乳腺超声图像中同时利用有提示和无提示的数据进行训练，通过可空提示机制在缺少文本元数据时回退到仅图像模式，在三个公开数据集上达到 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 许多公开的乳腺超声（BUS）数据集缺乏可靠的元数据或报告，导致基于提示的分割方法只能在小规模多模态子集上训练，限制了模型的鲁棒性。

Method: 提出 NullBUS 框架，引入“可空提示”机制——使用可学习的空嵌入和存在掩码，在元数据缺失时回退至仅图像证据，而在元数据存在时利用文本提示，实现单模型对有/无提示图像的统一学习。

Result: 在三个公开 BUS 数据集的统一测试池中，NullBUS 达到平均 IoU 0.8568 和平均 Dice 0.9103，展现出在混合提示可用条件下的最先进性能。

Conclusion: NullBUS 有效解决了乳腺超声图像中提示信息缺失的问题，提升了模型在真实场景中的泛化能力和分割精度。

Abstract: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

Abstract (中文翻译): 乳腺超声（BUS）分割为计算机辅助诊断和治疗规划提供了关键的病灶边界信息。尽管可提示方法在提供文本或空间提示时能够提升分割性能和肿瘤轮廓描绘效果，但许多公开的 BUS 数据集缺乏可靠的元数据或报告，导致训练只能局限于小规模的多模态子集，从而降低了模型的鲁棒性。我们提出了 NullBUS，一种多模态混合监督框架，能够在单一模型中同时从有提示和无提示的图像中学习。为处理缺失的文本信息，我们引入了“可空提示”机制，该机制通过可学习的空嵌入和存在掩码实现：当元数据缺失时，模型可回退至仅依赖图像证据；当元数据存在时，则可利用文本提示。在三个公开 BUS 数据集构成的统一测试池上的评估表明，NullBUS 实现了平均 IoU 为 0.8568、平均 Dice 为 0.9103 的性能，展示了在混合提示可用条件下最先进的分割效果。

</details>


### [5] [Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation](https://arxiv.org/abs/2512.20815)
*Reeshad Khan amd John Gauch*

Main category: cs.CV

TL;DR: 本文提出一种任务驱动的端到端RAW-to-task协同设计框架，将光学、传感器建模与轻量语义分割网络统一优化，在KITTI-360上显著提升mIoU，同时保持模型小巧（约1M参数）和实时性（~28 FPS），证明了全栈协同优化在自动驾驶感知中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶系统将相机设计与下游感知任务解耦，采用固定光学系统和人工设计的图像信号处理（ISP）流程，优先考虑人眼可视图像而非机器语义，导致在去马赛克、去噪或量化过程中丢失对感知任务有用的信息，并迫使模型适应传感器伪影。

Method: 提出一个端到端的RAW-to-task协同设计框架，整合真实手机级镜头模型、可学习彩色滤光阵列（CFA）、泊松-高斯噪声模型和量化过程，并直接针对语义分割目标联合优化光学、传感器与轻量分割网络。

Result: 在KITTI-360数据集上，该方法相比固定流水线持续提升mIoU，其中光学建模和CFA学习带来最大增益，尤其对细长或低光照敏感类别；模型仅含约100万参数，运行速度达28 FPS，具备边缘部署能力；视觉与定量分析表明协同设计的传感器能根据语义结构自适应采集，增强边界并维持在模糊、噪声和低位深条件下的精度。

Conclusion: 全栈协同优化（光学+传感器+网络）是实现高效、可靠且可部署的自动驾驶感知系统的有效路径。

Abstract: Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.

Abstract (中文翻译): 传统的自动驾驶系统将相机设计与下游感知任务解耦，依赖固定的光学系统和人工设计的图像信号处理（ISP）流程，这些流程优先考虑人眼可视图像而非机器语义。这种分离方式在去马赛克、去噪或量化过程中丢弃了有用信息，同时迫使模型去适应传感器引入的伪影。我们提出了一种任务驱动的协同设计框架，将光学、传感器建模与轻量级语义分割网络统一为单一的端到端RAW到任务（RAW-to-task）流水线。该系统基于DeepLens[19]，整合了逼真的手机级镜头模型、可学习的彩色滤光阵列（CFA）、泊松-高斯噪声过程以及量化操作，并直接针对分割目标进行联合优化。在KITTI-360上的评估表明，该方法相较于固定流水线始终取得更高的mIoU，其中光学建模和CFA学习带来了最大性能提升，尤其对细长物体或对低光照敏感的类别效果显著。重要的是，这些鲁棒性提升是在一个紧凑的约100万参数模型上实现的，运行速度约为28 FPS，展现出良好的边缘部署能力。视觉和定量分析进一步揭示，协同设计的传感器能够根据语义结构自适应地调整图像采集过程，在模糊、噪声和低位深条件下仍能锐化边界并保持准确率。这些发现共同表明，对光学、传感器和网络进行全栈协同优化，是实现高效、可靠且可部署的自动驾驶感知系统的有效途径。

</details>


### [6] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

TL;DR: 本文提出了CHAMMI-75数据集，包含来自75项研究的多通道显微图像，用于训练能适应不同成像条件的细胞形态学模型。


<details>
  <summary>Details</summary>
Motivation: 现有细胞形态学量化模型通常针对单一显微成像类型训练，难以跨研究复用，因技术规格（如通道数）或实验条件差异导致分布外问题。

Method: 构建并公开发布CHAMMI-75数据集，该数据集整合了75项不同生物研究的异构多通道显微图像，用于训练通道自适应的细胞形态模型。

Result: 在CHAMMI-75上训练的模型在多通道生物成像任务中表现更优，主要得益于其显微成像模态的高度多样性。

Conclusion: CHAMMI-75为开发适用于多种生物研究的新一代细胞形态学模型奠定了基础。

Abstract: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

Abstract (中文翻译): 利用图像和机器学习对细胞形态进行量化已被证明是研究细胞对处理响应的强大工具。然而，用于量化细胞形态的模型通常使用单一类型的显微成像进行训练，这导致了专用模型无法在不同生物学研究间复用，原因在于技术规格不匹配（例如通道数量不同）或目标实验条件超出分布范围。在此，我们提出了CHAMMI-75——一个开放获取的数据集，包含来自75项多样化生物学研究的异构多通道显微图像。我们从公开资源中整理了这一数据集，旨在研究能够自适应通道并处理任意显微图像类型的细胞形态模型。我们的实验表明，使用CHAMMI-75进行训练可显著提升多通道生物成像任务的性能，这主要归功于其在显微成像模态方面的高度多样性。本研究为构建适用于各类生物学研究的新一代细胞形态模型铺平了道路。

</details>


### [7] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

TL;DR: 本文提出一种自适应视觉预处理方法，可根据图像内容动态调整分辨率和空间覆盖范围，在不修改 FastVLM 架构或重新训练的前提下，显著降低视觉语言模型的推理时间和视觉 token 数量。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在多模态推理任务中表现优异，但部署时面临高推理延迟和计算成本问题，尤其在处理高分辨率图像时。现有方法采用静态视觉预处理，对简单图像仍进行冗余计算。

Method: 提出一种自适应视觉预处理方法，结合内容感知图像分析、自适应分辨率选择和内容感知裁剪，在送入视觉编码器前减少视觉冗余，并与 FastVLM 无缝集成，无需修改模型结构或重新训练。

Result: 在 DocVQA 数据集子集上的实验表明，该方法将每张图像的推理时间减少超过 50%，平均生成时间下降，视觉 token 数量减少超过 55%。

Conclusion: 内容感知的自适应预处理是一种轻量且有效的策略，可显著提升视觉语言模型在部署场景中的效率。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

Abstract (中文翻译): 视觉语言模型（VLMs）在多模态推理任务中展现出强大性能，但其部署仍面临高推理延迟和计算成本的挑战，尤其是在处理高分辨率视觉输入时。尽管近期如 FastVLM 等架构通过优化视觉编码器提升了效率，但现有流程仍依赖静态视觉预处理，导致对视觉上简单的输入产生冗余计算。本文提出一种自适应视觉预处理方法，可根据图像内容特征动态调整输入分辨率和空间覆盖范围。该方法结合内容感知的图像分析、自适应分辨率选择以及内容感知裁剪，在视觉编码前减少视觉冗余。重要的是，该方法无需修改 FastVLM 的架构或重新训练即可集成。我们在 DocVQA 数据集的一个子集上以仅推理方式评估了所提方法，重点关注面向效率的指标。实验结果表明，自适应预处理相比基线流程，将每张图像的推理时间减少超过 50%，降低了平均完整生成时间，并使视觉 token 数量持续减少超过 55%。这些结果表明，基于输入内容的预处理是一种有效且轻量的策略，可提升视觉语言模型在部署场景中的效率。为便于复现，我们的实现作为 FastVLM 仓库的一个分支提供，包含所提方法的相关文件，地址为 https://github.com/kmdavidds/mlfastlm。

</details>


### [8] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

TL;DR: ALIVE 是一个本地运行的交互式讲座视频引擎，结合神经化身、内容感知检索与多模态交互，将传统被动观看转变为实时互动学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统讲座视频缺乏实时答疑机制，学习者遇困惑需外部搜索；现有交互系统常缺乏对讲座内容的理解、依赖云端服务，或未能在保护隐私的前提下统一整合检索与化身讲解。

Method: ALIVE 在本地硬件上运行，整合三项核心技术：(1) 基于 ASR 转录、大语言模型优化和神经说话头合成的化身讲解；(2) 结合语义相似性与时间戳对齐的内容感知检索机制；(3) 支持文本/语音提问并以文本或化身形式返回答案的实时多模态交互。系统采用轻量嵌入模型、FAISS 检索及分段预加载策略保障响应速度。

Result: 在完整医学影像课程上验证了 ALIVE 的检索准确性、延迟表现与用户体验，证明其能提供准确、内容相关且具吸引力的实时学习支持。

Conclusion: ALIVE 展示了多模态 AI 与内容感知检索、本地部署相结合，可显著提升录播讲座的教学价值，为下一代交互式学习环境提供可扩展路径。

Abstract: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

Abstract (中文翻译): 传统的讲座视频虽具灵活性，但缺乏实时澄清机制，迫使学习者在困惑时需借助外部搜索。尽管大型语言模型和神经化身的最新进展为交互式学习提供了新机遇，但现有系统通常缺乏对讲座内容的理解、依赖云端服务，或未能在统一且保护隐私的流程中整合检索与化身讲解功能。  
我们提出了 ALIVE（Avatar-Lecture Interactive Video Engine），它将被动的讲座观看转变为动态的实时学习体验。ALIVE 完全在本地硬件上运行，集成了以下三项技术：(1) 通过自动语音识别（ASR）转录、大语言模型（LLM）优化和神经说话头合成生成的化身讲解；(2) 结合语义相似性与时间戳对齐的内容感知检索机制，用于定位上下文相关的讲座片段；(3) 实时多模态交互功能，允许学生暂停讲座并通过文本或语音提问，并以文本或化身形式接收基于内容的解释。  
为保证响应速度，ALIVE 采用轻量级嵌入模型、基于 FAISS 的检索方法，以及分段化身合成与渐进式预加载策略。我们在一门完整的医学影像课程上展示了该系统，并评估了其检索准确性、延迟特性和用户体验，结果表明 ALIVE 能够提供准确、内容相关且引人入胜的实时学习支持。  
ALIVE 表明，当多模态人工智能与内容感知检索和本地部署相结合时，可显著提升录播讲座的教学价值，为构建下一代交互式学习环境提供了一条可扩展的路径。

</details>


### [9] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

TL;DR: 本文提出一种用于地下管线3D探地雷达（GPR）智能检测的新框架，通过三视图联合分析、改进YOLO模型（DCO-YOLO）和3D-DIoU空间匹配算法，显著提升了小目标识别精度与多视图融合鲁棒性，在真实数据上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决3D探地雷达地下管线检测中多视图特征关联弱、小尺度目标识别精度低以及复杂场景下鲁棒性不足的问题。

Method: 1）建立基于B/C/D-Scan三视图联合分析的三维管线特征评估方法；2）提出DCO-YOLO框架，融合DySample、CGLU和OutlookAttention机制以增强小目标边缘特征提取；3）设计3D-DIoU空间特征匹配算法，结合三维几何约束与中心距离惩罚项实现多视图标注自动关联。

Result: 在真实城市地下管线数据实验中，所提方法在复杂多管线场景下达到96.2%准确率、93.3%召回率和96.7%mAP，分别比基线高2.0%、2.1%和0.9%；消融实验和Grad-CAM++可视化验证了模型对管线几何特征的关注能力提升。

Conclusion: 该研究将深度学习优化策略与3D GPR物理特性相结合，为地下管线的智能识别与定位提供了一种高效可靠的新技术框架。

Abstract: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

Abstract (中文翻译): 为解决利用三维探地雷达（3D GPR）进行地下管线检测时存在的多视图特征相关性弱、小尺度目标识别精度低以及复杂场景下鲁棒性不足等问题，本文提出了一种三维管线智能检测框架。首先，基于B/C/D-Scan三视图联合分析策略，通过FDTD方法获得的正演模拟结果与实际测量数据交叉验证，建立了三维管线三视图特征评估方法。其次，提出了DCO-YOLO框架，将DySample、CGLU和OutlookAttention等跨维度关联机制集成到原始YOLOv11算法中，显著提升了对小尺度管线边缘特征的提取能力。此外，还提出了一种3D-DIoU空间特征匹配算法，融合三维几何约束与中心距离惩罚项，实现了多视图标注的自动关联，三视图融合策略有效解决了单视图检测中的固有模糊性。基于真实城市地下管线数据的实验表明，所提方法在复杂多管线场景下的准确率、召回率和平均精度均值（mAP）分别达到96.2%、93.3%和96.7%，较基线模型分别提高了2.0%、2.1%和0.9%。消融实验证实了动态特征增强模块的协同优化效果，Grad-CAM++热力图可视化也表明改进后的模型显著增强了对管线几何特征的关注能力。本研究将深度学习优化策略与3D GPR的物理特性相结合，为地下管线的智能识别与定位提供了一种高效且可靠的新技术框架。

</details>


### [10] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

TL;DR: NeRV360 是一种面向 360 度视频的高效隐式神经表示框架，通过仅解码用户视口而非整帧全景，显著降低内存消耗并提升解码速度，同时保持更优画质。


<details>
  <summary>Details</summary>
Motivation: 将 NeRV 应用于高分辨率 360 度视频时存在内存占用高和解码速度慢的问题，难以满足实时应用需求。

Method: 提出 NeRV360 框架，将视口提取集成到解码过程中，并引入时空仿射变换模块，根据视角和时间进行条件解码，仅重建用户选择的视口区域。

Result: 在 6K 分辨率视频上的实验表明，与代表性方法 HNeRV 相比，NeRV360 内存消耗减少 7 倍，解码速度提升 2.5 倍，且客观画质指标更优。

Conclusion: NeRV360 有效解决了高分辨率 360 度视频在隐式神经表示下的效率瓶颈，在保证高质量的同时实现了实用化的内存与速度性能。

Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

Abstract (中文翻译): 面向视频的隐式神经表示（NeRV）在视频压缩方面展现出强大潜力。然而，将 NeRV 应用于高分辨率 360 度视频会导致内存占用过高和解码速度缓慢，使得实时应用变得不切实际。我们提出了 NeRV360，这是一种端到端的框架，仅解码用户选定的视口，而不是重建整个全景帧。与传统流程不同，NeRV360 将视口提取集成到解码过程中，并引入了一个时空仿射变换模块，以根据视角和时间进行条件解码。在 6K 分辨率视频上的实验表明，与代表性先前工作 HNeRV 相比，NeRV360 实现了 7 倍的内存消耗降低和 2.5 倍的解码速度提升，同时在客观指标上提供了更优的图像质量。

</details>
