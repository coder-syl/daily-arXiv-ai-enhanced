{"id": "2601.14258", "pdf": "https://arxiv.org/pdf/2601.14258", "abs": "https://arxiv.org/abs/2601.14258", "authors": ["Ho Yin Au", "Junkun Jiang", "Jie Chen"], "title": "SOSControl: Enhancing Human Motion Generation through Saliency-Aware Symbolic Orientation and Timing Control", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by AAAI 2026", "summary": "Traditional text-to-motion frameworks often lack precise control, and existing approaches based on joint keyframe locations provide only positional guidance, making it challenging and unintuitive to specify body part orientations and motion timing. To address these limitations, we introduce the Salient Orientation Symbolic (SOS) script, a programmable symbolic framework for specifying body part orientations and motion timing at keyframes. We further propose an automatic SOS extraction pipeline that employs temporally-constrained agglomerative clustering for frame saliency detection and a Saliency-based Masking Scheme (SMS) to generate sparse, interpretable SOS scripts directly from motion data. Moreover, we present the SOSControl framework, which treats the available orientation symbols in the sparse SOS script as salient and prioritizes satisfying these constraints during motion generation. By incorporating SMS-based data augmentation and gradient-based iterative optimization, the framework enhances alignment with user-specified constraints. Additionally, it employs a ControlNet-based ACTOR-PAE Decoder to ensure smooth and natural motion outputs. Extensive experiments demonstrate that the SOS extraction pipeline generates human-interpretable scripts with symbolic annotations at salient keyframes, while the SOSControl framework outperforms existing baselines in motion quality, controllability, and generalizability with respect to motion timing and body part orientation control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSOS\uff08Salient Orientation Symbolic\uff09\u811a\u672c\u7684\u53ef\u7f16\u7a0b\u7b26\u53f7\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5173\u952e\u5e27\u4e0a\u7cbe\u786e\u6307\u5b9a\u8eab\u4f53\u90e8\u4f4d\u671d\u5411\u548c\u8fd0\u52a8\u65f6\u5e8f\uff0c\u5e76\u914d\u5957\u5f00\u53d1\u4e86\u81ea\u52a8\u63d0\u53d6SOS\u811a\u672c\u7684\u6d41\u7a0b\u548cSOSControl\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u53ef\u63a7\u6027\u3001\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u7cbe\u786e\u63a7\u5236\uff1b\u73b0\u6709\u57fa\u4e8e\u5173\u8282\u5173\u952e\u5e27\u4f4d\u7f6e\u7684\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u4f4d\u7f6e\u5f15\u5bfc\uff0c\u96be\u4ee5\u76f4\u89c2\u5730\u6307\u5b9a\u8eab\u4f53\u90e8\u4f4d\u671d\u5411\u548c\u8fd0\u52a8\u65f6\u5e8f\u3002", "method": "1. \u63d0\u51faSOS\u811a\u672c\uff1a\u4e00\u79cd\u53ef\u7f16\u7a0b\u7b26\u53f7\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5173\u952e\u5e27\u6307\u5b9a\u8eab\u4f53\u90e8\u4f4d\u671d\u5411\u548c\u8fd0\u52a8\u65f6\u5e8f\u30022. \u8bbe\u8ba1\u81ea\u52a8SOS\u63d0\u53d6\u6d41\u7a0b\uff1a\u91c7\u7528\u65f6\u95f4\u7ea6\u675f\u7684\u51dd\u805a\u805a\u7c7b\u8fdb\u884c\u5e27\u663e\u8457\u6027\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u663e\u8457\u6027\u7684\u63a9\u7801\u65b9\u6848\uff08SMS\uff09\u4ece\u52a8\u4f5c\u6570\u636e\u4e2d\u751f\u6210\u7a00\u758f\u3001\u53ef\u89e3\u91ca\u7684SOS\u811a\u672c\u30023. \u6784\u5efaSOSControl\u6846\u67b6\uff1a\u5c06SOS\u811a\u672c\u4e2d\u7684\u671d\u5411\u7b26\u53f7\u89c6\u4e3a\u663e\u8457\u7ea6\u675f\uff0c\u5728\u52a8\u4f5c\u751f\u6210\u4e2d\u4f18\u5148\u6ee1\u8db3\uff1b\u7ed3\u5408SMS\u6570\u636e\u589e\u5f3a\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u8fed\u4ee3\u4f18\u5316\u4ee5\u63d0\u5347\u5bf9\u7528\u6237\u7ea6\u675f\u7684\u5bf9\u9f50\uff1b\u4f7f\u7528\u57fa\u4e8eControlNet\u7684ACTOR-PAE\u89e3\u7801\u5668\u786e\u4fdd\u52a8\u4f5c\u8f93\u51fa\u7684\u5e73\u6ed1\u81ea\u7136\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSOS\u63d0\u53d6\u6d41\u7a0b\u80fd\u751f\u6210\u5728\u663e\u8457\u5173\u952e\u5e27\u4e0a\u5e26\u6709\u7b26\u53f7\u6ce8\u91ca\u7684\u3001\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u811a\u672c\uff1bSOSControl\u6846\u67b6\u5728\u52a8\u4f5c\u8d28\u91cf\u3001\u53ef\u63a7\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u65f6\u5e8f\u548c\u8eab\u4f53\u90e8\u4f4d\u671d\u5411\u63a7\u5236\u4e0a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684SOS\u811a\u672c\u53ca\u5176\u914d\u5957\u7684SOSControl\u6846\u67b6\u4e3a\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u3001\u76f4\u89c2\u4e14\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u671d\u5411\u548c\u65f6\u5e8f\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\u3002", "summary_cn": "\u4f20\u7edf\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u901a\u5e38\u7f3a\u4e4f\u7cbe\u786e\u63a7\u5236\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u5173\u8282\u5173\u952e\u5e27\u4f4d\u7f6e\u7684\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u4f4d\u7f6e\u5f15\u5bfc\uff0c\u96be\u4ee5\u76f4\u89c2\u5730\u6307\u5b9a\u8eab\u4f53\u90e8\u4f4d\u7684\u671d\u5411\u548c\u8fd0\u52a8\u65f6\u5e8f\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u663e\u8457\u671d\u5411\u7b26\u53f7\uff08Salient Orientation Symbolic, SOS\uff09\u811a\u672c\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u7f16\u7a0b\u7684\u7b26\u53f7\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5173\u952e\u5e27\u4e0a\u6307\u5b9a\u8eab\u4f53\u90e8\u4f4d\u7684\u671d\u5411\u548c\u8fd0\u52a8\u65f6\u5e8f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8SOS\u63d0\u53d6\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u91c7\u7528\u65f6\u95f4\u7ea6\u675f\u7684\u51dd\u805a\u805a\u7c7b\u8fdb\u884c\u5e27\u663e\u8457\u6027\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u7684\u63a9\u7801\u65b9\u6848\uff08Saliency-based Masking Scheme, SMS\uff09\u76f4\u63a5\u4ece\u52a8\u4f5c\u6570\u636e\u4e2d\u751f\u6210\u7a00\u758f\u4e14\u53ef\u89e3\u91ca\u7684SOS\u811a\u672c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SOSControl\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u7a00\u758fSOS\u811a\u672c\u4e2d\u53ef\u7528\u7684\u671d\u5411\u7b26\u53f7\u89c6\u4e3a\u663e\u8457\u7ea6\u675f\uff0c\u5e76\u5728\u52a8\u4f5c\u751f\u6210\u8fc7\u7a0b\u4e2d\u4f18\u5148\u6ee1\u8db3\u8fd9\u4e9b\u7ea6\u675f\u3002\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8eSMS\u7684\u6570\u636e\u589e\u5f3a\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u8be5\u6846\u67b6\u589e\u5f3a\u4e86\u4e0e\u7528\u6237\u6307\u5b9a\u7ea6\u675f\u7684\u5bf9\u9f50\u3002\u540c\u65f6\uff0c\u5b83\u91c7\u7528\u57fa\u4e8eControlNet\u7684ACTOR-PAE\u89e3\u7801\u5668\u6765\u786e\u4fdd\u8f93\u51fa\u52a8\u4f5c\u7684\u5e73\u6ed1\u4e0e\u81ea\u7136\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSOS\u63d0\u53d6\u6d41\u7a0b\u80fd\u591f\u751f\u6210\u5728\u663e\u8457\u5173\u952e\u5e27\u4e0a\u5e26\u6709\u7b26\u53f7\u6ce8\u91ca\u7684\u4eba\u7c7b\u53ef\u89e3\u91ca\u811a\u672c\uff0c\u800cSOSControl\u6846\u67b6\u5728\u52a8\u4f5c\u8d28\u91cf\u3001\u53ef\u63a7\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728\u8fd0\u52a8\u65f6\u5e8f\u548c\u8eab\u4f53\u90e8\u4f4d\u671d\u5411\u63a7\u5236\u65b9\u9762\u3002"}}
{"id": "2601.14259", "pdf": "https://arxiv.org/pdf/2601.14259", "abs": "https://arxiv.org/abs/2601.14259", "authors": ["Ziwen Zhong", "Zhitao Shu", "Yue Zhao"], "title": "A Cloud-Based Cross-Modal Transformer for Emotion Recognition and Adaptive Human-Computer Interaction", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e91\u7684\u8de8\u6a21\u6001Transformer\uff08CMT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u4fe1\u53f7\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u5e76\u5728\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u60c5\u611f\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u60c5\u7eea\u8bc6\u522b\u7cfb\u7edf\u591a\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\uff08\u5982\u9762\u90e8\u8868\u60c5\u3001\u8bed\u97f3\u8bed\u8c03\u6216\u6587\u672c\u60c5\u611f\uff09\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u5dee\u3001\u6cdb\u5316\u80fd\u529b\u5f31\uff1b\u540c\u65f6\u7f3a\u4e4f\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u90e8\u7f72\u65b9\u6848\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff08ViT\u3001Wav2Vec2\u3001BERT\uff09\u5206\u522b\u63d0\u53d6\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5efa\u6a21\u5f02\u6784\u7279\u5f81\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5229\u7528Kubernetes\u4e0eTensorFlow Serving\u6784\u5efa\u4e91\u7aef\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0e\u63a8\u7406\u67b6\u6784\u3002", "result": "\u5728IEMOCAP\u3001MELD\u548cAffectNet\u6570\u636e\u96c6\u4e0a\uff0cCMT\u76f8\u6bd4\u5f3a\u591a\u6a21\u6001\u57fa\u7ebf\u63d0\u5347F1\u5206\u65703.0%\uff0c\u964d\u4f4e\u4ea4\u53c9\u71b5\u635f\u593112.9%\uff1b\u4e91\u7aef\u90e8\u7f72\u5e73\u5747\u54cd\u5e94\u5ef6\u8fdf\u4e3a128ms\uff0c\u8f83\u4f20\u7edfTransformer\u878d\u5408\u7cfb\u7edf\u964d\u4f4e35%\u3002", "conclusion": "\u6240\u63d0CMT\u6846\u67b6\u6709\u6548\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\u4e0e\u81ea\u9002\u5e94\u53cd\u9988\uff0c\u4e3a\u667a\u80fd\u5ba2\u670d\u3001\u865a\u62df\u6559\u5b66\u548c\u60c5\u611f\u8ba1\u7b97\u63a5\u53e3\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4e91\u539f\u751f\u60c5\u611f\u8ba1\u7b97\u7684\u53d1\u5c55\u3002", "summary_cn": "\u60c5\u7eea\u8bc6\u522b\u662f\u4e0b\u4e00\u4ee3\u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u4f7f\u673a\u5668\u80fd\u591f\u611f\u77e5\u3001\u7406\u89e3\u5e76\u54cd\u5e94\u7528\u6237\u7684\u60c5\u611f\u72b6\u6001\u3002\u7136\u800c\uff0c\u73b0\u6709\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u5206\u6790\uff08\u5982\u9762\u90e8\u8868\u60c5\u3001\u8bed\u97f3\u8bed\u8c03\u6216\u6587\u672c\u60c5\u611f\uff09\uff0c\u5bfc\u81f4\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u9c81\u68d2\u6027\u6709\u9650\u3001\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e91\u7684\u8de8\u6a21\u6001Transformer\uff08CMT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u7eea\u8bc6\u522b\u4e0e\u81ea\u9002\u5e94\u4eba\u673a\u4ea4\u4e92\u3002\u8be5\u6a21\u578b\u5229\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff08Vision Transformer\u3001Wav2Vec2\u548cBERT\uff09\u6574\u5408\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5f02\u6784\u7279\u5f81\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002\u901a\u8fc7\u5229\u7528\u4e91\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff0c\u7ed3\u5408Kubernetes\u4e0a\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e0eTensorFlow Serving\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9762\u5411\u5927\u89c4\u6a21\u7528\u6237\u4ea4\u4e92\u7684\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u60c5\u7eea\u8bc6\u522b\u3002\u5728IEMOCAP\u3001MELD\u548cAffectNet\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCMT\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff0c\u76f8\u8f83\u5f3a\u5927\u7684\u591a\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\uff0cF1\u5206\u6570\u63d0\u5347\u4e863.0%\uff0c\u4ea4\u53c9\u71b5\u635f\u5931\u964d\u4f4e\u4e8612.9%\u3002\u6b64\u5916\uff0c\u4e91\u7aef\u90e8\u7f72\u8bc4\u4f30\u663e\u793a\u5176\u5e73\u5747\u54cd\u5e94\u5ef6\u8fdf\u4e3a128\u6beb\u79d2\uff0c\u6bd4\u4f20\u7edf\u57fa\u4e8eTransformer\u7684\u878d\u5408\u7cfb\u7edf\u964d\u4f4e\u4e8635%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u5b9e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u5728\u667a\u80fd\u5ba2\u670d\u3001\u865a\u62df\u8f85\u5bfc\u7cfb\u7edf\u548c\u60c5\u611f\u8ba1\u7b97\u754c\u9762\u7b49\u5e94\u7528\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u60c5\u7eea\u8bc6\u522b\u4e0e\u81ea\u9002\u5e94\u53cd\u9988\uff0c\u6807\u5fd7\u7740\u5411\u4e91\u539f\u751f\u60c5\u611f\u8ba1\u7b97\u548c\u60c5\u611f\u667a\u80fd\u4ea4\u4e92\u7cfb\u7edf\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2601.14261", "pdf": "https://arxiv.org/pdf/2601.14261", "abs": "https://arxiv.org/abs/2601.14261", "authors": ["Taoliang Tan", "Chengwei Ma", "Zhen Tian", "Zhao Lin", "Dongdong Li", "Si Shi"], "title": "Intelligent Power Grid Design Review via Active Perception-Enabled Multimodal Large Language Models", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "The intelligent review of power grid engineering design drawings is crucial for power system safety. However, current automated systems struggle with ultra-high-resolution drawings due to high computational demands, information loss, and a lack of holistic semantic understanding for design error identification. This paper proposes a novel three-stage framework for intelligent power grid drawing review, driven by pre-trained Multimodal Large Language Models (MLLMs) through advanced prompt engineering. Mimicking the human expert review process, the first stage leverages an MLLM for global semantic understanding to intelligently propose domain-specific semantic regions from a low-resolution overview. The second stage then performs high-resolution, fine-grained recognition within these proposed regions, acquiring detailed information with associated confidence scores. In the final stage, a comprehensive decision-making module integrates these confidence-aware results to accurately diagnose design errors and provide a reliability assessment. Preliminary results on real-world power grid drawings demonstrate our approach significantly enhances MLLM's ability to grasp macroscopic semantic information and pinpoint design errors, showing improved defect discovery accuracy and greater reliability in review judgments compared to traditional passive MLLM inference. This research offers a novel, prompt-driven paradigm for intelligent and reliable power grid drawing review.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u667a\u80fd\u5ba1\u9605\u7535\u7f51\u5de5\u7a0b\u56fe\u7eb8\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u7eb8\u5e26\u6765\u7684\u8ba1\u7b97\u8d1f\u62c5\u3001\u4fe1\u606f\u4e22\u5931\u53ca\u7f3a\u4e4f\u6574\u4f53\u8bed\u4e49\u7406\u89e3\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u7cfb\u7edf\u5728\u5ba1\u9605\u8d85\u9ad8\u5206\u8fa8\u7387\u7535\u7f51\u5de5\u7a0b\u56fe\u7eb8\u65f6\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u4fe1\u606f\u4e22\u5931\u4ee5\u53ca\u7f3a\u4e4f\u6574\u4f53\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7b49\u6311\u6218\uff0c\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u8bbe\u8ba1\u9519\u8bef\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u7684\u667a\u80fd\u5ba1\u56fe\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528MLLM\u4ece\u4f4e\u5206\u8fa8\u7387\u6982\u89c8\u56fe\u4e2d\u8fdb\u884c\u5168\u5c40\u8bed\u4e49\u7406\u89e3\u5e76\u63d0\u51fa\u5173\u952e\u8bed\u4e49\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u8fd9\u4e9b\u533a\u57df\u4e2d\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u7ec6\u7c92\u5ea6\u8bc6\u522b\uff0c\u5e76\u83b7\u53d6\u5e26\u7f6e\u4fe1\u5ea6\u7684\u8be6\u7ec6\u4fe1\u606f\uff1b\u7b2c\u4e09\u9636\u6bb5\u901a\u8fc7\u51b3\u7b56\u6a21\u5757\u6574\u5408\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u7ed3\u679c\uff0c\u8bca\u65ad\u8bbe\u8ba1\u9519\u8bef\u5e76\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "result": "\u5728\u771f\u5b9e\u7535\u7f51\u56fe\u7eb8\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLLM\u5bf9\u5b8f\u89c2\u8bed\u4e49\u4fe1\u606f\u7684\u7406\u89e3\u80fd\u529b\u4e0e\u8bbe\u8ba1\u9519\u8bef\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5728\u7f3a\u9677\u53d1\u73b0\u51c6\u786e\u7387\u548c\u5ba1\u56fe\u5224\u65ad\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u88ab\u52a8\u5f0fMLLM\u63a8\u7406\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u7531\u63d0\u793a\u9a71\u52a8\u7684\u667a\u80fd\u7535\u7f51\u56fe\u7eb8\u5ba1\u9605\u8303\u5f0f\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "summary_cn": "\u7535\u7f51\u5de5\u7a0b\u8bbe\u8ba1\u56fe\u7eb8\u7684\u667a\u80fd\u5ba1\u9605\u5bf9\u7535\u529b\u7cfb\u7edf\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u7531\u4e8e\u8ba1\u7b97\u9700\u6c42\u9ad8\u3001\u4fe1\u606f\u4e22\u5931\u4ee5\u53ca\u7f3a\u4e4f\u7528\u4e8e\u8bc6\u522b\u8bbe\u8ba1\u9519\u8bef\u7684\u6574\u4f53\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u9ad8\u6e05\u56fe\u7eb8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u63d0\u793a\u5de5\u7a0b\u9a71\u52a8\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8fdb\u884c\u667a\u80fd\u7535\u7f51\u56fe\u7eb8\u5ba1\u9605\u3002\u8be5\u6846\u67b6\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u5ba1\u9605\u6d41\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528MLLM\u4ece\u4f4e\u5206\u8fa8\u7387\u6982\u89c8\u56fe\u4e2d\u8fdb\u884c\u5168\u5c40\u8bed\u4e49\u7406\u89e3\uff0c\u667a\u80fd\u5730\u63d0\u51fa\u7279\u5b9a\u9886\u57df\u7684\u8bed\u4e49\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u8fd9\u4e9b\u533a\u57df\u4e2d\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u3001\u7ec6\u7c92\u5ea6\u7684\u8bc6\u522b\uff0c\u83b7\u53d6\u5e26\u6709\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u7684\u8be6\u7ec6\u4fe1\u606f\uff1b\u7b2c\u4e09\u9636\u6bb5\u901a\u8fc7\u4e00\u4e2a\u7efc\u5408\u51b3\u7b56\u6a21\u5757\u6574\u5408\u8fd9\u4e9b\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u7ed3\u679c\uff0c\u51c6\u786e\u8bca\u65ad\u8bbe\u8ba1\u9519\u8bef\u5e76\u63d0\u4f9b\u53ef\u9760\u6027\u8bc4\u4f30\u3002\u5728\u771f\u5b9e\u7535\u7f51\u56fe\u7eb8\u4e0a\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86MLLM\u628a\u63e1\u5b8f\u89c2\u8bed\u4e49\u4fe1\u606f\u548c\u5b9a\u4f4d\u8bbe\u8ba1\u9519\u8bef\u7684\u80fd\u529b\uff0c\u5728\u7f3a\u9677\u53d1\u73b0\u51c6\u786e\u7387\u548c\u5ba1\u9605\u5224\u65ad\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u88ab\u52a8\u5f0fMLLM\u63a8\u7406\u3002\u672c\u7814\u7a76\u4e3a\u667a\u80fd\u3001\u53ef\u9760\u7684\u7535\u7f51\u56fe\u7eb8\u5ba1\u9605\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u7531\u63d0\u793a\u9a71\u52a8\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2601.14330", "pdf": "https://arxiv.org/pdf/2601.14330", "abs": "https://arxiv.org/abs/2601.14330", "authors": ["Mengyu Sun", "Ziyuan Yang", "Andrew Beng Jin Teoh", "Junxu Liu", "Haibo Hu", "Yi Zhang"], "title": "LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLURE\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u6f5c\u5728\u7a7a\u95f4\u548c\u5f15\u5bfc\u91c7\u6837\u8f68\u8ff9\uff0c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u9ad8\u6548\u5730\u91cd\u65b0\u5524\u9192\u88ab\u64e6\u9664\u7684\u591a\u4e2a\u6982\u5ff5\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5b58\u5728\u6f0f\u6d1e\uff0c\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u4ecd\u53ef\u88ab\u91cd\u65b0\u5524\u9192\uff1b\u800c\u5f53\u524d\u7684\u91cd\u5524\u9192\u65b9\u6cd5\u4ec5\u5173\u6ce8\u63d0\u793a\u8bcd\u5c42\u9762\u7684\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5176\u4ed6\u5173\u952e\u56e0\u7d20\uff0c\u5bfc\u81f4\u5bf9\u64e6\u9664\u673a\u5236\u7684\u7406\u89e3\u4e0d\u5168\u9762\u3002", "method": "\u5c06\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9690\u51fd\u6570\uff0c\u7406\u8bba\u5206\u6790\u6587\u672c\u6761\u4ef6\u3001\u6a21\u578b\u53c2\u6570\u548c\u6f5c\u5728\u72b6\u6001\u7b49\u56e0\u7d20\u5bf9\u6982\u5ff5\u91cd\u5524\u9192\u7684\u5f71\u54cd\uff1b\u63d0\u51faLURE\u65b9\u6cd5\uff0c\u5305\u62ec\u8bed\u4e49\u91cd\u7ed1\u5b9a\u673a\u5236\u3001\u68af\u5ea6\u573a\u6b63\u4ea4\u5316\uff08\u89e3\u51b3\u591a\u6982\u5ff5\u4e0b\u7684\u68af\u5ea6\u51b2\u7a81\uff09\u548c\u6f5c\u5728\u8bed\u4e49\u8bc6\u522b\u5f15\u5bfc\u91c7\u6837\uff08LSIS\uff09\u4ee5\u786e\u4fdd\u91cd\u5524\u9192\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLURE\u80fd\u5728\u591a\u79cd\u64e6\u9664\u4efb\u52a1\u548c\u65b9\u6cd5\u4e0b\uff0c\u540c\u65f6\u9ad8\u4fdd\u771f\u5730\u91cd\u5524\u9192\u591a\u4e2a\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86\u5f53\u524d\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u7684\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u91cd\u5524\u9192\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u5728\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u7684\u91cd\u8981\u6027\uff0c\u5bf9\u63d0\u5347\u64e6\u9664\u9c81\u68d2\u6027\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002", "summary_cn": "\u6982\u5ff5\u64e6\u9664\u65e8\u5728\u6291\u5236\u6269\u6563\u6a21\u578b\u4e2d\u7684\u654f\u611f\u5185\u5bb9\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u4ecd\u53ef\u80fd\u88ab\u91cd\u65b0\u5524\u9192\uff0c\u66b4\u9732\u51fa\u64e6\u9664\u65b9\u6cd5\u7684\u8106\u5f31\u6027\u3002\u73b0\u6709\u7684\u91cd\u5524\u9192\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u63d0\u793a\u8bcd\u5c42\u9762\u7684\u4f18\u5316\u6765\u64cd\u7eb5\u91c7\u6837\u8f68\u8ff9\uff0c\u5ffd\u89c6\u4e86\u5176\u4ed6\u751f\u6210\u56e0\u7d20\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5bf9\u5e95\u5c42\u52a8\u6001\u673a\u5236\u7684\u5168\u9762\u7406\u89e3\u3002\u672c\u6587\u5c06\u751f\u6210\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u4e00\u4e2a\u9690\u51fd\u6570\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5305\u62ec\u6587\u672c\u6761\u4ef6\u3001\u6a21\u578b\u53c2\u6570\u548c\u6f5c\u5728\u72b6\u6001\u5728\u5185\u7684\u591a\u79cd\u56e0\u7d20\u7684\u7efc\u5408\u7406\u8bba\u5206\u6790\u3002\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\uff0c\u6270\u52a8\u4efb\u4e00\u56e0\u7d20\u5747\u53ef\u91cd\u65b0\u5524\u9192\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u3002\u57fa\u4e8e\u6b64\u6d1e\u5bdf\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6982\u5ff5\u91cd\u5524\u9192\u65b9\u6cd5\uff1a\u7528\u4e8e\u6982\u5ff5\u91cd\u5524\u9192\u7684\u6f5c\u5728\u7a7a\u95f4\u89e3\u5c01\uff08LURE\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u6784\u6f5c\u5728\u7a7a\u95f4\u5e76\u5f15\u5bfc\u91c7\u6837\u8f68\u8ff9\u6765\u91cd\u5524\u9192\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u8bed\u4e49\u91cd\u7ed1\u5b9a\u673a\u5236\u901a\u8fc7\u5c06\u53bb\u566a\u9884\u6d4b\u4e0e\u76ee\u6807\u5206\u5e03\u5bf9\u9f50\u6765\u91cd\u6784\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u800c\u91cd\u5efa\u88ab\u5207\u65ad\u7684\u6587\u672c-\u89c6\u89c9\u5173\u8054\u3002\u7136\u800c\uff0c\u5728\u591a\u6982\u5ff5\u573a\u666f\u4e2d\uff0c\u7b80\u5355\u7684\u91cd\u6784\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u51b2\u7a81\u548c\u7279\u5f81\u7ea0\u7f20\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u68af\u5ea6\u573a\u6b63\u4ea4\u5316\uff0c\u901a\u8fc7\u5f3a\u5236\u7279\u5f81\u6b63\u4ea4\u6027\u6765\u9632\u6b62\u76f8\u4e92\u5e72\u6270\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6f5c\u5728\u8bed\u4e49\u8bc6\u522b\u5f15\u5bfc\u91c7\u6837\uff08LSIS\uff09\u901a\u8fc7\u540e\u9a8c\u5bc6\u5ea6\u9a8c\u8bc1\u786e\u4fdd\u91cd\u5524\u9192\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLURE\u80fd\u591f\u5728\u591a\u79cd\u64e6\u9664\u4efb\u52a1\u548c\u65b9\u6cd5\u4e0b\uff0c\u540c\u65f6\u9ad8\u4fdd\u771f\u5730\u91cd\u5524\u9192\u591a\u4e2a\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u3002"}}
{"id": "2601.14339", "pdf": "https://arxiv.org/pdf/2601.14339", "abs": "https://arxiv.org/abs/2601.14339", "authors": ["Haotian Xu", "Yue Hu", "Zhengqiu Zhu", "Chen Gao", "Ziyou Wang", "Junreng Rao", "Wenhao Lu", "Weishi Li", "Quanjun Yin", "Yong Li"], "title": "CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CityCube\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u8de8\u89c6\u89d2\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dVLMs\u4e0e\u4eba\u7c7b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5ba4\u5185\u6216\u8857\u9053\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u5f00\u653e\u57ce\u5e02\u7a7a\u95f4\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u3001\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u548c\u89c6\u89d2\u53d8\u5316\u6240\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u57ce\u5e02\u73af\u5883\u7684\u8de8\u89c6\u89d2\u63a8\u7406\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86CityCube\u57fa\u51c6\uff0c\u6574\u5408\u4e86\u56db\u79cd\u89c6\u89d2\u52a8\u6001\u4ee5\u6a21\u62df\u76f8\u673a\u8fd0\u52a8\uff0c\u5e76\u6db5\u76d6\u6765\u81ea\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u536b\u661f\u7b49\u591a\u79cd\u5e73\u53f0\u7684\u5e7f\u6cdb\u89c6\u89d2\u3002\u8be5\u57fa\u51c6\u5305\u542b5,022\u4e2a\u7cbe\u5fc3\u6807\u6ce8\u7684\u591a\u89c6\u89d2\u95ee\u7b54\u5bf9\uff0c\u5206\u4e3a\u4e94\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\u548c\u4e09\u79cd\u7a7a\u95f4\u5173\u7cfb\u8868\u8fbe\u5f62\u5f0f\uff0c\u5e76\u5bf933\u4e2aVLMs\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u5927\u89c4\u6a21VLMs\u7684\u51c6\u786e\u7387\u4e5f\u672a\u8d85\u8fc754.1%\uff0c\u6bd4\u4eba\u7c7b\u8868\u73b0\u4f4e34.2%\uff1b\u800c\u5c0f\u89c4\u6a21\u5fae\u8c03VLMs\u5219\u80fd\u8fbe\u523060.0%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u63ed\u793a\u4e86\u4efb\u52a1\u95f4\u7684\u76f8\u5173\u6027\u4ee5\u53caVLMs\u4e0e\u4eba\u7c7b\u63a8\u7406\u4e4b\u95f4\u7684\u6839\u672c\u8ba4\u77e5\u5dee\u5f02\u3002", "conclusion": "CityCube\u6709\u6548\u63ed\u793a\u4e86\u5f53\u524dVLMs\u5728\u57ce\u5e02\u8de8\u89c6\u89d2\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u8d34\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "summary_cn": "\u8de8\u89c6\u89d2\u7a7a\u95f4\u63a8\u7406\u5bf9\u4e8e\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u662f\u590d\u6742\u73af\u5883\u4e2d\u7a7a\u95f4\u7406\u89e3\u3001\u5fc3\u7406\u6a21\u62df\u548c\u89c4\u5212\u7684\u57fa\u7840\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5ba4\u5185\u6216\u8857\u9053\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u5f00\u653e\u5f0f\u57ce\u5e02\u7a7a\u95f4\u6240\u7279\u6709\u7684\u4e30\u5bcc\u8bed\u4e49\u3001\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u548c\u89c6\u89d2\u53d8\u5316\u5e26\u6765\u7684\u72ec\u7279\u6311\u6218\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CityCube\u2014\u2014\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u63a2\u7a76\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u8de8\u89c6\u89d2\u63a8\u7406\u80fd\u529b\u3002CityCube\u6574\u5408\u4e86\u56db\u79cd\u89c6\u89d2\u52a8\u6001\u4ee5\u6a21\u62df\u76f8\u673a\u8fd0\u52a8\uff0c\u5e76\u6db5\u76d6\u6765\u81ea\u591a\u79cd\u5e73\u53f0\uff08\u5982\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u536b\u661f\uff09\u7684\u5e7f\u6cdb\u89c6\u89d2\u3002\u4e3a\u5b9e\u73b0\u5168\u9762\u8bc4\u4f30\uff0c\u8be5\u57fa\u51c6\u5305\u542b5,022\u4e2a\u7cbe\u5fc3\u6807\u6ce8\u7684\u591a\u89c6\u89d2\u95ee\u7b54\u5bf9\uff0c\u6309\u4e94\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\u548c\u4e09\u79cd\u7a7a\u95f4\u5173\u7cfb\u8868\u8fbe\u65b9\u5f0f\u8fdb\u884c\u5206\u7c7b\u3002\u5bf933\u4e2aVLMs\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u5176\u6027\u80fd\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u5373\u4f7f\u662f\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4e5f\u672a\u80fd\u8d85\u8fc754.1%\uff0c\u6bd4\u4eba\u7c7b\u8868\u73b0\u4f4e34.2%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5c0f\u89c4\u6a21\u5fae\u8c03\u7684VLMs\u51c6\u786e\u7387\u53ef\u8d85\u8fc760.0%\uff0c\u51f8\u663e\u4e86\u672c\u57fa\u51c6\u7684\u5fc5\u8981\u6027\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u63ed\u793a\u4e86\u4efb\u52a1\u95f4\u7684\u76f8\u5173\u6027\u4ee5\u53caVLMs\u4e0e\u7c7b\u4eba\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u7684\u6839\u672c\u8ba4\u77e5\u5dee\u5f02\u3002"}}
{"id": "2601.14406", "pdf": "https://arxiv.org/pdf/2601.14406", "abs": "https://arxiv.org/abs/2601.14406", "authors": ["Yixiong Chen", "Zongwei Zhou", "Wenxuan Li", "Alan Yuille"], "title": "Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data", "categories": ["cs.CV", "eess.IV"], "comment": "ISBI 2026 accepted", "summary": "Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.", "AI": {"tldr": "\u63d0\u51faSegAE\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6807\u7b7e\u8d28\u91cf\uff0c\u63d0\u5347\u6570\u636e\u6548\u7387\u4e0e\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u5e38\u6df7\u5408\u4eba\u5de5\u6807\u6ce8\u548c\u4f2a\u6807\u7b7e\uff0c\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u5f00\u53d1SegAE\uff08Segmentation Assessment Engine\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u8fc7400\u4e07\u5e26\u8d28\u91cf\u8bc4\u5206\u56fe\u50cf-\u6807\u7b7e\u5bf9\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u81ea\u52a8\u9884\u6d4b142\u79cd\u89e3\u5256\u7ed3\u6784\u7684\u6807\u7b7e\u8d28\u91cf\u3002", "result": "SegAE\u4e0e\u771f\u5b9eDice\u76f8\u4f3c\u5ea6\u7684\u76f8\u5173\u7cfb\u6570\u8fbe0.902\uff0c\u5355\u4e2a3D\u63a9\u7801\u8bc4\u4f30\u4ec5\u97000.06\u79d2\uff1b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e2d\u53d1\u73b0\u666e\u904d\u5b58\u5728\u4f4e\u8d28\u91cf\u6807\u6ce8\uff1b\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u51cf\u5c11\u4e09\u5206\u4e4b\u4e00\u6807\u6ce8\u6210\u672c\u548c70%\u7684\u8d28\u91cf\u68c0\u67e5\u65f6\u95f4\u3002", "conclusion": "SegAE\u4e3a\u5927\u89c4\u6a21\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7b80\u4fbf\u7684\u6807\u7b7e\u8d28\u91cf\u63a7\u5236\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u5229\u7528\u6548\u7387\u4e0e\u6a21\u578b\u8bad\u7ec3\u9c81\u68d2\u6027\u3002", "summary_cn": "\u5927\u89c4\u6a21\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u901a\u5e38\u7ed3\u5408\u4e86\u4eba\u5de5\u6807\u6ce8\u548c\u8d28\u91cf\u4e0d\u4e00\u7684\u4f2a\u6807\u7b7e\uff0c\u8fd9\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u6548\u679c\u3002\u4f4e\u8d28\u91cf\u6807\u7b7e\u4f1a\u963b\u788d\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SegAE\uff08\u5206\u5272\u8bc4\u4f30\u5f15\u64ce\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u53ef\u81ea\u52a8\u9884\u6d4b142\u79cd\u89e3\u5256\u7ed3\u6784\u7684\u6807\u7b7e\u8d28\u91cf\u3002\u8be5\u6a21\u578b\u5728\u8d85\u8fc7\u56db\u767e\u4e07\u4e2a\u5e26\u6709\u8d28\u91cf\u8bc4\u5206\u7684\u56fe\u50cf-\u6807\u7b7e\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e0e\u771f\u5b9eDice\u76f8\u4f3c\u5ea6\u7684\u76f8\u5173\u7cfb\u6570\u9ad8\u8fbe0.902\uff0c\u5e76\u53ef\u57280.06\u79d2\u5185\u8bc4\u4f30\u4e00\u4e2a3D\u63a9\u7801\u3002SegAE\u5c55\u73b0\u51fa\u591a\u9879\u5b9e\u7528\u4f18\u52bf\uff1a\uff08I\uff09\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e2d\u666e\u904d\u5b58\u5728\u4f4e\u8d28\u91cf\u6807\u6ce8\uff1b\uff08II\uff09SegAE\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u548c\u8bad\u7ec3\u6027\u80fd\uff0c\u5c06\u6570\u636e\u96c6\u6807\u6ce8\u6210\u672c\u964d\u4f4e\u4e09\u5206\u4e4b\u4e00\uff0c\u6bcf\u4e2a\u6807\u7b7e\u7684\u8d28\u91cf\u68c0\u67e5\u65f6\u95f4\u51cf\u5c1170%\u3002\u8be5\u5de5\u5177\u4e3a\u5927\u89c4\u6a21\u533b\u5b66\u5206\u5272\u6570\u636e\u96c6\u7684\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u76f8\u5173\u6570\u636e\u96c6\u3001\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u5df2\u53d1\u5e03\u4e8e https://github.com/Schuture/SegAE\u3002"}}
{"id": "2601.14438", "pdf": "https://arxiv.org/pdf/2601.14438", "abs": "https://arxiv.org/abs/2601.14438", "authors": ["Danial Sadrian Zadeh", "Otman A. Basir", "Behzad Moshiri"], "title": "Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Under review at Computer Vision and Image Understanding (submitted July 25, 2025)", "summary": "Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06\u5355\u5f20\u524d\u89c6\u6444\u50cf\u5934\u56fe\u50cf\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u4ee5\u6355\u6349\u4ea4\u901a\u573a\u666f\u7684\u7a7a\u95f4\u5e03\u5c40\u3001\u8bed\u4e49\u5173\u7cfb\u548c\u9a7e\u9a76\u76f8\u5173\u7ebf\u7d22\uff0c\u5e76\u6784\u5efa\u4e86\u57fa\u4e8eBDD100K\u7684\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u51c6\u786e\u611f\u77e5\u548c\u7406\u89e3\u4ea4\u901a\u573a\u666f\u4ee5\u786e\u4fdd\u5b89\u5168\u884c\u9a76\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5355\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u4e30\u5bcc\u8bed\u4e49\u63cf\u8ff0\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\uff0c\u589e\u5f3a\u7a7a\u95f4\u4e0e\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b\u540c\u65f6\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eBDD100K\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u8ba8\u8bba\u4e86\u9002\u7528\u4e8e\u8be5\u4efb\u52a1\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7CIDEr\u3001SPICE\u7b49\u81ea\u52a8\u6307\u6807\u53ca\u4eba\u5de5\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6a21\u578b\u5728\u751f\u6210\u51c6\u786e\u3001\u8be6\u7ec6\u4ea4\u901a\u573a\u666f\u63cf\u8ff0\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ea4\u901a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u4ece\u5355\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u80fd\u529b\uff0c\u8fd8\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u8d44\u6e90\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "summary_cn": "\u4ea4\u901a\u573a\u666f\u7406\u89e3\u5bf9\u4e8e\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u591f\u51c6\u786e\u611f\u77e5\u548c\u89e3\u91ca\u5176\u73af\u5883\uff0c\u4ece\u800c\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u53ef\u5c06\u5355\u5f20\u524d\u89c6\u6444\u50cf\u5934\u56fe\u50cf\u8f6c\u5316\u4e3a\u7b80\u6d01\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u6709\u6548\u6355\u6349\u7a7a\u95f4\u5e03\u5c40\u3001\u8bed\u4e49\u5173\u7cfb\u4ee5\u53ca\u4e0e\u9a7e\u9a76\u76f8\u5173\u7684\u7ebf\u7d22\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5229\u7528\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u7a7a\u95f4\u548c\u8bed\u4e49\u7279\u5f81\u7684\u63d0\u53d6\uff0c\u5e76\u878d\u5408\u8fd9\u4e9b\u7279\u5f81\u4ee5\u751f\u6210\u4e0a\u4e0b\u6587\u4e30\u5bcc\u4e14\u8be6\u7ec6\u7684\u573a\u666f\u63cf\u8ff0\u3002\u4e3a\u89e3\u51b3\u8be5\u9886\u57df\u4e13\u7528\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u672c\u6587\u57fa\u4e8eBDD100K\u6570\u636e\u96c6\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u8be6\u5c3d\u7684\u6784\u5efa\u6307\u5357\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6df1\u5165\u63a2\u8ba8\u4e86\u76f8\u5173\u8bc4\u4f30\u6307\u6807\uff0c\u786e\u5b9a\u4e86\u6700\u9002\u5408\u8be5\u4efb\u52a1\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002\u901a\u8fc7CIDEr\u548cSPICE\u7b49\u5b9a\u91cf\u6307\u6807\u4ee5\u53ca\u4eba\u5de5\u5224\u65ad\u8bc4\u4f30\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u65b0\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u9884\u671f\u76ee\u6807\u3002"}}
{"id": "2601.14448", "pdf": "https://arxiv.org/pdf/2601.14448", "abs": "https://arxiv.org/abs/2601.14448", "authors": ["A. Enes Doruk"], "title": "Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": "Master Thesis", "summary": "The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u7684\u81ea\u9002\u5e94\u76f8\u673a-LiDAR\u591a\u6a21\u60013D\u5360\u636e\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u6709\u6548\u878d\u5408\u8bed\u4e49\u4e0e\u51e0\u4f55\u4fe1\u606f\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7a00\u758f\u76ee\u6807\u68c0\u6d4b\u96be\u4ee5\u5e94\u5bf9\u957f\u5c3e\u5b89\u5168\u6311\u6218\uff0c\u800c\u73b0\u6709\u5bc6\u96c63D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b9\u6cd5\u5728\u4f53\u7d20\u5316\u8fc7\u7a0b\u4e2d\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u878d\u5408\u8fc7\u7a0b\u8106\u5f31\u4e14\u9759\u6001\u3001\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u7684\u81ea\u9002\u5e94\u591a\u6a21\u60013D\u5360\u636e\u9884\u6d4b\u6a21\u578b\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) LiDAR\u6df1\u5ea6\u7279\u5f81\u805a\u5408\uff08LDFA\uff09\uff0c\u91c7\u7528\u6df1\u5ea6\u53ef\u53d8\u5f62\u91c7\u6837\u5904\u7406\u51e0\u4f55\u7a00\u758f\u6027\uff1b(2) \u57fa\u4e8e\u71b5\u7684\u7279\u5f81\u5e73\u6ed1\uff0c\u5229\u7528\u4ea4\u53c9\u71b5\u6291\u5236\u57df\u7279\u5f02\u6027\u566a\u58f0\uff1b(3) \u81ea\u9002\u5e94\u76f8\u673a-LiDAR\u878d\u5408\uff0c\u6839\u636e\u6a21\u578b\u8f93\u51fa\u52a8\u6001\u91cd\u6821\u51c6\u4f20\u611f\u5668\u6570\u636e\uff1b(4) Gauss-Mamba Head\uff0c\u91c7\u7528\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u89e3\u7801\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u9ad8\u6548\u878d\u5408\u76f8\u673a\u7684\u8bed\u4e49\u4f18\u52bf\u4e0eLiDAR\u7684\u51e0\u4f55\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u76843D\u5360\u636e\u9884\u6d4b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u957f\u5c3e\u5b89\u5168\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u76843D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u65b0\u8303\u5f0f\uff0c\u5176\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u8bbe\u8ba1\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\u3002", "summary_cn": "\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u957f\u5c3e\u5b89\u5168\u6311\u6218\uff0c\u4ece\u7a00\u758f\u76ee\u6807\u68c0\u6d4b\u8303\u5f0f\u8f6c\u5411\u5bc6\u96c63D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\u662f\u5fc5\u8981\u7684\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u4f53\u7d20\u5316\u65b9\u6cd5\u901a\u5e38\u9762\u4e34\u8fc7\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5176\u878d\u5408\u8fc7\u7a0b\u8106\u5f31\u3001\u9759\u6001\uff0c\u4e14\u5728\u52a8\u6001\u73af\u5883\u8bbe\u7f6e\u4e0b\u5bb9\u6613\u5931\u6548\u3002\u4e3a\u6b64\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u9ad8\u65af\u7684\u81ea\u9002\u5e94\u76f8\u673a-LiDAR\u591a\u6a21\u60013D\u5360\u636e\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5185\u5b58\u9ad8\u6548\u76843D\u9ad8\u65af\u6a21\u578b\u65e0\u7f1d\u7ed3\u5408\u76f8\u673a\u6a21\u6001\u7684\u8bed\u4e49\u4f18\u52bf\u4e0eLiDAR\u6a21\u6001\u7684\u51e0\u4f55\u4f18\u52bf\u3002\u6240\u63d0\u65b9\u6848\u5305\u542b\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) LiDAR\u6df1\u5ea6\u7279\u5f81\u805a\u5408\uff08LDFA\uff09\uff0c\u91c7\u7528\u6df1\u5ea6\u65b9\u5411\u7684\u53ef\u53d8\u5f62\u91c7\u6837\u4ee5\u5e94\u5bf9\u51e0\u4f55\u7a00\u758f\u6027\uff1b(2) \u57fa\u4e8e\u71b5\u7684\u7279\u5f81\u5e73\u6ed1\uff0c\u5229\u7528\u4ea4\u53c9\u71b5\u5904\u7406\u7279\u5b9a\u57df\u7684\u566a\u58f0\uff1b(3) \u81ea\u9002\u5e94\u76f8\u673a-LiDAR\u878d\u5408\uff0c\u6839\u636e\u6a21\u578b\u8f93\u51fa\u5bf9\u4f20\u611f\u5668\u8f93\u51fa\u8fdb\u884c\u52a8\u6001\u91cd\u6821\u51c6\uff1b(4) Gauss-Mamba Head\uff0c\u91c7\u7528\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8fdb\u884c\u5168\u5c40\u4e0a\u4e0b\u6587\u89e3\u7801\uff0c\u5e76\u4eab\u6709\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2601.14475", "pdf": "https://arxiv.org/pdf/2601.14475", "abs": "https://arxiv.org/abs/2601.14475", "authors": ["Yajvan Ravan", "Aref Malek", "Chester Dolph", "Nikhil Behari"], "title": "Real-Time Wildfire Localization on the NASA Autonomous Modular Sensor using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 9 figures, published at AIAA SciTech 2026", "summary": "High-altitude, multi-spectral, aerial imagery is scarce and expensive to acquire, yet it is necessary for algorithmic advances and application of machine learning models to high-impact problems such as wildfire detection. We introduce a human-annotated dataset from the NASA Autonomous Modular Sensor (AMS) using 12-channel, medium to high altitude (3 - 50 km) aerial wildfire images similar to those used in current US wildfire missions. Our dataset combines spectral data from 12 different channels, including infrared (IR), short-wave IR (SWIR), and thermal. We take imagery from 20 wildfire missions and randomly sample small patches to generate over 4000 images with high variability, including occlusions by smoke/clouds, easily-confused false positives, and nighttime imagery.\n  We demonstrate results from a deep-learning model to automate the human-intensive process of fire perimeter determination. We train two deep neural networks, one for image classification and the other for pixel-level segmentation. The networks are combined into a unique real-time segmentation model to efficiently localize active wildfire on an incoming image feed. Our model achieves 96% classification accuracy, 74% Intersection-over-Union(IoU), and 84% recall surpassing past methods, including models trained on satellite data and classical color-rule algorithms. By leveraging a multi-spectral dataset, our model is able to detect active wildfire at nighttime and behind clouds, while distinguishing between false positives. We find that data from the SWIR, IR, and thermal bands is the most important to distinguish fire perimeters. Our code and dataset can be found here: https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main and https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNASA\u591a\u5149\u8c31\u9ad8\u7a7a\u6c14\u7403\u56fe\u50cf\u7684\u65b0\u578b\u91ce\u706b\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u5206\u7c7b\u4e0e\u5206\u5272\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u591c\u95f4\u548c\u6709\u4e91\u906e\u6321\u6761\u4ef6\u4e0b\u4ecd\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u8bc6\u522b\u6d3b\u8dc3\u91ce\u706b\u533a\u57df\u3002", "motivation": "\u9ad8\u6d77\u62d4\u591a\u5149\u8c31\u822a\u7a7a\u56fe\u50cf\u7a00\u7f3a\u4e14\u6602\u8d35\uff0c\u4f46\u5bf9\u91ce\u706b\u68c0\u6d4b\u7b49\u9ad8\u5f71\u54cd\u529b\u4efb\u52a1\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u536b\u661f\u6570\u636e\u6216\u57fa\u4e8e\u989c\u8272\u89c4\u5219\u7684\u4f20\u7edf\u7b97\u6cd5\uff09\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u6709\u9650\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b12\u901a\u9053\uff08\u542b\u7ea2\u5916\u3001\u77ed\u6ce2\u7ea2\u5916\u548c\u70ed\u7ea2\u5916\uff09\u7684\u9ad8\u7a7a\u6c14\u7403\u91ce\u706b\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u6db5\u76d620\u6b21\u4efb\u52a1\u30014000\u4f59\u5f20\u5e26\u6807\u6ce8\u7684\u5c0f\u56fe\u5757\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u4e24\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff1a\u4e00\u4e2a\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u5e76\u5c06\u4e8c\u8005\u878d\u5408\u4e3a\u5b9e\u65f6\u5206\u5272\u6a21\u578b\u4ee5\u5b9a\u4f4d\u6d3b\u8dc3\u91ce\u706b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5206\u7c7b\u51c6\u786e\u7387\u8fbe96%\uff0cIoU\u4e3a74%\uff0c\u53ec\u56de\u7387\u4e3a84%\uff0c\u4f18\u4e8e\u4ee5\u5f80\u57fa\u4e8e\u536b\u661f\u6570\u636e\u6216\u4f20\u7edf\u989c\u8272\u89c4\u5219\u7684\u65b9\u6cd5\u3002\u6a21\u578b\u80fd\u6709\u6548\u5728\u591c\u95f4\u53ca\u4e91\u5c42\u906e\u6321\u4e0b\u68c0\u6d4b\u91ce\u706b\uff0c\u5e76\u533a\u5206\u6613\u6df7\u6dc6\u7684\u5047\u9633\u6027\u6837\u672c\u3002\u7814\u7a76\u8fd8\u53d1\u73b0SWIR\u3001IR\u548c\u70ed\u7ea2\u5916\u6ce2\u6bb5\u5bf9\u706b\u573a\u8fb9\u754c\u8bc6\u522b\u6700\u4e3a\u5173\u952e\u3002", "conclusion": "\u5229\u7528\u591a\u5149\u8c31\u9ad8\u7a7a\u56fe\u50cf\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u663e\u8457\u63d0\u5347\u91ce\u706b\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u5e94\u7528\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002", "summary_cn": "\u9ad8\u6d77\u62d4\u591a\u5149\u8c31\u822a\u7a7a\u56fe\u50cf\u83b7\u53d6\u56f0\u96be\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u5bf9\u4e8e\u63a8\u52a8\u7b97\u6cd5\u8fdb\u6b65\u5e76\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5e94\u7528\u4e8e\u91ce\u706b\u68c0\u6d4b\u7b49\u9ad8\u5f71\u54cd\u529b\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7531NASA\u81ea\u4e3b\u6a21\u5757\u5316\u4f20\u611f\u5668\uff08AMS\uff09\u91c7\u96c6\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5305\u542b12\u901a\u9053\u3001\u4e2d\u9ad8\u6d77\u62d4\uff083\u201350\u516c\u91cc\uff09\u7684\u822a\u7a7a\u91ce\u706b\u56fe\u50cf\uff0c\u4e0e\u5f53\u524d\u7f8e\u56fd\u91ce\u706b\u4efb\u52a1\u6240\u7528\u56fe\u50cf\u7c7b\u4f3c\u3002\u8be5\u6570\u636e\u96c6\u878d\u5408\u4e86\u6765\u81ea12\u4e2a\u4e0d\u540c\u901a\u9053\u7684\u5149\u8c31\u6570\u636e\uff0c\u5305\u62ec\u7ea2\u5916\uff08IR\uff09\u3001\u77ed\u6ce2\u7ea2\u5916\uff08SWIR\uff09\u548c\u70ed\u7ea2\u5916\u6ce2\u6bb5\u3002\u6211\u4eec\u4ece20\u6b21\u91ce\u706b\u4efb\u52a1\u4e2d\u63d0\u53d6\u56fe\u50cf\uff0c\u5e76\u968f\u673a\u91c7\u6837\u5c0f\u56fe\u5757\uff0c\u751f\u6210\u4e86\u8d85\u8fc74000\u5f20\u5177\u6709\u9ad8\u5ea6\u591a\u6837\u6027\u7684\u56fe\u50cf\uff0c\u6db5\u76d6\u70df\u96fe/\u4e91\u5c42\u906e\u6321\u3001\u6613\u6df7\u6dc6\u7684\u5047\u9633\u6027\u6837\u672c\u4ee5\u53ca\u591c\u95f4\u56fe\u50cf\u3002\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u539f\u672c\u4f9d\u8d56\u4eba\u5de5\u7684\u706b\u573a\u8fb9\u754c\u5224\u5b9a\u8fc7\u7a0b\u3002\u6211\u4eec\u8bad\u7ec3\u4e86\u4e24\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u4e00\u4e2a\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u5e76\u5c06\u4e8c\u8005\u7ed3\u5408\u6210\u4e00\u79cd\u72ec\u7279\u7684\u5b9e\u65f6\u5206\u5272\u6a21\u578b\uff0c\u4ee5\u9ad8\u6548\u5730\u5728\u8f93\u5165\u56fe\u50cf\u6d41\u4e2d\u5b9a\u4f4d\u6d3b\u8dc3\u91ce\u706b\u533a\u57df\u3002\u6211\u4eec\u7684\u6a21\u578b\u5b9e\u73b0\u4e8696%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u300174%\u7684\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u548c84%\u7684\u53ec\u56de\u7387\uff0c\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u536b\u661f\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u4f20\u7edf\u7684\u989c\u8272\u89c4\u5219\u7b97\u6cd5\u3002\u901a\u8fc7\u5229\u7528\u591a\u5149\u8c31\u6570\u636e\u96c6\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u5728\u591c\u95f4\u548c\u4e91\u5c42\u906e\u6321\u4e0b\u68c0\u6d4b\u6d3b\u8dc3\u91ce\u706b\uff0c\u540c\u65f6\u6709\u6548\u533a\u5206\u5047\u9633\u6027\u6837\u672c\u3002\u6211\u4eec\u53d1\u73b0\uff0cSWIR\u3001IR\u548c\u70ed\u7ea2\u5916\u6ce2\u6bb5\u7684\u6570\u636e\u5bf9\u4e8e\u533a\u5206\u706b\u573a\u8fb9\u754c\u6700\u4e3a\u5173\u952e\u3002\u6211\u4eec\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u53ef\u901a\u8fc7\u4ee5\u4e0b\u94fe\u63a5\u83b7\u53d6\uff1ahttps://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main \u548c https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link\u3002"}}
{"id": "2601.14477", "pdf": "https://arxiv.org/pdf/2601.14477", "abs": "https://arxiv.org/abs/2601.14477", "authors": ["Frank Bieder", "Hendrik K\u00f6nigshof", "Haohao Hu", "Fabian Immel", "Yinzhe Shen", "Jan-Hendrik Pauls", "Christoph Stiller"], "title": "XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Until open-world foundation models match the performance of specialized approaches, the effectiveness of deep learning models remains heavily dependent on dataset availability. Training data must align not only with the target object categories but also with the sensor characteristics and modalities. To bridge the gap between available datasets and deployment domains, domain adaptation strategies are widely used. In this work, we propose a novel approach to transferring sensor-specific knowledge from an image dataset to LiDAR, an entirely different sensing domain. Our method XD-MAP leverages detections from a neural network on camera images to create a semantic parametric map. The map elements are modeled to produce pseudo labels in the target domain without any manual annotation effort. Unlike previous domain transfer approaches, our method does not require direct overlap between sensors and enables extending the angular perception range from a front-view camera to a full 360 view. On our large-scale road feature dataset, XD-MAP outperforms single shot baseline approaches by +19.5 mIoU for 2D semantic segmentation, +19.5 PQth for 2D panoptic segmentation, and +32.3 mIoU in 3D semantic segmentation. The results demonstrate the effectiveness of our approach achieving strong performance on LiDAR data without any manual labeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aXD-MAP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u7ed3\u679c\u6784\u5efa\u8bed\u4e49\u53c2\u6570\u5316\u5730\u56fe\uff0c\u4ece\u800c\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5c06\u76f8\u673a\u56fe\u50cf\u4e2d\u7684\u8bed\u4e49\u77e5\u8bc6\u8fc1\u79fb\u5230LiDAR\u70b9\u4e91\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e862D/3D\u8bed\u4e49\u548c\u5168\u666f\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0e\u76ee\u6807\u57df\uff08\u5305\u62ec\u5bf9\u8c61\u7c7b\u522b\u3001\u4f20\u611f\u5668\u7279\u6027\u53ca\u6a21\u6001\uff09\u7684\u4e00\u81f4\u6027\u3002\u73b0\u6709\u6570\u636e\u96c6\u5f80\u5f80\u96be\u4ee5\u8986\u76d6\u5b9e\u9645\u90e8\u7f72\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u8de8\u57df\u8fc1\u79fb\u65b9\u6cd5\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faXD-MAP\u65b9\u6cd5\uff1a\u5229\u7528\u76f8\u673a\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u68c0\u6d4b\u7ed3\u679c\u751f\u6210\u8bed\u4e49\u53c2\u6570\u5316\u5730\u56fe\uff1b\u8be5\u5730\u56fe\u53ef\u81ea\u52a8\u4e3aLiDAR\u76ee\u6807\u57df\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff1b\u4e14\u4e0d\u8981\u6c42\u4f20\u611f\u5668\u4e4b\u95f4\u5b58\u5728\u76f4\u63a5\u91cd\u53e0\uff0c\u8fd8\u80fd\u5c06\u524d\u89c6\u76f8\u673a\u7684\u611f\u77e5\u8303\u56f4\u6269\u5c55\u81f3360\u5ea6\u5168\u666f\u3002", "result": "\u5728\u5927\u89c4\u6a21\u9053\u8def\u7279\u5f81\u6570\u636e\u96c6\u4e0a\uff0cXD-MAP\u76f8\u6bd4\u5355\u6b21\u68c0\u6d4b\u57fa\u7ebf\u65b9\u6cd5\uff0c\u57282D\u8bed\u4e49\u5206\u5272\u4e0a\u63d0\u5347+19.5 mIoU\uff0c2D\u5168\u666f\u5206\u5272\u63d0\u5347+19.5 PQth\uff0c3D\u8bed\u4e49\u5206\u5272\u63d0\u5347+32.3 mIoU\u3002", "conclusion": "XD-MAP\u80fd\u6709\u6548\u5b9e\u73b0\u4ece\u56fe\u50cf\u5230LiDAR\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5728\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u7684\u524d\u63d0\u4e0b\uff0c\u5728LiDAR\u6570\u636e\u4e0a\u53d6\u5f97\u4f18\u5f02\u7684\u5206\u5272\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5f00\u653e\u4e16\u754c\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "summary_cn": "\u5728\u5f00\u653e\u4e16\u754c\u57fa\u7840\u6a21\u578b\u8fbe\u5230\u4e13\u7528\u65b9\u6cd5\u7684\u6027\u80fd\u6c34\u5e73\u4e4b\u524d\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6709\u6548\u6027\u4ecd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u7684\u53ef\u7528\u6027\u3002\u8bad\u7ec3\u6570\u636e\u4e0d\u4ec5\u9700\u4e0e\u76ee\u6807\u7269\u4f53\u7c7b\u522b\u4e00\u81f4\uff0c\u8fd8\u9700\u5339\u914d\u4f20\u611f\u5668\u7279\u6027\u548c\u6a21\u6001\u3002\u4e3a\u5f25\u5408\u73b0\u6709\u6570\u636e\u96c6\u4e0e\u90e8\u7f72\u9886\u57df\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9886\u57df\u81ea\u9002\u5e94\u7b56\u7565\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5c06\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u7279\u5b9a\u4e8e\u4f20\u611f\u5668\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230\u5b8c\u5168\u4e0d\u540c\u7684\u611f\u77e5\u57df\u2014\u2014LiDAR\u3002\u6211\u4eec\u7684\u65b9\u6cd5XD-MAP\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5728\u76f8\u673a\u56fe\u50cf\u4e0a\u7684\u68c0\u6d4b\u7ed3\u679c\u6784\u5efa\u8bed\u4e49\u53c2\u6570\u5316\u5730\u56fe\uff0c\u5e76\u636e\u6b64\u5728\u76ee\u6807\u57df\u4e2d\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u3002\u4e0e\u4ee5\u5f80\u7684\u57df\u8fc1\u79fb\u65b9\u6cd5\u4e0d\u540c\uff0c\u672c\u65b9\u6cd5\u4e0d\u8981\u6c42\u4f20\u611f\u5668\u4e4b\u95f4\u5b58\u5728\u76f4\u63a5\u91cd\u53e0\uff0c\u5e76\u80fd\u5c06\u524d\u89c6\u76f8\u673a\u7684\u611f\u77e5\u89d2\u5ea6\u6269\u5c55\u81f3\u5b8c\u6574\u7684360\u5ea6\u89c6\u91ce\u3002\u5728\u6211\u4eec\u6784\u5efa\u7684\u5927\u89c4\u6a21\u9053\u8def\u7279\u5f81\u6570\u636e\u96c6\u4e0a\uff0cXD-MAP\u57282D\u8bed\u4e49\u5206\u5272\u4e0a\u6bd4\u5355\u6b21\u68c0\u6d4b\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa+19.5 mIoU\uff0c\u57282D\u5168\u666f\u5206\u5272\u4e0a\u63d0\u5347+19.5 PQth\uff0c\u57283D\u8bed\u4e49\u5206\u5272\u4e0a\u63d0\u5347+32.3 mIoU\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u5728LiDAR\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
