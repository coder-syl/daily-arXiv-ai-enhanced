<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的对抗性补丁生成、优化与评估流程，用于攻击面部生物识别系统，并结合ViT-GPT2模型生成语义描述以支持法证分析，同时利用感知哈希和分割技术实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 提升对人脸识别系统在对抗攻击下的脆弱性理解，并为法证分析和安全测试提供可解释、可检测的对抗样本生成与分析方法。

Method: 使用FGSM生成针对身份分类器的对抗噪声，通过扩散模型进行反向扩散，结合高斯平滑与自适应亮度校正提升不可感知性；将优化后的补丁应用于人脸图像，并利用ViT-GPT2生成身份语义描述；采用感知哈希与分割技术检测对抗样本。

Result: 生成的对抗补丁在保持视觉自然性的同时有效规避人脸识别系统，SSIM达0.95，并能通过语义描述支持法证解读，同时实现对对抗样本的高效检测与分析。

Conclusion: 所提端到端流程不仅有效生成高隐蔽性对抗补丁，还能支持法证文档生成与系统漏洞分析，为人脸识别系统的安全性评估与防御提供实用工具。

Abstract: This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

Abstract (中文翻译): 本研究提出了一种端到端的流程，用于生成、优化和评估对抗性补丁，以攻击面部生物识别系统，并应用于法证分析与安全测试。我们采用FGSM方法生成针对身份分类器的对抗噪声，并利用扩散模型通过反向扩散过程，结合高斯平滑与自适应亮度校正，提升对抗补丁的不可感知性，从而实现合成对抗补丁的隐蔽规避。优化后的补丁被应用于人脸图像，以测试其在保持自然视觉特征的同时规避识别系统的能力。此外，采用Vision Transformer（ViT）-GPT2模型为对抗图像生成语义描述，以支持对身份规避与识别攻击的法证解释与文档记录。该流程评估了在对抗条件下身份分类、图像描述结果的变化，以及面部身份验证与表情识别系统的脆弱性。我们进一步利用感知哈希与图像分割技术，实现了对对抗补丁及对抗样本的有效检测与分析，取得了0.95的SSIM指标。

</details>


### [2] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TL;DR: LCF3D是一种新型的RGB与LiDAR融合框架，通过后期融合减少LiDAR误检，并通过级联融合恢复漏检目标，在KITTI和nuScenes数据集上显著提升了对行人、骑行者等难检测类别的3D检测性能。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，准确地定位3D物体（如行人、骑行者和其他车辆）至关重要。虽然自动驾驶车辆通常结合使用RGB摄像头和LiDAR传感器以确保高检测性能，但如何有效融合这两种数据源进行3D目标检测仍是一个挑战。

Method: 提出LCF3D框架，将RGB图像上的2D目标检测器与LiDAR点云上的3D目标检测器相结合。该方法包含两个核心原则：(i) 后期融合，通过将LiDAR 3D检测结果与RGB 2D检测结果匹配，过滤未匹配的LiDAR检测以减少误报；(ii) 级联融合，通过为未匹配的RGB检测生成新的3D视锥体提案，以恢复LiDAR漏检的目标。

Result: 实验表明，LCF3D在域泛化方面表现优异，能有效处理训练和测试域之间不同的传感器配置。在KITTI数据集上对行人和骑行者，以及在nuScenes数据集上对摩托车和自行车等类别，LCF3D相比纯LiDAR方法取得了显著性能提升。

Conclusion: LCF3D通过创新的多模态融合策略，有效提升了3D目标检测的准确性和鲁棒性，尤其在处理难检测类别和不同传感器配置场景下表现出色。

Abstract: Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

Abstract (中文翻译): 在自动驾驶中，准确地定位行人、骑行者及其他车辆等3D物体至关重要。为确保高检测性能，自动驾驶车辆通常将RGB摄像头与LiDAR传感器相结合，但如何有效融合这些数据源以实现3D目标检测仍具挑战性。我们提出了LCF3D，一种新颖的传感器融合框架，将RGB图像上的2D目标检测器与LiDAR点云上的3D目标检测器相结合。通过利用多模态融合原理，我们弥补了LiDAR目标检测网络中的不准确性。我们的解决方案结合了两个关键原则：(i) 后期融合，通过将LiDAR 3D检测结果与RGB 2D检测结果匹配，并滤除未匹配的LiDAR检测，从而减少LiDAR的误报；(ii) 级联融合，通过为未匹配的RGB检测生成新的3D视锥体提案，以恢复LiDAR漏检的目标。实验表明，LCF3D在域泛化方面具有优势，能够成功应对训练域与测试域之间不同的传感器配置。LCF3D相比基于LiDAR的方法取得了显著改进，尤其在KITTI数据集上的行人和骑行者，以及nuScenes数据集上的摩托车和自行车等具有挑战性的类别上表现突出。代码可从以下网址下载：https://github.com/CarloSgaravatti/LCF3D。

</details>


### [3] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

TL;DR: 本研究比较了DenseNet121和EfficientNet-B0两种CNN模型在儿童肺炎X光片自动诊断中的性能，结果表明EfficientNet-B0在准确率、F1分数和MCC上均优于DenseNet121，且两者均具有高召回率；结合Grad-CAM与LIME可解释性方法验证了模型关注区域的临床相关性，提升了AI辅助诊断的可信度。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎仍是全球导致发病率和死亡率的主要原因，亟需高效准确的诊断辅助工具。深度学习在医学影像分析中展现出巨大潜力，特别是在胸部X光解读方面，因此有必要评估先进CNN架构在该任务中的表现。

Method: 使用包含5,863张儿童胸部X光图像的公开数据集，通过归一化、调整尺寸和数据增强进行预处理。在相同训练设置下，基于ImageNet预训练权重对DenseNet121和EfficientNet-B0进行微调，并采用准确率、F1分数、MCC和召回率评估性能。同时利用Grad-CAM和LIME进行可解释性分析，可视化影响预测的关键图像区域。

Result: EfficientNet-B0表现更优，准确率达84.6%，F1分数为0.8899，MCC为0.6849；DenseNet121准确率为79.7%，F1分数为0.8597，MCC为0.5852。两模型召回率均超过0.99，表明对肺炎高度敏感。Grad-CAM和LIME显示模型聚焦于临床相关的肺部区域。

Conclusion: EfficientNet-B0在性能和计算效率上优于DenseNet121，更适合临床部署。结合可解释性技术有助于提升AI辅助儿童肺炎诊断的透明度与可信度。

Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

Abstract (中文翻译): 背景：肺炎仍是全球儿童发病和死亡的主要原因之一，凸显了对准确高效诊断辅助工具的需求。深度学习在医学图像分析中展现出强大潜力，尤其是在胸部X光片解读方面。本研究比较了两种最先进的卷积神经网络（CNN）架构在儿童肺炎自动检测中的表现。方法：使用一个包含5,863张儿童胸部X光图像的公开数据集。图像经过归一化、尺寸调整和数据增强等预处理以提升泛化能力。在相同的训练设置下，利用ImageNet预训练权重对DenseNet121和EfficientNet-B0进行微调。性能评估指标包括准确率、F1分数、马修斯相关系数（MCC）和召回率。同时采用梯度加权类激活映射（Grad-CAM）和局部可解释模型无关解释（LIME）技术进行模型可解释性分析，以可视化影响预测的图像区域。结果：EfficientNet-B0优于DenseNet121，准确率达到84.6%，F1分数为0.8899，MCC为0.6849；而DenseNet121的准确率为79.7%，F1分数为0.8597，MCC为0.5852。两个模型的召回率均高于0.99，表明其在肺炎检测方面具有很高的敏感性。Grad-CAM和LIME可视化结果显示，模型一致聚焦于具有临床意义的肺部区域，支持了模型决策的可靠性。结论：与DenseNet121相比，EfficientNet-B0提供了更均衡且计算效率更高的性能，是临床部署的有力候选方案。可解释性技术的整合增强了人工智能辅助儿童肺炎诊断的透明度和可信度。

</details>


### [4] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

TL;DR: 提出NanoSD，一种轻量级扩散基础模型家族，通过联合优化U-Net与VAE结构，在保持生成先验的同时实现边缘设备上的实时图像恢复。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级扩散模型多通过压缩U-Net或缩短扩散轨迹来降低计算开销，但会破坏潜在流形结构，限制泛化能力；同时全管道部署在边缘设备上仍面临效率挑战。

Method: 通过网络剪枝、逐特征生成式蒸馏和结构化架构缩放，对Stable Diffusion 1.5的U-Net与VAE编解码器进行联合协同设计，构建Pareto最优的轻量模型家族NanoSD。

Result: NanoSD在多个任务（超分、去模糊、人脸修复、单目深度估计）上达到SOTA，参数量130M–315M，在移动端NPU上推理低至20ms，兼顾感知质量与部署效率。

Conclusion: NanoSD证明了全管道协同设计对边缘设备高效扩散模型的重要性，为实时视觉生成与恢复提供了通用基础模型。

Abstract: Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

Abstract (中文翻译): 诸如Stable Diffusion 1.5之类的潜在扩散模型提供了强大的生成先验，对图像恢复极具价值，但其完整流程计算开销过大，难以部署于边缘设备。现有的轻量级变体主要通过压缩去噪U-Net或缩短扩散轨迹来减小模型规模，但这会破坏底层潜在流形结构，并限制其在单一任务之外的泛化能力。我们提出了NanoSD——一个帕累托最优的扩散基础模型家族，通过对Stable Diffusion 1.5进行网络手术、逐特征生成式蒸馏以及U-Net与VAE编解码器的结构化架构缩放，实现全管道协同设计。该方法在保留生成先验的同时，生成了一系列在精度-延迟-模型大小前沿上占据不同工作点的模型（例如参数量130M–315M，在移动级NPU上实现低至20ms的实时推理）。我们指出，单纯减少参数量并不等同于硬件效率提升，并通过分析揭示了架构平衡、特征路由和潜在空间保持如何共同影响真实的端侧延迟。当作为即插即用主干网络时，NanoSD在图像超分辨率、图像去模糊、人脸修复和单目深度估计等多个任务上均达到当前最优性能，在感知质量和实际部署能力方面均优于以往的轻量级扩散模型。NanoSD建立了一个适用于边缘设备实时视觉生成与恢复的通用扩散基础模型家族。

</details>


### [5] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

TL;DR: 本文提出UniHash，一种结合pointwise和pairwise训练范式的双分支哈希框架，通过双向知识迁移提升在已见和未见类别上的图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度哈希方法通常局限于单一训练范式（pointwise或pairwise），前者在已见类别上表现好，后者在未见类别上泛化能力强。为兼顾两者优势，需统一两种范式以实现更均衡的检索性能。

Method: 提出Unified Hashing (UniHash)双分支框架：一个基于中心的pointwise分支和一个pairwise分支；引入互学习损失对齐哈希表示，并设计Split-Merge Mixture of Hash Experts (SM-MoH)模块增强跨分支哈希表示交换，实现双向知识迁移。

Result: 在CIFAR-10、MSCOCO和ImageNet上的实验表明，UniHash在已见与未见类别图像检索任务中均达到领先性能。

Conclusion: UniHash有效融合pointwise与pairwise范式的优势，显著提升哈希码的判别性与泛化能力，在跨类别图像检索中表现卓越。

Abstract: Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

Abstract (中文翻译): 在已见和未见类别上实现高效检索对现代图像检索系统至关重要。已见类别的检索确保对已知类别的精确识别，而未见类别的检索则促进模型在有限监督下对新类别的泛化能力。然而，大多数现有的深度哈希方法局限于单一训练范式——要么是逐点（pointwise）范式，要么是成对（pairwise）范式，前者在已见类别上表现优异，后者在未见类别上具有更好的泛化能力。为克服这一局限，我们提出了统一哈希（Unified Hashing, UniHash），这是一种双分支框架，融合两种范式的优点，以在已见和未见类别上实现均衡的检索性能。UniHash包含两个互补分支：遵循逐点范式的基于中心的分支和遵循成对范式的成对分支。我们引入了一种新颖的哈希码学习方法，以实现分支间的双向知识迁移，从而提升哈希码的判别性和泛化能力。该方法采用互学习损失来对齐哈希表示，并引入“分割-合并哈希专家混合”（Split-Merge Mixture of Hash Experts, SM-MoH）模块，以增强跨分支的哈希表示交换。理论分析验证了UniHash的有效性，且在CIFAR-10、MSCOCO和ImageNet上的大量实验表明，UniHash在已见和未见图像检索场景中均持续取得最先进的性能。

</details>


### [6] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 本文提出了一种名为ViSIL的新指标，用于评估多模态视频摘要（如关键帧+文本）的信息覆盖度，通过衡量未被摘要捕获的视频信息量，实现跨模态格式的统一比较，并在VQA任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如BLEU、ROUGE）无法有效衡量多模态视频摘要（如文本与关键帧组合）在不同模态间的信息覆盖程度，缺乏对跨模态语义一致性的量化能力。

Method: 提出Video Summary Information Loss (ViSIL)评分，基于信息论框架，利用视觉-语言模型（VLM）推理来量化视频摘要未能捕捉的视频信息量，从而提供一种统一的跨模态摘要评估指标。

Result: ViSIL评分与人类及VLM在视频问答（VQA）任务中的表现具有统计显著相关性；利用ViSIL可选出在信息损失与处理速度之间最优权衡的摘要，在不增加计算负载的情况下，使VQA准确率比纯文本摘要提升7%。

Conclusion: ViSIL为多模态视频摘要提供了一种有效的统一评估方法，不仅支持跨模态比较，还能指导高效摘要生成，在保持处理效率的同时提升下游任务性能。

Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

Abstract (中文翻译): 多模态视频字幕将密集的视频内容压缩为关键帧与自然语言组成的结构化形式。通过构建连贯的多模态摘要，该方法以丰富的语义证据为基础锚定生成式AI，并作为高效检索的轻量级代理。然而，传统的BLEU或ROUGE等指标无法量化不同模态间的信息覆盖度，例如难以比较一段文本与一串关键帧之间的信息一致性。为此，我们提出了视频摘要信息损失（ViSIL）评分，这是一种基于信息论的框架，通过视觉-语言模型（VLM）推理来量化摘要未能捕获的视频信息量。通过衡量信息损失，ViSIL成为一种统一的指标，能够直接比较结构差异显著的多模态摘要格式。实验结果表明，ViSIL评分与人类及VLM在视频问答（VQA）任务中的表现具有统计显著的相关性。此外，ViSIL还能用于摘要选择，以优化信息损失与处理速度之间的权衡，在不增加处理负担的前提下，相比纯文本摘要，VQA准确率提升7%，达到帕累托最优前沿。

</details>


### [7] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

TL;DR: 本文提出TuneCLIP，一种自监督微调框架，可在不重新训练的情况下提升开源CLIP模型在多种下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 改进CLIP模型通常需要从头开始在数十亿样本上训练，成本高昂。作者探索是否能仅利用现有自监督数据集来提升开源CLIP模型在多种下游任务中的通用性能。

Method: TuneCLIP包含两个关键阶段：(1) 受理论分析启发的预热阶段，用于恢复优化统计量以减少冷启动偏差；(2) 微调阶段，采用新的对比损失函数以缓解对假负样本对的惩罚。

Result: 实验表明，TuneCLIP在不同模型架构和规模下均能稳定提升性能。例如，在ImageNet及其分布外基准上提升高达+2.5%，在DataComp基准上提升+1.2%。

Conclusion: TuneCLIP为高效地进行CLIP模型后预训练适应提供了新范式，显著提升了开源CLIP模型的通用性能，建立了新的强基线。

Abstract: CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

Abstract (中文翻译): CLIP已成为多模态表征学习的基石，但提升其性能通常需要在数十亿样本上从头开始训练，成本极高。我们提出了一个不同的问题：能否仅使用现有的自监督数据集来提升开源CLIP模型在各种下游任务中的性能？与针对单一任务的有监督微调不同，我们的目标是提升模型在多种任务上的通用性能。然而，正如我们的实验和先前研究所揭示的那样，直接对开源CLIP模型应用标准训练协议往往会导致性能下降。为此，我们提出了TuneCLIP——一种自监督微调框架，有效克服了性能退化问题。TuneCLIP包含两个关键组件：(1) 受理论分析启发的预热阶段，用于恢复优化统计量以减少冷启动偏差；(2) 微调阶段，通过优化一种新的对比损失函数来缓解对假负样本对的惩罚。大量实验表明，TuneCLIP在不同模型架构和规模下均能持续提升性能。尤其值得注意的是，它显著提升了SigLIP（ViT-B/16）等领先的开源模型，在ImageNet及其相关分布外基准上最高提升+2.5%，在极具竞争力的DataComp基准上提升+1.2%，为高效的后预训练适应设立了新的强基线。

</details>


### [8] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

TL;DR: VibrantSR 是一种基于生成式超分辨率的方法，可从10米Sentinel-2影像中生成0.5米冠层高度模型（CHM），在西部美国22个生态区评估中MAE为4.39米，优于多个卫星基线方法，虽略逊于航拍方法，但具备大范围、低成本、高频次的森林监测能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于航拍影像的冠层高度建模方法受限于获取频率低、覆盖不连续，难以支持大尺度、高频次的森林监测与碳核算。因此，亟需一种能利用广泛可用、周期性强的卫星数据（如Sentinel-2）进行高精度冠层高度估算的方法。

Method: 提出 VibrantSR 框架，一种生成式超分辨率方法，利用全球可获取的 Sentinel-2 季节性合成影像，将10米分辨率影像提升至0.5米冠层高度模型（CHM）。

Result: 在西部美国22个EPA三级生态区上，使用空间分离的验证集评估，VibrantSR 对高度≥2米的冠层实现了4.39米的平均绝对误差（MAE），优于Meta（4.83米）、LANDFIRE（5.96米）和ETH（7.05米）等卫星基线方法；虽不及航拍方法VibrantVS（2.71米MAE），但具备运营级大陆尺度监测能力。

Conclusion: VibrantSR 在精度与实用性之间取得良好平衡，能够在不依赖昂贵且稀疏的航拍数据的前提下，实现大范围、季节至年度频率的森林冠层高度监测和碳核算，具有重要的实际应用价值。

Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

Abstract (中文翻译): 我们提出了 VibrantSR（Vibrant 超分辨率），这是一种生成式超分辨率框架，用于从10米分辨率的Sentinel-2影像中估算0.5米分辨率的冠层高度模型（CHM）。与依赖航拍影像的方法不同——后者受限于获取频率低且时间不规律，VibrantSR 利用全球可获取的Sentinel-2季节性合成影像，能够以季节至年度的频率进行持续监测。在对美国西部22个EPA三级生态区进行评估时，采用空间分离的验证划分，VibrantSR 对高度大于等于2米的冠层实现了4.39米的平均绝对误差（MAE），优于Meta（4.83米）、LANDFIRE（5.96米）和ETH（7.05米）等基于卫星的基准方法。尽管基于航拍的VibrantVS方法（MAE为2.71米）在精度上仍具优势，但VibrantSR无需依赖成本高昂且时间稀疏的航拍数据，即可实现大陆尺度的业务化森林监测与碳核算。

</details>


### [9] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: 本文提出MedVL-SAM2，一个统一的3D医学多模态模型，同时支持报告生成、视觉问答（VQA）和多种分割任务（语义、指代和交互式），通过结合图像级推理与像素级感知，在3D医学影像中实现细粒度视觉定位与空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型在图像级文本任务上表现良好，但在3D场景下实现细粒度视觉定位与体素级空间推理仍具挑战，尤其缺乏能统一这些能力的通用框架。

Method: 提出MedVL-SAM2模型，采用多阶段训练：首先在大规模3D CT图像-文本对上预训练以对齐视觉与语言特征；随后在包含语言理解和分割目标的3D CT分割数据集上联合优化，并引入基于SAM2的体素分割模块，支持语言、点或框提示的灵活交互。

Result: 模型在报告生成、VQA及多种3D分割任务上均达到SOTA性能，展现出可靠的3D视觉定位能力、可控的交互式分割效果以及强健的跨模态推理能力。

Conclusion: 高阶语义推理与精确3D定位可在统一的3D医学视觉语言模型中协同实现，MedVL-SAM2为此提供了有效框架。

Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

Abstract (中文翻译): 近期医学视觉-语言模型（VLM）在图像级文本中心任务（如报告生成和视觉问答VQA）上取得了显著进展。然而，在3D医学VLM中实现细粒度的视觉定位与体素级空间推理仍然具有挑战性，尤其是在试图将这些能力整合到单一、可泛化的框架中时。为应对这一挑战，我们提出了MedVL-SAM2——一个统一的3D医学多模态模型，能够同时支持报告生成、视觉问答（VQA）以及多范式分割任务，包括语义分割、指代表达分割和交互式分割。MedVL-SAM2通过专为3D医学影像设计的统一架构，融合了图像级推理与像素级感知，并引入基于SAM2的体素分割模块，以实现精确的多粒度空间推理。该模型采用多阶段训练流程：首先在大规模3D CT图像-文本对语料库上进行预训练，以对齐体素视觉特征与放射学语言嵌入；随后利用全面的3D CT分割数据集，对语言理解与分割目标进行联合优化。这种联合训练使模型能够通过语言、点或框提示进行灵活交互，从而将高层视觉推理与空间精确定位统一起来。我们的统一架构在报告生成、VQA以及多种3D分割任务上均取得了最先进的性能。大量分析进一步表明，该模型具备可靠的3D视觉定位能力、可控的交互式分割功能以及稳健的跨模态推理能力，证明了在统一的3D医学VLM中可以同时实现高层语义推理与精确的3D定位。

</details>


### [10] [Transition Matching Distillation for Fast Video Generation](https://arxiv.org/abs/2601.09881)
*Weili Nie,Julius Berner,Nanye Ma,Chao Liu,Saining Xie,Arash Vahdat*

Main category: cs.CV

TL;DR: 本文提出Transition Matching Distillation（TMD），一种将视频扩散模型蒸馏为高效少步生成器的新框架，通过匹配多步去噪轨迹与少步概率转移过程，在保持高质量的同时显著提升生成速度。


<details>
  <summary>Details</summary>
Motivation: 大型视频扩散和流模型虽能生成高质量视频，但其多步采样过程效率低下，难以用于实时交互应用，因此需要一种高效的蒸馏方法来减少推理步骤。

Method: TMD框架将扩散模型的多步去噪轨迹匹配到一个由轻量级条件流建模的少步概率转移过程中；将原扩散主干分解为主干部分（提取语义表示）和流头部分（执行多次内部流更新），并通过分布匹配蒸馏对带流头的学生模型进行训练。

Result: 在Wan2.1 1.3B和14B文本到视频模型上的实验表明，TMD在生成速度与视觉质量之间实现了灵活且优越的权衡，在相近推理成本下优于现有蒸馏模型。

Conclusion: TMD是一种有效且灵活的视频扩散模型蒸馏方法，能够显著加速推理过程，同时保持高视觉保真度和提示一致性，适用于实时视频生成场景。

Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd

Abstract (中文翻译): 大型视频扩散模型和流模型在高质量视频生成方面取得了显著成功，但由于其低效的多步采样过程，它们在实时交互应用中的使用仍然受限。在本研究中，我们提出了过渡匹配蒸馏（Transition Matching Distillation, TMD），这是一种将视频扩散模型蒸馏为高效少步生成器的新框架。TMD的核心思想是将扩散模型的多步去噪轨迹与一个少步概率转移过程相匹配，其中每一步转移都建模为一个轻量级的条件流。为了实现高效蒸馏，我们将原始扩散主干网络分解为两个组件：(1) 主干部分，包含大部分早期层，在每个外部转移步骤中提取语义表示；(2) 流头部分，由最后几层组成，利用这些表示执行多次内部流更新。给定一个预训练的视频扩散模型，我们首先为其引入一个流头，并将其适配为条件流映射。然后，我们在每个转移步骤中对带有流头展开的学生模型应用分布匹配蒸馏。在Wan2.1 1.3B和14B文本到视频模型上的大量实验表明，TMD在生成速度和视觉质量之间提供了灵活而强大的权衡。特别是在相近的推理成本下，TMD在视觉保真度和提示遵循方面优于现有的蒸馏模型。项目页面：https://research.nvidia.com/labs/genair/tmd

</details>
