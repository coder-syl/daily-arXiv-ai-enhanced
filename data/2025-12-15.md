<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification](https://arxiv.org/abs/2512.11015)
*Anoop Krishnan*

Main category: cs.CV

TL;DR: 本文提出利用图像标题中的语义信息，通过图文匹配引导和图文融合两种策略，在无需人口统计标签的情况下提升面部图像性别分类算法的公平性与准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于面部图像的性别分类算法存在显著的人口统计偏见，尤其在不同性别和种族群体间表现不均。为解决这一问题，作者探索引入文本引导方法以增强模型的公平性和泛化能力。

Method: 提出两种文本引导策略：1）图像-文本匹配（ITM）引导，训练模型识别图像与文本间的细粒度对齐关系，以获得增强的多模态表示；2）图像-文本融合，将两种模态结合为综合表示以提升公平性。该方法无需人口统计标签，且适用于多种应用场景。

Result: 在多个基准数据集上的大量实验表明，所提方法相比现有技术能有效减轻偏见，并在不同性别和种族群体中提升分类准确率。

Conclusion: 通过整合文本语义信息，该研究提供了一种可解释、直观的训练范式，有助于构建更公平的面部分析算法，并为缓解面部性别分类中的人口统计偏见提供了有效途径。

Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.

Abstract (中文翻译): 在追求人工智能公平性的过程中，本文提出了利用文本引导方法来提升基于面部图像的性别分类算法公平性的新思路。核心方法是在模型训练过程中利用图像标题中的语义信息以增强模型的泛化能力。文中提出了两种关键策略：图像-文本匹配（ITM）引导和图像-文本融合。ITM引导训练模型识别图像与文本之间的细粒度对齐关系，从而获得增强的多模态表示；图像-文本融合则将两种模态结合成综合表示，以提升公平性。在多个基准数据集上进行的大量实验表明，这些方法能有效缓解偏见，并在不同性别和种族群体中相比现有方法显著提升准确率。此外，文本引导的独特整合为计算机视觉系统提供了一种可解释且直观的训练范式。通过深入分析语义信息在多大程度上减少差异，本研究为构建更加公平的面部分析算法提供了宝贵见解。所提出的方法不仅解决了面部图像性别分类中人口统计偏见这一关键挑战，而且无需依赖人口统计标签，具有应用无关性。

</details>


### [2] [SoccerMaster: A Vision Foundation Model for Soccer Understanding](https://arxiv.org/abs/2512.11016)
*Haolin Yang,Jiayuan Rao,Haoning Wu,Weidi Xie*

Main category: cs.CV

TL;DR: 本文提出SoccerMaster，首个面向足球领域的视觉基础模型，通过多任务预训练统一处理从细粒度感知到语义推理的多种任务，并构建了大规模预训练数据集SoccerFactory，实验表明其在多个下游任务上优于专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖孤立的、针对特定任务的专家模型，难以应对足球视觉理解中多样且复杂的任务需求；因此，亟需一个统一框架来整合多种理解任务，提升模型泛化能力和实用性。

Method: 提出SoccerMaster模型，采用监督式多任务预训练策略，在单一框架内统一处理多种足球视觉理解任务；同时开发自动化数据整理流程，构建包含空间标注的大规模预训练数据集SoccerFactory。

Result: 在多个下游任务上的广泛评估显示，SoccerMaster始终优于专门设计的任务特定模型，展现出更强的通用性和性能优势。

Conclusion: SoccerMaster作为首个足球专用视觉基础模型，成功实现了多任务统一建模，并通过高质量预训练数据显著提升了各项任务的性能，为足球智能分析提供了新范式。

Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.

Abstract (中文翻译): 足球理解因其领域特有的复杂性和独特挑战，近年来引起了越来越多的研究关注。与以往通常依赖孤立的、任务特定专家模型的工作不同，本研究旨在提出一个统一模型，以处理从细粒度感知（如运动员检测）到语义推理（如事件分类）等多样化的足球视觉理解任务。具体而言，我们的贡献有三方面：（i）我们提出了SoccerMaster——首个面向足球领域的视觉基础模型，通过监督式多任务预训练，在单一框架内统一处理多种理解任务；（ii）我们开发了一套自动化数据整理流程，用于生成可扩展的空间标注，并将其与多个现有足球视频数据集整合，构建了全面的预训练数据资源SoccerFactory；（iii）我们进行了大量实验评估，结果表明SoccerMaster在多种下游任务上始终优于任务特定的专家模型，凸显了其广泛适用性与优越性。相关数据、代码和模型将公开发布。

</details>


### [3] [Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation](https://arxiv.org/abs/2512.11057)
*Marshal Ashif Shawkat,Moidul Hasan,Taufiq Hasan*

Main category: cs.CV

TL;DR: 本文提出一种基于知识蒸馏的方法，在无需边界框标注的情况下训练CNN模型以减少虚假关联并定位结核病（TB）相关异常，学生模型在TBX11k数据集上取得了0.2428 mIOU，并优于教师模型。


<details>
  <summary>Details</summary>
Motivation: 结核病是全球主要死因之一，尤其在资源有限地区。胸部X光（CXR）虽为经济有效的诊断工具，但依赖专家解读且常不可得；现有机器学习模型易受虚假关联影响、泛化能力差，且高质量医学图像标注成本高昂。

Method: 采用知识蒸馏技术构建教师-学生框架，使用ResNet50架构，在TBX11k数据集上训练CNN模型，无需边界框标注即可减少虚假关联并定位TB异常区域。

Result: 所提方法在TBX11k数据集上达到0.2428的mIOU分数，且学生模型性能始终优于教师模型。

Conclusion: 该方法提升了模型鲁棒性，具有在多样化临床环境中部署的潜力。

Abstract: Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.

Abstract (中文翻译): 结核病（TB）仍是全球主要致死原因之一，尤其是在资源有限的国家。胸部X光（CXR）成像是一种易于获取且成本低廉的诊断工具，但其解读依赖专家，而专家往往不可得。尽管机器学习模型在TB分类中表现出高性能，但它们常常依赖虚假关联，难以泛化。此外，构建带有高质量标注的大规模医学图像数据集需要大量资源和领域专家参与，通常还需多名标注者达成一致，造成巨大的财务和后勤负担。本研究重新利用知识蒸馏技术来训练CNN模型，在无需边界框标注的情况下减少虚假关联并定位TB相关异常。通过采用基于ResNet50架构的教师-学生框架，所提出的方法在TBX11k数据集上取得了0.2428的mIOU分数。实验结果进一步表明，学生模型始终优于教师模型，体现出更强的鲁棒性，并具备在多样化临床环境中更广泛应用的潜力。

</details>


### [4] [Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning](https://arxiv.org/abs/2512.11060)
*Chenjun Li,Cheng Wan,Laurin Lux,Alexander Berger,Richard B. Rosen,Martin J. Menten,Johannes C. Paetzold*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Synthetic Vasculature Reasoning (SVR) 的框架，通过可控合成带有糖尿病视网膜病变特征的视网膜血管图像及其细粒度推理文本，构建了包含10万对样本的 OCTA-100K-SVR 数据集。在该数据集上训练的通用视觉语言模型（Qwen3-VL-8b）在真实OCTA图像上实现了89.67%的零样本平衡分类准确率，并显著提升了临床解释质量和病灶定位能力。


<details>
  <summary>Details</summary>
Motivation: 医学领域（如OCTA图像解读）缺乏大规模、带有精准病理描述的图像-文本配对数据，限制了视觉语言模型（VLMs）在可解释性医疗诊断中的应用。

Method: 提出Synthetic Vasculature Reasoning (SVR)框架，可控地合成具有糖尿病视网膜病变（DR）特征（如毛细血管无灌注、微动脉瘤、新生血管和血管迂曲）的真实感视网膜血管图像，并自动生成细粒度的推理文本，构建OCTA-100K-SVR数据集。

Result: 在OCTA-100K-SVR数据集上训练的通用VLM（Qwen3-VL-8b）在真实OCTA图像上达到89.67%的零样本平衡分类准确率，优于监督基线；专家评估表明其显著提升了临床数据上的解释质量和病理定位能力。

Conclusion: 通过可控合成数据可以有效解决医学领域高质量图文数据稀缺的问题，所提出的SVR框架能显著提升VLM在医学图像理解与可解释诊断方面的性能。

Abstract: Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.

Abstract (中文翻译): 视觉语言模型（VLMs）通过允许用户在预测的同时跨模态询问临床解释，为可解释的医学诊断提供了一条有前景的路径。然而，训练用于详细推理的VLMs需要大规模的图像-文本数据集。在许多专业领域，例如解读光学相干断层扫描血管成像（OCTA）图像时，这种带有精确病理描述的文本数据十分稀缺甚至不存在。为克服这一瓶颈，我们提出了合成血管推理（Synthetic Vasculature Reasoning, SVR）框架，该框架能够可控地合成图像及其对应的文本，具体而言，即合成具有糖尿病视网膜病变（DR）特征（包括毛细血管无灌注、微动脉瘤、新生血管和血管迂曲）的真实感视网膜血管图像，并自动生成细粒度的推理文本。基于此，我们构建了OCTA-100K-SVR数据集，包含10万对OCTA图像-推理文本。实验表明，在该数据集上训练的通用VLM（Qwen3-VL-8b）在真实OCTA图像上实现了89.67%的零样本平衡分类准确率，优于监督学习基线。通过人类专家评估，我们还证明了该模型在临床数据上显著提升了解释质量和病理定位能力。

</details>


### [5] [VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation](https://arxiv.org/abs/2512.11061)
*Felix O'Mahony,Roberto Cipolla,Ayush Tewari*

Main category: cs.CV

TL;DR: 提出VDAWorld框架，利用视觉语言模型智能构建可模拟的抽象场景表示，并结合适配的物理引擎进行高质量动态仿真。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频模型存在违背物理与逻辑规则、缺乏交互性以及不可解释等问题，难以构建结构化、可查询的世界模型。

Method: 提出新范式，通过视觉语言模型（VLM）作为智能代理，从图像-文本对中自主构建具象化的2D/3D场景表示，选择合适的视觉工具和物理模拟器（如刚体、流体），并基于静态场景推断潜在动态以预测未来状态。

Result: 实验表明，该方法在多种动态场景下均能生成高质量的仿真结果，展现出强大的通用性和适应性。

Conclusion: 结合智能抽象与自适应仿真的VDAWorld框架有效克服了传统生成式视频模型的局限，为构建结构化、可交互且符合物理规律的世界模型提供了新路径。

Abstract: Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.

Abstract (中文翻译): 生成式视频模型作为世界建模的主流方法，存在根本性局限：常违背物理与逻辑规则、缺乏交互性，且作为黑箱难以构建结构化、可查询的世界。为解决这些问题，我们提出一种新范式，将图像-文本对提炼为适合仿真的、可处理的抽象表示。我们引入VDAWorld框架，其中视觉语言模型（VLM）作为智能代理协调整个过程：VLM自主从一系列视觉工具中选择，构建具象化的（2D或3D）场景表示，并据此选用兼容的物理模拟器（如刚体、流体）对其进行操作。随后，VDAWorld可从静态场景中推断潜在动态，以预测合理的未来状态。实验表明，这种智能抽象与自适应仿真的结合，能够生成适用于广泛动态场景的高质量仿真，展现出强大的通用世界建模能力。

</details>


### [6] [E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring](https://arxiv.org/abs/2512.11076)
*Jack Brady,Andrew Dailey,Kristen Schang,Zo Vic Shong*

Main category: cs.CV

TL;DR: 本文综述了城市动态研究的发展，重点探讨了基于事件的相机在该领域的潜力，包括其优势、挑战及与多传感器融合的可能性。


<details>
  <summary>Details</summary>
Motivation: 传统城市监测方法存在局限，而新兴技术如基于事件的相机可能提供更高效、隐私友好的城市动态感知手段。

Method: 对基于事件的相机的技术特性、应用场景、优缺点以及在机器学习中的应用进行系统分析，并提出将其作为城市动态研究的新媒介，同时建议与红外、事件激光雷达或振动传感器等进行多模态融合。

Result: 基于事件的相机具备低光工作能力、高时间分辨率和隐私保护等优势，适合用于城市动态监测；多传感器融合可进一步提升其性能并克服其固有局限。

Conclusion: 基于事件的相机是一种有前景的城市动态研究工具，结合多传感器融合策略可显著增强其在实际城市监测中的适用性和效果。

Abstract: Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.

Abstract (中文翻译): 理解人类活动与城市动态一直是一项挑战。从传统的人工观察城市居民，到使用普通摄像头，再到如今采用各类传感器和更复杂的技术，城市监测领域已取得长足发展。然而，在提升城市动态理解方面仍有更多可探索的空间。本文综述了城市动态研究手段的演进历程，特别聚焦于基于事件的相机（event-based cameras）。这类相机捕捉的是光强度的变化，而非传统相机所记录的RGB值，具备在低光照条件下工作的独特优势，相较其他传感器更具潜力。通过对基于事件的相机的技术原理、应用场景、优缺点及其在机器学习中的应用进行分析，本文提出将此类相机作为一种用于研究城市动态的信息采集媒介。它们不仅能够捕获关键信息，还能有效保护隐私。此外，本文还建议将基于事件的相机与其他传感器（如红外、事件激光雷达或振动传感器）进行多模态融合，以增强其能力并克服当前面临的挑战。

</details>


### [7] [Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description](https://arxiv.org/abs/2512.11098)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: 本文提出VLM-IRIS框架，通过将红外图像预处理为RGB兼容格式并结合CLIP模型，在无需重新训练的情况下实现红外图像的零样本工件检测。


<details>
  <summary>Details</summary>
Motivation: 传统视觉系统在低光或封闭制造环境中效果不佳，而红外相机在此类场景中具有优势；然而现有视觉-语言模型（VLM）仅基于RGB数据训练，无法直接处理红外图像。同时，监督式AI依赖大量标注数据，限制了其在红外应用中的实用性，因此需要一种适用于红外图像的零样本学习方法。

Method: 提出VLM-IRIS框架，将FLIR Boson传感器捕获的红外图像转换为magma表示形式，使其兼容基于CLIP的编码器（如ViT-B/32），并通过质心提示集成（centroid prompt ensembling）实现零样本推理。

Result: 在3D打印平台上的工件存在性检测任务中，利用热成像温差特性，VLM-IRIS在无需任何模型再训练的情况下实现了高准确率的零样本检测。

Conclusion: 所提出的VLM-IRIS方法有效扩展了视觉-语言模型在热成像领域的应用，为无标签工业监控提供了一种可行的零样本解决方案。

Abstract: Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.

Abstract (中文翻译): 许多制造环境处于低光照条件或封闭设备内部，传统视觉系统在这些场景中表现不佳。红外相机在此类环境中具有互补优势。同时，监督式人工智能系统需要大量标注数据，使得零样本学习框架在包括红外相机在内的应用中更具实用性。近期视觉-语言基础模型（VLM）的发展为基于图像-文本配对表示的零样本预测提供了新路径。然而，当前的VLM仅在RGB数据上训练，无法理解红外相机数据。本研究提出了VLM-IRIS（面向红外工业感知的视觉-语言模型），这是一种零样本框架，通过将FLIR Boson传感器捕获的红外图像预处理为与RGB兼容的输入，使其适用于基于CLIP的编码器。我们在3D打印机平台上展示了零样本工件存在性检测，该任务因构建平台与工件之间的温差而非常适合热成像。VLM-IRIS将红外图像转换为magma表示，并结合CLIP ViT-B/32编码器使用质心提示集成策略，在无需任何模型再训练的情况下，在红外图像上实现了高准确率。这些结果表明，所提出的VLM改进方法可有效拓展至热成像应用，实现无标签监控。

</details>


### [8] [VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction](https://arxiv.org/abs/2512.11099)
*Weitai Kang,Jason Kuen,Mengwei Ren,Zijun Wei,Yan Yan,Kangning Liu*

Main category: cs.CV

TL;DR: VGent是一种模块化编码器-解码器架构，通过冻结的多模态大语言模型（MLLM）进行高层推理，并利用检测器提供的高质量边界框作为查询，在编码器隐藏状态上进行交叉注意力以选择目标框。该方法避免了自回归解码的缺点，实现了快速推理，并在多目标视觉定位任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位模型要么依赖于自回归解码的多模态大语言模型（MLLM），存在速度慢和幻觉风险；要么通过对齐LLM与视觉特征学习新的特殊或对象标记，可能削弱LLM预训练的推理能力。

Method: 提出VGent架构：使用冻结的MLLM作为编码器保留其强大推理能力，解码器以检测器生成的高质量边界框为查询，通过交叉注意力机制从编码器隐藏状态中选择目标框。此外引入QuadThinker（基于强化学习增强多目标推理）、mask-aware标签（解决检测-分割歧义）和全局目标识别（提升所有目标的识别能力）。

Result: 在多目标视觉定位基准上，VGent相比之前方法F1分数提升+20.6%，在视觉参考挑战下gIoU提升+8.2%、cIoU提升+5.8%，同时保持恒定且快速的推理延迟。

Conclusion: VGent通过解耦高层推理与低层边界框预测，有效结合了目标检测与MLLM的优势，避免了自回归解码的问题，支持模块化升级，并在性能和效率上均取得显著进步。

Abstract: Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.

Abstract (中文翻译): 当前的视觉定位模型要么基于执行自回归解码的多模态大语言模型（MLLM），这种方法速度慢且存在幻觉风险；要么通过对齐LLM与视觉特征来学习用于定位的新特殊或对象标记，这可能会削弱LLM预训练的推理能力。相比之下，我们提出了VGent，一种模块化的编码器-解码器架构，显式地将高层推理与低层边界框预测解耦。具体而言，一个冻结的MLLM作为编码器，提供未受影响的强大推理能力，而解码器则以检测器提出的高质量边界框作为查询，通过在编码器隐藏状态上进行交叉注意力来选择目标框。该设计充分利用了目标检测和MLLM两方面的最新进展，避免了自回归解码的缺陷，并实现了快速推理。此外，它还支持对编码器和解码器进行模块化升级以提升整体系统性能：我们引入了（i）QuadThinker，一种基于强化学习的训练范式，用于增强编码器的多目标推理能力；（ii）掩码感知标签，用于解决检测与分割之间的歧义；以及（iii）全局目标识别，以提升对所有目标的识别能力，从而有助于在增强的候选框中进行选择。在多目标视觉定位基准上的实验表明，VGent相比先前方法实现了+20.6%的F1分数提升，并在视觉参考挑战下进一步将gIoU提高了+8.2%、cIoU提高了+5.8%，同时保持了恒定且快速的推理延迟。

</details>


### [9] [Information-driven Fusion of Pathology Foundation Models for Enhanced Disease Characterization](https://arxiv.org/abs/2512.11104)
*Brennan Flannery,Thomas DeSilvio,Jane Nguyen,Satish E. Viswanath*

Main category: cs.CV

TL;DR: 本文提出一种基于相关性引导的智能融合策略，整合多个病理基础模型（FMs），在肾癌、前列腺癌和直肠癌的分级/分期任务中显著优于单一模型或简单拼接方法，同时提升性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管病理基础模型（FMs）在多种任务中表现优异，但对其嵌入空间的互补性、冗余性及生物学意义仍缺乏系统理解，亟需有效融合策略以提升下游任务效果。

Method: 研究采用三种融合方案（多数投票集成、朴素特征拼接、基于相关性引导剪枝的智能融合），在tile级和slide级分别整合多个病理FM，并在三类癌症的H&E全切片图像上训练下游分类器，通过患者分层交叉验证评估性能。

Result: 智能融合在所有三类癌症中均优于最佳单一FM和朴素融合；全局相似性高但局部邻域一致性低，表明FM间存在细粒度互补信息；注意力图显示融合后更聚焦肿瘤区域，减少对良性区域的误关注。

Conclusion: 基于相关性引导的智能融合能生成紧凑且任务定制的表示，在提升计算病理任务预测性能的同时增强模型可解释性。

Abstract: Foundation models (FMs) have demonstrated strong performance across diverse pathology tasks. While there are similarities in the pre-training objectives of FMs, there is still limited understanding of their complementarity, redundancy in embedding spaces, or biological interpretation of features. In this study, we propose an information-driven, intelligent fusion strategy for integrating multiple pathology FMs into a unified representation and systematically evaluate its performance for cancer grading and staging across three distinct diseases. Diagnostic H&E whole-slide images from kidney (519 slides), prostate (490 slides), and rectal (200 slides) cancers were dichotomized into low versus high grade or stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were considered to train downstream classifiers. We then evaluated three FM fusion schemes at both tile and slide levels: majority-vote ensembling, naive feature concatenation, and intelligent fusion based on correlation-guided pruning of redundant features. Under patient-stratified cross-validation with hold-out testing, intelligent fusion of tile-level embeddings yielded consistent gains in classification performance across all three cancers compared with the best single FMs and naive fusion. Global similarity metrics revealed substantial alignment of FM embedding spaces, contrasted by lower local neighborhood agreement, indicating complementary fine-grained information across FMs. Attention maps showed that intelligent fusion yielded concentrated attention on tumor regions while reducing spurious focus on benign regions. Our findings suggest that intelligent, correlation-guided fusion of pathology FMs can yield compact, task-tailored representations that enhance both predictive performance and interpretability in downstream computational pathology tasks.

Abstract (中文翻译): 基础模型（FMs）在多种病理任务中展现出强大性能。尽管这些模型的预训练目标存在相似性，但对其嵌入空间中的互补性、冗余性以及特征的生物学解释仍知之甚少。本研究提出一种信息驱动的智能融合策略，将多个病理基础模型整合为统一表征，并在三种不同癌症（肾癌、前列腺癌和直肠癌）的分级与分期任务中系统评估其性能。研究使用了来自肾癌（519张切片）、前列腺癌（490张）和直肠癌（200张）的诊断性H&E全切片图像，并将其二分为低级别/早期与高级别/晚期。研究同时考虑了tile级别的FMs（Conch v1.5、MUSK、Virchow2、H-Optimus1、Prov-Gigapath）和slide级别的FMs（TITAN、CHIEF、MADELEINE）用于训练下游分类器。随后在tile和slide两个层面评估了三种FM融合方案：多数投票集成、朴素特征拼接，以及基于相关性引导剪枝冗余特征的智能融合。在采用患者分层交叉验证并保留独立测试集的设置下，tile级嵌入的智能融合在所有三种癌症中均持续优于最佳单一FM和朴素融合方法。全局相似性指标显示FM嵌入空间高度对齐，但局部邻域一致性较低，表明不同FM提供了互补的细粒度信息。注意力图进一步表明，智能融合能更集中地关注肿瘤区域，同时减少对良性区域的虚假关注。研究结果表明，基于相关性引导的智能融合策略可生成紧凑且面向特定任务的表征，从而在下游计算病理任务中同时提升预测性能与可解释性。

</details>


### [10] [Learning from a Generative Oracle: Domain Adaptation for Restoration](https://arxiv.org/abs/2512.11121)
*Yuyang Hu,Mojtaba Sahraee-Ardakan,Arpit Bansal,Kangfu Mei,Christian Qi,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: LEGO是一种无需配对数据的三阶段后训练域自适应框架，通过利用大型生成模型作为“神谕”来生成伪真值，从而将无监督域自适应问题转化为伪监督学习任务，在不修改模型结构的前提下显著提升预训练图像恢复模型在真实世界分布外数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练图像恢复模型在面对真实世界中分布外（out-of-distribution）的退化时往往表现不佳，因为存在显著的域差距；而对这些未见域进行自适应具有挑战性，因为缺乏真实标签，且传统方法通常需要复杂的架构改动。

Method: 提出LEGO（Learning from a Generative Oracle）框架，包含三个阶段：1）用预训练模型生成初始恢复结果；2）利用冻结的大规模生成式“神谕”模型将这些结果精炼为高质量的伪真值；3）采用混合监督策略，结合原始域内数据和新生成的伪配对数据对原模型进行微调。

Result: 实验表明，LEGO能有效弥合域间差距，在多种真实世界基准上显著提升了模型性能。

Conclusion: LEGO提供了一种实用、无需配对数据且无需修改模型结构的后训练域自适应方法，成功提升了图像恢复模型在分布外真实场景中的泛化能力。

Abstract: Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.

Abstract (中文翻译): 预训练的图像恢复模型由于显著的域间差距，常常在真实世界的分布外退化数据上表现不佳。针对这些未见过的域进行自适应具有挑战性，因为分布外数据缺乏真实标签，而传统的自适应方法通常需要复杂的架构更改。我们提出了LEGO（从生成式神谕中学习），这是一种无需配对数据的实用三阶段后训练域自适应框架。LEGO将这一无监督挑战转化为一个可处理的伪监督问题。首先，我们从预训练模型中获得初始恢复结果；其次，我们利用一个冻结的大规模生成式神谕模型，将这些估计结果精炼成高质量的伪真值；第三，我们采用混合监督策略，结合域内数据与这些新生成的伪配对数据，对原始模型进行微调。该方法在无需牺牲模型原始鲁棒性或进行架构修改的情况下，实现了对新分布的自适应。实验表明，LEGO能有效弥合域间差距，在多种真实世界基准上显著提升了性能。

</details>
