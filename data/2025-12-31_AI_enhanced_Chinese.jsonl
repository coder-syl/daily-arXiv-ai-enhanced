{"id": "2512.22175", "pdf": "https://arxiv.org/pdf/2512.22175", "abs": "https://arxiv.org/abs/2512.22175", "authors": ["Vatsal Baherwani", "Yixuan Ren", "Abhinav Shrivastava"], "title": "Characterizing Motion Encoding in Video Diffusion Timesteps", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "10 pages, 4 figures", "summary": "Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9a\u91cf\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u8fd0\u52a8\u4e0e\u5916\u89c2\u5728\u53bb\u566a\u65f6\u95f4\u6b65\u4e0a\u7684\u89e3\u8026\u89c4\u5f8b\uff0c\u9a8c\u8bc1\u5e76\u5f62\u5f0f\u5316\u4e86\u201c\u65e9\u671f\u65f6\u95f4\u6b65\u4e3b\u5bfc\u8fd0\u52a8\u3001\u540e\u671f\u4e3b\u5bfc\u5916\u89c2\u201d\u7684\u7ecf\u9a8c\u6cd5\u5219\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e00\u79cd\u7b80\u5316\u4e14\u9ad8\u6548\u7684\u8fd0\u52a8\u5b9a\u5236\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5bf9\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u8fd0\u52a8\u5982\u4f55\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u4e0a\u7f16\u7801\u7684\u7406\u89e3\u5c1a\u4e0d\u5145\u5206\uff0c\u5c3d\u7ba1\u5b9e\u8df5\u4e2d\u5e38\u4f7f\u7528\u201c\u65e9\u671f\u65f6\u95f4\u6b65\u63a7\u5236\u8fd0\u52a8\u3001\u540e\u671f\u7ec6\u5316\u5916\u89c2\u201d\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u548c\u7406\u8bba\u652f\u6491\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5728\u6307\u5b9a\u65f6\u95f4\u6b65\u8303\u56f4\u5185\u6ce8\u5165\u65b0\u6761\u4ef6\uff0c\u4ee5\u7531\u6b64\u5f15\u53d1\u7684\u5916\u89c2\u7f16\u8f91\u4e0e\u8fd0\u52a8\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861\u4f5c\u4e3a\u8fd0\u52a8\u7f16\u7801\u7684\u4ee3\u7406\u6307\u6807\uff0c\u5f00\u5c55\u5927\u89c4\u6a21\u5b9a\u91cf\u5b9e\u9a8c\uff0c\u4ece\u800c\u523b\u753b\u8fd0\u52a8\u4e0e\u5916\u89c2\u5728\u53bb\u566a\u8f68\u8ff9\u4e0a\u7684\u7ade\u4e89\u5173\u7cfb\u3002", "result": "\u5728\u591a\u79cd\u67b6\u6784\u4e0b\u5747\u53d1\u73b0\u5b58\u5728\u65e9\u671f\u7684\u201c\u8fd0\u52a8\u4e3b\u5bfc\u9636\u6bb5\u201d\u548c\u540e\u671f\u7684\u201c\u5916\u89c2\u4e3b\u5bfc\u9636\u6bb5\u201d\uff0c\u636e\u6b64\u5b9a\u4e49\u4e86\u64cd\u4f5c\u6027\u7684\u8fd0\u52a8-\u5916\u89c2\u65f6\u95f4\u6b65\u8fb9\u754c\uff1b\u57fa\u4e8e\u8be5\u53d1\u73b0\uff0c\u4ec5\u5728\u8fd0\u52a8\u4e3b\u5bfc\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\u5373\u53ef\u5b9e\u73b0\u5f3a\u6548\u8fd0\u52a8\u8fc1\u79fb\uff0c\u65e0\u9700\u989d\u5916\u53bb\u504f\u6a21\u5757\u6216\u7279\u6b8a\u76ee\u6807\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u5e7f\u6cdb\u4f7f\u7528\u7684\u7ecf\u9a8c\u6cd5\u5219\u8f6c\u5316\u4e3a\u65f6\u7a7a\u89e3\u8026\u539f\u5219\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u8fd0\u52a8\u8fc1\u79fb\u4e0e\u7f16\u8f91\u65b9\u6cd5\u4e2d\u7684\u65f6\u95f4\u6b65\u7ea6\u675f\u65b9\u6848\u3002", "summary_cn": "\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u5408\u6210\u65f6\u95f4\u8fd0\u52a8\u548c\u7a7a\u95f4\u5916\u89c2\uff0c\u7136\u800c\u8fd0\u52a8\u5728\u5404\u4e2a\u65f6\u95f4\u6b65\u4e0a\u5982\u4f55\u88ab\u7f16\u7801\u4ecd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u5b9e\u8df5\u4e2d\uff0c\u4eba\u4eec\u5e38\u4f9d\u8d56\u4e00\u79cd\u7ecf\u9a8c\u6cd5\u5219\uff1a\u65e9\u671f\u65f6\u95f4\u6b65\u4e3b\u8981\u5851\u9020\u8fd0\u52a8\u548c\u5e03\u5c40\uff0c\u800c\u540e\u671f\u65f6\u95f4\u6b65\u5219\u7ec6\u5316\u5916\u89c2\uff0c\u4f46\u8fd9\u4e00\u884c\u4e3a\u5c1a\u672a\u88ab\u7cfb\u7edf\u6027\u5730\u523b\u753b\u3002\u672c\u6587\u901a\u8fc7\u5728\u6307\u5b9a\u65f6\u95f4\u6b65\u8303\u56f4\u5185\u6ce8\u5165\u65b0\u6761\u4ef6\u6240\u5f15\u53d1\u7684\u5916\u89c2\u7f16\u8f91\u4e0e\u8fd0\u52a8\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4f5c\u4e3a\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u8fd0\u52a8\u7f16\u7801\u7684\u4ee3\u7406\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9a\u91cf\u7814\u7a76\u5bf9\u8be5\u4ee3\u7406\u6307\u6807\u8fdb\u884c\u523b\u753b\u3002\u8be5\u65b9\u6cd5\u4f7f\u6211\u4eec\u80fd\u591f\u6cbf\u7740\u53bb\u566a\u8f68\u8ff9\u5b9a\u91cf\u5730\u5206\u79bb\u8fd0\u52a8\u4e0e\u5916\u89c2\u3002\u5728\u591a\u79cd\u4e0d\u540c\u67b6\u6784\u4e2d\uff0c\u6211\u4eec\u4e00\u81f4\u5730\u8bc6\u522b\u51fa\u4e00\u4e2a\u65e9\u671f\u7684\u201c\u8fd0\u52a8\u4e3b\u5bfc\u9636\u6bb5\u201d\u548c\u4e00\u4e2a\u540e\u671f\u7684\u201c\u5916\u89c2\u4e3b\u5bfc\u9636\u6bb5\u201d\uff0c\u4ece\u800c\u5728\u65f6\u95f4\u6b65\u7a7a\u95f4\u4e2d\u786e\u7acb\u4e86\u4e00\u4e2a\u53ef\u64cd\u4f5c\u7684\u8fd0\u52a8-\u5916\u89c2\u8fb9\u754c\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u5c06\u5f53\u524d\u7684\u4e00\u6b21\u6027\u8fd0\u52a8\u5b9a\u5236\u8303\u5f0f\u7b80\u5316\u4e3a\u4ec5\u5728\u8fd0\u52a8\u4e3b\u5bfc\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u5373\u53ef\u5728\u65e0\u9700\u8f85\u52a9\u53bb\u504f\u6a21\u5757\u6216\u4e13\u95e8\u4f18\u5316\u76ee\u6807\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u8fd0\u52a8\u8fc1\u79fb\u6548\u679c\u3002\u6211\u4eec\u7684\u5206\u6790\u5c06\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u8f6c\u5316\u4e3a\u4e00\u79cd\u65f6\u7a7a\u89e3\u8026\u539f\u5219\uff0c\u800c\u6240\u63d0\u51fa\u7684\u65f6\u95f4\u6b65\u7ea6\u675f\u65b9\u6848\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u7684\u8fd0\u52a8\u8fc1\u79fb\u4e0e\u7f16\u8f91\u65b9\u6cd5\u4e2d\u3002"}}
{"id": "2512.22177", "pdf": "https://arxiv.org/pdf/2512.22177", "abs": "https://arxiv.org/abs/2512.22177", "authors": ["Dawnena Key"], "title": "Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment", "categories": ["cs.CV"], "comment": "10 pages, 1 figure, 2 tables. Patent pending (US 63/918,518). Code available at https://github.com/dawnenakey/spokhandSLR", "summary": "This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D CNN\u4e0eLSTM\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u8bc6\u522b\u7f8e\u5f0f\u624b\u8bed\uff08ASL\uff09\u5355\u8bcd\u7ea7\u522b\u624b\u52bf\u3002\u7cfb\u7edf\u57fa\u4e8e\u7f51\u7edc\u6444\u50cf\u5934\u89c6\u9891\u6d41\uff0c\u5728WLASL\u3001ASL-LEX\u53ca\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cF1\u5206\u6570\u8fbe0.71\u20130.99\uff0c\u5e76\u53ef\u5728OAK-D\u76f8\u673a\u4e0a\u5b9e\u73b0\u8fb9\u7f18\u90e8\u7f72\u3002", "motivation": "\u5168\u7403\u6709\u8d85\u8fc77000\u4e07\u804b\u4eba\u548c\u542c\u969c\u4eba\u58eb\u9762\u4e34\u6c9f\u901a\u969c\u788d\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\u4ee5\u63d0\u5347\u65e0\u969c\u788d\u4ea4\u6d41\u80fd\u529b\u3002", "method": "\u91c7\u75283D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff083D CNN\uff09\u63d0\u53d6\u89c6\u9891\u5e27\u4e2d\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u518d\u901a\u8fc7\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u5efa\u6a21\u624b\u8bed\u624b\u52bf\u4e2d\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff1b\u6a21\u578b\u5728WLASL\u6570\u636e\u96c6\uff082000\u4e2a\u5e38\u7528\u8bcd\uff09\u3001ASL-LEX\u8bcd\u6c47\u5e93\uff08\u7ea62700\u4e2a\u624b\u52bf\uff09\u4ee5\u53ca100\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u624b\u52bf\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u90e8\u7f72\u4e8eAWS\u5e73\u53f0\uff0c\u652f\u6301OAK-D\u76f8\u673a\u7684\u8fb9\u7f18\u63a8\u7406\u3002", "result": "\u7cfb\u7edf\u5728\u5404\u7c7b\u624b\u8bed\u7c7b\u522b\u4e0a\u7684F1\u5206\u6570\u4ecb\u4e8e0.71\u81f30.99\u4e4b\u95f4\uff0c\u5c55\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u80fd\u6709\u6548\u5b9e\u73b0\u5b9e\u65f6\u7f8e\u5f0f\u624b\u8bed\u8bc6\u522b\uff0c\u5177\u5907\u826f\u597d\u7684\u90e8\u7f72\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u524d\u666f\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u542c\u969c\u4eba\u7fa4\u7684\u6c9f\u901a\u53ef\u53ca\u6027\u3002", "summary_cn": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u7f8e\u5f0f\u624b\u8bed\uff08ASL\uff09\u8bc6\u522b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u7ed3\u5408\u4e09\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff083D CNN\uff09\u4e0e\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002\u8be5\u7cfb\u7edf\u5904\u7406\u7f51\u7edc\u6444\u50cf\u5934\u89c6\u9891\u6d41\uff0c\u4ee5\u8bc6\u522b\u5355\u8bcd\u7ea7\u522b\u7684ASL\u624b\u52bf\uff0c\u65e8\u5728\u7f13\u89e3\u5168\u7403\u8d85\u8fc77000\u4e07\u804b\u4eba\u53ca\u542c\u969c\u4eba\u58eb\u6240\u9762\u4e34\u7684\u6c9f\u901a\u969c\u788d\u3002\u8be5\u67b6\u6784\u5229\u75283D\u5377\u79ef\u4ece\u89c6\u9891\u5e27\u4e2d\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff0c\u968f\u540e\u901a\u8fc7LSTM\u5c42\u5efa\u6a21\u624b\u8bed\u624b\u52bf\u4e2d\u56fa\u6709\u7684\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\u3002\u7cfb\u7edf\u5728WLASL\u6570\u636e\u96c6\uff08\u5305\u542b2000\u4e2a\u5e38\u7528\u8bcd\uff09\u3001ASL-LEX\u8bcd\u6c47\u6570\u636e\u5e93\uff08\u7ea62700\u4e2a\u624b\u52bf\uff09\u4ee5\u53ca\u4e00\u7ec4\u7ecf\u4e13\u5bb6\u6807\u6ce8\u7684100\u4e2aASL\u624b\u52bf\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u5404\u7c7b\u624b\u52bf\u4e0a\u7684F1\u5206\u6570\u8fbe\u52300.71\u81f30.99\u3002\u8be5\u6a21\u578b\u90e8\u7f72\u4e8eAWS\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u652f\u6301\u5728OAK-D\u76f8\u673a\u4e0a\u8fdb\u884c\u8fb9\u7f18\u90e8\u7f72\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002\u6587\u4e2d\u8fd8\u8ba8\u8bba\u4e86\u8be5\u67b6\u6784\u7684\u8bbe\u8ba1\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u9762\u5411\u5b9e\u9645\u65e0\u969c\u788d\u5e94\u7528\u573a\u666f\u7684\u90e8\u7f72\u8003\u91cf\u3002"}}
{"id": "2512.22182", "pdf": "https://arxiv.org/pdf/2512.22182", "abs": "https://arxiv.org/abs/2512.22182", "authors": ["Hassan Khalid", "Muhammad Mahad Khaliq", "Muhammad Jawad Bashir"], "title": "Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "5 pages, 3 figures. Accepted and published at 2024 19th International Conference on Emerging Technologies (ICET)", "summary": "The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u4e0e\u5c40\u90e8\u7ebf\u6027\u5d4c\u5165\uff08LLE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u533b\u7597\u8ba1\u8d39\u548c\u8f6c\u5f55\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u5904\u7406\u9ad8\u7ef4\u533b\u7597\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u8ba1\u8d39\u548c\u8f6c\u5f55\u7b49\u73af\u8282\uff0c\u4ee5\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u5e76\u63d0\u5347\u6574\u4f53\u8fd0\u8425\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cdAI\u589e\u5f3a\u7684\u5c40\u90e8\u7ebf\u6027\u5d4c\u5165\uff08LLE\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u8be5\u6a21\u578b\u5bf9\u9ad8\u7ef4\u533b\u7597\u6570\u636e\u8fdb\u884c\u964d\u7ef4\u4e0e\u5904\u7406\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u533b\u7597\u8ba1\u8d39\u548c\u8f6c\u5f55\u6d41\u7a0b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6570\u636e\u5904\u7406\u51c6\u786e\u6027\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AI\u589e\u5f3a\u7684LLE\u65b9\u6cd5\u5728\u533b\u7597\u6570\u636e\u5904\u7406\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e0d\u4ec5\u53ef\u4f18\u5316\u5f53\u524d\u7684\u8ba1\u8d39\u4e0e\u8f6c\u5f55\u7cfb\u7edf\uff0c\u4e5f\u4e3a\u672a\u6765\u66f4\u5e7f\u6cdb\u7684\u533b\u7597\u5e94\u7528\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "summary_cn": "\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9886\u57df\u7684\u8fc5\u901f\u53d1\u5c55\u4e3a\u63d0\u5347\u5404\u7c7b\u6d41\u7a0b\uff08\u5305\u62ec\u533b\u7597\u8ba1\u8d39\u548c\u8f6c\u5f55\uff09\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u5c06\u4eba\u5de5\u667a\u80fd\u4e0e\u5c40\u90e8\u7ebf\u6027\u5d4c\u5165\uff08LLE\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u9769\u65b0\u9ad8\u7ef4\u533b\u7597\u6570\u636e\u7684\u5904\u7406\u65b9\u5f0f\u3002\u8be5AI\u589e\u5f3a\u7684LLE\u6a21\u578b\u4e13\u95e8\u9488\u5bf9\u63d0\u5347\u533b\u7597\u8ba1\u8d39\u7cfb\u7edf\u548c\u8f6c\u5f55\u670d\u52a1\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u800c\u8bbe\u8ba1\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u8fd9\u4e9b\u6d41\u7a0b\uff0c\u8be5\u6a21\u578b\u65e8\u5728\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u5e76\u7b80\u5316\u64cd\u4f5c\uff0c\u4ece\u800c\u52a0\u5feb\u60a3\u8005\u62a4\u7406\u6587\u6863\u8bb0\u5f55\u548c\u8d22\u52a1\u4ea4\u6613\u7684\u901f\u5ea6\u4e0e\u51c6\u786e\u6027\u3002\u672c\u6587\u63d0\u4f9b\u4e86AI\u589e\u5f3aLLE\u7684\u5b8c\u6574\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u5904\u7406\u51c6\u786e\u6027\u548c\u8fd0\u8425\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u672c\u7814\u7a76\u4e0d\u4ec5\u7a81\u663e\u4e86AI\u589e\u5f3aLLE\u5728\u533b\u7597\u6570\u636e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e5f\u4e3a\u672a\u6765\u66f4\u5e7f\u6cdb\u7684\u533b\u7597\u5e94\u7528\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.22183", "pdf": "https://arxiv.org/pdf/2512.22183", "abs": "https://arxiv.org/abs/2512.22183", "authors": ["Zhaonan Li", "Shijie Lu", "Fei Wang", "Jacob Dineen", "Xiao Ye", "Zhikun Xu", "Siyi Liu", "Young Min Cho", "Bangzheng Li", "Daniel Chang", "Kenny Nguyen", "Qizheng Yang", "Muhao Chen", "Ben Zhou"], "title": "Unbiased Visual Reasoning with Controlled Visual Inputs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.", "AI": {"tldr": "VISTA is a modular framework that separates visual perception from reasoning to reduce reliance on spurious correlations in vision-language models, improving robustness and interpretability.", "motivation": "End-to-end vision-language models often rely on spurious correlations rather than true visual evidence, especially after fine-tuning, leading to shortcut learning and poor generalization.", "method": "VISTA decouples perception and reasoning via an explicit information bottleneck: a frozen VLM handles short, objective perception queries, while a text-only LLM decomposes questions, plans queries, and aggregates visual facts. The system is trained with reinforcement learning (GRPO) using only 641 curated multi-step questions.", "result": "VISTA significantly improves robustness on SpuriVerse (+16.29% with Qwen-2.5-VL-7B, +6.77% with Llama-3.2-Vision-11B), maintains competitive performance on MMVP and SeedBench, transfers well to unseen VLMs, and recovers from perception failures. Human evaluation shows more neutral, evidence-grounded reasoning.", "conclusion": "By enforcing a modular, bottlenecked interface between perception and reasoning, VISTA enables more reliable, interpretable, and robust visual question answering that resists spurious correlations.", "summary_cn": "\u7aef\u5230\u7aef\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u901a\u5e38\u901a\u8fc7\u5229\u7528\u865a\u5047\u76f8\u5173\u6027\u800c\u975e\u56e0\u679c\u89c6\u89c9\u8bc1\u636e\u6765\u56de\u7b54\u89c6\u89c9\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u5fae\u8c03\u540e\u66f4\u5bb9\u6613\u4f9d\u8d56\u6377\u5f84\u3002\u6211\u4eec\u63d0\u51fa\u4e86VISTA\uff08\u7528\u4e8e\u6587\u672c\u5206\u6790\u7684\u89c6\u89c9\u4fe1\u606f\u5206\u79bb\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7684\u4fe1\u606f\u74f6\u9888\u5c06\u611f\u77e5\u4e0e\u63a8\u7406\u89e3\u8026\u3002\u4e00\u4e2a\u51bb\u7ed3\u7684VLM\u4f20\u611f\u5668\u4ec5\u5904\u7406\u7b80\u77ed\u3001\u5ba2\u89c2\u7684\u611f\u77e5\u67e5\u8be2\uff0c\u800c\u7eaf\u6587\u672c\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u5668\u5219\u8d1f\u8d23\u5206\u89e3\u95ee\u9898\u3001\u89c4\u5212\u67e5\u8be2\uff0c\u5e76\u4ee5\u81ea\u7136\u8bed\u8a00\u805a\u5408\u89c6\u89c9\u4e8b\u5b9e\u3002\u8fd9\u79cd\u53d7\u63a7\u63a5\u53e3\u4e3a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65e0\u504f\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u5956\u52b1\u5bf9\u9f50\u7684\u73af\u5883\u3002\u5728\u4ec5\u4f7f\u7528641\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u6b65\u95ee\u9898\u5e76\u901a\u8fc7GRPO\u8fdb\u884c\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u4e8eQwen2.5-VL\u548cLlama3.2-Vision\u4f20\u611f\u5668\u5b9e\u73b0\u7684VISTA\u5728SpuriVerse\u4e0a\u5bf9\u73b0\u5b9e\u4e16\u754c\u865a\u5047\u76f8\u5173\u6027\u7684\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\uff08Qwen-2.5-VL-7B\u63d0\u534716.29%\uff0cLlama-3.2-Vision-11B\u63d0\u53476.77%\uff09\uff0c\u540c\u65f6\u5728MMVP\u548c\u5e73\u8861\u7248SeedBench\u5b50\u96c6\u4e0a\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\u3002VISTA\u80fd\u7a33\u5065\u5730\u8fc1\u79fb\u5230\u672a\u89c1\u8fc7\u7684VLM\u4f20\u611f\u5668\uff0c\u5e76\u80fd\u8bc6\u522b\u548c\u4eceVLM\u611f\u77e5\u5931\u8d25\u4e2d\u6062\u590d\u3002\u4eba\u5de5\u5206\u6790\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u4e0e\u7aef\u5230\u7aefVLM\u57fa\u7ebf\u76f8\u6bd4\uff0cVISTA\u7684\u63a8\u7406\u8f68\u8ff9\u66f4\u52a0\u4e2d\u7acb\u3001\u66f4\u5c11\u4f9d\u8d56\u865a\u5047\u5c5e\u6027\uff0c\u5e76\u66f4\u660e\u786e\u5730\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u3002"}}
{"id": "2512.22185", "pdf": "https://arxiv.org/pdf/2512.22185", "abs": "https://arxiv.org/abs/2512.22185", "authors": ["Antara Titikhsha", "Divyanshu Tak"], "title": "SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening", "categories": ["cs.CV"], "comment": null, "summary": "Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that \"more augmentation is always better\" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \\$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.", "AI": {"tldr": "SAMM2D\u662f\u4e00\u79cd\u53cc\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5728\u9885\u5185\u52a8\u8109\u7624\u68c0\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4e34\u5e8a\u57fa\u7ebf\uff08AUC\u63d0\u534732%\uff09\uff0c\u5e76\u53d1\u73b0\u5f3a\u9884\u8bad\u7ec3\u6a21\u578b\u4e0b\u6570\u636e\u589e\u5f3a\u53cd\u800c\u635f\u5bb3\u6027\u80fd\uff1b\u901a\u8fc7\u9608\u503c\u6821\u51c6\uff0c\u6a21\u578b\u8fbe\u523095%\u654f\u611f\u5ea6\uff0c\u8d85\u8d8a\u666e\u901a\u653e\u5c04\u79d1\u533b\u751f\uff0c\u5e76\u5728\u53ef\u89c6\u5316\u4e0a\u5c55\u73b0\u51fa\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "motivation": "\u52a8\u8109\u7624\u68c0\u6d4b\u5bf9\u9884\u9632\u81f4\u547d\u6027\u51fa\u8840\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56e0\u52a8\u8109\u7624\u5f62\u6001\u7ec6\u5fae\u3001\u7c7b\u522b\u6781\u5ea6\u4e0d\u5e73\u8861\u53ca\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u800c\u6781\u5177\u6311\u6218\u3002", "method": "\u63d0\u51faSAMM2D\u53cc\u7f16\u7801\u5668\u6846\u67b6\uff0c\u7ed3\u5408\u5f3aImageNet\u9884\u8bad\u7ec3\u4e3b\u5e72\uff0c\u5728\u65e0\u6570\u636e\u589e\u5f3a\u6761\u4ef6\u4e0b\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u51b3\u7b56\u9608\u503c\u6821\u51c6\u4f18\u5316\u654f\u611f\u5ea6\uff1b\u540c\u65f6\u8fdb\u884c\u516d\u79cd\u589e\u5f3a\u7b56\u7565\u7684\u6d88\u878d\u5b9e\u9a8c\u4ee5\u8bc4\u4f30\u589e\u5f3a\u6548\u679c\u3002", "result": "SAMM2D\u5728RSNA\u6570\u636e\u96c6\u4e0aAUC\u8fbe0.686\uff0c\u6bd4\u4e34\u5e8a\u57fa\u7ebf\u9ad832%\uff1b\u672a\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u6240\u6709\u589e\u5f3a\u53d8\u4f53\uff08\u63d0\u53471.75\u20132.23\u4e2a\u767e\u5206\u70b9\uff0cp<0.01\uff09\uff1b\u6821\u51c6\u540e\u654f\u611f\u5ea6\u8fbe95%\uff0c\u8d85\u8fc7\u5e73\u5747\u653e\u5c04\u79d1\u533b\u751f\u6c34\u5e73\uff1bGrad-CAM\u663e\u793a85%\u771f\u9633\u6027\u805a\u7126\u4e8e\u76f8\u5173\u8840\u7ba1\u533a\u57df\uff08\u4e0e\u4e13\u5bb6\u6807\u6ce8IoU\u4e3a62%\uff09\uff1b\u9884\u8ba1\u6bcf\u5343\u540d\u7b5b\u67e5\u60a3\u8005\u53ef\u8282\u77011390\u4e07\u7f8e\u5143\u3002", "conclusion": "\u5728\u4f4e\u6570\u636e\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\uff0c\u5f3a\u9884\u8bad\u7ec3\u53ef\u80fd\u6bd4\u590d\u6742\u7684\u6570\u636e\u589e\u5f3a\u66f4\u6709\u6548\uff1b\u672a\u6765\u5de5\u4f5c\u5e94\u4f18\u5148\u5229\u7528\u9ad8\u8d28\u91cf\u9884\u8bad\u7ec3\u7279\u5f81\uff0c\u800c\u975e\u8fc7\u5ea6\u4f9d\u8d56\u589e\u5f3a\u7b56\u7565\u3002", "summary_cn": "\u6709\u6548\u7684\u52a8\u8109\u7624\u68c0\u6d4b\u5bf9\u4e8e\u9884\u9632\u5371\u53ca\u751f\u547d\u7684\u51fa\u8840\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u52a8\u8109\u7624\u5f62\u6001\u7ec6\u5fae\u3001\u7c7b\u522b\u4e25\u91cd\u4e0d\u5e73\u8861\u4ee5\u53ca\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86SAMM2D\uff0c\u4e00\u79cd\u53cc\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5728RSNA\u9885\u5185\u52a8\u8109\u7624\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.686\u7684AUC\uff0c\u6bd4\u4e34\u5e8a\u57fa\u7ebf\u63d0\u9ad8\u4e8632%\u3002\u5728\u5bf9\u516d\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\u8fdb\u884c\u5168\u9762\u6d88\u878d\u5b9e\u9a8c\u65f6\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u4e00\u4e2a\u60ca\u4eba\u73b0\u8c61\uff1a\u5f53\u4e0e\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u4e3b\u5e72\u7f51\u7edc\u7ed3\u5408\u65f6\uff0c\u4efb\u4f55\u5f62\u5f0f\u7684\u6570\u636e\u589e\u5f3a\u90fd\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u672a\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u7684\u57fa\u7ebf\u6a21\u578b\u5728\u6240\u6709\u589e\u5f3a\u53d8\u4f53\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u6027\u80fd\u9ad8\u51fa1.75\u81f32.23\u4e2a\u767e\u5206\u70b9\uff08p < 0.01\uff09\uff0c\u63a8\u7ffb\u4e86\u201c\u5728\u4f4e\u6570\u636e\u533b\u5b66\u573a\u666f\u4e2d\uff0c\u8d8a\u591a\u589e\u5f3a\u8d8a\u597d\u201d\u7684\u4f20\u7edf\u5047\u8bbe\u3002\u6211\u4eec\u63a8\u6d4b\uff0cImageNet\u9884\u8bad\u7ec3\u7279\u5f81\u5df2\u5177\u5907\u8db3\u591f\u7684\u9c81\u68d2\u4e0d\u53d8\u6027\uff0c\u989d\u5916\u7684\u6570\u636e\u589e\u5f3a\u4e0d\u4ec5\u5197\u4f59\uff0c\u8fd8\u4f1a\u7834\u574f\u6240\u5b66\u7279\u5f81\u6d41\u5f62\u3002\u901a\u8fc7\u6821\u51c6\u51b3\u7b56\u9608\u503c\uff0cSAMM2D\u8fbe\u5230\u4e8695%\u7684\u654f\u611f\u5ea6\uff0c\u8d85\u8d8a\u4e86\u666e\u901a\u653e\u5c04\u79d1\u533b\u751f\u7684\u8868\u73b0\uff0c\u5e76\u5728\u7b5b\u67e5\u5e94\u7528\u4e2d\u9884\u8ba1\u6bcf1000\u540d\u60a3\u8005\u53ef\u8282\u77011390\u4e07\u7f8e\u5143\u3002Grad-CAM\u53ef\u89c6\u5316\u8bc1\u5b9e\uff0c85%\u7684\u771f\u9633\u6027\u6837\u672c\u805a\u7126\u4e8e\u76f8\u5173\u7684\u8840\u7ba1\u533a\u57df\uff08\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684IoU\u4e3a62%\uff09\uff0c\u8868\u660e\u6a21\u578b\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u5173\u6ce8\u70b9\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u672a\u6765\u7684\u533b\u5b66\u5f71\u50cf\u5de5\u4f5c\u6d41\u7a0b\u6216\u8bb8\u5e94\u66f4\u91cd\u89c6\u5f3a\u9884\u8bad\u7ec3\uff0c\u800c\u975e\u65e5\u76ca\u590d\u6742\u7684\u6570\u636e\u589e\u5f3a\u6d41\u7a0b\u3002"}}
{"id": "2512.22188", "pdf": "https://arxiv.org/pdf/2512.22188", "abs": "https://arxiv.org/abs/2512.22188", "authors": ["Xitong Ling", "Minxi Ouyang", "Xiaoxiao Li", "Jiawen Li", "Ying Chen", "Yuxuan Sun", "Xinrui Chen", "Tian Guan", "Xiaoping Liu", "Yonghong He"], "title": "HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHookMIL\uff0c\u4e00\u79cd\u57fa\u4e8e\u53ef\u5b66\u4e60\u201c\u94a9\u5b50\u201d\uff08hook\uff09token\u7684\u9ad8\u6548\u591a\u793a\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u5207\u7247\u56fe\u50cf\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u521d\u59cb\u5316\u3001\u7ebf\u6027\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u673a\u5236\u3001\u591a\u6837\u6027\u635f\u5931\u548ctoken\u95f4\u901a\u4fe1\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edfMIL\u65b9\u6cd5\u5728\u75c5\u7406\u5168\u5207\u7247\u56fe\u50cf\u5206\u6790\u4e2d\u4e22\u5931\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u800c\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u867d\u8868\u8fbe\u80fd\u529b\u5f3a\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u5b58\u5728\u5197\u4f59\u3002", "method": "\u63d0\u51faHookMIL\u6846\u67b6\uff1a\u5f15\u5165\u7d27\u51d1\u3001\u53ef\u5b66\u4e60\u7684hook token\u8fdb\u884c\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u805a\u5408\uff1b\u652f\u6301\u4ece\u5173\u952e\u56fe\u50cf\u5757\u7279\u5f81\u3001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6587\u672c\u5d4c\u5165\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4-\u89c6\u89c9\u6a21\u578b\u7279\u5f81\u4e09\u79cd\u65b9\u5f0f\u521d\u59cb\u5316\uff1b\u91c7\u7528\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\uff1b\u8bbe\u8ba1Hook Diversity Loss\u4fc3\u8fdbtoken\u4e13\u4e1a\u5316\uff1b\u52a0\u5165hook-to-hook\u901a\u4fe1\u673a\u5236\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u75c5\u7406\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "HookMIL\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709MIL\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u5f31\u76d1\u7763\u75c5\u7406\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u6848\u3002", "summary_cn": "\u591a\u793a\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5df2\u63a8\u52a8\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u7684\u5f31\u76d1\u7763\u5206\u6790\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684MIL\u65b9\u6cd5\u5e38\u5e38\u4e22\u5931\u5173\u952e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u800c\u57fa\u4e8eTransformer\u7684\u53d8\u4f53\u867d\u7136\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\uff0c\u5374\u9762\u4e34\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\u548c\u5197\u4f59\u8ba1\u7b97\u7684\u95ee\u9898\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\uff0c\u6211\u4eec\u63d0\u51fa\u4e86HookMIL\u2014\u2014\u4e00\u79cd\u517c\u987e\u4e0a\u4e0b\u6587\u611f\u77e5\u4e0e\u8ba1\u7b97\u6548\u7387\u7684MIL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u7d27\u51d1\u4e14\u53ef\u5b66\u4e60\u7684\u201c\u94a9\u5b50\u201d\uff08hook\uff09token\u8fdb\u884c\u7ed3\u6784\u5316\u7684\u4e0a\u4e0b\u6587\u805a\u5408\u3002\u8fd9\u4e9btoken\u53ef\u901a\u8fc7\u4ee5\u4e0b\u4e09\u79cd\u65b9\u5f0f\u521d\u59cb\u5316\uff1a(i) \u5173\u952e\u56fe\u50cf\u5757\u7684\u89c6\u89c9\u7279\u5f81\uff0c(ii) \u89c6\u89c9-\u8bed\u8a00\u75c5\u7406\u6a21\u578b\u7684\u6587\u672c\u5d4c\u5165\uff0c\u4ee5\u53ca(iii) \u7a7a\u95f4\u8f6c\u5f55\u7ec4-\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u7684\u7a7a\u95f4\u5b9a\u4f4d\u7279\u5f81\u3002\u8fd9\u79cd\u591a\u6a21\u6001\u521d\u59cb\u5316\u4f7fHook Token\u80fd\u591f\u878d\u5165\u4e30\u5bcc\u7684\u6587\u672c\u4e0e\u7a7a\u95f4\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u52a0\u901f\u6536\u655b\u5e76\u63d0\u5347\u8868\u5f81\u8d28\u91cf\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cHook token\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u5b9e\u4f8b\u8fdb\u884c\u4ea4\u4e92\u3002\u4e3a\u8fdb\u4e00\u6b65\u4fc3\u8fdb\u4e13\u4e1a\u5316\uff0c\u6211\u4eec\u5f15\u5165\u4e86Hook\u591a\u6837\u6027\u635f\u5931\uff08Hook Diversity Loss\uff09\uff0c\u9f13\u52b1\u6bcf\u4e2atoken\u805a\u7126\u4e8e\u4e0d\u540c\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u4e00\u79cdtoken\u95f4\u7684\u901a\u4fe1\u673a\u5236\u53ef\u4f18\u5316\u4e0a\u4e0b\u6587\u4ea4\u4e92\u5e76\u6700\u5c0f\u5316\u5197\u4f59\u3002\u5728\u56db\u4e2a\u516c\u5f00\u75c5\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHookMIL\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/lingxitong/HookMIL\u3002"}}
{"id": "2512.22193", "pdf": "https://arxiv.org/pdf/2512.22193", "abs": "https://arxiv.org/abs/2512.22193", "authors": ["Kenneth Xu", "Songhan Wu"], "title": "Tiny-YOLOSAM: Fast Hybrid Image Segmentation", "categories": ["cs.CV"], "comment": "7 pages, 6 figures, 5 tables. Code available at: https://github.com/Kenneth-Xu11566/tiny-yolosam", "summary": "The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its \"segment-everything\" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense \"segment-everything\" prompting for practical full-scene segmentation.", "AI": {"tldr": "Tiny-YOLOSAM combines YOLOv12 detection with TinySAM to achieve fast, high-coverage segmentation by using box prompts for salient objects and sparse point prompts only where needed, greatly improving speed and coverage over standard \"segment-everything\" approaches.", "motivation": "The Segment Anything Model (SAM) is too slow for latency-sensitive applications. Although TinySAM offers a lightweight alternative, its \"segment-everything\" mode remains inefficient due to requiring hundreds of prompts. A faster yet effective full-scene segmentation method is needed.", "method": "The authors first validate TinySAM on COCO val2017. Then they propose Tiny-YOLOSAM\u2014a hybrid pipeline that uses YOLOv12 to generate box prompts for foreground objects and adds sparse point prompts only in regions not covered by YOLO-guided masks.", "result": "On COCO val2017, Tiny-YOLOSAM boosts class-agnostic AR from 16.4% to 77.1% and mIoU from 19.2% to 67.8%, while reducing runtime from 49.20s/image to 10.39s/image (4.7\u00d7 faster) on an Apple M1 Pro CPU.", "conclusion": "Detector-guided prompting with targeted sparse sampling is a practical and efficient alternative to dense \u201csegment-everything\u201d strategies for full-scene segmentation.", "summary_cn": "Segment Anything Model\uff08SAM\uff09\u652f\u6301\u53ef\u63d0\u793a\u7684\u9ad8\u8d28\u91cf\u5206\u5272\uff0c\u4f46\u5728\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u573a\u666f\u4e2d\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\u3002TinySAM \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u84b8\u998f\u7248 SAM\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u96f6\u6837\u672c\u63a9\u7801\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5176\u201c\u5168\u56fe\u5206\u5272\u201d\u6a21\u5f0f\u4ecd\u9700\u6570\u767e\u4e2a\u63d0\u793a\uff0c\u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u8f83\u6162\u3002\u4f5c\u8005\u9996\u5148\u4f7f\u7528\u5b98\u65b9\u68c0\u67e5\u70b9\u5728 COCO val2017 \u4e0a\u590d\u73b0\u4e86 TinySAM\uff0c\u5176 AP \u6307\u6807\u4e0e\u539f\u6587\u62a5\u544a\u7ed3\u679c\u76f8\u5dee\u4e0d\u8d85\u8fc7 0.03%\uff0c\u5efa\u7acb\u4e86\u53ef\u9760\u7684\u5b9e\u9a8c\u57fa\u7ebf\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u6df7\u5408\u6d41\u6c34\u7ebf Tiny-YOLOSAM\uff1a\u5229\u7528\u6700\u65b0\u7684 YOLO \u68c0\u6d4b\u5668\uff08YOLOv12\uff09\u4e3a\u663e\u8457\u524d\u666f\u76ee\u6807\u751f\u6210\u6846\u63d0\u793a\uff0c\u5e76\u4ec5\u5728 YOLO \u5f15\u5bfc\u7684\u63a9\u7801\u672a\u8986\u76d6\u533a\u57df\u8865\u5145\u7a00\u758f\u70b9\u63d0\u793a\u3002\u5728 COCO val2017 \u4e0a\uff0c\u8be5\u6df7\u5408\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u522b\u65e0\u5173\u7684\u8986\u76d6\u7387\uff08AR \u4ece 16.4% \u63d0\u5347\u81f3 77.1%\uff0cmIoU \u4ece 19.2% \u63d0\u5347\u81f3 67.8%\uff09\uff0c\u540c\u65f6\u5728 Apple M1 Pro CPU \u4e0a\u5c06\u7aef\u5230\u7aef\u63a8\u7406\u65f6\u95f4\u4ece\u6bcf\u56fe 49.20 \u79d2\u964d\u81f3 10.39 \u79d2\uff08\u63d0\u901f 4.7 \u500d\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0c\u68c0\u6d4b\u5668\u5f15\u5bfc\u63d0\u793a\u7ed3\u5408\u9488\u5bf9\u6027\u7a00\u758f\u91c7\u6837\u662f\u5b9e\u73b0\u9ad8\u6548\u5168\u573a\u666f\u5206\u5272\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.22197", "pdf": "https://arxiv.org/pdf/2512.22197", "abs": "https://arxiv.org/abs/2512.22197", "authors": ["Shivum Telang"], "title": "Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy", "categories": ["cs.CV"], "comment": "4 pages, 6 figures", "summary": "Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u53ef\u89e3\u91ca\u6027\u6a21\u578b\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u6790\u773c\u5e95\u56fe\u50cf\u4e2d\u89c6\u7f51\u819c\u8c61\u9650\u7684\u75c5\u7076\u5206\u5e03\u6765\u6a21\u62df\u773c\u79d1\u533b\u751f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u751f\u6210\u914d\u5bf9\u7684Grad-CAM\u70ed\u56fe\uff0c\u540c\u65f6\u6574\u5408OCT\u548c\u773c\u5e95\u56fe\u50cf\u4ee5\u63d0\u5347\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u5206\u7c7b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u8bca\u65ad\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u4f9d\u8d56\u5355\u4e00\u6210\u50cf\u6a21\u6001\u7684AI\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u4ec5\u80fd\u9ad8\u4eae\u75c5\u7076\u4f4d\u7f6e\u800c\u65e0\u6cd5\u63d0\u4f9b\u63a8\u7406\u4f9d\u636e\uff1b\u4e8c\u662f\u4eba\u5de5\u6807\u6ce8\u75c5\u7076\u5bf9\u4e34\u5e8a\u533b\u751f\u800c\u8a00\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u4ee5\u81ea\u7136\u8bed\u8a00\u8bc6\u522b\u5e76\u89e3\u91caDR\u75c5\u7076\u3001\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u7684\u5b9a\u91cf\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u7b5b\u67e5\u3001\u6cbb\u7597\u548c\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u591a\u6a21\u6001\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5206\u6790\u773c\u5e95\u56fe\u50cf\u4e2d\u89c6\u7f51\u819c\u56db\u4e2a\u8c61\u9650\u5185\u7684\u75c5\u7076\u5206\u5e03\uff0c\u6a21\u62df\u773c\u79d1\u533b\u751f\u7684\u8bca\u65ad\u903b\u8f91\uff0c\u5e76\u751f\u6210\u914d\u5bf9\u7684Grad-CAM\u70ed\u56fe\uff0c\u53ef\u89c6\u5316OCT\u548c\u773c\u5e95\u56fe\u50cf\u4e2d\u5bf9DR\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u8d77\u5173\u952e\u4f5c\u7528\u7684\u533a\u57df\u3002", "result": "\u5728\u5305\u542b3,000\u5f20\u773c\u5e95\u56fe\u50cf\u548c1,000\u5f20OCT\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709DR\u8bca\u65ad\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u548c\u591a\u6a21\u6001\u878d\u5408\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u5b9e\u7528\u7684\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u53ef\u89e3\u91ca\u6027\u6a21\u578b\u4e0d\u4ec5\u63d0\u5347\u4e86DR\u5206\u7c7b\u7684\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u53ef\u4fe1\u5ea6\uff0c\u8fd8\u4e3a\u672a\u6765\u5728\u7b5b\u67e5\u3001\u6cbb\u7597\u89c4\u5212\u548c\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u663e\u8457\u6539\u5584\u60a3\u8005\u9884\u540e\u3002", "summary_cn": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u5168\u7403\u81f4\u76f2\u7684\u4e3b\u8981\u539f\u56e0\u4e4b\u4e00\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u4fdd\u62a4\u89c6\u529b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u533b\u7597\u8d44\u6e90\u6709\u9650\uff0cDR\u5e38\u88ab\u6f0f\u8bca\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u73b0\u6709AI\u6a21\u578b\u5e38\u91c7\u7528\u75c5\u7076\u5206\u5272\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u75c5\u7076\u5bf9\u4e34\u5e8a\u533b\u751f\u800c\u8a00\u4e0d\u5207\u5b9e\u9645\u3002\u533b\u751f\u66f4\u9700\u8981\u7684\u662f\u80fd\u591f\u89e3\u91ca\u5206\u7c7b\u4f9d\u636e\u7684\u6a21\u578b\uff0c\u800c\u975e\u4ec5\u6807\u51fa\u75c5\u7076\u4f4d\u7f6e\u3002\u6b64\u5916\uff0c\u5f53\u524d\u6a21\u578b\u591a\u4e3a\u5355\u6a21\u6001\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u6548\u679c\u6709\u9650\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e00\u79cd\u80fd\u4ee5\u81ea\u7136\u8bed\u8a00\u8bc6\u522b\u4e2a\u4f53DR\u75c5\u7076\u7684\u5b9a\u91cf\u68c0\u6d4b\u7cfb\u7edf\u5c06\u7a81\u7834\u4e0a\u8ff0\u9650\u5236\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u7b5b\u67e5\u3001\u6cbb\u7597\u548c\u7814\u7a76\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u53ef\u89e3\u91ca\u6027\u6a21\u578b\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u6790\u773c\u5e95\u56fe\u50cf\u4e2d\u89c6\u7f51\u819c\u8c61\u9650\u5185\u7684\u75c5\u7076\u5206\u5e03\u6765\u6a21\u62df\u773c\u79d1\u533b\u751f\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u8be5\u6a21\u578b\u751f\u6210\u914d\u5bf9\u7684Grad-CAM\u70ed\u56fe\uff0c\u5c55\u793aOCT\u548c\u773c\u5e95\u56fe\u50cf\u4e2d\u5404\u795e\u7ecf\u5143\u6743\u91cd\uff0c\u76f4\u89c2\u7a81\u51fa\u5f71\u54cdDR\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u7684\u5173\u952e\u533a\u57df\u3002\u57fa\u4e8e3,000\u5f20\u773c\u5e95\u56fe\u50cf\u548c1,000\u5f20OCT\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u8be5\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524dDR\u8bca\u65ad\u4e2d\u7684\u5173\u952e\u5c40\u9650\uff0c\u4e3a\u6539\u5584\u60a3\u8005\u9884\u540e\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u5168\u9762\u7684\u5de5\u5177\u3002"}}
{"id": "2512.22203", "pdf": "https://arxiv.org/pdf/2512.22203", "abs": "https://arxiv.org/abs/2512.22203", "authors": ["Qiang Guo", "Rubo Zhang", "Bingbing Zhang", "Junjie Liu", "Jianqing Liu"], "title": "TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTCFormer\uff0c\u4e00\u4e2a\u4ec5\u542b500\u4e07\u53c2\u6570\u7684\u8d85\u8f7b\u91cf\u7ea7\u5f31\u76d1\u7763Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u5bc6\u5ea6\u52a0\u6743\u5e73\u5747\u6a21\u5757\u548c\u5bc6\u5ea6\u7b49\u7ea7\u5206\u7c7b\u635f\u5931\uff0c\u5728\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u8ba1\u6570\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u4eba\u7fa4\u8ba1\u6570\u3002", "motivation": "\u4eba\u7fa4\u8ba1\u6570\u901a\u5e38\u4f9d\u8d56\u8017\u65f6\u8d39\u529b\u7684\u70b9\u7ea7\u6807\u6ce8\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u89c6\u89c9Transformer\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff1b\u8bbe\u8ba1\u53ef\u5b66\u4e60\u5bc6\u5ea6\u52a0\u6743\u5e73\u5747\uff08Learnable Density-Weighted Averaging\uff09\u6a21\u5757\u4ee5\u52a8\u6001\u91cd\u52a0\u6743\u5c40\u90e8token\uff1b\u5f15\u5165\u5bc6\u5ea6\u7b49\u7ea7\u5206\u7c7b\u635f\u5931\uff0c\u5c06\u4eba\u7fa4\u5bc6\u5ea6\u79bb\u6563\u4e3a\u591a\u4e2a\u7b49\u7ea7\u4ee5\u6b63\u5219\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728ShanghaiTech A/B\u3001UCF-QNRF\u548cNWPU\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTCFormer\u5728\u53c2\u6570\u6548\u7387\u4e0e\u8ba1\u6570\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\u3002", "conclusion": "TCFormer\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u9ad8\u6548\u4eba\u7fa4\u8ba1\u6570\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u5f31\u76d1\u7763\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\u3002", "summary_cn": "\u4eba\u7fa4\u8ba1\u6570\u901a\u5e38\u4f9d\u8d56\u4e8e\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u70b9\u7ea7\u6807\u6ce8\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u80fd\u529b\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86TCFormer\u2014\u2014\u4e00\u79cd\u5fae\u578b\u3001\u8d85\u8f7b\u91cf\u7ea7\u7684\u5f31\u76d1\u7763Transformer\u4eba\u7fa4\u8ba1\u6570\u6846\u67b6\uff0c\u4ec5\u5305\u542b500\u4e07\u53c2\u6570\u5374\u80fd\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u9996\u5148\uff0c\u91c7\u7528\u4e00\u4e2a\u5f3a\u5927\u800c\u9ad8\u6548\u7684\u89c6\u89c9Transformer\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5176\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u53ef\u5728\u6781\u5c0f\u5185\u5b58\u5360\u7528\u4e0b\u63d0\u4f9b\u8bed\u4e49\u4e30\u5bcc\u7684\u4eba\u7fa4\u7279\u5f81\u3002\u5176\u6b21\uff0c\u4e3a\u5f25\u8865\u7a7a\u95f4\u76d1\u7763\u4fe1\u606f\u7684\u7f3a\u5931\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u53ef\u5b66\u4e60\u5bc6\u5ea6\u52a0\u6743\u5e73\u5747\u201d\uff08Learnable Density-Weighted Averaging\uff09\u7684\u7279\u5f81\u805a\u5408\u673a\u5236\uff0c\u8be5\u6a21\u5757\u6839\u636e\u9884\u6d4b\u7684\u5bc6\u5ea6\u5206\u6570\u52a8\u6001\u5730\u5bf9\u5c40\u90e8token\u8fdb\u884c\u91cd\u52a0\u6743\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u6839\u636e\u5404\u533a\u57df\u7684\u5177\u4f53\u5bc6\u5ea6\u7279\u6027\u81ea\u9002\u5e94\u5730\u8c03\u8282\u7279\u5f81\uff0c\u800c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002\u6b64\u5916\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u5bc6\u5ea6\u7b49\u7ea7\u5206\u7c7b\u635f\u5931\uff0c\u5c06\u4eba\u7fa4\u5bc6\u5ea6\u79bb\u6563\u4e3a\u82e5\u5e72\u7b49\u7ea7\uff0c\u4ece\u800c\u6b63\u5219\u5316\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u540c\u5bc6\u5ea6\u7ea7\u522b\u4e0b\u7684\u5206\u7c7b\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u5c3d\u7ba1TCFormer\u4ec5\u5229\u7528\u56fe\u50cf\u7ea7\u5168\u5c40\u8ba1\u6570\u8fdb\u884c\u5f31\u76d1\u7763\u8bad\u7ec3\uff0c\u4f46\u901a\u8fc7\u5bf9\u8ba1\u6570\u635f\u5931\u548c\u5bc6\u5ea6\u7b49\u7ea7\u635f\u5931\u7684\u8054\u5408\u4f18\u5316\uff0c\u8be5\u6846\u67b6\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u4f30\u8ba1\u3002\u5728\u5305\u62ecShanghaiTech A/B\u3001UCF-QNRF\u548cNWPU\u5728\u5185\u7684\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53c2\u6570\u6548\u7387\u4e0e\u8ba1\u6570\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u5e73\u8861\uff0c\u53ef\u6210\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4eba\u7fa4\u8ba1\u6570\u4efb\u52a1\u7684\u826f\u597d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22205", "pdf": "https://arxiv.org/pdf/2512.22205", "abs": "https://arxiv.org/abs/2512.22205", "authors": ["Md. Ismiel Hossen Abir", "Awolad Hossain"], "title": "A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Malaria remains a prevalent health concern in regions with tropical and subtropical climates. The cause of malaria is the Plasmodium parasite, which is transmitted through the bites of infected female Anopheles mosquitoes. Traditional diagnostic methods, such as microscopic blood smear analysis, are low in sensitivity, depend on expert judgment, and require resources that may not be available in remote settings. To overcome these limitations, this study proposes a deep learning-based approach utilizing a custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. This study also compares the custom CNN with established deep learning architectures, including ResNet50, VGG16, MobileNetV2, and DenseNet121. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied. The proposed system shows how deep learning can provide quick, accurate and understandable malaria diagnosis, especially in areas with limited resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u5b9a\u4e49CNN\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u759f\u539f\u866b\u611f\u67d3\u7684\u8840\u7ec6\u80de\u56fe\u50cf\uff0c\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u5730\u533a\u3002", "motivation": "\u4f20\u7edf\u759f\u75be\u8bca\u65ad\u65b9\u6cd5\uff08\u5982\u663e\u5fae\u955c\u8840\u6d82\u7247\u5206\u6790\uff09\u7075\u654f\u5ea6\u4f4e\u3001\u4f9d\u8d56\u4e13\u5bb6\u5224\u65ad\u4e14\u5728\u504f\u8fdc\u5730\u533a\u7f3a\u4e4f\u5fc5\u8981\u8d44\u6e90\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u90e8\u7f72\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u65b9\u6848\u3002", "method": "\u91c7\u7528\u81ea\u5b9a\u4e49\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u8840\u7ec6\u80de\u56fe\u50cf\u8fdb\u884c\u81ea\u52a8\u5206\u7c7b\uff08\u611f\u67d3/\u672a\u611f\u67d3\uff09\uff0c\u5e76\u4e0eResNet50\u3001VGG16\u3001MobileNetV2\u548cDenseNet121\u7b49\u4e3b\u6d41\u67b6\u6784\u8fdb\u884c\u6bd4\u8f83\uff1b\u540c\u65f6\u5e94\u7528SHAP\u3001LIME\u548c\u663e\u8457\u56fe\u7b49\u53ef\u89e3\u91caAI\u6280\u672f\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6240\u63d0\u6a21\u578b\u51c6\u786e\u7387\u8fbe96%\uff0c\u4e24\u7c7b\u522b\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u5747\u8d85\u8fc70.95\uff0c\u4f18\u4e8e\u6216\u5ab2\u7f8e\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5176\u51b3\u7b56\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u6df1\u5ea6\u5b66\u4e60\u53ef\u5728\u8d44\u6e90\u53d7\u9650\u5730\u533a\u5b9e\u73b0\u5feb\u901f\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u759f\u75be\u8bca\u65ad\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u4e0e\u516c\u5171\u536b\u751f\u5e94\u7528\u6f5c\u529b\u3002", "summary_cn": "\u759f\u75be\u5728\u70ed\u5e26\u548c\u4e9a\u70ed\u5e26\u5730\u533a\u4ecd\u662f\u4e00\u4e2a\u666e\u904d\u7684\u5065\u5eb7\u95ee\u9898\uff0c\u7531\u53d7\u611f\u67d3\u7684\u96cc\u6027\u6309\u868a\u53ee\u54ac\u4f20\u64ad\u759f\u539f\u866b\u5f15\u8d77\u3002\u4f20\u7edf\u7684\u8bca\u65ad\u65b9\u6cd5\uff08\u5982\u663e\u5fae\u955c\u4e0b\u8840\u6d82\u7247\u5206\u6790\uff09\u7075\u654f\u5ea6\u8f83\u4f4e\uff0c\u4f9d\u8d56\u4e13\u5bb6\u5224\u65ad\uff0c\u4e14\u5728\u504f\u8fdc\u5730\u533a\u5f80\u5f80\u7f3a\u4e4f\u6240\u9700\u8d44\u6e90\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u5b9a\u4e49\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u81ea\u52a8\u5c06\u8840\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u4e3a\u611f\u67d3\u6216\u672a\u611f\u67d3\u3002\u8be5\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523096%\uff0c\u4e24\u7c7b\u522b\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u5747\u8d85\u8fc70.95\u3002\u7814\u7a76\u8fd8\u5c06\u5176\u4e0eResNet50\u3001VGG16\u3001MobileNetV2\u548cDenseNet121\u7b49\u6210\u719f\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u4e3a\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u91c7\u7528\u4e86SHAP\u3001LIME\u548c\u663e\u8457\u56fe\u7b49\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u3002\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5982\u4f55\u5728\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u63d0\u4f9b\u5feb\u901f\u3001\u51c6\u786e\u4e14\u53ef\u7406\u89e3\u7684\u759f\u75be\u8bca\u65ad\u3002"}}
