<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UI-Venus-1.5 Technical Report](https://arxiv.org/abs/2602.09082)
*Veuns-Team,:,Changlong Gao,Zhangxuan Gu,Yulin Liu,Xinyu Qiu,Shuheng Shen,Yue Wen,Tianyu Xia,Zhenyu Xu,Zhengwen Zeng,Beitong Zhou,Xingran Zhou,Weizhi Chen,Sunhao Dai,Jingya Dou,Yichen Gong,Yuan Guo,Zhenlin Guo,Feng Li,Qian Li,Jinzhen Lin,Yuqi Zhou,Linchao Zhu,Liang Chen,Zhenyu Guo,Changhua Meng,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus-1.5 是一个统一的端到端 GUI 智能体，在多个基准测试中达到 SOTA，并在中文移动应用中展现出强大的真实场景执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有 GUI 智能体难以同时兼顾通用性和任务性能，因此需要构建一个更强大、统一且适用于真实世界应用的智能体。

Method: UI-Venus-1.5 引入三项关键技术：（1）基于 30+ 数据集、100 亿 token 的中期训练以建立 GUI 语义基础；（2）使用完整轨迹展开的在线强化学习，对齐长视野动态导航目标；（3）通过模型融合将领域专用模型（定位、网页、移动端）整合为单一统一智能体。模型包含 2B、8B 密集版本和 30B-A3B 混合专家版本。

Result: 在 ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和 AndroidWorld（77.6%）等基准上显著超越先前强基线，同时在多种中文移动应用中展现稳健的导航与指令执行能力。

Conclusion: UI-Venus-1.5 通过统一架构和三项关键技术，成功实现了高通用性与高性能的结合，为真实世界 GUI 自动化提供了有效解决方案。

Abstract: GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus

Abstract (中文翻译): GUI 智能体已成为自动化数字环境交互的强大范式，但实现广泛的通用性和持续强劲的任务性能仍具挑战。在本报告中，我们提出了 UI-Venus-1.5，这是一个面向鲁棒现实世界应用的统一端到端 GUI 智能体。该模型系列包含两个密集版本（2B 和 8B）和一个混合专家版本（30B-A3B），以满足不同的下游应用场景。相比前一版本，UI-Venus-1.5 引入了三项关键技术进展：（1）一个全面的中期训练阶段，利用 30 多个数据集中的 100 亿 token 建立基础 GUI 语义；（2）采用完整轨迹展开的在线强化学习，使训练目标与大规模环境中长视野、动态导航对齐；（3）通过模型融合构建单一统一的 GUI 智能体，将领域专用模型（定位、网页和移动端）整合为一个连贯的检查点。大量评估表明，UI-Venus-1.5 在 ScreenSpot-Pro（69.6%）、VenusBench-GD（75.0%）和 AndroidWorld（77.6%）等基准上建立了新的最先进性能，显著优于此前的强基线。此外，UI-Venus-1.5 在多种中文移动应用中展现出稳健的导航能力，能够有效在真实场景中执行用户指令。代码：https://github.com/inclusionAI/UI-Venus；模型：https://huggingface.co/collections/inclusionAI/ui-venus

</details>


### [2] [Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling](https://arxiv.org/abs/2602.09084)
*Ruijie Ye,Jiayi Zhang,Zhuoxin Liu,Zihao Zhu,Siyuan Yang,Li Li,Tianfu Fu,Franck Dernoncourt,Yue Zhao,Jiacheng Zhu,Ryan Rossi,Wenhao Chai,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出Agent Banana框架，通过Context Folding和Image Layer Decomposition机制，解决专业图像编辑中过度编辑、多轮编辑对象失真及高清评估缺失三大挑战，在新构建的4K高分辨率多轮对话基准HDD-Bench上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法在专业工作流中面临三大问题：(i) 编辑器常过度修改，超出用户意图；(ii) 多数模型仅支持单轮编辑，而多轮编辑易导致对象失真；(iii) 当前评估通常在约1K分辨率下进行，与实际使用的超高清（如4K）图像工作流不匹配。

Method: 提出Agent Banana，一种分层的智能体规划-执行框架，包含两个核心机制：(1) Context Folding，将长交互历史压缩为结构化记忆，以实现稳定的长程控制；(2) Image Layer Decomposition，通过局部图层编辑保留非目标区域，并支持原生分辨率输出。同时构建了高分辨率对话基准HDD-Bench用于评估。

Result: 在HDD-Bench上，Agent Banana在多轮一致性（IC 0.871）和背景保真度（SSIM-OM 0.84, LPIPS-OM 0.12）方面表现最佳，同时在指令遵循任务上保持竞争力，并在标准单轮编辑基准上也取得优异性能。

Conclusion: 本工作通过提出Agent Banana框架和HDD-Bench基准，推动了可靠、专业级智能体图像编辑的发展，并促进其在真实工作流中的集成。

Abstract: We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.

Abstract (中文翻译): 我们研究了专业工作流下的基于指令的图像编辑，并识别出三个持续存在的挑战：(i) 编辑器经常过度编辑，修改超出用户意图的内容；(ii) 现有模型大多是单轮的，而多轮编辑会改变对象的真实性；(iii) 在约1K分辨率下的评估与通常操作超高清图像（例如4K）的真实工作流不一致。我们提出了Agent Banana，一种用于高保真、对象感知、深思熟虑编辑的分层智能体规划-执行框架。Agent Banana引入了两个关键机制：(1) 上下文折叠（Context Folding），将长交互历史压缩成结构化记忆，以实现稳定的长程控制；(2) 图像图层分解（Image Layer Decomposition），执行局部图层编辑以保留非目标区域，同时支持原生分辨率输出。为了支持严格的评估，我们构建了HDD-Bench，一个高分辨率、基于对话的基准，具有可验证的逐步目标和原生4K图像（1180万像素），用于诊断长程失败。在HDD-Bench上，Agent Banana在多轮一致性和背景保真度方面达到最佳（例如，IC 0.871，SSIM-OM 0.84，LPIPS-OM 0.12），同时在指令遵循方面保持竞争力，并在标准单轮编辑基准上也取得了强劲的性能。我们希望这项工作能推动可靠、专业级智能体图像编辑及其在真实工作流中的集成。

</details>


### [3] [SemanticMoments: Training-Free Motion Similarity via Third Moment Features](https://arxiv.org/abs/2602.09146)
*Saar Huberman,Kfir Goldberg,Or Patashnik,Sagie Benaim,Ron Mokady*

Main category: cs.CV

TL;DR: 现有视频检索方法过于依赖静态外观而忽视语义运动，本文提出无需训练的SemanticMoments方法，通过在预训练语义特征上计算高阶时间统计量，在新构建的SimMotion基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频表征方法因训练数据和目标的偏差，过度依赖静态外观和场景上下文，而传统基于光流等运动中心的方法又缺乏高层语义理解能力，导致在语义运动检索任务上表现不佳。

Method: 提出SemanticMoments方法：一种无需训练的策略，通过对预训练语义模型提取的特征计算时间维度上的高阶矩（如方差、偏度等）来捕捉语义运动信息。

Result: 在新构建的SimMotion基准（包含合成数据与人工标注真实数据）上，SemanticMoments一致优于现有的RGB、光流及文本监督方法，证明了其有效性。

Conclusion: 在语义特征空间中使用时间统计量，为以运动为中心的视频理解提供了一种可扩展且感知合理的解决方案。

Abstract: Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

Abstract (中文翻译): 基于语义运动的视频检索是一个基础但尚未解决的问题。现有的视频表征方法过度依赖静态外观和场景上下文，而非运动动态，这种偏差源于其训练数据和目标。相反，传统的以运动为中心的输入（如光流）缺乏理解高层运动所需的语义基础。为揭示这一固有偏差，我们提出了SimMotion基准，结合了受控的合成数据和新的人工标注真实世界数据集。我们发现现有模型在这些基准上表现不佳，常常无法将运动与外观解耦。为弥补这一差距，我们提出了SemanticMoments——一种简单且无需训练的方法，通过对预训练语义模型的特征计算时间统计量（特别是高阶矩）来表征语义运动。在我们的基准测试中，SemanticMoments始终优于现有的RGB、光流和文本监督方法。这表明，在语义特征空间中使用时间统计量，为以运动为中心的视频理解提供了可扩展且感知合理的基石。

</details>


### [4] [A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video](https://arxiv.org/abs/2602.09154)
*Andrea Filiberto Lucas,Dylan Seychell*

Main category: cs.CV

TL;DR: 本文提出了一种可解释、模块化的新闻视频人名提取框架，在保证可审计性和无幻觉的前提下，实现了与生成式方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 随着视频新闻内容激增，亟需透明可靠的方法从屏幕中提取信息；但新闻图形布局多样、平台设计差异大，使得人工标注不现实，而现有生成式方法缺乏可审计性，难以满足新闻业对数据溯源的要求。

Method: 构建了一个涵盖当代新闻图形多样性的标注帧语料库，并设计了一个可解释、模块化的确定性提取流程，用于自动检测和提取广播及社交媒体原生新闻视频中的个人姓名。

Result: 所提方法在图形元素定位上达到95.8% mAP@0.5；人名提取F1为77.08%（精度79.9%，召回率74.4%），虽略低于生成式方法（84.18%），但具备全程可追溯性且无幻觉；用户调研显示59%受访者难以在快节奏播报中看清屏幕人名。

Conclusion: 该研究为现代新闻媒体中的混合多模态信息提取建立了方法严谨、可解释的基线，平衡了性能与透明度，更适合新闻分析等需审计的应用场景。

Abstract: The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.
  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.

Abstract (中文翻译): 视频新闻内容的不断增长，提高了对透明且可靠方法的需求，以从中提取屏幕上的信息。然而，图形布局、排版惯例和平台特定设计模式的多样性，使得人工索引变得不切实际。本研究提出了一种全面的框架，用于自动检测和提取广播及社交媒体原生新闻视频中的个人姓名。该框架引入了一个经过精心整理且均衡的标注帧语料库，捕捉了当代新闻图形的多样性，并设计了一种可解释、模块化的提取流水线，可在确定性和可审计的条件下运行。该流水线与一类生成式多模态方法进行了对比评估，揭示了确定性可审计性与随机推理之间的明显权衡。底层检测器在图形元素定位任务中达到了95.8%的mAP@0.5，展现出稳健的实用性能。尽管生成式系统在原始准确率上略高（F1：84.18% 对比 77.08%），但它们缺乏新闻和分析场景所需的透明数据溯源能力。所提出的流水线在精确率（79.9%）和召回率（74.4%）之间取得了良好平衡，避免了幻觉，并在每个处理阶段提供完整的可追溯性。补充的用户调研表明，59%的受访者表示在快节奏的新闻播报中难以看清屏幕上的姓名，凸显了该任务的实际意义。研究结果为现代新闻媒体中的混合多模态信息提取建立了一个方法严谨且可解释的基线。

</details>


### [5] [Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images](https://arxiv.org/abs/2602.09155)
*Ahmed Rahu,Brian Shula,Brandon Combs,Aqsa Sultana,Surendra P. Singh,Vijayan K. Asari,Derrick Forchetti*

Main category: cs.CV

TL;DR: 本研究利用卷积神经网络（CNN）分析低级别管状腺瘤的全切片图像，以识别可预测患者未来罹患结直肠癌风险的细微组织学特征。


<details>
  <summary>Details</summary>
Motivation: 尽管已有筛查手段可降低结直肠癌（CRC）发病率，但部分最初被诊断为低级别腺瘤的患者仍会在日后发展为CRC。目前缺乏有效方法识别这些“低风险”患者中实际具有高进展风险的个体，从而实现精准监测和预防。

Method: 采用卷积神经网络（CNN）对低级别管状腺瘤的全切片图像（WSI）进行深度学习分析，以挖掘传统病理学难以察觉的、与恶性转化相关的细微组织结构或细胞学特征。

Result: 论文尚未报告具体结果，但提出了一种基于机器学习的方法，有望从常规病理图像中提取出预测CRC长期风险的生物标志物。

Conclusion: 结合数字病理与深度学习技术，有望超越传统组织学评估的局限，为低级别腺瘤患者提供更精准的风险分层，指导个体化随访和干预策略。

Abstract: Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.

Abstract (中文翻译): 尽管已广泛实施旨在检测和切除癌前息肉的预防性措施，结直肠癌（CRC）仍然是癌症相关死亡的重要原因。虽然筛查能有效降低发病率，但相当一部分最初被诊断为低级别腺瘤性息肉的患者，即使不存在已知的高危综合征，仍会在日后发展为CRC。识别哪些低风险患者具有更高的进展风险，是实现个体化监测和预防性治疗策略的关键未满足需求。传统的腺瘤组织学评估虽为基础，但可能无法充分捕捉预示恶性潜能的细微结构或细胞学特征。数字病理学和机器学习的进步为全面、客观地分析全切片图像（WSIs）提供了契机。本研究探讨了机器学习算法，特别是卷积神经网络（CNNs），是否能够检测低级别管状腺瘤WSIs中可预测患者长期CRC风险的细微组织学特征。

</details>


### [6] [All-in-One Conditioning for Text-to-Image Synthesis](https://arxiv.org/abs/2602.09165)
*Hirunima Jayasekara,Chuong Huynh,Yixuan Ren,Christabel Acquaye,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文提出一种基于场景图的零样本条件机制，通过ASQL Conditioner在推理时生成软视觉引导，提升文本到图像生成模型对复杂提示的语义保真度与结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在处理包含多个对象、属性和空间关系的复杂提示时，难以保持语义准确性和结构连贯性；现有方法依赖预定义布局图，限制了组合灵活性和多样性。

Method: 引入一种零样本、基于场景图的条件机制，在推理阶段通过轻量级语言模型生成Attribute-Size-Quantity-Location（ASQL）视觉条件，并结合扩散模型进行优化，实现软引导图像生成。

Result: 该方法在保持文本-图像对齐的同时，支持轻量、一致且多样化的图像合成，有效提升了对复杂提示的理解与可视化能力。

Conclusion: 基于场景图的ASQL条件机制为文本到图像合成提供了一种灵活而有效的解决方案，显著增强了模型对复杂语义结构的组合生成能力。

Abstract: Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

Abstract (中文翻译): 准确理解和可视化包含多个对象、属性及空间关系的复杂提示是文本到图像合成中的关键挑战。尽管近期在生成逼真图像方面取得了进展，但现有模型在处理复杂文本输入时仍常难以维持语义保真度和结构一致性。我们提出一种新方法，将文本到图像合成置于场景图结构框架下，以增强现有模型的组合能力。尽管先前方法尝试通过从提示中提取的预定义布局图来解决此问题，但此类刚性约束往往限制了组合的灵活性与多样性。相比之下，我们引入了一种零样本、基于场景图的条件机制，在推理过程中生成软性视觉引导。该方法的核心是“属性-尺寸-数量-位置”（ASQL）条件模块，它通过一个轻量级语言模型生成视觉条件，并在推理时通过优化引导扩散模型生成图像。这使得模型在保持文本-图像对齐的同时，能够实现轻量、一致且多样化的图像合成。

</details>


### [7] [Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain](https://arxiv.org/abs/2602.09209)
*Michael D. Murray,James Tung,Richard W. Nuckols*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级CNN-RNN模型，利用RGB-D相机视觉数据在脚触地前250ms内预测足底前后方向的压力中心（COP）和触地时间（TOI），在楼梯上行过渡任务中实现了高精度预测，并可在消费级设备上实时运行（60 FPS）。


<details>
  <summary>Details</summary>
Motivation: 现有基于计算机视觉的环境分类方法虽常用于辅助系统控制，但对足部如何与变化环境接触的预测能力尚未充分探索。本研究旨在填补这一空白，通过视觉信息提前预测足底接触特性，为辅助系统的前瞻性控制提供支持。

Method: 受试者佩戴RGB-D相机（置于右小腿）和仪器化鞋垫，在平地到楼梯上行过渡过程中行走。研究团队训练了一个CNN-RNN模型，在脚触地前250ms的时间窗口内连续预测前后方向的足底压力中心（COP）和触地时间（TOI）。

Result: 在150、100和50ms的预测时域下，COP的平均绝对误差分别为29.42mm、26.82mm和23.72mm；TOI的误差分别为21.14ms、20.08ms和17.73ms。更快的脚趾摆动速度可提高COP预测精度，但对TOI无显著影响；更靠前的落脚位置会降低COP预测精度，但不影响TOI。所提轻量模型可在笔记本或边缘设备上以60 FPS运行。

Conclusion: 利用视觉数据通过轻量级模型预测COP和TOI是可行的，该方法有望用于提升辅助系统的前瞻性控制能力。

Abstract: Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

Abstract (中文翻译): 计算机视觉（CV）已被用于步态过程中的环境分类，并常用于辅助系统的控制决策；然而，预测足部如何与变化环境接触的能力仍鲜有研究。我们评估了在从平地过渡到上楼梯过程中，于脚触地前预测前后方向（AP）足底压力中心（COP）和触地时间（TOI）的可行性。八名受试者在右小腿佩戴RGB-D相机并穿着仪器化鞋垫，执行踏上楼梯的任务。我们训练了一个CNN-RNN模型，在脚触地前250毫秒的时间窗口（称为预测时域，FH）内连续预测COP和TOI。在150、100和50毫秒的FH下，COP的平均绝对误差（MAE）分别为29.42毫米、26.82毫米和23.72毫米；TOI的MAE则分别为21.14毫秒、20.08毫秒和17.73毫秒。躯干速度对两项任务的预测误差均无影响；而脚触地前更快的脚趾摆动速度可提高COP预测精度，但对TOI预测无显著影响。此外，更靠前的落脚位置会降低COP预测精度，但不影响TOI预测精度。我们还发现，所提出的轻量级模型可在消费级笔记本电脑或边缘计算设备上以60帧每秒（FPS）的速度运行。本研究表明，利用视觉数据通过轻量级模型预测COP和TOI是可行的，这对辅助系统中的前瞻性控制具有重要意义。

</details>


### [8] [VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models](https://arxiv.org/abs/2602.09214)
*Chenyu Wang,Tianle Chen,H. M. Sabbir Ahmad,Kayhan Batmanghelich,Wenchao Li*

Main category: cs.CV

TL;DR: 本文提出了VLM-UQBench，一个用于评估视觉语言模型（VLM）中模态特异性和跨模态数据不确定性的基准，并通过扰动实验发现现有不确定性量化（UQ）方法在细粒度、模态感知的不确定性识别方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 为了确保视觉语言模型（VLM）的安全可靠运行，需要对其不确定性进行量化（UQ），但当前缺乏能定位不确定性来源（图像、文本或两者对齐问题）的有效方法和评估基准。

Method: 作者构建了VLM-UQBench基准，包含600个来自VizWiz的真实样本，划分为干净、图像不确定性、文本不确定性和跨模态不确定性子集，并设计了一个包含8种视觉、5种文本和3种跨模态扰动的可扩展扰动流程。同时提出了两个新指标，用于衡量UQ分数对扰动的敏感性及其与幻觉的相关性，并在4个VLM和3个数据集上评估多种UQ方法。

Result: 研究发现：(i) 现有UQ方法具有强烈的模态特异性且高度依赖底层VLM；(ii) 模态特异性不确定性常伴随幻觉，但当前UQ分数仅提供微弱且不一致的风险信号；(iii) UQ方法虽能在明显、群体层面的模糊性上媲美思维链基线，却难以检测扰动引入的细微、实例级模糊性。

Conclusion: 当前UQ实践与可靠部署VLM所需的细粒度、模态感知不确定性之间存在显著差距，亟需发展更精细的不确定性量化方法。

Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

Abstract (中文翻译): 不确定性量化（UQ）对于确保视觉语言模型（VLM）安全可靠地运行至关重要。一个核心挑战在于将不确定性定位到其来源，即判断不确定性是源于图像、文本，还是两者之间的错位。我们提出了VLM-UQBench，这是一个用于评估VLM中模态特异性和跨模态数据不确定性的基准。它包含从VizWiz数据集中抽取的600个真实世界样本，这些样本被整理为干净、图像不确定性、文本不确定性和跨模态不确定性子集，并配备了一个可扩展的扰动流程，包含8种视觉、5种文本和3种跨模态扰动。我们进一步提出了两个简单指标，用于量化UQ分数对这些扰动的敏感性及其与幻觉的相关性，并利用它们在四个VLM和三个数据集上评估了一系列UQ方法。实证结果表明：(i) 现有的UQ方法表现出强烈的模态特异性专业化，并且严重依赖于底层VLM；(ii) 模态特异性不确定性经常与幻觉共存，而当前的UQ分数仅提供微弱且不一致的风险信号；(iii) 尽管UQ方法在明显的、群体层面的模糊性上可以与基于推理的思维链基线相媲美，但它们在检测由我们的扰动流程引入的细微、实例层面的模糊性方面基本失败。这些结果突显了当前UQ实践与可靠部署VLM所需的细粒度、模态感知不确定性之间的显著差距。

</details>


### [9] [VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models](https://arxiv.org/abs/2602.09252)
*Ange Lou,Yamin Li,Qi Chang,Nan Xi,Luyuan Xie,Zichao Li,Tianyu Luan*

Main category: cs.CV

TL;DR: 提出IR-SIS：首个支持自然语言交互、具备自适应迭代优化能力的手术图像分割系统，在域内和域外数据上均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有手术图像分割方法受限于预定义类别、缺乏自适应优化机制，且无法与临床医生进行交互，难以满足实际手术场景需求。

Method: IR-SIS系统结合微调后的SAM3进行初始分割，利用视觉-语言模型识别器械并评估分割质量，并通过智能体工作流自适应选择优化策略；同时支持医生通过自然语言反馈参与分割过程。此外，构建了基于EndoVis2017/2018的多粒度语言标注数据集。

Result: 在域内和分布外数据上均取得当前最优性能，且引入临床医生交互后性能进一步提升。

Conclusion: 本研究建立了首个基于语言、具备自适应自我优化能力的手术图像分割框架，为机器人辅助手术和术中导航提供了新范式。

Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

Abstract (中文翻译): 手术图像分割对于机器人辅助手术和术中导航至关重要。然而，现有方法局限于预定义类别，仅提供一次性预测而缺乏自适应优化能力，并且缺少与临床医生交互的机制。我们提出了IR-SIS——一种支持自然语言描述的手术图像分割迭代优化系统。IR-SIS利用微调后的SAM3进行初始分割，采用视觉-语言模型检测手术器械并评估分割质量，并通过智能体工作流自适应地选择优化策略。该系统支持临床医生通过自然语言反馈参与分割过程。我们还基于EndoVis2017和EndoVis2018基准构建了一个多粒度语言标注数据集。实验表明，该方法在域内和分布外数据上均达到了最先进的性能，且临床医生的交互进一步提升了分割效果。我们的工作建立了首个基于语言、具备自适应自我优化能力的手术图像分割框架。

</details>


### [10] [Rethinking Global Text Conditioning in Diffusion Transformers](https://arxiv.org/abs/2602.09268)
*Nikita Starodubcev,Daniil Pakhomov,Zongze Wu,Ilya Drobyshevskiy,Yuchen Liu,Zhonghao Wang,Yuqian Zhou,Zhe Lin,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: 本文探讨了在扩散Transformer中基于调制（modulation）的文本条件是否必要，发现传统用法下其贡献有限，但若将其作为引导信号以可控方式调整生成结果，则可在无需训练的情况下显著提升多种任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer模型通常通过注意力机制和基于池化文本嵌入的调制机制引入文本信息，但近期方法倾向于仅使用注意力机制。作者旨在探究调制机制是否仍有必要，以及是否能带来性能优势。

Method: 作者分析了传统调制机制中池化文本嵌入的作用，并提出一种新视角：将池化嵌入用作引导信号，以实现对生成结果的可控调整，该方法无需额外训练、易于实现且运行开销极小。

Result: 实验表明，在传统用法下池化嵌入对性能提升有限；但作为引导信号使用时，可在文本到图像/视频生成和图像编辑等多种任务中带来显著改进。

Conclusion: 调制机制在传统形式下并非必要，但若重新设计其用途（如作为引导），则可有效增强扩散模型的可控性与性能，且具有通用性和高效性。

Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

Abstract (中文翻译): 扩散Transformer通常通过注意力层和使用池化文本嵌入的调制机制来融合文本信息。然而，近期方法摒弃了基于调制的文本条件，仅依赖注意力机制。本文探讨了基于调制的文本条件是否必要，以及是否能带来性能优势。我们的分析表明，在其传统用法中，池化嵌入对整体性能贡献甚微，说明仅靠注意力机制通常已足以准确传递提示信息。然而，我们发现若从不同视角使用池化嵌入——将其作为引导信号，实现向更理想属性的可控偏移——则可带来显著增益。该方法无需训练、实现简单、运行时开销可忽略，并可应用于多种扩散模型，在包括文本到图像/视频生成和图像编辑在内的多种任务中均取得改进。

</details>
