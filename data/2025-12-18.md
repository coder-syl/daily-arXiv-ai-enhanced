<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation](https://arxiv.org/abs/2512.14755)
*Paul Weinmann,Ferdinand Schenck,Martin Šiklar*

Main category: cs.CV

TL;DR: 本文提出SkyCap数据集，融合高分辨率光学与SAR影像用于线性基础设施变化检测，并通过光学到SAR的标签迁移生成SAR变化标签。实验发现，经适当预处理的光学基础模型在SAR幅度变化检测任务上优于专门针对SAR微调的基础模型。


<details>
  <summary>Details</summary>
Motivation: 光学高分辨率影像易受云层干扰，而SAR虽可全天候成像但难以标注。为解决这一矛盾，需构建融合两类数据的基准并探索无需SAR专家标注的变化检测方法。

Method: 构建SkyCap双时相VHR光学-SAR数据集，通过光学至SAR的标签迁移获得SAR幅度变化检测标签；对SARATR-X模型在SAR数据上继续预训练，并在不同预处理策略下评估多种基础模型在SkyCap上的性能。

Result: 在所有评估模型中，采用dB+Z-score预处理的光学基础模型MTP(ViT-B+RVSA)表现最佳（F1_c = 45.06），优于直接在Capella SAR数据上微调的SAR专用基础模型；同时发现模型性能对预处理与预训练统计的一致性高度敏感，且光学模型在光学变化检测中的排序不能直接迁移到SAR任务。

Conclusion: 这是首个在VHR SAR幅度变化检测任务上对基础模型的系统评估，表明精心设计的预处理可使通用光学模型在SAR任务中超越专用SAR模型，突显了数据预处理与模型适配的重要性。

Abstract: Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.

Abstract (中文翻译): 用于线性基础设施监测的变化检测需要可靠且高分辨率的数据以及定期的获取频率。光学超高分辨率（VHR）影像具有良好的可解释性且易于标注，但云层会破坏其获取节奏。合成孔径雷达（SAR）可实现全天候成像，但难以进行标注。我们提出了SkyCap，这是一个双时相VHR光学-SAR数据集，通过归档匹配和共配准（光学）SkySat与Capella Space（SAR）场景构建而成。我们利用光学到SAR的标签迁移方法，在无需SAR专家标注的情况下获得SAR幅度变化检测（ACD）标签。我们在自有SAR数据上对SARATR-X模型进行持续预训练，并将所得的SAR专用基础模型（FMs）与原始SARATR-X一起，在不同预处理选择下与光学基础模型在SkyCap数据集上进行基准测试。在所评估的模型中，采用dB+Z-score预处理的光学基础模型MTP(ViT-B+RVSA)取得了最佳结果（F1_c = 45.06），优于直接在Capella数据上进一步预训练的SAR专用基础模型。我们观察到模型性能对预处理与预训练统计量的一致性高度敏感，且光学模型在光学变化检测任务中的性能排序并不能一对一地迁移到SAR幅度变化检测任务中。据我们所知，这是首次在VHR SAR幅度变化检测任务上对基础模型进行评估。

</details>


### [2] [SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning](https://arxiv.org/abs/2512.14757)
*Tomohito Kawabata,Xinyu Zhang,Ling Xiao*

Main category: cs.CV

TL;DR: 本文提出SocialNav-MoE，一种基于小型混合专家视觉语言模型的高效社交导航方法，结合强化微调和语义相似性奖励，在保证导航性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航研究多关注安全性，忽视了对人类舒适度、社会规范和情境适当性的考量；同时，大型视觉语言模型因计算开销大，难以部署于资源受限的机器人平台。

Method: 提出SocialNav-MoE模型，采用小型混合专家（MoE）视觉语言架构，结合强化微调（RFT）与新设计的语义相似性奖励（SSR），并系统评估不同小语言模型（Phi、Qwen、StableLM）、路由策略及视觉编码器（CLIP vs. SigLIP，冻结 vs. 微调）的效果。

Result: 在SNEI数据集上的实验表明，SocialNav-MoE在导航准确性和计算效率之间取得良好平衡，且所提出的SSR优于硬级别和字符级别奖励。

Conclusion: 小型混合专家视觉语言模型结合语义相似性奖励能有效实现高效且符合社交规范的机器人导航，适用于资源受限的实时应用场景。

Abstract: For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.

Abstract (中文翻译): 对于在人类环境中导航的机器人而言，安全性和社交合规性同样重要，但以往的研究主要强调安全性。考虑到人类舒适度、社会规范和情境适当性的社交合规导航仍鲜有探索。视觉语言模型（VLMs）在此任务中展现出潜力，然而大规模模型带来显著的计算开销，导致更高的推理延迟和能耗，使其难以在资源受限的机器人平台上实现实时部署。为解决此问题，我们研究了小型VLM的有效性，并提出了SocialNav-MoE——一种用于社交合规导航的高效混合专家（Mixture-of-Experts）视觉语言模型，结合强化微调（RFT）。我们进一步引入语义相似性奖励（SSR），以更有效地利用RFT提升决策能力。此外，我们还系统评估了不同类型的小型语言模型（Phi、Qwen和StableLM）、路由策略以及视觉编码器（CLIP与SigLIP，冻结与微调）的效果。在SNEI数据集上的实验表明，SocialNav-MoE在导航准确性和效率之间取得了出色的平衡，所提出的SSR函数优于硬级别和字符级别奖励。论文被接收后将公开源代码。

</details>


### [3] [The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics](https://arxiv.org/abs/2512.14758)
*Fan Bu,Rongfeng Li,Zijin Li,Ya Li,Linfeng Fan,Pei Huang*

Main category: cs.CV

TL;DR: 本文提出了一种无需大量标注数据的模块化专家系统，用于将带歌词的印刷版简谱乐谱高精度转换为MusicXML和MIDI格式，在旋律和歌词识别上均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 大规模光学乐谱识别（OMR）研究主要集中在西方五线谱，而中国简谱及其丰富的歌词资源尚未得到充分探索。

Method: 采用自上而下的专家系统设计，结合传统计算机视觉技术（如短语相关性、骨架分析）利用先验知识，并集成无监督深度学习模块进行图像特征嵌入，形成兼顾可解释性与准确性的混合策略。

Result: 在《中国民歌选集》上评估，系统成功数字化了5000多首纯旋律歌曲（超30万音符）和1400多首带歌词歌曲（超10万音符），旋律识别F1达0.951，歌词对齐F1达0.931。

Conclusion: 所提出的模块化专家系统能高效、高精度地实现简谱乐谱及其歌词的数字化，为非西方乐谱的OMR研究提供了有效范式。

Abstract: Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).

Abstract (中文翻译): 大规模光学乐谱识别（OMR）研究主要聚焦于西方五线谱，而中国简谱（数字谱）及其丰富的歌词资源尚未得到充分探索。我们提出了一种模块化专家系统流水线，可在无需大量标注训练数据的情况下，将带歌词的印刷版简谱乐谱转换为机器可读的MusicXML和MIDI格式。该方法采用自上而下的专家系统设计，利用传统计算机视觉技术（例如短语相关性、骨架分析）来充分利用先验知识，同时集成无监督深度学习模块以提取图像特征嵌入。这种混合策略在可解释性与准确性之间取得了良好平衡。在《中国民歌选集》上的评估表明，我们的系统大规模数字化了（i）包含5000多首歌曲（超过30万音符）的纯旋律集合，以及（ii）包含1400多首歌曲（超过10万音符）的带歌词精选子集。该系统在旋律识别（音符级F1 = 0.951）和对齐歌词（字符级F1 = 0.931）方面均实现了高精度识别。

</details>


### [4] [AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion](https://arxiv.org/abs/2512.14760)
*Afrah Shaahid,Muzammil Behzad*

Main category: cs.CV

TL;DR: AquaDiff 是一种基于扩散模型的水下图像增强方法，通过引入色度先验引导的颜色补偿策略和条件扩散过程，在多个水下数据集上实现了优于现有方法的色彩校正与整体图像质量。


<details>
  <summary>Details</summary>
Motivation: 水下图像因波长依赖的光吸收和散射而严重退化，表现为颜色失真、对比度低和细节丢失，阻碍了基于视觉的水下应用。因此亟需有效方法来同时恢复色彩和结构信息。

Method: 提出 AquaDiff 框架，结合色度先验引导的颜色补偿策略与条件扩散过程；在每一步去噪中使用交叉注意力动态融合退化输入与带噪潜在状态；采用带有残差密集块和多分辨率注意力的增强去噪主干网络；并设计一种新颖的跨域一致性损失，联合约束像素级精度、感知相似性、结构完整性和频域保真度。

Result: 在多个具有挑战性的水下基准数据集上的实验表明，AquaDiff 在色彩校正方面表现优异，并在整体图像质量上与当前最先进的传统方法、CNN、GAN 和扩散模型方法相比具有竞争力。

Conclusion: AquaDiff 能有效解决水下图像的颜色失真和细节丢失问题，在多种水下环境下均展现出优越的增强性能，为水下视觉任务提供了有力支持。

Abstract: Underwater images are severely degraded by wavelength-dependent light absorption and scattering, resulting in color distortion, low contrast, and loss of fine details that hinder vision-based underwater applications. To address these challenges, we propose AquaDiff, a diffusion-based underwater image enhancement framework designed to correct chromatic distortions while preserving structural and perceptual fidelity. AquaDiff integrates a chromatic prior-guided color compensation strategy with a conditional diffusion process, where cross-attention dynamically fuses degraded inputs and noisy latent states at each denoising step. An enhanced denoising backbone with residual dense blocks and multi-resolution attention captures both global color context and local details. Furthermore, a novel cross-domain consistency loss jointly enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity. Extensive experiments on multiple challenging underwater benchmarks demonstrate that AquaDiff provides good results as compared to the state-of-the-art traditional, CNN-, GAN-, and diffusion-based methods, achieving superior color correction and competitive overall image quality across diverse underwater conditions.

Abstract (中文翻译): 水下图像由于波长依赖的光吸收和散射而严重退化，导致颜色失真、对比度低以及精细细节的丢失，从而阻碍了基于视觉的水下应用。为应对这些挑战，我们提出了 AquaDiff——一种基于扩散模型的水下图像增强框架，旨在校正色度失真，同时保持结构和感知保真度。AquaDiff 将色度先验引导的颜色补偿策略与条件扩散过程相结合，在每一步去噪过程中通过交叉注意力机制动态融合退化输入与带噪潜在状态。其增强的去噪主干网络采用残差密集块和多分辨率注意力机制，能够同时捕捉全局色彩上下文和局部细节。此外，该方法还引入了一种新颖的跨域一致性损失，联合约束像素级精度、感知相似性、结构完整性以及频域保真度。在多个具有挑战性的水下基准数据集上的大量实验表明，AquaDiff 相较于当前最先进的传统方法、CNN、GAN 以及扩散模型方法，取得了良好的效果，在多样化的水下条件下实现了更优的色彩校正和具有竞争力的整体图像质量。

</details>


### [5] [Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification](https://arxiv.org/abs/2512.14770)
*Xixian Wu,Yang Ou,Pengchao Tian,Zian Yang,Jielei Zhang,Peiyi Li,Longwen Gao*

Main category: cs.CV

TL;DR: 提出DAVR框架，通过自反思与跨模型验证提升视觉语言模型在VQA任务中的可靠性，显著减少幻觉问题，在ICCV-CLVL 2025挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在视觉问答（VQA）中易产生幻觉，导致回答过于自信但错误，严重损害答案可靠性。

Method: 提出DAVR框架，包含双路径架构：一路使用双选择器模块融合VLM潜在特征与问答嵌入以评估回答可靠性；另一路引入外部参考模型进行事实交叉验证以缓解幻觉。

Result: 在ICCV-CLVL 2025的Reliable VQA Challenge中，DAVR取得Φ₁₀₀得分39.64和100-AUC得分97.22，排名第一。

Conclusion: DAVR有效提升了VLM回答的可信度，为解决VLM幻觉问题提供了新思路。

Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $Φ_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.

Abstract (中文翻译): 视觉语言模型（VLMs）在视觉问答（VQA）任务中展现出巨大潜力。然而，VLMs容易产生幻觉，导致其给出过于自信却错误的答案，严重削弱了回答的可靠性。为解决这一问题，我们提出了“用于VLM可靠性的双重评估”（DAVR）框架，该框架结合自反思与跨模型验证，实现全面的不确定性估计。DAVR框架采用双路径架构：一条路径利用双选择器模块，通过融合VLM的潜在特征与问答嵌入来评估回答的可靠性；另一条路径则部署外部参考模型进行事实交叉核查，以减轻幻觉现象。在ICCV-CLVL 2025举办的Reliable VQA Challenge中，DAVR取得了Φ₁₀₀得分为39.64、100-AUC得分为97.22的优异成绩，位列第一，充分证明了其在提升VLM回答可信度方面的有效性。

</details>


### [6] [HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering](https://arxiv.org/abs/2512.14870)
*Dan Ben-Ami,Gabriele Serussi,Kobi Cohen,Chaim Baskin*

Main category: cs.CV

TL;DR: 提出HERBench视频问答基准，专门评估模型跨时间整合多条视觉证据的能力；现有13个先进Video-LLM在此基准上表现仅略高于随机猜测，揭示其在关键帧检索和信息融合两方面存在严重瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答（VideoQA）基准通常允许仅凭单一显著线索回答问题，未能充分测试模型整合多个、时间上分离的视觉证据进行推理的能力。

Method: 构建HERBench基准，包含26K个五选一多项选择题，涵盖12种组合任务，要求每个问题必须整合至少三个非重叠的、来自不同视频片段的证据线索。引入“最小所需帧集”（MRFS）指标来量化模型正确回答所需的最少帧数。

Result: HERBench的平均MRFS为5.5，远高于以往数据集（2.6-4.2）。在13个最先进的Video-LLM上评估，其准确率仅为31-42%，仅略高于20%的随机猜测基线。分析发现失败源于两个瓶颈：关键证据检索不足和信息融合失败。

Conclusion: HERBench通过使跨时间证据整合变得不可避免且可量化，为推动鲁棒、组合式的视频理解能力提供了一个明确的目标和评估标准。

Abstract: Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.

Abstract (中文翻译): 视频大语言模型（Video-LLMs）正在快速发展，但当前的视频问答（VideoQA）基准通常允许仅凭单一显著线索回答问题，未能充分测试模型整合多个、时间上分离的视觉证据进行推理的能力。我们提出了HERBench，一个专门为评估跨时间多证据整合能力而构建的VideoQA基准。其中每个问题都要求整合至少三个来自不同视频片段的、非重叠的证据线索，因此仅靠语言先验或单帧快照都无法作答。HERBench包含2.6万个五选一多项选择题，组织成十二个组合任务，用以考察身份绑定、跨实体关系、时间排序、共现验证和计数等能力。为了使证据需求可度量，我们引入了“最小所需帧集”（MRFS），即模型正确作答所需融合的最少帧数，并证明HERBench的要求远高于以往数据集（平均MRFS为5.5，而此前为2.6-4.2）。在HERBench上对13个最先进的Video-LLM进行评估，结果显示其准确率仅为31-42%，仅略高于20%的随机猜测基线。我们将此失败归结为两个关键瓶颈：（1）检索缺陷，即帧选择器忽略了关键证据；（2）融合缺陷，即即使提供了所有必要证据，模型也无法有效整合信息。通过使跨时间证据整合变得不可避免且可量化，HERBench为推进鲁棒、组合式的视频理解能力确立了一个有原则的目标。

</details>


### [7] [Isolated Sign Language Recognition with Segmentation and Pose Estimation](https://arxiv.org/abs/2512.14876)
*Daniel Perkins,Davis Hunter,Dhrumil Patel,Galen Flanagan*

Main category: cs.CV

TL;DR: 本文提出了一种用于孤立手语识别（ISLR）的新模型，通过结合姿态估计、信息分割和ResNet-Transformer骨干网络，在降低计算成本的同时保持对不同手语者变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在口语和书面语翻译方面取得进展，但美国手语（ASL）用户因依赖复杂视觉线索而难以受益。现有ISLR方法受限于每类手语样本稀少、手语者差异大以及计算开销高。

Method: 该方法整合了三个模块：(i) 姿态估计流程以提取手部和面部关节坐标；(ii) 分割模块以隔离相关信息；(iii) ResNet-Transformer骨干网络联合建模时空依赖关系。

Result: 该模型在降低计算需求的同时，对不同手语者的差异表现出良好的鲁棒性。

Conclusion: 所提出的ISLR模型有效缓解了数据稀缺、个体差异和计算成本高等挑战，为手语识别提供了一种高效且鲁棒的解决方案。

Abstract: The recent surge in large language models has automated translations of spoken and written languages. However, these advances remain largely inaccessible to American Sign Language (ASL) users, whose language relies on complex visual cues. Isolated sign language recognition (ISLR) - the task of classifying videos of individual signs - can help bridge this gap but is currently limited by scarce per-sign data, high signer variability, and substantial computational costs. We propose a model for ISLR that reduces computational requirements while maintaining robustness to signer variation. Our approach integrates (i) a pose estimation pipeline to extract hand and face joint coordinates, (ii) a segmentation module that isolates relevant information, and (iii) a ResNet-Transformer backbone to jointly model spatial and temporal dependencies.

Abstract (中文翻译): 近期大型语言模型的迅猛发展已实现了对口语和书面语言翻译的自动化。然而，这些进展对于依赖复杂视觉线索的美国手语（ASL）用户而言仍基本无法触及。孤立手语识别（ISLR）——即对单个手语动作视频进行分类的任务——有助于弥合这一鸿沟，但目前受限于每类手语样本稀缺、手语者间差异显著以及高昂的计算成本。我们提出了一种用于ISLR的模型，在降低计算需求的同时保持对手语者差异的鲁棒性。我们的方法整合了以下三个部分：(i) 一个姿态估计流程，用于提取手部和面部关节坐标；(ii) 一个分割模块，用于隔离相关信息；(iii) 一个ResNet-Transformer骨干网络，用于联合建模空间和时间依赖关系。

</details>


### [8] [Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris](https://arxiv.org/abs/2512.14878)
*Wenshuo Li,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 该论文提出一种结合皮肤纹理（dermatoglyphic）文本描述与图像的跨模态动物重识别（Re-ID）方法，利用人类可解释的语言标签编码虎的斑纹拓扑结构，并通过文本-图像协同合成生成虚拟个体以缓解数据稀缺问题，显著提升AI在跨模态身份检索中的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有AI重识别方法主要依赖图像，难以融合生态学家长期使用的文本描述；作者希望引入法医学中使用的精确皮肤纹理文本描述，提升Re-ID的可解释性与跨模态能力。

Method: 基于185只老虎共3,355张图像和84,264个手动标注的细节特征，构建视觉-文本联合模型；开发文本-图像协同合成管道，生成包含逼真图像与对应皮肤纹理文本描述的“虚拟个体”用于数据增强。

Result: 该方法在真实场景基准测试中显著提高了跨模态身份检索的准确率，有效缓解了数据稀缺问题，并实现了人类可验证的身份匹配。

Conclusion: 引入皮肤纹理语言引导的生物识别方法可突破纯视觉方法的局限，实现文本到视觉的身份恢复，推动Re-ID系统的可解释性，并促进生态监测中描述模态的语言统一。

Abstract: Biologists have long combined visuals with textual field notes to re-identify (Re-ID) animals. Contemporary AI tools automate this for species with distinctive morphological features but remain largely image-based. Here, we extend Re-ID methodologies by incorporating precise dermatoglyphic textual descriptors-an approach used in forensics but new to ecology. We demonstrate that these specialist semantics abstract and encode animal coat topology using human-interpretable language tags. Drawing on 84,264 manually labelled minutiae across 3,355 images of 185 tigers (Panthera tigris), we evaluate this visual-textual methodology, revealing novel capabilities for cross-modal identity retrieval. To optimise performance, we developed a text-image co-synthesis pipeline to generate 'virtual individuals', each comprising dozens of life-like visuals paired with dermatoglyphic text. Benchmarking against real-world scenarios shows this augmentation significantly boosts AI accuracy in cross-modal retrieval while alleviating data scarcity. We conclude that dermatoglyphic language-guided biometrics can overcome vision-only limitations, enabling textual-to-visual identity recovery underpinned by human-verifiable matchings. This represents a significant advance towards explainability in Re-ID and a language-driven unification of descriptive modalities in ecological monitoring.

Abstract (中文翻译): 生物学家长期以来结合视觉图像与文字野外笔记来进行动物重识别（Re-ID）。当前的人工智能工具虽已能自动化处理具有独特形态特征的物种，但主要仍基于图像。本文通过引入精确的皮肤纹理（dermatoglyphic）文本描述——这一在法医学中常用但在生态学中尚属新颖的方法——拓展了Re-ID技术。我们证明，这类专业语义可通过人类可理解的语言标签对动物皮毛拓扑结构进行抽象与编码。基于185只老虎（Panthera tigris）的3,355张图像中手动标注的84,264个细节特征，我们评估了这种视觉-文本方法，揭示其在跨模态身份检索中的新能力。为优化性能，我们开发了一种文本-图像协同合成流程，生成“虚拟个体”，每个个体包含数十张逼真的图像及其对应的皮肤纹理文本描述。在真实场景下的基准测试表明，这种数据增强显著提升了AI在跨模态检索中的准确率，同时缓解了数据稀缺问题。我们得出结论：皮肤纹理语言引导的生物识别方法能够克服纯视觉方法的局限，实现基于人类可验证匹配的文本到视觉身份恢复。这标志着Re-ID可解释性的重要进展，也是生态监测中描述模态语言驱动统一的重要一步。

</details>


### [9] [Vibe Spaces for Creatively Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2512.14884)
*Huzheng Yang,Katherine Xu,Andrew Lu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TL;DR: 本文提出“氛围融合”（Vibe Blending）任务及“氛围空间”（Vibe Space）方法，通过在CLIP等特征空间中构建分层图流形，学习低维测地线以实现不同视觉概念间的语义连贯融合，并通过结合人类判断、大语言模型推理和几何路径难度评分的评估框架验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以在潜在空间中识别并穿越连接远距离概念的非线性路径，从而无法有效生成具有共享属性（即“氛围”）的连贯且有意义的新视觉概念。

Method: 提出“氛围空间”（Vibe Space），一种分层图流形结构，在CLIP等特征空间中学习低维测地线，以实现概念之间平滑且语义一致的过渡。

Result: 实验表明，Vibe Space 生成的概念融合结果在人类评价中始终比现有方法更具创意性和连贯性。

Conclusion: Vibe Space 能有效捕捉并利用图像间的共享氛围属性，显著提升跨概念视觉融合的创造性和语义一致性。

Abstract: Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.

Abstract (中文翻译): 创造新的视觉概念通常需要通过其最相关的共享属性——即“氛围”（vibe）——将不同的想法联系起来。我们提出了“氛围融合”（Vibe Blending）这一新任务，用于生成连贯且有意义的混合图像，从而揭示图像之间的共享属性。当前方法难以实现此类融合，因其难以在潜在空间中识别并穿越连接遥远概念的非线性路径。为此，我们提出了“氛围空间”（Vibe Space），一种分层图流形结构，可在CLIP等特征空间中学习低维测地线，从而实现概念之间平滑且语义一致的过渡。为评估生成结果的创造性质量，我们设计了一个受认知启发的评估框架，结合人类判断、大语言模型推理以及基于几何路径的难度评分。研究发现，Vibe Space 生成的融合结果在人类评价中始终比现有方法更具创意性和连贯性。

</details>


### [10] [PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis](https://arxiv.org/abs/2512.14922)
*Joshua L. Ebbert,Dennis Della Corte*

Main category: cs.CV

TL;DR: 本文提出了PANDA-PLUS-Bench，一个专门用于评估前列腺癌Gleason分级中AI基础模型是否依赖切片特异性伪影而非生物学特征的基准数据集。通过在九位患者的活检切片上提取不同分辨率和增强条件下的图像块，作者测试了七个基础模型对生物信号与切片混杂因素的区分能力。结果显示模型间鲁棒性差异显著：HistoEncoder（专为前列腺组织训练）表现最佳，而Virchow2虽切片编码最低但跨切片准确率也较低。所有模型均存在明显的片内与跨片准确率差距。该基准填补了临床相关任务中模型鲁棒性评估的空白。


<details>
  <summary>Details</summary>
Motivation: 当前用于前列腺癌Gleason分级的人工智能基础模型可能通过学习切片特异性伪影（如染色、制片工艺等）而非可泛化的生物学特征来获得高验证准确率，导致其在真实临床场景中性能受限。因此，亟需一个专门设计的基准来量化这种失败模式，以评估模型对生物信号与切片混杂因素的区分能力。

Method: 作者构建了PANDA-PLUS-Bench基准数据集，包含来自9位独特患者的专家标注前列腺活检全切片图像，从中提取非重叠组织图像块（512x512和224x224像素），并在8种增强条件下生成多样本。利用该基准，评估7个基础模型在分离生物信号与切片级混杂因素方面的能力，主要指标包括跨切片准确率和切片级编码强度。

Result: 实验结果显示模型鲁棒性存在显著差异：HistoEncoder（专为前列腺组织训练）取得最高的跨切片准确率（59.7%）和最强的切片级编码（90.3%）；Virchow2在大模型中切片编码最低（81.0%），但跨切片准确率也较低（47.2%）。所有模型均表现出片内与跨片准确率差距（19.9–26.9个百分点）。作者还提供了开源Colab笔记本以支持后续评估。

Conclusion: PANDA-PLUS-Bench为评估Gleason分级任务中基础模型的鲁棒性提供了一个专门构建的资源，揭示了当前模型普遍存在的对切片特异性伪影的依赖问题，并表明组织特异性训练可能有助于提升模型对真实生物特征的捕捉能力。该基准填补了临床AI模型评估中的关键空白。

Abstract: Artificial intelligence foundation models are increasingly deployed for prostate cancer Gleason grading, where GP3/GP4 distinction directly impacts treatment decisions. However, these models may achieve high validation accuracy by learning specimen-specific artifacts rather than generalizable biological features, limiting real-world clinical utility. We introduce PANDA-PLUS-Bench, a curated benchmark dataset derived from expert-annotated prostate biopsies designed specifically to quantify this failure mode. The benchmark comprises nine carefully selected whole slide images from nine unique patients containing diverse Gleason patterns, with non-overlapping tissue patches extracted at both 512x512 and 224x224 pixel resolutions across eight augmentation conditions. Using this benchmark, we evaluate seven foundation models on their ability to separate biological signal from slide-level confounders. Our results reveal substantial variation in robustness across models: Virchow2 achieved the lowest slide-level encoding among large-scale models (81.0%) yet exhibited the second-lowest cross-slide accuracy (47.2%). HistoEncoder, trained specifically on prostate tissue, demonstrated the highest cross-slide accuracy (59.7%) and the strongest slide-level encoding (90.3%), suggesting tissue-specific training may enhance both biological feature capture and slide-specific signatures. All models exhibited measurable within-slide vs. cross-slide accuracy gaps, though the magnitude varied from 19.9 percentage points to 26.9 percentage points. We provide an open-source Google Colab notebook enabling researchers to evaluate additional foundation models against our benchmark using standardized metrics. PANDA-PLUS-Bench addresses a critical gap in foundation model evaluation by providing a purpose-built resource for robustness assessment in the clinically important context of Gleason grading.

Abstract (中文翻译): 人工智能基础模型越来越多地被应用于前列腺癌Gleason分级任务，其中GP3与GP4的区分直接影响治疗决策。然而，这些模型可能通过学习切片特异性伪影而非可泛化的生物学特征来实现较高的验证准确率，从而限制了其在真实临床环境中的实用性。我们提出了PANDA-PLUS-Bench——一个由专家标注的前列腺活检样本构建的精选基准数据集，专门用于量化此类失效模式。该基准包含来自九位不同患者的九张全切片图像，涵盖多种Gleason模式，并在八种增强条件下提取了512×512和224×224像素分辨率的非重叠组织图像块。利用该基准，我们评估了七个基础模型在分离生物信号与切片级混杂因素方面的能力。结果表明，各模型的鲁棒性存在显著差异：Virchow2在大规模模型中具有最低的切片级编码（81.0%），但其跨切片准确率也位居第二低（47.2%）；而专为前列腺组织训练的HistoEncoder则展现出最高的跨切片准确率（59.7%）和最强的切片级编码（90.3%），表明组织特异性训练可能同时增强模型对生物特征的捕捉能力和对切片特异性信号的学习。所有模型均表现出可测量的片内与跨片准确率差距，差距幅度从19.9个百分点到26.9个百分点不等。我们提供了一个开源的Google Colab笔记本，使研究人员能够使用标准化指标在该基准上评估更多基础模型。PANDA-PLUS-Bench通过提供一个专为此目的构建的资源，在Gleason分级这一临床重要任务中填补了基础模型评估的关键空白。

</details>
