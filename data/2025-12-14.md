<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Neuromorphic Eye Tracking for Low-Latency Pupil Detection](https://arxiv.org/abs/2512.09969)
*Paul Hueber,Luca Peres,Florian Pitters,Alejandro Gloriani,Oliver Rhodes*

Main category: cs.CV

TL;DR: 本文提出一种基于脉冲神经网络（SNN）的高效事件驱动眼动追踪模型，通过用轻量级LIF层替代传统循环和注意力模块，并采用深度可分离卷积，在保持接近现有最优精度的同时，显著降低模型大小与计算开销，适用于低功耗可穿戴AR/VR设备。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的眼动追踪方法在可穿戴系统中面临运动模糊、高计算成本和时间分辨率有限等问题，难以满足AR/VR等新兴技术对低延迟、低功耗和高响应性的需求。尽管神经形态传感器和SNN提供了潜在解决方案，但现有SNN方法要么过于专用，要么性能不及现代ANN架构。

Method: 将当前性能领先的基于事件的眼动追踪模型转换为神经形态版本，使用轻量级LIF（Leaky Integrate-and-Fire）层替代原有的循环和注意力模块，并引入深度可分离卷积以降低模型复杂度。

Result: 所提模型平均误差为3.7–4.1像素，接近专用神经形态系统Retina（3.24像素）的精度，同时模型体积缩小20倍，理论计算量减少850倍；预计可在1 kHz下实现3.9–4.9 mW功耗和3 ms延迟。

Conclusion: 高性能的事件驱动眼动追踪架构可以成功重构为SNN，在大幅提高能效的同时保留适用于实时可穿戴部署的精度。

Abstract: Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.

Abstract (中文翻译): 面向可穿戴系统的眼动追踪需要低延迟和毫瓦级功耗，但传统的基于帧的处理流程在运动模糊、高计算开销和有限的时间分辨率方面存在困难。这些能力对于增强现实（AR）和虚拟现实（VR）等新兴技术中实现无缝且响应迅速的交互至关重要，因为理解用户注视点是提升沉浸感和界面设计的关键。神经形态传感器和脉冲神经网络（SNN）提供了一种有前景的替代方案，然而现有的SNN方法要么过于专用，要么性能无法达到现代人工神经网络（ANN）架构的水平。本文提出了当前顶尖的基于事件的眼动追踪模型的神经形态版本，用轻量级LIF层替代其循环和注意力模块，并利用深度可分离卷积来降低模型复杂度。我们的模型获得了3.7–4.1像素的平均误差，接近专用神经形态系统Retina（3.24像素）的精度，同时相比最接近的ANN变体，模型大小减少了20倍，理论计算量减少了850倍。这些高效变体预计可在1 kHz频率下以3.9–4.9毫瓦的功耗和3毫秒的延迟运行。本研究结果表明，高性能的事件驱动眼动追踪架构可以被重新设计为SNN，在显著提升能效的同时，仍保持适用于实时可穿戴部署的精度。

</details>


### [2] [ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects](https://arxiv.org/abs/2512.10031)
*Woojin Lee,Hyugjae Chang,Jaeho Moon,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: 本文提出ABBSPO框架，通过自适应边界框缩放和对称先验角度损失，提升弱监督方向目标检测（WS-OOD）的性能，尤其在仅使用水平边界框标注的情况下达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 弱监督方向目标检测（WS-OOD）是一种成本更低但精度较高的替代方案。现有基于水平边界框（HBox）监督的方法在尺度估计上存在不准确问题，且在数据增强时容易因预测一致错误而导致学习崩溃。

Method: 提出ABBSPO框架，包含：(i) 自适应边界框缩放（ABBS），根据预测旋转框（RBox）动态调整真实HBox大小；(ii) 对称先验角度（SPA）损失，利用航拍目标的对称性进行自监督学习，防止多视图预测错误导致的学习崩溃。

Result: 大量实验表明，ABBSPO在HBox弱监督设定下显著优于现有方法，达到当前最优性能。

Conclusion: ABBSPO有效解决了现有HBox监督OOR方法在尺度估计和学习稳定性方面的不足，在弱监督方向目标检测任务中实现了领先性能。

Abstract: Weakly supervised oriented object detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing both efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox)-supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO, a framework for WS-OOD. Our ABBSPO addresses limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with the minimum circumscribed rectangles of predicted RBoxes, often leading to inaccurate scale estimation. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS), which appropriately scales GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate scale prediction; and (ii) a Symmetric Prior Angle (SPA) loss that exploits inherent symmetry of aerial objects for self-supervised learning, resolving issues in previous methods where learning collapses when predictions for all three augmented views (original, rotated, and flipped) are consistently incorrect. Extensive experimental results demonstrate that ABBSPO achieves state-of-the-art performance, outperforming existing methods.

Abstract (中文翻译): 弱监督方向目标检测（WS-OOD）作为一种兼顾效率与高精度的成本效益替代方案，近年来受到广泛关注。在各类弱监督方法中，基于水平边界框（HBox）监督的OOR因其能直接利用现有HBox标注，并在弱监督设定下实现最高精度而脱颖而出。本文提出一种名为ABBSPO的框架，结合自适应边界框缩放与基于对称先验的方向预测。ABBSPO针对以往HBox监督OOR方法的局限性进行了改进——这些方法通常直接将真实HBox与预测旋转框（RBox）的最小外接矩形进行比较，导致尺度估计不准确。为此，我们提出：(i) 自适应边界框缩放（ABBS），可根据每个预测RBox的尺寸动态调整真实HBox的尺度，从而实现更精确的尺度预测；(ii) 对称先验角度（SPA）损失，利用航拍目标固有的对称性进行自监督学习，解决以往方法在原始、旋转和翻转三种增强视图预测均错误时出现的学习崩溃问题。大量实验结果表明，ABBSPO在性能上超越现有方法，达到了当前最优水平。

</details>


### [3] [Diffusion Is Your Friend in Show, Suggest and Tell](https://arxiv.org/abs/2512.10038)
*Jia Cheng Hu,Roberto Cavicchioli,Alessandro Capotondi*

Main category: cs.CV

TL;DR: 本文提出将扩散模型作为自回归模型的“建议”模块，而非替代方案，在图像描述任务中实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 扩散去噪模型在生成式计算机视觉任务中表现出色，但在离散域（如文本生成）中仍无法超越标准的自回归方法。作者旨在结合两者优势，探索一种新的范式。

Method: 提出“展示、建议与讲述”（Show, Suggest and Tell, SST）框架，利用扩散模型为自回归生成过程提供双向、可优化的建议，同时保留自回归模型强大的语言结构能力。

Result: SST在COCO数据集上取得了125.1的CIDEr-D分数（未使用强化学习），比当前自回归和扩散模型的SOTA分别高出1.5和2.5分。实验还验证了建议模块对生成质量的正向影响。

Conclusion: 将扩散模型作为自回归模型的辅助建议机制是一种有前景且尚未充分探索的研究方向。

Abstract: Diffusion Denoising models demonstrated impressive results across generative Computer Vision tasks, but they still fail to outperform standard autoregressive solutions in the discrete domain, and only match them at best. In this work, we propose a different paradigm by adopting diffusion models to provide suggestions to the autoregressive generation rather than replacing them. By doing so, we combine the bidirectional and refining capabilities of the former with the strong linguistic structure provided by the latter. To showcase its effectiveness, we present Show, Suggest and Tell (SST), which achieves State-of-the-Art results on COCO, among models in a similar setting. In particular, SST achieves 125.1 CIDEr-D on the COCO dataset without Reinforcement Learning, outperforming both autoregressive and diffusion model State-of-the-Art results by 1.5 and 2.5 points. On top of the strong results, we performed extensive experiments to validate the proposal and analyze the impact of the suggestion module. Results demonstrate a positive correlation between suggestion and caption quality, overall indicating a currently underexplored but promising research direction. Code will be available at: https://github.com/jchenghu/show\_suggest\_tell.

Abstract (中文翻译): 扩散去噪模型在各类生成式计算机视觉任务中展现了令人印象深刻的效果，但在离散域（如文本生成）中仍未能超越标准的自回归方法，最多只能与之持平。在本研究中，我们提出了一种新范式：采用扩散模型为自回归生成过程提供建议，而非取代它。通过这种方式，我们将扩散模型的双向性和优化能力与自回归模型强大的语言结构能力相结合。为验证该方法的有效性，我们提出了“展示、建议与讲述”（Show, Suggest and Tell, SST）模型，在相似设定下于COCO数据集上取得了最先进的结果。具体而言，SST在未使用强化学习的情况下，在COCO数据集上达到了125.1的CIDEr-D分数，分别比当前自回归模型和扩散模型的最先进结果高出1.5分和2.5分。除了取得优异性能外，我们还进行了大量实验以验证所提方法，并分析建议模块的影响。结果表明，建议质量与生成描述质量之间存在正相关关系，整体显示出一个目前尚未被充分探索但极具前景的研究方向。代码将发布于：https://github.com/jchenghu/show_suggest_tell。

</details>


### [4] [MetaVoxel: Joint Diffusion Modeling of Imaging and Clinical Metadata](https://arxiv.org/abs/2512.10041)
*Yihao Liu,Chenyu Gao,Lianrui Zuo,Michael E. Kim,Brian D. Boyd,Lisa L. Barnes,Walter A. Kukull,Lori L. Beason-Held,Susan M. Resnick,Timothy J. Hohman,Warren D. Taylor,Bennett A. Landman*

Main category: cs.CV

TL;DR: MetaVoxel 是一个生成式联合扩散模型框架，通过学习图像和临床元数据的联合分布，实现多任务统一建模与零样本灵活推理。


<details>
  <summary>Details</summary>
Motivation: 当前大多数医学AI方法针对特定预测方向和输入变量训练条件模型，难以泛化到其他任务或输入组合，限制了临床应用的灵活性。

Method: 提出 MetaVoxel 框架，利用单一扩散过程对成像数据与临床元数据的联合分布进行建模。

Result: 在超过10,000例T1加权MRI及九个数据集的临床元数据上，单个MetaVoxel模型在图像生成、年龄估计和性别预测任务中达到与专用基线模型相当的性能，并支持任意输入子集的零样本推理。

Conclusion: 联合多模态扩散模型为统一医学AI系统并提升其临床适用性提供了有前景的新方向。

Abstract: Modern deep learning methods have achieved impressive results across tasks from disease classification, estimating continuous biomarkers, to generating realistic medical images. Most of these approaches are trained to model conditional distributions defined by a specific predictive direction with a specific set of input variables. We introduce MetaVoxel, a generative joint diffusion modeling framework that models the joint distribution over imaging data and clinical metadata by learning a single diffusion process spanning all variables. By capturing the joint distribution, MetaVoxel unifies tasks that traditionally require separate conditional models and supports flexible zero-shot inference using arbitrary subsets of inputs without task-specific retraining. Using more than 10,000 T1-weighted MRI scans paired with clinical metadata from nine datasets, we show that a single MetaVoxel model can perform image generation, age estimation, and sex prediction, achieving performance comparable to established task-specific baselines. Additional experiments highlight its capabilities for flexible inference.Together, these findings demonstrate that joint multimodal diffusion offers a promising direction for unifying medical AI models and enabling broader clinical applicability.

Abstract (中文翻译): 现代深度学习方法在疾病分类、连续生物标志物估计以及生成逼真医学图像等任务中取得了令人瞩目的成果。然而，这些方法大多被训练用于建模由特定预测方向和特定输入变量定义的条件分布。本文提出了 MetaVoxel，一种生成式联合扩散建模框架，通过学习覆盖所有变量（包括影像数据和临床元数据）的单一扩散过程，对它们的联合分布进行建模。通过捕捉联合分布，MetaVoxel 统一了传统上需要单独条件模型的任务，并支持使用任意输入子集进行灵活的零样本推理，而无需针对特定任务重新训练。我们在来自九个数据集的超过10,000例T1加权MRI扫描及其配对的临床元数据上验证了该方法，结果表明单个 MetaVoxel 模型即可执行图像生成、年龄估计和性别预测，并在各项任务中达到与现有专用基线模型相当的性能。额外实验进一步突显了其灵活推理能力。综上所述，这些发现表明联合多模态扩散模型为统一医学人工智能模型并拓展其临床应用前景提供了有力支持。

</details>


### [5] [Independent Density Estimation](https://arxiv.org/abs/2512.10067)
*Jiahao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为独立密度估计（IDE）的新方法，通过学习句子中每个词与图像特征之间的联系，提升视觉-语言模型在未见过组合上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言模型在图像描述和条件图像生成等任务上表现优异，但在类人组合泛化能力方面仍存在不足。

Method: 提出独立密度估计（IDE）方法，构建两个基于IDE的模型：一个使用完全解耦的视觉表示，另一个利用变分自编码器从原始图像中提取部分解耦特征；并设计基于熵的组合推理方法整合各词预测结果。

Result: 在多个数据集上的评估表明，所提模型在未见组合上的泛化性能优于现有模型。

Conclusion: IDE方法有效提升了视觉-语言模型的组合泛化能力，为实现更接近人类水平的理解与生成提供了新思路。

Abstract: Large-scale Vision-Language models have achieved remarkable results in various domains, such as image captioning and conditioned image generation. Nevertheless, these models still encounter difficulties in achieving human-like compositional generalization. In this study, we propose a new method called Independent Density Estimation (IDE) to tackle this challenge. IDE aims to learn the connection between individual words in a sentence and the corresponding features in an image, enabling compositional generalization. We build two models based on the philosophy of IDE. The first one utilizes fully disentangled visual representations as input, and the second leverages a Variational Auto-Encoder to obtain partially disentangled features from raw images. Additionally, we propose an entropy-based compositional inference method to combine predictions of each word in the sentence. Our models exhibit superior generalization to unseen compositions compared to current models when evaluated on various datasets.

Abstract (中文翻译): 大规模视觉-语言模型在图像描述和条件图像生成等多个领域取得了显著成果。然而，这些模型在实现类人的组合泛化能力方面仍面临挑战。本研究提出了一种名为独立密度估计（Independent Density Estimation, IDE）的新方法来应对这一问题。IDE旨在学习句子中各个词语与图像中对应特征之间的关联，从而实现组合泛化。我们基于IDE理念构建了两个模型：第一个模型采用完全解耦的视觉表征作为输入，第二个模型则利用变分自编码器从原始图像中提取部分解耦的特征。此外，我们还提出了一种基于熵的组合推理方法，用于融合句子中每个词的预测结果。在多个数据集上的实验评估表明，与当前模型相比，我们的模型在未见过的组合上展现出更优的泛化能力。

</details>


### [6] [TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing](https://arxiv.org/abs/2512.10095)
*Jiachen Tao,Junyi Wu,Haoxuan Wang,Zongxin Yang,Dawen Cai,Yan Yan*

Main category: cs.CV

TL;DR: TraceFlow 是一个用于高保真渲染动态镜面场景的新框架，通过精确估计反射方向和物理准确的反射建模，在复杂动态环境中生成更清晰、逼真的镜面反射效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态镜面场景渲染中难以同时实现精确的反射方向估计与物理上准确的反射建模，导致镜面反射失真或不真实。

Method: 提出一种残差材质增强的2D高斯泼溅表示以建模动态几何与材质属性；引入动态环境高斯与混合渲染管线，将渲染分解为漫反射与镜面反射分量，并结合光栅化与光线追踪实现物理可信的镜面合成；采用由粗到精的训练策略提升优化稳定性与物理分解合理性。

Result: 在多个动态场景基准上的实验表明，TraceFlow 在定量和定性指标上均优于现有方法，能生成更清晰、逼真的动态镜面反射。

Conclusion: TraceFlow 有效解决了动态镜面场景高保真渲染的关键挑战，显著提升了镜面反射的真实感与准确性。

Abstract: We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.

Abstract (中文翻译): 我们提出了 TraceFlow，这是一个用于高保真渲染动态镜面场景的新框架，旨在解决两个关键挑战：精确的反射方向估计和物理上准确的反射建模。为此，我们提出了一种残差材质增强的2D高斯泼溅表示方法，用于建模动态几何结构和材质属性，从而实现精确的反射光线计算。此外，我们引入了动态环境高斯表示和一种混合渲染管线，将渲染过程分解为漫反射和镜面反射两个分量，通过光栅化与光线追踪相结合的方式实现基于物理的镜面反射合成。最后，我们设计了一种由粗到精的训练策略，以提高优化稳定性并促进具有物理意义的反射分量分解。在多个动态场景基准上的大量实验表明，TraceFlow 在定量和定性评估中均优于先前方法，在复杂动态环境中生成更清晰、更逼真的镜面反射效果。

</details>


### [7] [Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information](https://arxiv.org/abs/2512.10102)
*Neelima Prasad,Jarek Reynolds,Neel Karsanbhai,Tanusree Sharma,Lotus Zhang,Abigale Stangl,Yang Wang,Leah Findlater,Danna Gurari*

Main category: cs.CV

TL;DR: 提出了一个新的任务“层次化实例跟踪”，并发布了首个支持该任务的基准数据集，包含552个视频中的2765个实体，涵盖40个类别。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对对象及其组成部分之间层次关系的联合跟踪能力，因此需要定义新任务并提供相应基准。

Method: 提出“层次化实例跟踪”任务，并构建包含对象与部件共40类、2765个被跟踪实体的视频数据集；评估了四种模型的七种变体在该任务上的表现。

Result: 实验表明，所提出的任务具有挑战性，现有模型在新数据集上的性能仍有较大提升空间。

Conclusion: 该工作为层次化实例跟踪提供了首个基准，推动了对对象与其组成部分联合建模的研究。

Abstract: We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/

Abstract (中文翻译): 我们提出了一项新任务——层次化实例跟踪，该任务要求跟踪预定义类别中的所有对象及其组成部分的实例，同时保持它们之间的层次关系。我们发布了首个支持该任务的基准数据集，包含552个视频中被跟踪的2765个唯一实体，涵盖40个类别（包括对象和部件）。针对该任务定制的四种模型的七种变体在该数据集上的评估结果表明，该数据集具有挑战性。我们的数据集可在 https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/ 获取。

</details>


### [8] [Topological Conditioning for Mammography Models via a Stable Wavelet-Persistence Vectorization](https://arxiv.org/abs/2512.10151)
*Charles Fanning,Mehmet Emin Aktas*

Main category: cs.CV

TL;DR: 本文提出一种基于小波持久同调的条件信号，用于提升乳腺癌筛查模型在不同数据集上的泛化性能。通过拓扑数据分析生成对强度扰动稳定的多尺度空间图，并将其整合到两阶段检测流程中，在跨域数据集上显著提升了AUC。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线摄影筛查虽可降低死亡率，但在实际应用中仍存在较高的假阴性和假阳性率，且模型在不同扫描设备、成像模态和人群间部署时性能常显著下降。因此，亟需提升模型的外部泛化能力。

Method: 作者利用拓扑数据分析（TDA）提取图像在不同强度阈值下持续存在的结构信息，将其转化为对小强度扰动具有理论稳定性的多尺度空间图；随后将这些图作为小波持久性通道，通过输入层通道拼接方式整合进两阶段检测流程中。

Result: 模型在美国CBIS-DDSM胶片数字化乳腺数据集上训练和验证，并在葡萄牙（INbreast）和中国（CMMD）两个独立全视野数字乳腺数据集上评估。在INbreast数据集上，使用有限训练预算时，加入小波持久性通道使ConvNeXt Tiny的患者级AUC从0.55提升至0.75。

Conclusion: 所提出的基于小波持久同调的条件信号能有效提升乳腺癌检测模型在跨域数据上的性能，证明了拓扑特征在医学图像分析中的潜力。

Abstract: Breast cancer is the most commonly diagnosed cancer in women and a leading cause of cancer death worldwide. Screening mammography reduces mortality, yet interpretation still suffers from substantial false negatives and false positives, and model accuracy often degrades when deployed across scanners, modalities, and patient populations. We propose a simple conditioning signal aimed at improving external performance based on a wavelet based vectorization of persistent homology. Using topological data analysis, we summarize image structure that persists across intensity thresholds and convert this information into spatial, multi scale maps that are provably stable to small intensity perturbations. These maps are integrated into a two stage detection pipeline through input level channel concatenation. The model is trained and validated on the CBIS DDSM digitized film mammography cohort from the United States and evaluated on two independent full field digital mammography cohorts from Portugal (INbreast) and China (CMMD), with performance reported at the patient level. On INbreast, augmenting ConvNeXt Tiny with wavelet persistence channels increases patient level AUC from 0.55 to 0.75 under a limited training budget.

Abstract (中文翻译): 乳腺癌是女性中最常被诊断出的癌症，也是全球癌症死亡的主要原因。乳腺X线摄影筛查可降低死亡率，但其解读仍存在大量假阴性和假阳性问题，且模型在不同扫描仪、成像模态和患者群体中部署时，准确性往往下降。我们提出一种基于小波持久同调向量化的简单条件信号，旨在提升模型的外部泛化性能。通过拓扑数据分析，我们总结了在不同强度阈值下持续存在的图像结构，并将该信息转化为对小强度扰动具有理论稳定性的空间多尺度图。这些图通过输入层通道拼接的方式整合到两阶段检测流程中。该模型在美国CBIS-DDSM胶片数字化乳腺数据集上进行训练和验证，并在葡萄牙（INbreast）和中国（CMMD）两个独立的全视野数字乳腺数据集上进行评估，性能以患者级别报告。在INbreast数据集上，在有限训练预算下，将小波持久性通道加入ConvNeXt Tiny模型后，患者级别的AUC从0.55提升至0.75。

</details>


### [9] [Feature Coding for Scalable Machine Vision](https://arxiv.org/abs/2512.10209)
*Md Eimran Hossain Eimon,Juan Merlos,Ashan Perera,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: 本文提出了一种用于压缩深度神经网络中间特征的编码方法（FCTM），在保持模型精度的同时平均减少85.14%的比特率，支持边缘-云协同推理，适用于带宽受限和注重隐私的应用场景。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络计算需求高，在边缘设备上部署困难。传统方案（全在设备端运行或完全卸载到云端）在延迟、带宽和隐私方面存在权衡。将推理任务拆分到边缘和云端虽可平衡这些因素，但传输中间特征会带来新的带宽挑战。

Method: 采用MPEG制定的面向机器的特征编码标准（FCM），设计并实现特征编码测试模型（FCTM），通过专门的比特流语法和编解码流程对中间特征进行高效压缩。

Result: 在多个视觉任务中，FCTM平均实现了85.14%的比特率降低，同时保持了模型精度。

Conclusion: FCM为在带宽受限和隐私敏感的消费级应用中高效、互操作地部署智能视觉功能提供了可扩展的解决方案。

Abstract: Deep neural networks (DNNs) drive modern machine vision but are challenging to deploy on edge devices due to high compute demands. Traditional approaches-running the full model on-device or offloading to the cloud face trade-offs in latency, bandwidth, and privacy. Splitting the inference workload between the edge and the cloud offers a balanced solution, but transmitting intermediate features to enable such splitting introduces new bandwidth challenges. To address this, the Moving Picture Experts Group (MPEG) initiated the Feature Coding for Machines (FCM) standard, establishing a bitstream syntax and codec pipeline tailored for compressing intermediate features. This paper presents the design and performance of the Feature Coding Test Model (FCTM), showing significant bitrate reductions-averaging 85.14%-across multiple vision tasks while preserving accuracy. FCM offers a scalable path for efficient and interoperable deployment of intelligent features in bandwidth-limited and privacy-sensitive consumer applications.

Abstract (中文翻译): 深度神经网络（DNN）推动了现代机器视觉的发展，但由于其高昂的计算需求，在边缘设备上的部署颇具挑战。传统方法——在设备端运行完整模型或将任务卸载至云端——在延迟、带宽和隐私方面面临权衡。将推理工作负载在边缘与云端之间拆分提供了一种平衡方案，但为了实现这种拆分而传输中间特征又带来了新的带宽挑战。为此，动态图像专家组（MPEG）启动了“面向机器的特征编码”（FCM）标准，制定了专门用于压缩中间特征的比特流语法和编解码器流程。本文介绍了特征编码测试模型（FCTM）的设计与性能，在多个视觉任务中平均实现了85.14%的比特率降低，同时保持了模型精度。FCM为在带宽受限且注重隐私的消费级应用中高效、可互操作地部署智能视觉功能提供了一条可扩展的路径。

</details>


### [10] [Latent Chain-of-Thought World Modeling for End-to-End Driving](https://arxiv.org/abs/2512.10226)
*Shuhan Tan,Kashyap Chitta,Yuxiao Chen,Ran Tian,Yurong You,Yan Wang,Wenjie Luo,Yulong Cao,Philipp Krahenbuhl,Marco Pavone,Boris Ivanovic*

Main category: cs.CV

TL;DR: 本文提出LCDrive，一种在隐空间中进行推理的视觉-语言-动作模型，通过动作对齐的隐式语言替代自然语言进行链式推理，从而提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的链式思维（CoT）推理在自动驾驶中效率不高，作者希望探索更高效的推理表示方式以提升驾驶性能与安全性。

Method: 提出Latent-CoT-Drive（LCDrive）模型，在动作对齐的隐空间中统一CoT推理与决策：使用动作提议token和世界模型token交替进行推理；先通过真实未来轨迹冷启动训练，再通过闭环强化学习后训练。

Result: 在大规模端到端驾驶基准上，LCDrive相比无推理和基于文本推理的基线方法，具有更快的推理速度、更优的轨迹质量，以及在交互式强化学习中更大的性能提升。

Conclusion: 使用隐式语言而非自然语言进行链式推理能更高效地支持自动驾驶决策，LCDrive在多个指标上显著优于现有方法。

Abstract: Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.

Abstract (中文翻译): 近期用于自动驾驶的视觉-语言-动作（VLA）模型探索了推理时推理机制，以在具有挑战性的场景中提升驾驶性能与安全性。以往大多数工作使用自然语言在生成驾驶动作前表达链式思维（CoT）推理。然而，文本可能并非最高效的推理表示方式。本文提出了Latent-CoT-Drive（LCDrive）：一种在隐式语言中表达CoT的模型，该隐式语言能够捕捉所考虑驾驶动作的可能结果。我们的方法通过在动作对齐的隐空间中统一CoT推理与决策过程来实现这一目标。模型不再使用自然语言，而是通过交替使用（1）动作提议token（与模型输出动作共享词汇表）和（2）世界模型token（基于学习到的隐式世界模型，并表达这些动作的未来结果）来进行推理。我们通过监督模型的动作提议和世界模型token（基于场景的真实未来轨迹）来冷启动隐式CoT。随后，采用闭环强化学习进行后训练以增强推理能力。在大规模端到端驾驶基准测试中，LCDrive相比无推理和基于文本推理的基线方法，实现了更快的推理速度、更高质量的轨迹，以及在交互式强化学习中更大的性能提升。

</details>
