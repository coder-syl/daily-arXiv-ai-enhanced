<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility](https://arxiv.org/abs/2512.19711)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.CV

TL;DR: 本文提出PHANTOM，一种利用变形艺术生成视角依赖的物理对抗样本的新方法，可在黑盒条件下欺骗多种主流目标检测器，并通过V2X通信引发网联自动驾驶车辆系统的全局安全风险。


<details>
  <summary>Details</summary>
Motivation: 网联自动驾驶车辆（CAVs）依赖基于视觉的深度神经网络和低延迟V2X通信，但易受物理对抗攻击。现有攻击方法通常需白盒访问或缺乏跨模型泛化能力，且未充分考虑对整个CAV通信生态的影响。

Method: 提出PHANTOM框架，利用变形艺术（anamorphic art）构造几何畸变图案，这些图案对人类看似自然，却能高置信度误导目标检测器；该方法无需目标模型信息（黑盒），并测试其在YOLOv5、SSD、Faster R-CNN和RetinaNet四种检测器上的迁移性；在CARLA仿真环境中评估不同速度、天气和光照条件下的攻击效果，并通过SUMO-OMNeT++联合仿真分析V2X通信层面的连锁影响。

Result: 在理想条件下攻击成功率超90%，恶劣环境下仍保持60–80%有效性；攻击在距目标6–10米内触发，留给车辆的安全反应时间不足；通过V2X传播虚假紧急消息，使信息峰值年龄增加68–89%，严重干扰安全关键通信。

Conclusion: PHANTOM揭示了CAV系统在感知层和通信层均存在严重安全漏洞，强调需同时加强物理对抗鲁棒性和V2X消息验证机制。

Abstract: Connected autonomous vehicles (CAVs) rely on vision-based deep neural networks (DNNs) and low-latency (Vehicle-to-Everything) V2X communication to navigate safely and efficiently. Despite their advances, these systems remain vulnerable to physical adversarial attacks. In this paper, we introduce PHANTOM (PHysical ANamorphic Threats Obstructing connected vehicle Mobility), a novel framework for crafting and deploying perspective-dependent adversarial examples using \textit{anamorphic art}. PHANTOM exploits geometric distortions that appear natural to humans but are misclassified with high confidence by state-of-the-art object detectors. Unlike conventional attacks, PHANTOM operates in black-box settings without model access and demonstrates strong transferability across four diverse detector architectures (YOLOv5, SSD, Faster R-CNN, and RetinaNet). Comprehensive evaluation in CARLA across varying speeds, weather conditions, and lighting scenarios shows that PHANTOM achieves over 90\% attack success rate under optimal conditions and maintains 60-80\% effectiveness even in degraded environments. The attack activates within 6-10 meters of the target, providing insufficient time for safe maneuvering. Beyond individual vehicle deception, PHANTOM triggers network-wide disruption in CAV systems: SUMO-OMNeT++ co-simulation demonstrates that false emergency messages propagate through V2X links, increasing Peak Age of Information by 68-89\% and degrading safety-critical communication. These findings expose critical vulnerabilities in both perception and communication layers of CAV ecosystems.

Abstract (中文翻译): 网联自动驾驶车辆（CAVs）依赖基于视觉的深度神经网络（DNNs）和低延迟的车联网（V2X）通信来实现安全高效行驶。尽管技术不断进步，这些系统仍容易受到物理对抗攻击的影响。本文提出了PHANTOM（利用变形艺术制造阻碍网联车辆移动的物理变形威胁），这是一种利用“变形艺术”（anamorphic art）构建和部署视角依赖型对抗样本的新框架。PHANTOM利用几何畸变图案，这些图案对人类而言看起来自然，却能被当前最先进的目标检测器以高置信度错误识别。与传统攻击不同，PHANTOM可在无需访问目标模型的黑盒设置下运行，并在四种不同的检测器架构（YOLOv5、SSD、Faster R-CNN 和 RetinaNet）上展现出强大的迁移能力。在CARLA仿真平台中，针对不同车速、天气条件和光照场景的全面评估表明，PHANTOM在理想条件下攻击成功率超过90%，即使在环境退化的情况下仍能保持60%至80%的有效性。该攻击在距离目标6至10米范围内激活，留给车辆的安全操控时间不足。此外，PHANTOM不仅欺骗单个车辆，还会通过V2X链路触发全网范围的干扰：SUMO-OMNeT++联合仿真显示，虚假的紧急消息在网络中传播，导致信息峰值年龄（Peak Age of Information）增加68%至89%，严重损害了安全关键通信。这些发现揭示了CAV生态系统在感知层和通信层均存在关键安全漏洞。

</details>


### [2] [Generating the Past, Present and Future from a Motion-Blurred Image](https://arxiv.org/abs/2512.19817)
*SaiKiran Tedla,Kelly Zhu,Trevor Canham,Felix Taubner,Michael S. Brown,Kiriakos N. Kutulakos,David B. Lindell*

Main category: cs.CV

TL;DR: 本文提出一种新方法，利用预训练的视频扩散模型从单张运动模糊图像中恢复出包含复杂动态信息的视频，揭示拍摄时刻及前后可能发生的场景变化，并在多项下游任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从运动模糊图像中恢复清晰内容或预测视频序列时，依赖手工设计的先验或特定网络结构，难以处理复杂动态场景，也无法重建图像拍摄前后的事件。此外，这些方法未充分利用大规模图像与视频数据中的先验知识。

Method: 该方法重新利用在互联网规模数据集上预训练的视频扩散模型，从单张运动模糊图像中恢复出展现拍摄瞬间及前后短时间内的复杂场景动态视频。

Result: 该方法在恢复视频质量上优于以往方法，能泛化到真实复杂场景，并支持相机轨迹、物体运动和动态3D场景结构等下游任务。

Conclusion: 利用大规模预训练视频扩散模型可有效从运动模糊图像中挖掘丰富的时空信息，为理解场景过去、现在与未来提供强大工具。

Abstract: We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io

Abstract (中文翻译): 我们试图回答这样一个问题：一张运动模糊的图像能揭示场景的过去、现在和未来哪些信息？尽管运动模糊会掩盖图像细节并降低视觉质量，但它也编码了曝光期间场景与相机运动的信息。以往的技术利用这些信息从输入的模糊图像中估计出清晰图像，或预测一段视频帧序列以展示图像捕获时刻可能发生的情况。然而，这些方法依赖手工设计的先验或特定网络架构来解决这一逆问题中的歧义，且未结合大规模数据集上的图像与视频先验。因此，现有方法难以再现复杂的场景动态，也无法恢复图像拍摄之前或之后发生的内容。本文提出了一种新技术，通过重新利用在互联网规模数据集上预训练的视频扩散模型，从单张模糊图像中恢复出能揭示拍摄时刻复杂动态以及紧邻过去或未来可能事件的视频。该方法具有鲁棒性和通用性：在该任务上优于以往方法，能泛化至具有挑战性的野外图像，并支持恢复相机轨迹、物体运动和动态3D场景结构等下游任务。代码和数据可在 https://blur2vid.github.io 获取。

</details>


### [3] [Learning to Refocus with Video Diffusion Models](https://arxiv.org/abs/2512.19823)
*SaiKiran Tedla,Zhoutong Zhang,Xuaner Zhang,Shumian Xin*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频扩散模型的新方法，可从单张失焦图像生成逼真的焦距堆栈视频，实现拍摄后的交互式重对焦，并发布了大规模真实场景手机拍摄的焦距堆栈数据集。


<details>
  <summary>Details</summary>
Motivation: 自动对焦系统常无法准确捕捉用户意图的主体，且用户希望在拍摄后能调整焦点。

Method: 利用视频扩散模型，从单张失焦图像生成感知上准确的焦距堆栈（以视频序列表示）。

Result: 该方法在感知质量和鲁棒性方面均优于现有方法，适用于多种具有挑战性的场景。

Conclusion: 该研究为日常摄影中更高级的焦点编辑能力奠定了基础。

Abstract: Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io

Abstract (中文翻译): 对焦是摄影的核心要素，然而自动对焦系统常常无法准确捕捉用户预期的主体，而且用户经常希望在拍摄后调整焦点。我们提出了一种利用视频扩散模型实现逼真拍摄后重对焦的新方法。该方法仅需一张失焦图像，即可生成在感知上准确的焦距堆栈，并以视频序列表示，从而支持交互式重对焦并拓展多种下游应用。为支持本研究及未来相关工作，我们发布了一个在多样化真实世界智能手机条件下采集的大规模焦距堆栈数据集。我们的方法在感知质量和鲁棒性方面始终优于现有方法，即使在具有挑战性的场景中也表现优异，为日常摄影中更先进的焦点编辑功能铺平了道路。代码和数据可在 www.learn2refocus.github.io 获取。

</details>


### [4] [RANSAC Scoring Functions: Analysis and Reality Check](https://arxiv.org/abs/2512.19850)
*A. Shekhovtsov*

Main category: cs.CV

TL;DR: 本文重新审视了RANSAC中用于评估候选几何模型拟合质量的评分函数。作者将经典的高斯噪声下的几何误差推广至球形噪声，并在鲁棒设定下通过高斯-均匀混合模型统一了基于似然和M估计器的方法。文章指出当前SOTA方法MAGSAC++实际上等价于一个简单的高斯-均匀似然模型，且实验表明包括学习型内点分布在内的各种评分函数性能并无显著差异，MAGSAC++并不优于简单方法，也未对阈值超参数更鲁棒。


<details>
  <summary>Details</summary>
Motivation: RANSAC中评分函数的设计对鲁棒几何拟合至关重要，但现有方法（如MAGSAC++）的理论基础和实际优势尚不清晰，需系统性重审以指导未来研究。

Method: 从概率建模出发，将几何误差扩展到球形噪声；引入高斯-均匀混合模型，在鲁棒设定下统一似然与M估计框架；并通过大验证集或小随机验证集的实验设计评估不同评分函数。

Result: MAGSAC++的评分函数在数值上等价于简单的高斯-均匀似然模型；所有评分函数（含学习型）在实验中表现一致，MAGSAC++既不优于简单方法，也未对阈值更鲁棒。

Conclusion: 当前SOTA评分函数并无实质性优势，其理论与实验需被全面重审，这对后续改进或拓展鲁棒拟合方法至关重要。

Abstract: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.

Abstract (中文翻译): 我们重新审视了为候选几何模型分配评分（即拟合质量）的问题——这是RANSAC鲁棒几何拟合中的关键组成部分。在非鲁棒设定下，被称为“黄金标准”的评分函数即几何误差，它源于带有高斯噪声的概率模型。我们将该模型推广至球形噪声情形。在鲁棒设定下，我们考虑了包含均匀分布离群点的混合模型，并证明基于阈值的参数化方法能够统一基于似然和鲁棒M估计器及其相应的局部优化方案。  
接着，我们分析了MAGSAC++方法，该方法因两点而突出：一是根据现有基准取得了最佳结果；二是其建模假设和推导步骤与其他方法显著不同。然而我们发现，其推导并不符合严谨的概率原则，所得评分函数在数值上实际上等价于所提框架内一个简单的高斯-均匀似然模型。  
最后，我们提出了一种评估评分函数的实验方法：假设有大型验证集，或在期望意义下使用小型随机验证集。我们发现所有评分函数（包括使用学习得到的内点分布的方法）表现完全相同。特别是，MAGSAC++的评分函数既不比简单方法表现更好，也并未对阈值超参数的选择表现出更低的敏感性。  
因此，我们的理论与实验分析全面重审了当前最先进方法，这对于未来旨在改进这些方法或将其应用于其他鲁棒拟合问题的研究至关重要。

</details>


### [5] [HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction](https://arxiv.org/abs/2512.19871)
*Jong Wook Kim,Wonseok Roh,Ha Dam Baek,Pilhyeon Lee,Jonghyun Choi,Sangpil Kim*

Main category: cs.CV

TL;DR: 本文提出HyGE-Occ，一种结合3D高斯与边缘先验的混合视图变换框架，用于提升3D全景占据预测中的几何一致性和边界感知能力，在Occ3D-nuScenes数据集上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D全景占据预测中难以保持精确几何结构并准确捕捉实例的空间范围，影响了全景分割的鲁棒性。

Method: 提出HyGE-Occ框架，通过混合视图变换分支融合连续高斯深度表示与离散深度区间表示以增强BEV特征的几何一致性，并利用从BEV特征中提取的边缘图作为辅助信息提升边界感知。

Result: 在Occ3D-nuScenes数据集上的大量实验表明，HyGE-Occ优于现有方法，展现出更强的3D几何推理能力。

Conclusion: HyGE-Occ有效提升了3D全景占据预测中的几何一致性和实例边界准确性，为复杂场景下的密集3D语义理解提供了新思路。

Abstract: 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.

Abstract (中文翻译): 3D全景占据预测旨在通过预测三维空间中每个被占据区域的语义类别和实例身份，重建密集的体素化场景地图。实现这种细粒度的3D理解需要在复杂环境中进行精确的几何推理并保持空间一致的场景表示。然而，现有方法往往难以维持精确的几何结构，并且无法准确捕捉对鲁棒全景分离至关重要的3D实例空间范围。为克服这些局限，我们提出了HyGE-Occ，一种新颖的框架，它利用结合3D高斯与边缘先验的混合视图变换分支，以增强3D全景占据预测中的几何一致性和边界感知能力。HyGE-Occ采用混合视图变换分支，将基于连续高斯的深度表示与离散化的深度区间表示相融合，生成具有更高几何一致性和结构连贯性的鸟瞰图（BEV）特征。同时，我们从BEV特征中提取边缘图，并将其作为辅助信息用于学习边缘线索。在Occ3D-nuScenes数据集上的大量实验表明，HyGE-Occ优于现有方法，展现出卓越的3D几何推理能力。

</details>


### [6] [Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs](https://arxiv.org/abs/2512.19918)
*Houston H. Zhang,Tao Zhang,Baoze Lin,Yuanqi Xue,Yincheng Zhu,Huan Liu,Li Gu,Linfeng Ye,Ziqiang Wang,Xinxin Zuo,Yang Wang,Yuanhao Yu,Zhixiang Chi*

Main category: cs.CV

TL;DR: 本文提出Widget2Code任务，针对缺乏上下文、布局紧凑的App小部件（widgets）从图像生成代码的问题，构建了首个仅基于图像的小部件基准，并提出了包含感知理解与结构化代码生成的基线方法WidgetFactory，显著提升生成代码的视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有UI2Code研究主要聚焦于网页和移动界面，而App小部件因结构紧凑、上下文缺失、设计专有且缺乏可访问的标记数据，尚未被充分探索。同时，通用多模态大模型在该任务上表现不佳，生成的代码不可靠且视觉不一致。

Method: 作者构建了一个仅基于图像的小部件基准，并提出WidgetFactory基线系统：在感知层面，依据小部件设计原则组合原子组件，引入图标检索与可复用可视化模块；在系统层面，设计了与框架无关的领域特定语言WidgetDSL及其编译器，支持生成多种前端实现（如React、HTML/CSS），并通过自适应渲染模块优化空间紧凑性。

Result: 实验表明，尽管通用多模态大模型优于专用UI2Code方法，但仍存在可靠性与视觉一致性问题；所提出的WidgetFactory显著提升了生成代码的视觉保真度。

Conclusion: 本文正式定义了Widget2Code任务，提供了评估基准与统一基础设施，为未来研究奠定了基础。

Abstract: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.

Abstract (中文翻译): 用户界面到代码（UI2Code）旨在生成可忠实重建给定输入用户界面的可执行代码。以往的研究主要集中于网页和移动屏幕，而应用程序小部件（app widgets）则未被充分探索。与具有丰富层次上下文的网页或移动UI不同，小部件是紧凑、无上下文的微型界面，在严格的空间限制下通过密集布局和图标来概括关键信息。此外，尽管网页或移动UI存在大量（图像，代码）配对数据，但小部件设计属于专有内容，缺乏可访问的标记语言。本文将这一场景形式化为“小部件到代码”（Widget2Code）任务，并引入一个仅基于图像的小部件基准，配备细粒度、多维度的评估指标。基准测试表明，尽管通用多模态大语言模型（MLLMs）优于专门的UI2Code方法，其生成的代码仍不可靠且视觉不一致。为解决这些局限，我们开发了一种基线方法，同步推进感知理解与结构化代码生成。在感知层面，我们遵循小部件设计原则，将原子组件组装为完整布局，并配备图标检索与可复用的可视化模块。在系统层面，我们设计了一个端到端的基础设施WidgetFactory，其中包括一个与框架无关、面向小部件的领域特定语言（WidgetDSL）及其编译器，可将其翻译为多种前端实现（如React、HTML/CSS）。一个自适应渲染模块进一步优化空间尺寸以满足紧凑性约束。这些贡献共同显著提升了视觉保真度，为未来的Widget2Code研究建立了强有力的基线和统一基础设施。

</details>


### [7] [Unified Brain Surface and Volume Registration](https://arxiv.org/abs/2512.19928)
*S. Mazdak Abulnaga,Andrew Hoopes,Malte Hoffmann,Robin Magnet,Maks Ovsjanikov,Lilla Zöllei,John Guttag,Bruce Fischl,Adrian Dalca*

Main category: cs.CV

TL;DR: NeurAlign 是一种基于深度学习的新方法，通过统一的体积-表面表示，在球面坐标空间中同时对大脑皮层和皮下结构进行配准，显著优于现有方法，在准确率、速度和易用性方面均表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统脑MRI配准方法将体素和表面配准分开处理，导致结果不一致，限制了后续分析。因此需要一种能联合对齐皮层与皮下结构、保持解剖一致性的新方法。

Method: 提出 NeurAlign 框架，利用中间球面坐标空间，将解剖表面拓扑与体积解剖结构结合，通过端到端深度学习实现体积与表面的一致配准，并在训练中整合球面配准以保证几何一致性。

Result: 在域内和域外数据集上，NeurAlign 均优于传统和基于机器学习的方法，Dice 分数最高提升7个点，变形场更规则，推理速度快几个数量级，且仅需输入MRI图像即可使用。

Conclusion: NeurAlign 在联合皮层与皮下结构配准任务中实现了更高的准确性、更快的速度和更简便的使用流程，为脑影像分析设立了新标准。

Abstract: Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.

Abstract (中文翻译): 准确配准脑部MRI扫描是神经科学研究中跨被试分析的基础，这需要同时对齐大脑的皮层表面和内部体积。传统方法将体素配准和基于表面的配准分开处理，常常导致不一致性，从而限制了后续分析。我们提出了一种深度学习框架 NeurAlign，通过统一的体积-表面表示，在三维脑MRI图像中联合对齐皮层和皮下区域。该方法利用中间球面坐标空间，将解剖表面拓扑结构与体积解剖信息相连接，从而实现一致且解剖学上准确的对齐。通过在学习过程中整合球面配准，我们的方法确保了体积域与表面域之间的几何一致性。在一系列针对域内和域外数据集的实验中，该方法始终优于经典方法和基于机器学习的配准方法——Dice分数最多提高了7个点，同时保持了规则的形变场。此外，其推理速度比当前标准方法快几个数量级，并且使用更简便，仅需输入MRI扫描图像即可。凭借其卓越的准确性、快速的推理能力和易用性，NeurAlign 为皮层与皮下结构联合配准设立了新标准。

</details>


### [8] [Vehicle-centric Perception via Multimodal Structured Pre-training](https://arxiv.org/abs/2512.19934)
*Wentao Wu,Xiao Wang,Chenglong Li,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出VehicleMAE-V2，一种面向车辆感知的预训练大模型，通过引入对称性、轮廓和语义三种结构化先验知识指导掩码重建过程，显著提升车辆表征学习能力，并在五个下游任务中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预训练阶段缺乏对车辆相关知识的有效学习，导致难以建模通用的车辆感知表征。

Method: 提出VehicleMAE-V2模型，设计三个模块：对称引导掩码模块（SMM）、轮廓引导表征模块（CRM）和语义引导表征模块（SRM），分别利用车辆的对称性、轮廓和语义结构化先验指导掩码重建；同时构建包含400万车辆图像和12,693条文本描述的大规模数据集Autobot4M用于预训练。

Result: 在五个下游任务上的大量实验表明VehicleMAE-V2具有优越的性能。

Conclusion: 通过融合多模态结构化先验知识，VehicleMAE-V2能有效提升车辆中心感知的通用表征能力，为智能交通与自动驾驶等应用提供有力支持。

Abstract: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.

Abstract (中文翻译): 以车辆为中心的感知在许多智能系统中起着至关重要的作用，包括大规模监控系统、智能交通和自动驾驶。现有方法在预训练过程中缺乏对车辆相关知识的有效学习，导致其建模通用车辆感知表征的能力较差。为解决这一问题，我们提出了VehicleMAE-V2，一种新颖的以车辆为中心的预训练大模型。通过探索并利用与车辆相关的多模态结构化先验知识来指导掩码标记重建过程，我们的方法能够显著增强模型学习通用车辆感知表征的能力。具体而言，我们设计了对称引导掩码模块（SMM）、轮廓引导表征模块（CRM）和语义引导表征模块（SRM），分别将车辆的对称性、轮廓和语义三类结构化先验融入到标记重建中。SMM利用车辆对称性约束避免保留对称图像块，从而选择高质量的掩码图像块并减少信息冗余；CRM通过最小化轮廓特征与重建特征之间的概率分布差异，在像素级重建过程中保留完整的车辆结构信息；SRM则通过对比学习和跨模态蒸馏对齐图文特征，以缓解掩码重建过程中因语义理解不足导致的特征混淆问题。为支持VehicleMAE-V2的预训练，我们构建了Autobot4M——一个包含约400万张车辆图像和12,693条文本描述的大规模数据集。在五个下游任务上的大量实验验证了VehicleMAE-V2的卓越性能。

</details>


### [9] [Block-Recurrent Dynamics in Vision Transformers](https://arxiv.org/abs/2512.19941)
*Mozes Jacobs,Thomas Fel,Richard Hakim,Alessandra Brondetta,Demba Ba,T. Andy Keller*

Main category: cs.CV

TL;DR: 本文提出“块循环假设”（BRH），认为视觉Transformer（ViT）的深层结构可被压缩为少量可重复使用的模块，并通过训练递归代理模型Raptor验证该假设；实验表明ViT内部存在低复杂度的动态结构，支持用动力系统理论进行解释。


<details>
  <summary>Details</summary>
Motivation: 当前对Vision Transformer（ViT）内部计算机制的理解仍不充分，尽管其架构暗示了某种动态结构，但缺乏将模型深度解释为明确动力学流程的统一框架。作者旨在建立一种可解释ViT深度计算本质的新视角。

Method: 提出“块循环假设”（BRH），即ViT中L个块的计算可由k≪L个不同块递归实现；通过分析层间表征相似性识别相位结构，并训练递归代理模型Raptor来拟合预训练ViT；进一步开展动力学可解释性分析，研究表征轨迹、token动态及更新秩变化。

Result: 在小规模实验中发现随机深度和训练促进循环结构形成；成功训练仅含2个块的Raptor模型，在同等计算成本下恢复DINOv2在ImageNet-1k上线性探针96%的准确率；观察到类依赖的角盆地收敛、token特异性动态及晚期低秩更新等动力学特性。

Conclusion: ViT的深度计算可被理解为一个紧凑的递归程序，体现出低复杂度的规范解，这使其适合通过动力系统理论进行原则性分析，为Transformer的机制可解释性提供了新路径。

Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.

Abstract (中文翻译): 随着视觉Transformer（ViTs）成为标准的视觉骨干网络，对其计算现象进行机制性解释变得至关重要。尽管其架构线索暗示了某种动态结构，但目前尚无公认的框架能将Transformer的深度解释为一种具有良好特征的动力学流。在本研究中，我们提出了“块循环假设”（Block-Recurrent Hypothesis, BRH），认为经过训练的ViT具有块循环的深度结构，即原始L个块的计算可以被精确地重写为仅使用k≪L个不同块的递归应用。在多种ViT模型中，层间表征相似性矩阵显示出少数连续的阶段。为了判断这些阶段是否反映了真正可复用的计算，我们训练了预训练ViT的块循环代理模型：Raptor（Recurrent Approximations to Phase-structured TransfORmers）。在小规模实验中，我们证明了随机深度和训练过程促进了循环结构的形成，并与我们准确拟合Raptor的能力相关。随后，我们通过训练一个仅含2个块的Raptor模型，在同等计算成本下恢复了DINOv2在ImageNet-1k上线性探针96%的准确率，从而为BRH提供了经验性的存在性证明。最后，我们利用该假设开展了一项“动力学可解释性”研究计划，发现：(i) 表征沿方向收敛至类别依赖的角盆地，且在微小扰动下具有自校正轨迹；(ii) token特异性动态，其中cls token在后期执行急剧的方向重定向，而patch tokens在后期表现出朝向其均值方向的强一致性；(iii) 在深度后期，更新坍缩为低秩，符合向低维吸引子收敛的特征。总体而言，我们发现ViT深度中涌现出一个紧凑的递归程序，指向一种低复杂度的规范解，使得这些模型能够通过基于原理的动力系统分析方法加以研究。

</details>


### [10] [SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction](https://arxiv.org/abs/2512.19943)
*Haoyi Zhong,Fang-Lue Zhang,Andrew Chalmers,Taehyun Rhee*

Main category: cs.CV

TL;DR: 提出SE360框架，通过自动数据生成和两阶段优化策略，实现高质量、语义准确的360度全景图像多条件引导对象编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的360度全景图像编辑方法在等距柱状投影（ERP）和透视视图中常产生不合理结果，缺乏语义意义和几何一致性。

Method: 构建一种无需人工干预的粗到精自主数据生成流程，结合视觉-语言模型（VLM）与自适应投影调整进行分层分析；采用低成本两阶段数据优化策略提升真实感并减少擦除伪影；在此基础上训练基于Transformer的扩散模型，支持文本、掩码或参考图像引导的编辑。

Result: 实验表明，该方法在视觉质量和语义准确性方面均优于现有方法。

Conclusion: SE360有效解决了360度全景图像编辑中的语义与几何一致性问题，为多条件引导编辑提供了高质量解决方案。

Abstract: While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.

Abstract (中文翻译): 尽管基于指令的图像编辑正在兴起，但将其扩展到360度全景图像会带来额外挑战。现有方法在等距柱状投影（ERP）和透视视图中常常产生不合理的结果。为解决这些局限性，我们提出了SE360——一种用于360度全景图像中多条件引导对象编辑的新框架。其核心是一种无需人工干预的由粗到精的自主数据生成流程。该流程利用视觉-语言模型（VLM）和自适应投影调整进行分层分析，确保对象及其物理上下文的整体分割。所生成的数据对即使来自未标注的全景图像，也兼具语义意义和几何一致性。此外，我们引入了一种低成本的两阶段数据优化策略，以提升数据真实感并减轻模型对擦除伪影的过拟合。基于所构建的数据集，我们训练了一个基于Transformer的扩散模型，支持在360度全景图像中通过文本、掩码或参考图像进行灵活的对象编辑。实验表明，我们的方法在视觉质量和语义准确性方面均优于现有方法。

</details>
