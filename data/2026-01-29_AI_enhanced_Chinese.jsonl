{"id": "2601.20051", "pdf": "https://arxiv.org/pdf/2601.20051", "abs": "https://arxiv.org/abs/2601.20051", "authors": ["Gautham Vinod", "Bruce Coburn", "Siddeshwar Raghavan", "Jiangpeng He", "Fengqing Zhu"], "title": "Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4ece\u5355\u76ee\u56fe\u50cf\u4e2d\u6062\u590d\u771f\u5b9e\u5c3a\u5ea63D\u98df\u7269\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u98df\u7269\u4f53\u79ef\u4f30\u7b97\u7cbe\u5ea6\uff0c\u52a9\u529b\u7cbe\u51c6\u8425\u517b\u3002", "motivation": "\u73b0\u6709AI\u81b3\u98df\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u4ece\u5355\u76ee\u56fe\u50cf\u51c6\u786e\u4f30\u8ba1\u98df\u7269\u5206\u91cf\uff08\u201c\u5403\u4e86\u591a\u5c11\u201d\uff09\uff0c\u56e0\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u5c3a\u5ea6\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u5728\u7cbe\u51c6\u8425\u517b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u63d0\u53d6\u7684\u4e30\u5bcc\u89c6\u89c9\u7279\u5f81\uff0c\u4f30\u8ba13D\u91cd\u5efa\u7269\u4f53\u7684\u771f\u5b9e\u5c3a\u5ea6\uff0c\u5c06\u5355\u89c6\u89d23D\u91cd\u5efa\u8f6c\u6362\u4e3a\u5177\u6709\u7269\u7406\u610f\u4e49\u7684\u771f\u5b9e\u5c3a\u5bf8\u6a21\u578b\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u5e73\u5747\u4f53\u79ef\u4f30\u7b97\u8bef\u5dee\u964d\u4f4e\u8fd130%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5f25\u5408\u4e863D\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6570\u5b57\u5065\u5eb7\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5728\u7cbe\u51c6\u8425\u517b\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002", "summary_cn": "\u4e0e\u996e\u98df\u76f8\u5173\u7684\u6162\u6027\u75be\u75c5\uff08\u5982\u80a5\u80d6\u75c7\u548c\u7cd6\u5c3f\u75c5\uff09\u7684\u589e\u52a0\uff0c\u51f8\u663e\u4e86\u51c6\u786e\u76d1\u6d4b\u98df\u7269\u6444\u5165\u91cf\u7684\u5fc5\u8981\u6027\u3002\u5c3d\u7ba1\u8fd1\u5e74\u6765\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u81b3\u98df\u8bc4\u4f30\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u4ece\u5355\u76ee\u56fe\u50cf\u4e2d\u6062\u590d\u5c3a\u5bf8\uff08\u4efd\u91cf\uff09\u4fe1\u606f\u4ee5\u51c6\u786e\u56de\u7b54\u201c\u4f60\u5403\u4e86\u591a\u5c11\uff1f\u201d\u8fd9\u4e00\u95ee\u9898\uff0c\u56e0\u5176\u672c\u8eab\u662f\u4e00\u4e2a\u75c5\u6001\u95ee\u9898\uff0c\u4ecd\u662f\u4e00\u9879\u7d27\u8feb\u7684\u6311\u6218\u3002\u4e00\u4e9b3D\u91cd\u5efa\u65b9\u6cd5\u867d\u5728\u51e0\u4f55\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6548\u679c\uff0c\u5374\u672a\u80fd\u6062\u590d\u91cd\u5efa\u7269\u4f53\u7684\u5173\u952e\u771f\u5b9e\u4e16\u754c\u5c3a\u5ea6\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u7cbe\u51c6\u8425\u517b\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u4ece\u5355\u76ee\u56fe\u50cf\u4e2d\u6062\u590d\u771f\u5b9e\u5c3a\u5ea63D\u91cd\u5efa\u7269\u4f53\u7684\u65b9\u6cd5\uff0c\u5f25\u5408\u4e863D\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6570\u5b57\u5065\u5eb7\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u6240\u63d0\u53d6\u7684\u4e30\u5bcc\u89c6\u89c9\u7279\u5f81\u6765\u4f30\u8ba1\u91cd\u5efa\u7269\u4f53\u7684\u5c3a\u5ea6\u3002\u8fd9\u79cd\u5b66\u4e60\u5230\u7684\u5c3a\u5ea6\u4f7f\u6211\u4eec\u80fd\u591f\u5c06\u5355\u89c6\u89d23D\u91cd\u5efa\u8f6c\u6362\u4e3a\u771f\u5b9e\u3001\u5177\u6709\u7269\u7406\u610f\u4e49\u7684\u6a21\u578b\u3002\u5728\u4e24\u4e2a\u516c\u5f00\u53ef\u7528\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e73\u5747\u7edd\u5bf9\u4f53\u79ef\u4f30\u7b97\u8bef\u5dee\u964d\u4f4e\u4e86\u8fd130%\uff0c\u5c55\u793a\u4e86\u5176\u5728\u589e\u5f3a\u7cbe\u51c6\u8425\u517b\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.20064", "pdf": "https://arxiv.org/pdf/2601.20064", "abs": "https://arxiv.org/abs/2601.20064", "authors": ["Zhen Yao", "Xin Li", "Taotao Jing", "Shuai Zhang", "Mooi Choo Chuah"], "title": "DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "19 pages, 11 figures", "summary": "Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDiSa\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u663e\u8457\u6027\u7ebf\u7d22\u89e3\u8026\u524d\u666f\u4e0e\u80cc\u666f\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u5c42\u6b21\u5316\u7cbe\u70bc\u6a21\u5757\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u524d\u666f\u504f\u501a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u6a21\u7cca\u95ee\u9898\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a\u4e00\u662f\u5bf9\u663e\u8457\u524d\u666f\u533a\u57df\u7684\u504f\u597d\u5bfc\u81f4\u5ffd\u7565\u80cc\u666f\uff08\u524d\u666f\u504f\u501a\uff09\uff0c\u4e8c\u662f\u7f3a\u4e4f\u7cbe\u786e\u7684\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u5bfc\u81f4\u7269\u4f53\u8fb9\u754c\u6a21\u7cca\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDiSa\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u663e\u8457\u6027\u611f\u77e5\u89e3\u8026\u6a21\u5757\uff08SDM\uff09\uff0c\u5229\u7528\u663e\u8457\u6027\u7ebf\u7d22\u5c06\u524d\u666f\u4e0e\u80cc\u666f\u7279\u5f81\u5206\u522b\u5efa\u6a21\uff1b2\uff09\u5c42\u6b21\u5316\u7cbe\u70bc\u6a21\u5757\uff08HRM\uff09\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u66f4\u65b0\u8fdb\u884c\u50cf\u7d20\u7ea7\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u901a\u9053\u7ea7\u7279\u5f81\u7cbe\u70bc\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDiSa\u65b9\u6cd5\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u663e\u8457\u6027\u4fe1\u606f\u5e76\u89e3\u8026\u524d\u666f\u4e0e\u80cc\u666f\u7279\u5f81\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u7cbe\u70bc\u7b56\u7565\uff0cDiSa\u6709\u6548\u514b\u670d\u4e86VLM\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u56fa\u6709\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002", "summary_cn": "\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65e8\u5728\u6839\u636e\u6587\u672c\u6807\u7b7e\u4e3a\u56fe\u50cf\u4e2d\u7684\u6bcf\u4e2a\u50cf\u7d20\u5206\u914d\u8bed\u4e49\u6807\u7b7e\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u8fdb\u884c\u5bc6\u96c6\u9884\u6d4b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u5173\u6ce8\u663e\u8457\u7684\u3001\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u533a\u57df\uff0c\u5728\u5e94\u7528\u4e8e\u5206\u5272\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u4e24\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff1a\uff08i\uff09\u524d\u666f\u504f\u501a\uff0c\u5373\u5bb9\u6613\u5ffd\u7565\u80cc\u666f\u533a\u57df\uff1b\uff08ii\uff09\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u6709\u9650\uff0c\u5bfc\u81f4\u7269\u4f53\u8fb9\u754c\u6a21\u7cca\u3002\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DiSa\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u663e\u8457\u6027\u611f\u77e5\u524d\u666f-\u80cc\u666f\u533a\u5206\u89e3\u8026\u6846\u67b6\u3002\u901a\u8fc7\u5728\u6240\u8bbe\u8ba1\u7684\u663e\u8457\u6027\u611f\u77e5\u89e3\u8026\u6a21\u5757\uff08SDM\uff09\u4e2d\u663e\u5f0f\u5f15\u5165\u663e\u8457\u6027\u7ebf\u7d22\uff0cDiSa\u4ee5\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u5f0f\u5206\u522b\u5efa\u6a21\u524d\u666f\u4e0e\u80cc\u666f\u7684\u96c6\u6210\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c42\u6b21\u5316\u7cbe\u70bc\u6a21\u5757\uff08HRM\uff09\uff0c\u8be5\u6a21\u5757\u5229\u7528\u50cf\u7d20\u7ea7\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u591a\u5c42\u7ea7\u66f4\u65b0\u5b9e\u73b0\u901a\u9053\u7ea7\u7279\u5f81\u7cbe\u70bc\u3002\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDiSa\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.20072", "pdf": "https://arxiv.org/pdf/2601.20072", "abs": "https://arxiv.org/abs/2601.20072", "authors": ["Atik Faysal", "Mohammad Rostami", "Reihaneh Gh. Roshan", "Nikhil Muralidhar", "Huaxia Wang"], "title": "Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSSMAE\u6846\u67b6\uff0c\u5728\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u4f46\u65e0\u6807\u7b7e\u6570\u636e\u4e30\u5bcc\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u4e0e\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9a8c\u8bc1\u7684\u95e8\u63a7\u673a\u5236\u52a8\u6001\u542f\u7528\u4f2a\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347Vision Transformer\u5728\u4f4e\u6807\u7b7e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u4f46\u65e0\u6807\u7b7e\u6570\u636e\u4e30\u5bcc\u7684\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u8bad\u7ec3Vision Transformer\uff08ViT\uff09\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u4f2a\u6807\u7b7e\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\u65f6\u5bb9\u6613\u5f15\u5165\u786e\u8ba4\u504f\u8bef\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faSemi-Supervised Masked Autoencoder\uff08SSMAE\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u6709\u6807\u7b7e\u548c\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u8054\u5408\u4f18\u5316\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u4e0e\u5206\u7c7b\u4efb\u52a1\uff1b\u5f15\u5165\u9a8c\u8bc1\u9a71\u52a8\u7684\u95e8\u63a7\u673a\u5236\uff0c\u4ec5\u5f53\u6a21\u578b\u5728\u5f31\u589e\u5f3a\u548c\u5f3a\u589e\u5f3a\u89c6\u56fe\u4e0b\u5747\u4ea7\u751f\u9ad8\u7f6e\u4fe1\u5ea6\u4e14\u4e00\u81f4\u7684\u9884\u6d4b\u65f6\uff0c\u624d\u542f\u7528\u4f2a\u6807\u7b7e\uff0c\u4ee5\u51cf\u5c11\u786e\u8ba4\u504f\u8bef\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u4e0a\uff0cSSMAE\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u76d1\u7763ViT\u548c\u5fae\u8c03\u540e\u7684MAE\uff0c\u5c24\u5176\u5728\u4f4e\u6807\u7b7e\u6bd4\u4f8b\u4e0b\u63d0\u5347\u663e\u8457\uff08\u5982CIFAR-10\u4f7f\u752810%\u6807\u7b7e\u65f6\u6bd4ViT\u9ad8\u51fa9.24%\uff09\u3002", "conclusion": "\u5728\u6570\u636e\u9ad8\u6548\u7684Transformer\u8bad\u7ec3\u4e2d\uff0c\u4f2a\u6807\u7b7e\u7684\u5f15\u5165\u65f6\u673a\u4e0e\u5176\u751f\u6210\u65b9\u5f0f\u540c\u6837\u91cd\u8981\uff1b\u6240\u63d0SSMAE\u6709\u6548\u7f13\u89e3\u4e86\u786e\u8ba4\u504f\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763ViT\u7684\u6027\u80fd\u3002", "summary_cn": "\u6211\u4eec\u9488\u5bf9\u6807\u7b7e\u6570\u636e\u7a00\u7f3a\u4f46\u65e0\u6807\u7b7e\u6570\u636e\u4e30\u5bcc\u7684\u573a\u666f\u4e0b\u8bad\u7ec3Vision Transformer\uff08ViT\uff09\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u534a\u76d1\u7763\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08Semi-Supervised Masked Autoencoder, SSMAE\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u6709\u6807\u7b7e\u548c\u65e0\u6807\u7b7e\u6837\u672c\uff0c\u8054\u5408\u4f18\u5316\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u4e0e\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u9009\u62e9\u7684\u4f2a\u6807\u7b7e\u3002SSMAE\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a8c\u8bc1\u7684\u95e8\u63a7\u673a\u5236\uff1a\u53ea\u6709\u5f53\u6a21\u578b\u5728\u540c\u4e00\u6837\u672c\u7684\u5f31\u589e\u5f3a\u548c\u5f3a\u589e\u5f3a\u89c6\u56fe\u4e0b\u5747\u80fd\u4ea7\u751f\u53ef\u9760\u4e14\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u4e00\u81f4\u9884\u6d4b\u65f6\uff0c\u624d\u6fc0\u6d3b\u4f2a\u6807\u7b7e\uff0c\u4ece\u800c\u51cf\u5c11\u786e\u8ba4\u504f\u8bef\u3002\u5728CIFAR-10\u548cCIFAR-100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSMAE\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u76d1\u7763\u8bad\u7ec3\u7684ViT\u548c\u5fae\u8c03\u540e\u7684MAE\uff0c\u5c24\u5176\u5728\u4f4e\u6807\u7b7e\u6bd4\u4f8b\u4e0b\u63d0\u5347\u663e\u8457\uff08\u4f8b\u5982\u5728CIFAR-10\u4f7f\u752810%\u6807\u7b7e\u65f6\uff0c\u6027\u80fd\u6bd4ViT\u9ad8\u51fa9.24%\uff09\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6570\u636e\u9ad8\u6548\u7684Transformer\u8bad\u7ec3\u4e2d\uff0c\u4f2a\u6807\u7b7e\u7684\u5f15\u5165\u65f6\u673a\u4e0e\u5176\u751f\u6210\u65b9\u5f0f\u540c\u7b49\u91cd\u8981\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff1ahttps://github.com/atik666/ssmae\u3002"}}
{"id": "2601.20075", "pdf": "https://arxiv.org/pdf/2601.20075", "abs": "https://arxiv.org/abs/2601.20075", "authors": ["Chuan Qin", "Constantin Venhoff", "Sonia Joseph", "Fanyi Xiao", "Stefan Scherer"], "title": "Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.\n  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728CLIP\u8bad\u7ec3\u4e2d\u76f4\u63a5\u5f15\u5165\u7a00\u758f\u6027\uff0c\u4ee5\u540c\u65f6\u83b7\u5f97\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u6027\u80fd\u7684\u8868\u793a\uff0c\u4f18\u4e8e\u540e\u5904\u7406\u65b9\u6cd5\uff08\u5982\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff09\uff0c\u5e76\u4fdd\u7559\u591a\u6a21\u6001\u80fd\u529b\u3002", "motivation": "CLIP\u867d\u7136\u6210\u529f\uff0c\u4f46\u5176\u7a20\u5bc6\u4e14\u4e0d\u900f\u660e\u7684\u6f5c\u5728\u8868\u793a\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1b\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\uff08\u5982SAE\uff09\u867d\u63d0\u5347\u53ef\u89e3\u91ca\u6027\uff0c\u5374\u635f\u5bb3\u4e0b\u6e38\u6027\u80fd\u548c\u591a\u6a21\u6001\u80fd\u529b\u3002", "method": "\u5728CLIP\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u96c6\u6210\u7a00\u758f\u6027\u7ea6\u675f\uff0c\u751f\u6210\u7a00\u758f\u3001\u53ef\u89e3\u91ca\u4e14\u4fdd\u6301\u6027\u80fd\u7684\u8868\u793a\u3002", "result": "\u6240\u63d0\u51fa\u7684Sparse CLIP\u5728\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u591a\u6a21\u6001\u80fd\u529b\uff0c\u5e76\u652f\u6301\u8bed\u4e49\u6982\u5ff5\u5bf9\u9f50\u4e0e\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u5f15\u5bfc\u3002", "conclusion": "\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u5e76\u975e\u6b64\u6d88\u5f7c\u957f\uff0c\u800c\u662f\u53ef\u4ee5\u534f\u540c\u4f18\u5316\uff1b\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u539f\u5219\u3002", "summary_cn": "\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u5df2\u6210\u4e3a\u89c6\u89c9-\u8bed\u8a00\u8868\u5f81\u5b66\u4e60\u7684\u57fa\u77f3\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u7c7b\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u4f5c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u9ed8\u8ba4\u7684\u89c6\u89c9\u4e3b\u5e72\u7f51\u7edc\u3002\u5c3d\u7ba1\u53d6\u5f97\u6210\u529f\uff0cCLIP\u7684\u7a20\u5bc6\u4e14\u4e0d\u900f\u660e\u7684\u6f5c\u5728\u8868\u5f81\u5e26\u6765\u4e86\u663e\u8457\u7684\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002\u901a\u5e38\u8ba4\u4e3a\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u5f15\u5165\u7a00\u758f\u6027\u4f1a\u964d\u4f4e\u51c6\u786e\u7387\uff0c\u56e0\u6b64\u8fd1\u671f\u7814\u7a76\u8f6c\u5411\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u5982\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u540e\u5904\u7406\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u4e0b\u6e38\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u524a\u5f31CLIP\u56fa\u6709\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u4e14\u6240\u5b66\u7279\u5f81\u5927\u591a\u4ecd\u4e3a\u5355\u6a21\u6001\u3002\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c06\u7a00\u758f\u6027\u76f4\u63a5\u6574\u5408\u8fdbCLIP\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ece\u800c\u83b7\u5f97\u517c\u5177\u53ef\u89e3\u91ca\u6027\u4e0e\u9ad8\u6027\u80fd\u7684\u8868\u5f81\u3002\u4e0eSAEs\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u7a00\u758fCLIP\u8868\u5f81\u5728\u4fdd\u6301\u5f3a\u5927\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u4fdd\u7559\u4e86\u591a\u6a21\u6001\u80fd\u529b\u3002\u6211\u4eec\u5c55\u793a\u4e86\u591a\u6a21\u6001\u7a00\u758f\u7279\u5f81\u80fd\u591f\u5b9e\u73b0\u76f4\u63a5\u7684\u8bed\u4e49\u6982\u5ff5\u5bf9\u9f50\uff0c\u5e76\u63ed\u793a\u4e86\u8de8\u6a21\u6001\u77e5\u8bc6\u5982\u4f55\u5728\u8bad\u7ec3\u4e2d\u6d8c\u73b0\u7684\u52a8\u6001\u8fc7\u7a0b\u3002\u6700\u540e\uff0c\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u6211\u4eec\u5728\u7a00\u758fCLIP\u8868\u5f81\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u89c6\u89c9\u7684\u5f15\u5bfc\u80fd\u529b\u3002\u6211\u4eec\u7684\u53d1\u73b0\u6311\u6218\u4e86\u201c\u53ef\u89e3\u91ca\u6027\u5fc5\u987b\u727a\u7272\u51c6\u786e\u6027\u201d\u7684\u4f20\u7edf\u89c2\u5ff5\uff0c\u8bc1\u660e\u53ef\u89e3\u91ca\u6027\u4e0e\u6027\u80fd\u53ef\u4ee5\u534f\u540c\u4f18\u5316\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u539f\u5219\u3002"}}
{"id": "2601.20104", "pdf": "https://arxiv.org/pdf/2601.20104", "abs": "https://arxiv.org/abs/2601.20104", "authors": ["Nima Torbati", "Anastasia Meshcheryakova", "Ramona Woitek", "Sepideh Hatamikia", "Diana Mechtcheriakova", "Amirreza Mahbod"], "title": "NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation", "categories": ["cs.CV"], "comment": "31 pages", "summary": "Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.\n  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.\n  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.", "AI": {"tldr": "\u672c\u6587\u4e0d\u805a\u7126\u4e8e\u65b0\u6a21\u578b\u5f00\u53d1\uff0c\u800c\u662f\u7cfb\u7edf\u8bc4\u4f30\u548c\u6807\u51c6\u5316\u7528\u4e8eH&E\u67d3\u8272\u56fe\u50cf\u6838\u5b9e\u4f8b\u5206\u5272\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u8bad\u7ec3\u96c6\uff08NucFuse-train\uff09\u4e0e\u6d4b\u8bd5\u96c6\uff08NucFuse-test\uff09\uff0c\u4e3a\u8be5\u4efb\u52a1\u5efa\u7acb\u65b0\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u6838\u5b9e\u4f8b\u5206\u5272\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u65b0\u7b97\u6cd5\u5f00\u53d1\uff0c\u4e14\u5728\u6709\u9650\u3001\u4efb\u610f\u9009\u53d6\u7684\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u96c6\u672c\u8eab\u8d28\u91cf\u4e0e\u9002\u7528\u6027\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u963b\u788d\u4e86\u6a21\u578b\u6027\u80fd\u7684\u516c\u5e73\u6bd4\u8f83\u4e0e\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u6536\u96c6\u5e76\u6807\u51c6\u5316\u591a\u4e2a\u516c\u5f00\u7684\u624b\u52a8\u6807\u6ce8H&E\u67d3\u8272\u56fe\u50cf\u6570\u636e\u96c6\uff1b\u4f7f\u7528\u4e24\u79cd\u5148\u8fdb\u6a21\u578b\uff08CNN\u4e0eCNN-ViT\u6df7\u5408\u67b6\u6784\uff09\u5bf9\u5404\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u4e0e\u6392\u5e8f\uff1b\u6784\u5efa\u7edf\u4e00\u7684\u8bad\u7ec3\u96c6\uff08NucFuse-train\uff09\u548c\u6d4b\u8bd5\u96c6\uff08NucFuse-test\uff09\u4ee5\u652f\u6301\u8de8\u6570\u636e\u96c6\u516c\u5e73\u8bc4\u4f30\u4e0e\u6027\u80fd\u63d0\u5347\u3002", "result": "\u6210\u529f\u5bf9\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u4e0e\u6392\u5e8f\uff0c\u9a8c\u8bc1\u4e86\u878d\u5408\u591a\u6570\u636e\u96c6\u53ef\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5916\u90e8\u9a8c\u8bc1\u8bc1\u660e\u4e86\u6240\u63d0\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aH&E\u67d3\u8272\u7ec4\u7ec7\u56fe\u50cf\u4e2d\u7684\u6838\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u6570\u636e\u57fa\u51c6\u3001\u7edf\u4e00\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u5f00\u6e90\u5b9e\u73b0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u66f4\u53ef\u9760\u3001\u53ef\u590d\u73b0\u7684\u7814\u7a76\u3002", "summary_cn": "\u82cf\u6728\u7cbe-\u4f0a\u7ea2\uff08H&E\uff09\u67d3\u8272\u56fe\u50cf\u4e2d\u7684\u7ec6\u80de\u6838\u5b9e\u4f8b\u5206\u5272\u5728\u81ea\u52a8\u5316\u7ec4\u7ec7\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u626e\u6f14\u7740\u91cd\u8981\u89d2\u8272\uff0c\u5e76\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u5c3d\u7ba1\u5df2\u6709\u591a\u79cd\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u88ab\u63d0\u51fa\u7528\u4e8e\u7ec6\u80de\u6838\u5b9e\u4f8b\u5206\u5272\uff0c\u4f46\u8be5\u9886\u57df\u7684\u5927\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u4e8e\u5f00\u53d1\u65b0\u7684\u5206\u5272\u7b97\u6cd5\uff0c\u5e76\u4ec5\u5728\u6570\u91cf\u6709\u9650\u4e14\u4efb\u610f\u9009\u53d6\u7684\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002  \n\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5e76\u672a\u805a\u7126\u4e8e\u6a21\u578b\u5f00\u53d1\uff0c\u800c\u662f\u5173\u6ce8\u7528\u4e8e\u8be5\u4efb\u52a1\u7684\u6570\u636e\u96c6\u3002\u57fa\u4e8e\u5e7f\u6cdb\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u591a\u4e2a\u624b\u52a8\u6807\u6ce8\u3001\u516c\u5f00\u53ef\u7528\u7684H&E\u67d3\u8272\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u6807\u51c6\u5316\u4e3a\u7edf\u4e00\u7684\u8f93\u5165\u548c\u6807\u6ce8\u683c\u5f0f\u3002\u6211\u4eec\u5229\u7528\u4e24\u79cd\u6700\u5148\u8fdb\u7684\u5206\u5272\u6a21\u578b\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u53e6\u4e00\u79cd\u57fa\u4e8eCNN\u4e0e\u89c6\u89c9Transformer\u7684\u6df7\u5408\u67b6\u6784\u2014\u2014\u7cfb\u7edf\u5730\u8bc4\u4f30\u5e76\u6839\u636e\u5176\u7ec6\u80de\u6838\u5b9e\u4f8b\u5206\u5272\u6027\u80fd\u5bf9\u8fd9\u4e9b\u6570\u636e\u96c6\u8fdb\u884c\u6392\u5e8f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d4b\u8bd5\u96c6\uff08NucFuse-test\uff09\u7528\u4e8e\u516c\u5e73\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u96c6\uff08NucFuse-train\uff09\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u6570\u636e\u96c6\u7684\u56fe\u50cf\u4ee5\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002  \n\u901a\u8fc7\u8bc4\u4f30\u548c\u6392\u5e8f\u6570\u636e\u96c6\u3001\u5f00\u5c55\u5168\u9762\u5206\u6790\u3001\u751f\u6210\u878d\u5408\u6570\u636e\u96c6\u3001\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\u5e76\u5c06\u6211\u4eec\u7684\u5b9e\u73b0\u516c\u5f00\u53d1\u5e03\uff0c\u6211\u4eec\u4e3aH&E\u67d3\u8272\u7ec4\u7ec7\u5b66\u56fe\u50cf\u4e0a\u7684\u7ec6\u80de\u6838\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u7684\u8bad\u7ec3\u3001\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2601.20107", "pdf": "https://arxiv.org/pdf/2601.20107", "abs": "https://arxiv.org/abs/2601.20107", "authors": ["Zhuchenyang Liu", "Ziyu Hu", "Yao Zhang", "Yu Xiao"], "title": "Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing", "categories": ["cs.CV", "cs.CL", "cs.IR"], "comment": "18 pages, 6 figures, 11 tables", "summary": "Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u526a\u679d\u65b9\u6cd5 SAP\uff0c\u901a\u8fc7\u4ece\u4e2d\u5c42\u8bc6\u522b\u5173\u952e\u89c6\u89c9\u5757\uff0c\u5728 ViDoRe \u57fa\u51c6\u4e0a\u5b9e\u73b0\u8d85\u8fc7 90% \u7684\u7d22\u5f15\u5411\u91cf\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5f15\u5165 OSR \u534f\u8bae\u63ed\u793a\u4e2d\u5c42\u8bed\u4e49\u7ed3\u6784\u951a\u70b9\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u65e0\u8bad\u7ec3\u526a\u679d\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e EOS \u6ce8\u610f\u529b\u7684\u65b9\u6cd5\uff09\u5728\u9ad8\u538b\u7f29\u7387\uff08>80%\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u968f\u673a\u9009\u62e9\uff1b\u5148\u524d\u7814\u7a76\u8ba4\u4e3a\u89c6\u89c9 token \u7684\u91cd\u8981\u6027\u4f9d\u8d56\u4e8e\u67e5\u8be2\uff0c\u4ece\u800c\u8d28\u7591\u65e0\u8bad\u7ec3\u526a\u679d\u7684\u53ef\u884c\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u6709\u6548\u7684\u65e0\u8bad\u7ec3\u526a\u679d\u7b56\u7565\u3002", "method": "\u63d0\u51fa Structural Anchor Pruning (SAP) \u65b9\u6cd5\uff0c\u4ece\u6a21\u578b\u4e2d\u95f4\u5c42\u8bc6\u522b\u5173\u952e\u89c6\u89c9 patch \u8fdb\u884c\u526a\u679d\uff1b\u540c\u65f6\u5f15\u5165 Oracle Score Retention (OSR) \u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u5c42\u4fe1\u606f\u5bf9\u538b\u7f29\u6548\u7387\u7684\u5f71\u54cd\u3002", "result": "\u5728 ViDoRe \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAP \u65b9\u6cd5\u5728\u538b\u7f29\u7d22\u5f15\u5411\u91cf\u8d85\u8fc7 90% \u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u5927\u7684\u68c0\u7d22\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u8bad\u7ec3\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u89c9\u8bed\u4e49\u7ed3\u6784\u951a\u70b9\u5b58\u5728\u4e8e\u6a21\u578b\u4e2d\u95f4\u5c42\u800c\u975e\u6700\u7ec8\u5c42\uff0c\u8fd9\u4e3a\u9ad8\u6548\u65e0\u8bad\u7ec3\u526a\u679d\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff1bSAP \u4e3a\u89c6\u89c9 RAG \u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u8fd1\u671f\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982 ColPali\uff09\u80fd\u591f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u6587\u6863\u68c0\u7d22\uff08VDR\uff09\uff0c\u4f46\u4f1a\u5e26\u6765\u9ad8\u6602\u7684\u7d22\u5f15\u5411\u91cf\u5b58\u50a8\u5f00\u9500\u3002\u65e0\u9700\u8bad\u7ec3\u7684\u526a\u679d\u65b9\u6cd5\uff08\u4f8b\u5982\u57fa\u4e8e EOS \u6ce8\u610f\u529b\u7684\u65b9\u6cd5\uff09\u53ef\u5728\u4e0d\u8c03\u6574\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5c06\u7d22\u5f15\u5411\u91cf\u5927\u5c0f\u51cf\u5c11\u7ea6 60%\uff0c\u4f46\u5728\u9ad8\u538b\u7f29\u573a\u666f\uff08>80%\uff09\u4e0b\u5f80\u5f80\u8868\u73b0\u4e0d\u5982\u968f\u673a\u9009\u62e9\u3002\u5148\u524d\u7684\u7814\u7a76\uff08\u4f8b\u5982 Light-ColPali\uff09\u5c06\u6b64\u5f52\u56e0\u4e8e\u89c6\u89c9 token \u7684\u91cd\u8981\u6027\u672c\u8d28\u4e0a\u4f9d\u8d56\u4e8e\u67e5\u8be2\uff0c\u4ece\u800c\u8d28\u7591\u4e86\u65e0\u9700\u8bad\u7ec3\u526a\u679d\u7684\u53ef\u884c\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86 Structural Anchor Pruning\uff08SAP\uff09\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u6a21\u578b\u4e2d\u95f4\u5c42\u8bc6\u522b\u5173\u952e\u89c6\u89c9\u5757\u6765\u5b9e\u73b0\u9ad8\u6027\u80fd\u538b\u7f29\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86 Oracle Score Retention\uff08OSR\uff09\u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u9010\u5c42\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u538b\u7f29\u6548\u7387\u3002\u5728 ViDoRe \u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSAP \u5728\u5c06\u7d22\u5f15\u5411\u91cf\u51cf\u5c11\u8d85\u8fc7 90% \u7684\u540c\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5065\u7684\u68c0\u7d22\u4fdd\u771f\u5ea6\uff0c\u4e3a\u89c6\u89c9 RAG \u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u57fa\u4e8e OSR \u7684\u5206\u6790\u63ed\u793a\uff0c\u8bed\u4e49\u7ed3\u6784\u951a\u70b9\u5b58\u5728\u4e8e\u4e2d\u95f4\u5c42\uff0c\u800c\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u805a\u7126\u7684\u6700\u7ec8\u5c42\u4e2d\u8fd9\u4e9b\u7ed3\u6784\u4fe1\u53f7\u5df2\u7ecf\u6d88\u6563\u3002"}}
{"id": "2601.20168", "pdf": "https://arxiv.org/pdf/2601.20168", "abs": "https://arxiv.org/abs/2601.20168", "authors": ["Zhewen Wan", "Tianchen Song", "Chen Lin", "Zhiyong Zhao", "Xianpeng Lang"], "title": "Efficient Token Pruning for LLaDA-V", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u5f0f\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5982LLaDA-V\uff09\u7684\u7ed3\u6784\u5316\u89c6\u89c9token\u526a\u679d\u7b56\u7565\uff0c\u5728\u4e2d\u95f4\u5230\u540e\u671f\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u53ef\u51cf\u5c11\u9ad8\u8fbe65%\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u755995%\u7684\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5f0f\u591a\u6a21\u6001\u6a21\u578b\uff08\u5982LLaDA-V\uff09\u56e0\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u548c\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5e26\u6765\u9ad8\u8ba1\u7b97\u5f00\u9500\uff0c\u4e14\u5176\u8de8\u6a21\u6001\u4fe1\u606f\u805a\u5408\u4e3b\u8981\u53d1\u751f\u5728\u4e2d\u540e\u671f\u5c42\uff0c\u5bfc\u81f4\u8bed\u4e49\u5bf9\u9f50\u5ef6\u8fdf\uff0c\u4e9f\u9700\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u53d7FastV\u542f\u53d1\uff0c\u63d0\u51fa\u4e00\u79cd\u7ed3\u6784\u5316token\u526a\u679d\u7b56\u7565\uff0c\u5728\u7b2c\u4e00\u8f6e\u53bb\u566a\u7684\u4e2d\u540e\u671f\u5c42\u6709\u9009\u62e9\u5730\u79fb\u9664\u90e8\u5206\u89c6\u89c9token\uff0c\u4ee5\u5951\u5408LLaDA-V\u7684\u6ce8\u610f\u529b\u805a\u5408\u7279\u6027\uff0c\u4ece\u800c\u964d\u4f4e\u540e\u7eed\u6240\u6709\u6b65\u9aa4\u7684\u8ba1\u7b97\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\uff0c\u6700\u4f73\u914d\u7f6e\u53ef\u51cf\u5c11\u6700\u591a65%\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u5e73\u5747\u4fdd\u755995%\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u63a2\u7d22\u4e86\u6269\u6563\u5f0f\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u7684\u7ed3\u6784\u5316token\u526a\u679d\uff0c\u4e3aLLaDA-V\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u89c6\u89c9\u611f\u77e5\u526a\u679d\u5728\u8be5\u7c7b\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\u3002", "summary_cn": "\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08\u5982LLaDA-V\uff09\u5728\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u5c55\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\u3002\u7136\u800c\uff0c\u5176\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u548c\u6269\u6563\u5f0f\u7684\u8fed\u4ee3\u53bb\u566a\u8303\u5f0f\u5f15\u5165\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u56e0\u4e3a\u89c6\u89c9token\u5728\u6240\u6709\u5c42\u548c\u53bb\u566a\u6b65\u9aa4\u4e2d\u88ab\u53cd\u590d\u5904\u7406\u3002\u672c\u6587\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u6ce8\u610f\u529b\u5206\u6790\uff0c\u53d1\u73b0\u4e0e\u81ea\u56de\u5f52\u89e3\u7801\u5668\u4e0d\u540c\uff0cLLaDA-V\u4e3b\u8981\u5728\u4e2d\u540e\u671f\u5c42\u805a\u5408\u8de8\u6a21\u6001\u4fe1\u606f\uff0c\u5bfc\u81f4\u8bed\u4e49\u5bf9\u9f50\u5ef6\u8fdf\u3002\u53d7\u6b64\u89c2\u5bdf\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7FastV\u542f\u53d1\u7684\u7ed3\u6784\u5316token\u526a\u679d\u7b56\u7565\uff0c\u5728\u6307\u5b9a\u5c42\u6709\u9009\u62e9\u5730\u79fb\u9664\u90e8\u5206\u89c6\u89c9token\uff0c\u4ee5\u51cf\u5c11FLOPs\u540c\u65f6\u4fdd\u7559\u5173\u952e\u8bed\u4e49\u4fe1\u606f\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u5728\u6269\u6563\u5f0f\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7814\u7a76\u7ed3\u6784\u5316token\u526a\u679d\u7684\u5de5\u4f5c\u3002\u4e0d\u540c\u4e8eFastV\u805a\u7126\u4e8e\u6d45\u5c42\u526a\u679d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u9488\u5bf9\u7b2c\u4e00\u8f6e\u53bb\u566a\u7684\u4e2d\u540e\u671f\u5c42\u8fdb\u884c\u526a\u679d\uff0c\u4ee5\u5951\u5408LLaDA-V\u5ef6\u8fdf\u7684\u6ce8\u610f\u529b\u805a\u5408\u7279\u6027\uff0c\u4ece\u800c\u7ef4\u6301\u8f93\u51fa\u8d28\u91cf\uff1b\u5e76\u4e14\u7531\u4e8e\u5728\u7b2c\u4e00\u8f6e\u5c31\u8fdb\u884c\u526a\u679d\uff0c\u53ef\u964d\u4f4e\u6240\u6709\u540e\u7eed\u6b65\u9aa4\u7684\u8ba1\u7b97\u91cf\u3002\u6211\u4eec\u7684\u6846\u67b6\u4e3aLLaDA-V\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u5e76\u7a81\u663e\u4e86\u89c6\u89c9\u611f\u77e5\u526a\u679d\u5728\u6269\u6563\u5f0f\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u6f5c\u529b\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u6700\u4f73\u914d\u7f6e\u6700\u591a\u53ef\u51cf\u5c1165%\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5e73\u5747\u4fdd\u755995%\u7684\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2601.20175", "pdf": "https://arxiv.org/pdf/2601.20175", "abs": "https://arxiv.org/abs/2601.20175", "authors": ["Shiwen Zhang", "Xiaoyan Yang", "Bojia Zi", "Haibin Huang", "Chi Zhang", "Xuelong Li"], "title": "TeleStyle: Content-Preserving Style Transfer in Images and Videos", "categories": ["cs.CV"], "comment": null, "summary": "Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle", "AI": {"tldr": "TeleStyle \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u56fe\u50cf\u548c\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u6a21\u578b\uff0c\u57fa\u4e8e Qwen-Image-Edit \u6784\u5efa\uff0c\u901a\u8fc7\u8bfe\u7a0b\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u5728\u9ad8\u8d28\u91cf\u4e0e\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u5185\u5bb9\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u5bf9\u672a\u89c1\u98ce\u683c\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u98ce\u683c\u76f8\u4f3c\u6027\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u7f8e\u5b66\u8d28\u91cf\u65b9\u9762\u8fbe\u5230 SOTA\u3002", "motivation": "\u6269\u6563 Transformer\uff08DiT\uff09\u5728\u5185\u5bb9\u4fdd\u7559\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e2d\u9762\u4e34\u5185\u5bb9\u4e0e\u98ce\u683c\u7279\u5f81\u5728\u5185\u90e8\u8868\u793a\u4e2d\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u540c\u65f6\u517c\u987e\u98ce\u683c\u8868\u8fbe\u4e0e\u5185\u5bb9\u4fdd\u771f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u6765\u89e3\u8026\u6216\u534f\u8c03\u8fd9\u4e24\u8005\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u98ce\u683c\u8fc1\u79fb\u3002", "method": "TeleStyle \u57fa\u4e8e Qwen-Image-Edit \u6a21\u578b\uff0c\u5229\u7528\u5176\u5f3a\u5927\u7684\u5185\u5bb9\u4fdd\u7559\u548c\u98ce\u683c\u5b9a\u5236\u80fd\u529b\uff1b\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7cbe\u9009\u771f\u5b9e\u98ce\u683c\u6837\u672c\u548c\u5927\u91cf\u91ce\u5916\u98ce\u683c\u5408\u6210\u4e09\u5143\u7ec4\u7684\u6df7\u5408\u6570\u636e\u96c6\uff1b\u91c7\u7528\u8bfe\u7a0b\u6301\u7eed\u5b66\u4e60\uff08Curriculum Continual Learning\uff09\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\uff1b\u5e76\u5f15\u5165\u89c6\u9891\u5230\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u6a21\u5757\u4ee5\u63d0\u5347\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "result": "TeleStyle \u5728\u98ce\u683c\u76f8\u4f3c\u6027\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u7f8e\u5b66\u8d28\u91cf\u4e09\u9879\u6838\u5fc3\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u5f53\u524d\u6700\u4f18\uff08SOTA\uff09\u6027\u80fd\uff0c\u4e14\u80fd\u6709\u6548\u6cdb\u5316\u81f3\u672a\u89c1\u8fc7\u7684\u98ce\u683c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u5185\u5bb9\u4fdd\u771f\u3002", "conclusion": "TeleStyle \u901a\u8fc7\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u3001\u8bfe\u7a0b\u6301\u7eed\u5b66\u4e60\u7b56\u7565\u548c\u89c6\u9891\u65f6\u5e8f\u4f18\u5316\u6a21\u5757\uff0c\u6210\u529f\u89e3\u51b3\u4e86 DiT \u5728\u5185\u5bb9\u4fdd\u7559\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u56fe\u50cf\u4e0e\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u5185\u5bb9\u4fdd\u7559\u7684\u98ce\u683c\u8fc1\u79fb\u65e8\u5728\u6839\u636e\u5185\u5bb9\u53c2\u8003\u548c\u98ce\u683c\u53c2\u8003\u751f\u6210\u5177\u6709\u6307\u5b9a\u98ce\u683c\u7684\u8f93\u51fa\uff0c\u4f46\u7531\u4e8e\u6269\u6563 Transformer\uff08DiT\uff09\u5728\u5176\u5185\u90e8\u8868\u793a\u4e2d\u5b58\u5728\u5185\u5bb9\u4e0e\u98ce\u683c\u7279\u5f81\u7684\u56fa\u6709\u7ea0\u7f20\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002\u5728\u672c\u6280\u672f\u62a5\u544a\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TeleStyle\u2014\u2014\u4e00\u79cd\u8f7b\u91cf\u800c\u9ad8\u6548\u7684\u56fe\u50cf\u4e0e\u89c6\u9891\u98ce\u683c\u8fc1\u79fb\u6a21\u578b\u3002\u8be5\u6a21\u578b\u57fa\u4e8e Qwen-Image-Edit \u6784\u5efa\uff0c\u5145\u5206\u5229\u7528\u5176\u5728\u5185\u5bb9\u4fdd\u7559\u548c\u98ce\u683c\u5b9a\u5236\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002\u4e3a\u652f\u6301\u6709\u6548\u8bad\u7ec3\uff0c\u6211\u4eec\u6574\u7406\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u7279\u5b9a\u98ce\u683c\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u4e00\u6b65\u5229\u7528\u6570\u5343\u79cd\u591a\u6837\u5316\u7684\u91ce\u5916\u98ce\u683c\u7c7b\u522b\u5408\u6210\u4e86\u4e09\u5143\u7ec4\u6570\u636e\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u8bfe\u7a0b\u6301\u7eed\u5b66\u4e60\uff08Curriculum Continual Learning\uff09\u6846\u67b6\uff0c\u5728\u7531\u5e72\u51c0\uff08\u7cbe\u9009\uff09\u548c\u566a\u58f0\uff08\u5408\u6210\uff09\u4e09\u5143\u7ec4\u7ec4\u6210\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3 TeleStyle\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4e0d\u727a\u7272\u5185\u5bb9\u4fdd\u771f\u5ea6\u7684\u524d\u63d0\u4e0b\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u98ce\u683c\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u89c6\u9891\u5230\u89c6\u9891\u7684\u98ce\u683c\u8fc1\u79fb\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002TeleStyle \u5728\u98ce\u683c\u76f8\u4f3c\u6027\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u7f8e\u5b66\u8d28\u91cf\u4e09\u9879\u6838\u5fc3\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u6c34\u5e73\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5f00\u6e90\uff1ahttps://github.com/Tele-AI/TeleStyle\u3002"}}
{"id": "2601.20196", "pdf": "https://arxiv.org/pdf/2601.20196", "abs": "https://arxiv.org/abs/2601.20196", "authors": ["Brayden Hamilton", "Tim Cashmore", "Peter Driscoll", "Trevor Gee", "Henry Williams"], "title": "Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale", "categories": ["cs.CV"], "comment": "Australasian Conference on Robotics and Automation, ACRA2025 13 Pages, 8 Figures", "summary": "Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u5927\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u8bc4\u4f30\u8239\u4f53\u751f\u7269\u6c61\u635f\u4e25\u91cd\u7a0b\u5ea6\uff08LoF\u7b49\u7ea7\uff09\uff0c\u53d1\u73b0\u4e24\u8005\u5404\u6709\u4f18\u52bf\uff0c\u6df7\u5408\u65b9\u6cd5\u6700\u5177\u524d\u666f\u3002", "motivation": "\u4f20\u7edf\u4f9d\u8d56\u6f5c\u6c34\u5458\u7684\u8239\u4f53\u751f\u7269\u6c61\u635f\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u98ce\u9669\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8bc4\u4f30\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\u548c\u96f6\u6837\u672c\u5927\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\uff08LLMs\uff09\u5728\u65b0\u897f\u5170\u521d\u7ea7\u4ea7\u4e1a\u90e8\u63d0\u4f9b\u7684\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff1bLLMs\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u68c0\u7d22\u8fdb\u884c\u5f15\u5bfc\u3002", "result": "\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728LoF\u6781\u7aef\u7b49\u7ea7\u4e0a\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u4e2d\u95f4\u7b49\u7ea7\u8868\u73b0\u4e0d\u4f73\uff1bLLMs\u65e0\u9700\u8bad\u7ec3\u5373\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u8f93\u51fa\u3002", "conclusion": "\u4e0d\u540c\u65b9\u6cd5\u5177\u6709\u4e92\u8865\u4f18\u52bf\uff0c\u7ed3\u5408\u5206\u5272\u8986\u76d6\u5ea6\u4e0eLLM\u63a8\u7406\u7684\u6df7\u5408\u65b9\u6cd5\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u751f\u7269\u6c61\u635f\u8bc4\u4f30\u7684\u6709\u524d\u666f\u8def\u5f84\u3002", "summary_cn": "\u8239\u4f53\u4e0a\u7684\u6d77\u6d0b\u751f\u7269\u6c61\u635f\u5e26\u6765\u4e86\u91cd\u5927\u7684\u751f\u6001\u3001\u7ecf\u6d4e\u548c\u751f\u7269\u5b89\u5168\u98ce\u9669\u3002\u4f20\u7edf\u7684\u8c03\u67e5\u65b9\u6cd5\u4f9d\u8d56\u6f5c\u6c34\u5458\u68c0\u67e5\uff0c\u65e2\u5371\u9669\u53c8\u96be\u4ee5\u89c4\u6a21\u5316\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u81ea\u5b9a\u4e49\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u548c\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u751f\u7269\u6c61\u635f\u4e25\u91cd\u7a0b\u5ea6\uff08\u6309\u6c61\u635f\u7b49\u7ea7LoF\u91cf\u8868\uff09\u8fdb\u884c\u81ea\u52a8\u5206\u7c7b\u3002\u7814\u7a76\u5728\u6765\u81ea\u65b0\u897f\u5170\u521d\u7ea7\u4ea7\u4e1a\u90e8\u7684\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\uff0c\u8bc4\u4f30\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u57fa\u4e8eTransformer\u7684\u5206\u5272\u6a21\u578b\u4ee5\u53ca\u96f6\u6837\u672cLLMs\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728LoF\u91cf\u8868\u7684\u6781\u7aef\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u548c\u56fe\u50cf\u6784\u56fe\u95ee\u9898\uff0c\u5728\u4e2d\u95f4\u7b49\u7ea7\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u800c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u68c0\u7d22\u5f15\u5bfc\u7684LLMs\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u53d6\u5f97\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u3002\u7814\u7a76\u7ed3\u679c\u5c55\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u4e4b\u95f4\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u8868\u660e\u878d\u5408\u5206\u5272\u8986\u76d6\u7387\u4e0eLLM\u63a8\u7406\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u751f\u7269\u6c61\u635f\u8bc4\u4f30\u7684\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2601.20218", "pdf": "https://arxiv.org/pdf/2601.20218", "abs": "https://arxiv.org/abs/2601.20218", "authors": ["Haoyou Deng", "Keyu Yan", "Chaojie Mao", "Xiang Wang", "Yu Liu", "Changxin Gao", "Nong Sang"], "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2026", "summary": "Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \\textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.", "AI": {"tldr": "DenseGRPO improves human preference alignment in text-to-image generation by introducing dense rewards that evaluate each denoising step, addressing the sparse reward problem of prior GRPO methods.", "motivation": "Existing GRPO-based methods suffer from sparse reward issues where a single terminal reward is applied uniformly across all denoising steps, leading to misalignment between global feedback and fine-grained step contributions.", "method": "DenseGRPO introduces two key components: (1) step-wise reward prediction using an ODE-based approach to assign dense rewards based on intermediate clean images; (2) a reward-aware exploration scheme that adaptively adjusts stochasticity injection in the SDE sampler according to timestep-specific noise intensity.", "result": "Extensive experiments on standard benchmarks show DenseGRPO outperforms existing methods, validating the importance of dense rewards for effective alignment in flow matching models.", "conclusion": "DenseGRPO effectively addresses the sparse reward problem in GRPO-based alignment by leveraging dense, step-wise rewards and adaptive exploration, significantly improving text-to-image generation aligned with human preferences.", "summary_cn": "\u57fa\u4e8e\u6d41\u5339\u914d\u6a21\u578b\u7684\u8fd1\u671fGRPO\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u7684\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u6539\u8fdb\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u9762\u4e34\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff1a\u6574\u4e2a\u53bb\u566a\u8f68\u8ff9\u7684\u7ec8\u7aef\u5956\u52b1\u88ab\u5e94\u7528\u4e8e\u6240\u6709\u4e2d\u95f4\u6b65\u9aa4\uff0c\u5bfc\u81f4\u5168\u5c40\u53cd\u9988\u4fe1\u53f7\u4e0e\u5404\u4e2d\u95f4\u53bb\u566a\u6b65\u9aa4\u7684\u5b9e\u9645\u7ec6\u7c92\u5ea6\u8d21\u732e\u4e0d\u5339\u914d\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\\textbf{DenseGRPO}\uff0c\u4e00\u79cd\u901a\u8fc7\u5bc6\u96c6\u5956\u52b1\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8bc4\u4f30\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u7684\u7ec6\u7c92\u5ea6\u8d21\u732e\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\uff081\uff09\u901a\u8fc7\u57fa\u4e8eODE\u7684\u65b9\u6cd5\uff0c\u5728\u4e2d\u95f4\u5e72\u51c0\u56fe\u50cf\u4e0a\u5e94\u7528\u5956\u52b1\u6a21\u578b\uff0c\u9884\u6d4b\u6bcf\u4e00\u6b65\u7684\u5956\u52b1\u589e\u76ca\u4f5c\u4e3a\u5bc6\u96c6\u5956\u52b1\uff0c\u4ece\u800c\u786e\u4fdd\u53cd\u9988\u4fe1\u53f7\u4e0e\u5404\u6b65\u9aa4\u8d21\u732e\u7684\u4e00\u81f4\u6027\uff0c\u4fc3\u8fdb\u6709\u6548\u8bad\u7ec3\uff1b\uff082\uff09\u57fa\u4e8e\u6240\u4f30\u8ba1\u7684\u5bc6\u96c6\u5956\u52b1\uff0c\u63ed\u793a\u4e86\u73b0\u6709GRPO\u65b9\u6cd5\u4e2d\u5747\u5300\u63a2\u7d22\u7b56\u7565\u4e0e\u968f\u65f6\u95f4\u53d8\u5316\u7684\u566a\u58f0\u5f3a\u5ea6\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u7f3a\u9677\uff0c\u5bfc\u81f4\u63a2\u7d22\u7a7a\u95f4\u4e0d\u5408\u7406\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5956\u52b1\u611f\u77e5\u65b9\u6848\uff0c\u901a\u8fc7\u5728SDE\u91c7\u6837\u5668\u4e2d\u81ea\u9002\u5e94\u8c03\u6574\u7279\u5b9a\u65f6\u95f4\u6b65\u7684\u968f\u673a\u6027\u6ce8\u5165\uff0c\u6821\u51c6\u63a2\u7d22\u7a7a\u95f4\uff0c\u786e\u4fdd\u5728\u6240\u6709\u65f6\u95f4\u6b65\u4e0a\u90fd\u5177\u6709\u5408\u9002\u7684\u63a2\u7d22\u8303\u56f4\u3002\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0DenseGRPO\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u663e\u4e86\u6709\u6548\u5bc6\u96c6\u5956\u52b1\u5728\u6d41\u5339\u914d\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
