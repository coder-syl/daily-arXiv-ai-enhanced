<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data](https://arxiv.org/abs/2601.02392)
*Mo Chen*

Main category: cs.CV

TL;DR: 提出Dense-MAE，一种基于掩码自编码器的自监督学习框架，用于在缺乏标注数据的情况下提升冠状动脉钙化伪影去除和管腔狭窄估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉钙化在CTA中产生blooming伪影，严重影响管腔狭窄诊断；现有基于DCNN（如Dense-Unet）的方法依赖大量标注数据，而医学领域标注数据稀缺。

Method: 受3D点云Masked Autoencoder（MAE）启发，提出Dense-MAE：通过随机掩码血管管腔的3D块，并训练Dense-Unet重建缺失几何结构，实现无需人工标注的自监督预训练。

Result: 在临床CTA数据集上的实验表明，使用MAE预训练权重初始化钙化去除网络，在小样本场景下显著优于从零训练，提升了图像修复精度和狭窄程度估计效果。

Conclusion: Dense-MAE通过自监督预训练有效缓解医学图像中标注数据不足的问题，在冠状动脉钙化伪影去除任务中具有实用价值。

Abstract: Coronary calcification creates blooming artifacts in Computed Tomography Angiography (CTA), severely hampering the diagnosis of lumen stenosis. While Deep Convolutional Neural Networks (DCNNs) like Dense-Unet have shown promise in removing these artifacts via inpainting, they often require large labeled datasets which are scarce in the medical domain. Inspired by recent advancements in Masked Autoencoders (MAE) for 3D point clouds, we propose \textbf{Dense-MAE}, a novel self-supervised learning framework for volumetric medical data. We introduce a pre-training strategy that randomly masks 3D patches of the vessel lumen and trains the Dense-Unet to reconstruct the missing geometry. This forces the encoder to learn high-level latent features of arterial topology without human annotation. Experimental results on clinical CTA datasets demonstrate that initializing the Calcium Removal network with our MAE-based weights significantly improves inpainting accuracy and stenosis estimation compared to training from scratch, specifically in few-shot scenarios.

Abstract (中文翻译): 冠状动脉钙化在计算机断层扫描血管造影（CTA）中会产生blooming伪影，严重阻碍管腔狭窄的诊断。尽管Dense-Unet等深度卷积神经网络（DCNN）已通过图像修复方法在去除此类伪影方面展现出潜力，但它们通常需要大量标注数据，而这类数据在医学领域十分稀缺。受近期3D点云掩码自编码器（MAE）进展的启发，我们提出了Dense-MAE——一种面向体素医学数据的新型自监督学习框架。我们引入了一种预训练策略：随机掩码血管管腔的3D图像块，并训练Dense-Unet重建缺失的几何结构，从而迫使编码器在无需人工标注的情况下学习动脉拓扑的高层潜在特征。在临床CTA数据集上的实验结果表明，与从零开始训练相比，使用我们基于MAE的权重初始化钙化去除网络能显著提升图像修复精度和狭窄程度估计性能，尤其在小样本场景下表现更优。

</details>


### [2] [MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion](https://arxiv.org/abs/2601.02414)
*Jichao Zhu,Jun Yu*

Main category: cs.CV

TL;DR: 提出了一种新的多模态情感识别方法MIAR，通过模态间交互生成特征token，并利用对比学习与归一化策略对齐模态，在CMU-MOSI和CMU-MOSEI数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往的多模态情感识别方法在模态融合时未能充分处理模态间的分布差异，也未考虑各模态对任务贡献的不均衡性，且在不同文本模型特征下泛化能力不足，限制了性能。

Method: 提出Modality Interaction and Alignment Representation（MIAR）方法，通过跨模态特征交互生成代表全局信息的特征token，并采用对比学习与归一化策略对齐不同模态。

Result: 在CMU-MOSI和CMU-MOSEI两个基准数据集上的实验表明，MIAR优于当前最先进的多模态情感识别方法。

Conclusion: MIAR有效解决了模态分布差异与贡献不均问题，提升了多模态情感识别的性能与泛化能力。

Abstract: Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.

Abstract (中文翻译): 多模态情感识别（MER）旨在通过语言、视觉和音频三种模态感知人类情感。以往的方法主要关注模态融合，却未能充分处理模态之间显著的分布差异，也未考虑各模态对任务的不同贡献，同时在面对多样化的文本模型特征时缺乏良好的泛化能力，从而限制了其在多模态场景中的表现。为此，我们提出了一种名为“模态交互与对齐表示”（Modality Interaction and Alignment Representation, MIAR）的新方法。该网络通过跨模态的特征交互生成特征token，以表示某一模态从其他模态中提取信息的全局表征；这四个token分别代表每个模态如何从其他模态中提取信息的全局表示。MIAR还结合对比学习和归一化策略来对齐不同模态。我们在CMU-MOSI和CMU-MOSEI两个基准数据集上进行了实验，结果表明MIAR优于当前最先进的多模态情感识别方法。

</details>


### [3] [Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion](https://arxiv.org/abs/2601.02415)
*Wangyuan Zhu,Jun Yu*

Main category: cs.CV

TL;DR: 本文提出了一种结合多通道特征提取与对称互促（SMP）跨模态融合机制的多模态情感分析方法，通过增强模态内表示并促进模态间信息交互，在两个基准数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单模态特征提取上不够丰富，且在跨模态融合时忽视了模态间特征差异，导致融合效果受限。

Method: 首先在视觉和听觉模态中分别采用双通道特征提取以增强模态内表示；然后提出对称互促（SMP）跨模态融合方法，结合对称交叉注意力与自注意力机制，促进模态间有用信息交换；最后融合模态内与模态间特征，兼顾互补性与差异性。

Result: 在两个基准数据集上的实验表明，所提方法具有良好的性能和优越性。

Conclusion: 该方法通过更全面的特征提取和有效的跨模态融合机制，显著提升了多模态情感分析的准确性和鲁棒性。

Abstract: Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.

Abstract (中文翻译): 多模态情感分析是人机交互与情感计算领域的关键技术。准确识别人类情感状态对于实现人与机器之间的顺畅沟通至关重要。尽管多模态情感分析研究已取得一定进展，但仍面临诸多挑战。第一个挑战是单模态数据所提取的特征有限且不够丰富；其次，大多数研究仅关注模态间特征信息的一致性，忽略了特征之间的差异，导致特征融合不充分。本文首先提取多通道特征以获得更全面的特征信息，在视觉和听觉模态中均采用双通道特征以增强模态内特征表示。其次，提出一种对称互促（Symmetric Mutual Promotion, SMP）的跨模态特征融合方法，该方法结合对称交叉模态注意力机制与自注意力机制：交叉模态注意力机制用于捕捉其他模态中的有用信息，而自注意力机制则用于建模上下文信息，从而促进模态间有用信息的交换，加强模态间交互。此外，我们将模态内特征与跨模态融合后的特征进行整合，在充分利用模态间特征信息互补性的同时，也考虑了特征信息的差异性。在两个基准数据集上的实验结果验证了所提方法的有效性与优越性。

</details>


### [4] [Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning](https://arxiv.org/abs/2601.02422)
*Wenting Lu,Didi Zhu,Tao Shen,Donglin Zhu,Ayong Ye,Chao Wu*

Main category: cs.CV

TL;DR: 提出CoCoT框架，通过动态多区域定位和关系感知推理提升多模态推理性能，在多个基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought方法在跨模态场景中存在两大局限：过度依赖单一粗粒度图像区域，以及推理步骤间语义割裂。

Method: 提出CoCoT（协作式跨模态思维）框架，包含两项关键技术：1）动态多区域定位，根据问题自适应检测最相关图像区域；2）关系感知推理，通过迭代对齐视觉线索实现多区域协作，构建连贯逻辑推理链。同时构建了含74,691个样本的CoCoT-70K数据集。

Result: 在六个具有挑战性的基准测试中，CoCoT在LLaVA-1.5上平均准确率提升15.4%，在Qwen2-VL上提升4.0%。

Conclusion: CoCoT框架有效解决了多模态推理中的区域选择与语义连贯性问题，显著提升了复杂视觉推理能力。

Abstract: Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.

Abstract (中文翻译): 多模态推理需要视觉与语言线索的无缝融合，然而现有的思维链（Chain-of-Thought）方法在跨模态场景中存在两个关键局限：（1）过度依赖单一的粗粒度图像区域；（2）连续推理步骤之间存在语义碎片化问题。为解决这些问题，我们提出了CoCoT（协作式跨模态思维）框架，其核心包含两项创新：a) 动态多区域定位，可根据问题自适应地检测最相关的图像区域；b) 关系感知推理，通过迭代对齐视觉线索，实现多区域协作，从而形成连贯且逻辑清晰的推理链。基于该方法，我们构建了CoCoT-70K数据集，包含74,691个高质量样本，均带有多个图像区域标注和结构化的推理链。大量实验表明，CoCoT显著增强了复杂视觉推理能力，在六个具有挑战性的基准测试中，分别在LLaVA-1.5和Qwen2-VL模型上实现了平均15.4%和4.0%的准确率提升。相关数据与代码已开源：https://github.com/deer-echo/CoCoT。

</details>


### [5] [NitroGen: An Open Foundation Model for Generalist Gaming Agents](https://arxiv.org/abs/2601.02427)
*Loïc Magne,Anas Awadalla,Guanzhi Wang,Yinzhen Xu,Joshua Belofsky,Fengyuan Hu,Joohwan Kim,Ludwig Schmidt,Georgia Gkioxari,Jan Kautz,Yisong Yue,Yejin Choi,Yuke Zhu,Linxi "Jim" Fan*

Main category: cs.CV

TL;DR: NitroGen 是一个通用游戏智能体的视觉-动作基础模型，基于 4 万小时、1000 多款游戏的视频训练而成，在未见过的游戏中表现出色，并开源了数据集、评测环境和模型权重。


<details>
  <summary>Details</summary>
Motivation: 现有游戏智能体通常针对特定游戏设计，缺乏跨游戏泛化能力。为推动通用具身智能体研究，作者构建大规模多游戏视频-动作数据集并开发统一模型以实现跨游戏迁移。

Method: NitroGen 模型结合三个关键要素：1）从公开游戏视频自动提取动作构建的大规模视频-动作数据集；2）可评估跨游戏泛化能力的多游戏基准环境；3）使用大规模行为克隆训练的统一视觉-动作模型。

Result: NitroGen 在多种游戏类型（如3D动作游戏战斗、2D平台高精度控制、程序生成世界探索）中表现优异，在未见游戏中任务成功率最高提升52%（相比从头训练模型）。

Conclusion: NitroGen 展示了大规模视频-动作预训练在构建通用游戏智能体方面的潜力，其开源资源将促进通用具身智能体的研究。

Abstract: We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.

Abstract (中文翻译): 我们提出了 NitroGen，这是一个面向通用游戏智能体的视觉-动作基础模型，基于超过 1,000 款游戏共计 40,000 小时的游戏视频进行训练。我们融合了三个关键要素：1）通过自动从公开游戏视频中提取玩家动作构建的互联网规模视频-动作数据集；2）可衡量跨游戏泛化能力的多游戏基准测试环境；3）采用大规模行为克隆训练的统一视觉-动作模型。NitroGen 在多个领域展现出强大能力，包括 3D 动作游戏中的战斗场景、2D 平台游戏中的高精度控制以及程序生成世界中的探索任务。该模型能有效迁移到未见过的游戏，在任务成功率上相较从零训练的模型最高提升 52%。我们开源了数据集、评测套件和模型权重，以推动通用具身智能体的研究。

</details>


### [6] [TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers](https://arxiv.org/abs/2601.02437)
*Zhibo Wang,Zuoyuan Zhang,Xiaoyi Pang,Qile Zhang,Xuanyi Hao,Shuguo Zhuo,Peng Sun*

Main category: cs.CV

TL;DR: 本文提出TAP-ViTs，一种无需访问设备本地原始数据即可为不同设备生成任务自适应剪枝ViT模型的框架。


<details>
  <summary>Details</summary>
Motivation: 现有ViT剪枝方法要么为所有设备生成单一模型而忽略设备异构性，要么依赖设备本地数据微调，这在资源受限和隐私约束下不可行，难以实现隐私保护下的任务定制化剪枝。

Method: 提出基于高斯混合模型（GMM）的度量数据集构建机制：各设备用轻量GMM拟合私有数据分布并上传参数；云端利用这些参数从公开数据中选取分布一致样本构建设备专属代理数据集，并在此基础上设计双粒度重要性评估剪枝策略，联合衡量复合神经元重要性和自适应层重要性。

Result: 在多个ViT主干网络和数据集上的实验表明，TAP-ViTs在相同压缩比下始终优于当前最先进的剪枝方法。

Conclusion: TAP-ViTs有效解决了隐私保护移动计算场景中ViT模型的任务定制化剪枝难题，实现了高效、设备特定的模型压缩。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a wide range of vision tasks, yet their substantial computational and memory demands hinder efficient deployment on resource-constrained mobile and edge devices. Pruning has emerged as a promising direction for reducing ViT complexity. However, existing approaches either (i) produce a single pruned model shared across all devices, ignoring device heterogeneity, or (ii) rely on fine-tuning with device-local data, which is often infeasible due to limited on-device resources and strict privacy constraints. As a result, current methods fall short of enabling task-customized ViT pruning in privacy-preserving mobile computing settings. This paper introduces TAP-ViTs, a novel task-adaptive pruning framework that generates device-specific pruned ViT models without requiring access to any raw local data. Specifically, to infer device-level task characteristics under privacy constraints, we propose a Gaussian Mixture Model (GMM)-based metric dataset construction mechanism. Each device fits a lightweight GMM to approximate its private data distribution and uploads only the GMM parameters. Using these parameters, the cloud selects distribution-consistent samples from public data to construct a task-representative metric dataset for each device. Based on this proxy dataset, we further develop a dual-granularity importance evaluation-based pruning strategy that jointly measures composite neuron importance and adaptive layer importance, enabling fine-grained, task-aware pruning tailored to each device's computational budget. Extensive experiments across multiple ViT backbones and datasets demonstrate that TAP-ViTs consistently outperforms state-of-the-art pruning methods under comparable compression ratios.

Abstract (中文翻译): 视觉Transformer（ViTs）在各类视觉任务中展现出强大性能，但其巨大的计算和内存需求阻碍了在资源受限的移动和边缘设备上的高效部署。剪枝已成为降低ViT复杂度的一个有前景的方向。然而，现有方法要么（i）为所有设备生成一个共享的剪枝模型，忽略了设备异构性，要么（ii）依赖使用设备本地数据进行微调，而这通常由于设备资源有限和严格的隐私限制而不可行。因此，当前方法尚无法在隐私保护的移动计算环境中实现任务定制化的ViT剪枝。本文提出了TAP-ViTs，这是一种新颖的任务自适应剪枝框架，可在不访问任何原始本地数据的情况下生成设备特定的剪枝ViT模型。具体而言，为了在隐私约束下推断设备级任务特征，我们提出了一种基于高斯混合模型（GMM）的度量数据集构建机制。每个设备拟合一个轻量级GMM以近似其私有数据分布，并仅上传GMM参数。云端利用这些参数从公开数据中选择分布一致的样本，为每个设备构建一个任务代表性的度量数据集。基于该代理数据集，我们进一步开发了一种双粒度重要性评估剪枝策略，联合衡量复合神经元重要性和自适应层重要性，从而实现针对每个设备计算预算的细粒度、任务感知剪枝。在多个ViT主干网络和数据集上的大量实验表明，TAP-ViTs在可比压缩比下始终优于最先进的剪枝方法。

</details>


### [7] [Understanding Pure Textual Reasoning for Blind Image Quality Assessment](https://arxiv.org/abs/2601.02441)
*Yuan Li,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文从信息流角度探究文本在盲图像质量评估（BIQA）中的作用，通过对比三种范式（Chain-of-Thought、Self-Consistency、Autoencoder）发现仅用文本预测性能显著下降，其中Self-Consistency最有效缩小图像与文本条件预测间的差距。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚文本信息如何促进BIQA中的质量预测，以及文本能在多大程度上表征与评分相关的图像内容。

Method: 从信息流视角出发，设计并比较三种学习图像-文本-评分关系的范式：Chain-of-Thought、Self-Consistency 和 Autoencoder，并与现有BIQA模型进行对比实验。

Result: 仅使用文本信息时现有模型性能大幅下降；Chain-of-Thought对BIQA提升有限；Self-Consistency显著缩小图像与文本条件预测之间的PLCC/SRCC差距至0.02/0.03；Autoencoder范式效果较弱但提示了优化方向。

Conclusion: 研究揭示了文本推理在BIQA中的局限与潜力，为改进BIQA及高层视觉任务中的文本推理提供了新思路。

Abstract: Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.

Abstract (中文翻译): 最近，文本推理被广泛应用于盲图像质量评估（BIQA）中。然而，文本信息如何促进质量预测，以及文本能在多大程度上表征与评分相关的图像内容，目前仍不清楚。本研究从信息流的角度出发，通过对比现有BIQA模型与三种旨在学习图像-文本-评分关系的范式（思维链 Chain-of-Thought、自一致性 Self-Consistency 和自编码器 Autoencoder）来回答这些问题。实验表明，当仅使用文本信息进行预测时，现有模型的评分预测性能显著下降。其中，思维链范式对BIQA性能提升有限，而自一致性范式显著缩小了基于图像和基于文本的预测之间的差距，将PLCC/SRCC差异缩小至0.02/0.03。自编码器类范式在弥合图像-文本差距方面效果较弱，但揭示了进一步优化的方向。这些发现为改进BIQA及高层视觉任务中的文本推理提供了重要启示。

</details>


### [8] [Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative](https://arxiv.org/abs/2601.02443)
*Li Wang,Xi Chen,XiangWen Deng,HuaHui Yi,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLM）在医学视觉问答和报告生成方面表现良好，但在膝骨关节炎X光片分类任务中，其作为分类器的效果不如单独优化的视觉编码器；研究发现数据平衡与质量比数据规模更重要，建议在特定医学分类任务中优先优化视觉编码器并谨慎构建数据集。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在通用医学视觉任务中表现优异，但其在疾病特异性分类（如膝骨关节炎X光片分级）中的有效性尚未充分验证，且该任务在现有医学MLLM基准中代表性不足，而全球有3–4亿人受此病影响，亟需可靠诊断方法。

Method: 作者在膝骨关节炎X光片分类任务上系统评估不同MLLM架构，通过消融实验分别调整视觉编码器、连接模块和大语言模型（LLM），并在多种训练策略下衡量各组件对诊断准确率的贡献；同时比较了小规模平衡数据集（500张图像）与大规模不平衡数据集（5,778张图像）在LoRA微调下的性能差异。

Result: 在分类任务中，仅训练视觉编码器即可超越完整MLLM流程；对LLM进行微调相比基于提示的方法未带来显著提升；使用小而平衡的数据集进行LoRA微调效果优于使用大但不平衡的数据集。

Conclusion: 对于需要高确定性的特定医学图像分类任务，MLLM架构并不理想；LLM更适合作为解释器和报告生成器，而非主要分类器；应优先优化视觉编码器并注重高质量、类别平衡的数据集构建。

Abstract: Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.

Abstract (中文翻译): 多模态大语言模型（MLLM）在医学视觉问答（VQA）和报告生成方面展现出良好的性能，但这些生成与解释能力并不能可靠地迁移到疾病特异性分类任务中。我们在膝骨关节炎（OA）X光片分类任务上评估了多种MLLM架构——该任务在现有医学MLLM基准中仍缺乏充分代表，尽管全球约有3亿至4亿人受膝骨关节炎影响。通过系统性消融实验，我们分别操控视觉编码器、连接模块和大语言模型（LLM），并在多种训练策略下测量各组件对诊断准确率的贡献。结果表明，在本分类任务中，仅训练视觉编码器即可在分类准确率上超越完整的MLLM流程；而对LLM进行微调相比基于提示的引导并未带来有意义的提升。此外，在一个小型但类别平衡的数据集（500张图像）上进行LoRA微调，其效果优于在一个更大但类别不平衡的数据集（5,778张图像）上训练，这表明在此类任务中，数据的平衡性与质量可能比原始数据规模更为重要。这些发现表明，对于领域特定的医学分类任务，大语言模型更适合作为解释器和报告生成器，而非主要分类器。因此，MLLM架构似乎不太适用于需要高确定性的医学图像诊断分类任务。我们建议在开发临床可用系统时，优先优化视觉编码器并精心构建数据集。

</details>


### [9] [A Spatio-Temporal Deep Learning Approach For High-Resolution Gridded Monsoon Prediction](https://arxiv.org/abs/2601.02445)
*Parashjyoti Borah,Sanghamitra Sarkar,Ranjan Phukan*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的新方法，将印度夏季风（ISM）的网格化预测转化为时空计算机视觉任务，利用CNN架构从1-5月的大气和海洋场数据中预测6-9月高分辨率逐月及季节总降雨量。


<details>
  <summary>Details</summary>
Motivation: 传统长期预报方法通常仅预测一个空间平均的季风季节值，缺乏对区域资源管理至关重要的空间细节。为弥补这一不足，作者旨在开发一种能提供高分辨率网格化预测的新方法。

Method: 将多变量、季风前的大气和海洋场视为多通道图像序列，构建类似视频的输入张量，并使用基于卷积神经网络（CNN）的架构，从85年ERA5再分析数据（预测因子）和IMD降雨数据（目标）中学习从1-5月到后续季风季节高分辨率降雨的空间映射关系。

Result: 该框架成功生成了季风四个月（6-9月）各自以及整个季节平均降雨量的独立高分辨率预测，证明其在季节内和季节尺度展望中的有效性。

Conclusion: 将季风预测问题重构为时空计算机视觉任务是可行且有效的，所提出的深度学习框架能够提供传统方法所缺乏的精细空间信息，对区域层面的水资源和农业管理具有重要价值。

Abstract: The Indian Summer Monsoon (ISM) is a critical climate phenomenon, fundamentally impacting the agriculture, economy, and water security of over a billion people. Traditional long-range forecasting, whether statistical or dynamical, has predominantly focused on predicting a single, spatially-averaged seasonal value, lacking the spatial detail essential for regional-level resource management. To address this gap, we introduce a novel deep learning framework that reframes gridded monsoon prediction as a spatio-temporal computer vision task. We treat multi-variable, pre-monsoon atmospheric and oceanic fields as a sequence of multi-channel images, effectively creating a video-like input tensor. Using 85 years of ERA5 reanalysis data for predictors and IMD rainfall data for targets, we employ a Convolutional Neural Network (CNN)-based architecture to learn the complex mapping from the five-month pre-monsoon period (January-May) to a high-resolution gridded rainfall pattern for the subsequent monsoon season. Our framework successfully produces distinct forecasts for each of the four monsoon months (June-September) as well as the total seasonal average, demonstrating its utility for both intra-seasonal and seasonal outlooks.

Abstract (中文翻译): 印度夏季风（ISM）是一种关键的气候现象，对超过十亿人口的农业、经济和水安全有着根本性影响。传统的长期预报方法，无论是统计还是动力方法，主要集中在预测一个单一的空间平均季节值，缺乏区域层面资源管理所必需的空间细节。为了解决这一问题，我们引入了一种新颖的深度学习框架，将网格化的季风预测重新定义为一个时空计算机视觉任务。我们将多变量的季风前期大气和海洋场视为一个多通道图像序列，有效地创建了一个类似视频的输入张量。利用85年的ERA5再分析数据作为预测因子，IMD降雨数据作为目标，我们采用基于卷积神经网络（CNN）的架构，学习从五个月的季风前期（1月至5月）到随后季风季节高分辨率网格化降雨模式之间的复杂映射关系。我们的框架成功地为季风四个月（6月至9月）中的每一个月以及整个季节的平均降雨量生成了独立的预测，展示了其在季节内和季节展望方面的实用性。

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [10] [Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis](https://arxiv.org/abs/2601.02409)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh*

Main category: eess.IV

TL;DR: 本文提出EGxFSL和xGAL双框架，结合专家引导与可解释性，在少样本医学图像分析中同时提升性能与模型透明度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临标注数据稀缺和模型缺乏可解释性两大挑战，阻碍了临床AI部署。现有少样本学习方法缺乏预测透明度，而主动学习方法忽视所选样本的可解释性。

Method: 提出两个框架：1）EGxFSL，利用放射科医生定义的兴趣区域作为空间监督，通过基于Grad-CAM的Dice损失与原型分类联合优化；2）xGAL，迭代选择样本时兼顾预测不确定性和注意力错位，形成可解释性引导训练与采样的闭环。

Result: 在BraTS、VinDr-CXR和SIIM-COVID-19数据集上分别达到92%、76%和62%的准确率，均优于非引导基线。在极低数据量下（仅680样本），xGAL达76%准确率，远超随机采样的57%。Grad-CAM可视化显示模型聚焦于诊断相关区域，并在乳腺超声数据上验证了跨模态泛化能力。

Conclusion: 将专家知识与可解释性机制融合，能有效提升少样本医学图像分析的准确性与可信度，为临床AI部署提供可行路径。

Abstract: Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.

Abstract (中文翻译): 医学图像分析面临两个关键挑战：标注数据稀缺和模型缺乏可解释性，这两者都阻碍了临床AI的部署。少样本学习（FSL）虽能缓解数据不足问题，但其预测缺乏透明度；主动学习（AL）方法虽能优化数据获取，却忽视了所获取样本的可解释性。为此，我们提出了一个双框架解决方案：专家引导的可解释少样本学习（EGxFSL）和可解释性引导的主动学习（xGAL）。EGxFSL通过基于Grad-CAM的Dice损失，将放射科医生定义的兴趣区域作为空间监督信号，与原型分类联合优化，实现可解释的少样本学习。xGAL则在迭代样本获取过程中同时考虑预测不确定性与注意力错位，构建了一个可解释性协同指导训练与样本选择的闭环框架。在BraTS（MRI）、VinDr-CXR（胸部X光）和SIIM-COVID-19（胸部X光）数据集上，我们的方法分别取得了92%、76%和62%的准确率，在所有数据集上均持续优于非引导基线。在极端数据受限条件下，xGAL仅用680个样本就达到了76%的准确率，而随机采样仅为57%。Grad-CAM可视化表明，引导模型聚焦于具有诊断意义的区域，并在乳腺超声数据上的泛化实验进一步验证了该方法的跨模态适用性。

</details>
