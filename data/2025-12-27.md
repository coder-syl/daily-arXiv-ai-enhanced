<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

TL;DR: 本文提出了首个大规模视觉-语言模型（VLM）注视理解基准VL4Gaze，包含48.9万条自动生成的问答对，涵盖四个互补任务。实验表明，现有VLM在无专门监督下难以有效理解注视信息，而基于VL4Gaze的训练可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型（VLM）虽在多种视觉任务中表现出色，但在人类注视理解方面缺乏系统性评估和训练基准，尚不清楚通用视觉-语言预训练能否自然获得注视理解能力。

Method: 构建了名为VL4Gaze的大规模基准数据集，包含12.4万张图像上的48.9万个自动生成的问答对，并将注视理解建模为统一的视觉问答（VQA）问题，设计了四个互补任务：注视对象描述、注视方向描述、注视点定位和模糊问题识别。在上下文学习和微调设置下对多个商业和开源VLM进行了全面评估。

Result: 实验结果显示，即使大规模VLM在没有任务特定监督的情况下也难以可靠地推断注视语义和空间定位；而在VL4Gaze上训练后，所有任务的性能均获得显著且一致的提升。

Conclusion: 针对注视理解的多任务监督对提升VLM在此类任务上的能力至关重要，通用预训练本身不足以支持可靠的注视理解。

Abstract: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

Abstract (中文翻译): 人类注视为理解视觉场景中的注意力、意图和社会互动提供了关键线索，但当前的视觉-语言模型（VLM）在注视理解方面仍鲜有探索。尽管近期VLM在多种视觉任务中展现出强大的场景级推理能力，但尚无系统性评估或训练其注视理解能力的基准，因此尚不清楚注视理解是否能从通用视觉-语言预训练中自然涌现。为填补这一空白，我们提出了VL4Gaze——首个大规模基准，旨在探究、评估并释放VLM在注视理解方面的潜力。VL4Gaze包含12.4万张图像上的48.9万个自动生成的问答对，并通过四项互补任务将注视理解形式化为统一的视觉问答（VQA）问题：(1) 注视对象描述，(2) 注视方向描述，(3) 注视点定位，以及(4) 模糊问题识别。我们在上下文学习和微调设置下对商业和开源VLM进行了全面评估。结果表明，即使大规模VLM在缺乏任务特定监督的情况下也难以可靠地推断注视语义和空间定位；相比之下，在VL4Gaze上进行训练可在所有任务上带来显著且一致的性能提升，凸显了针对性多任务监督对发展VLM注视理解能力的重要性。我们将公开发布该数据集和代码，以支持该方向的进一步研究与开发。

</details>


### [2] [TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection](https://arxiv.org/abs/2512.20746)
*Tony Tran,Bin Hu*

Main category: cs.CV

TL;DR: 本文提出一种硬件感知的神经架构搜索框架，在TACO数据集上开发适用于TinyML设备的轻量级垃圾检测模型TrashDets。该方法通过迭代优化主干与检测头，结合种群传递机制和精度预测器，显著降低搜索成本并提升稳定性。最强变体TrashDet-l在五类子集上达到19.5 mAP50，参数仅30.5M，优于现有方法；整个模型族覆盖1.2M–30.5M参数范围，适用于不同资源预算。在MAX78002微控制器上，TrashDet-ResNet和TrashDet-MBNet分别在能效、延迟和精度方面大幅超越基线，最高节能88%、延迟降低78%。


<details>
  <summary>Details</summary>
Motivation: 在资源极度受限的TinyML边缘和IoT设备上实现高效、准确的垃圾检测具有挑战性。现有方法往往在精度、参数量或能效方面存在不足，难以兼顾部署灵活性与性能。因此，亟需一种可扩展、硬件感知的轻量级目标检测架构设计方法。

Method: 作者构建了一个Once-for-All风格的ResDets超网络，并采用迭代式进化搜索策略，交替优化骨干网络与检测头/颈部结构。该过程辅以种群传递机制和精度预测器，以降低搜索开销并增强稳定性，最终生成一系列适用于不同TinyML部署预算的TrashDets模型。

Result: 在TACO五类子集上，TrashDet-l达到19.5 mAP50，参数为30.5M，比先前方法最高提升3.6 mAP50且参数更少；模型族覆盖1.2M–30.5M参数，mAP50介于11.4–19.5之间。在MAX78002微控制器上，TrashDet-ResNet单次推理能耗7525 μJ、延迟26.7 ms、37.45 FPS；TrashDet-MBNet mAP50提升10.2%；两者相较现有TinyML检测器最多节能88%、延迟降低78%、平均功耗降低53%。

Conclusion: 所提出的硬件感知神经架构搜索框架成功生成了高性能、低功耗且可扩展的TrashDets模型族，显著优于现有TinyML垃圾检测方案，在精度、能效和部署灵活性方面取得良好平衡，适用于多种资源受限的边缘设备。

Abstract: This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.

Abstract (中文翻译): 本文针对TACO数据集上的垃圾检测任务，在严格的TinyML约束下，提出了一种面向边缘和物联网设备的迭代式硬件感知神经架构搜索框架。该方法构建了一个Once-for-All风格的ResDets超网络，并通过交替优化骨干网络与检测头/颈部的迭代进化搜索策略，在种群传递机制和精度预测器的支持下，有效降低了搜索成本并提升了稳定性。该框架生成了一系列即用型检测器，称为TrashDets。在包含纸张、塑料、瓶子、易拉罐和烟头的五类TACO子集上，性能最强的变体TrashDet-l取得了19.5 mAP50的精度，参数量为30.5M，相比先前的检测器在精度上最多提升3.6 mAP50，同时使用更少的参数。TrashDet模型族的参数量范围为1.2M至30.5M，对应的mAP50值介于11.4到19.5之间，为不同TinyML部署预算提供了可扩展的选择。在MAX78002微控制器上使用TrashNet数据集时，两种专用变体TrashDet-ResNet和TrashDet-MBNet共同超越了ai87-fpndetector基线：其中TrashDet-ResNet单次推理能耗为7525微焦，延迟26.7毫秒，帧率达37.45 FPS；而TrashDet-MBNet的mAP50提升了10.2%。两者相较现有TinyML检测器，最高可减少88%的能耗、78%的延迟以及53%的平均功耗。

</details>


### [3] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

TL;DR: 本文提出了OccuFly，首个基于相机的真实世界空中语义场景补全（SSC）基准数据集，并设计了一种无需LiDAR的数据生成框架，利用传统3D重建技术将2D标注自动提升至3D点云，显著减少人工标注成本，为高空视角下的3D场景理解提供新基准。


<details>
  <summary>Details</summary>
Motivation: 当前语义场景补全（SSC）研究主要集中在地面场景（如自动驾驶），而空中场景（如无人机飞行）尚未充分探索；此外，现有SSC依赖LiDAR传感器，但大多数无人机受限于法规、重量、能耗及高空LiDAR点云稀疏等问题，难以部署LiDAR。

Method: 提出OccuFly数据集，在30–50米高度采集涵盖城市、工业和乡村环境的四季图像，包含22个语义类别；同时开发一种纯相机模态的LiDAR-free数据生成框架，通过传统3D重建将部分2D掩码标注自动提升到重建点云中，实现高效3D标注。

Result: 构建了首个真实世界、基于相机的空中SSC基准OccuFly；实现了自动化3D标注流程，大幅降低人工成本；在该基准上评估了现有SOTA方法，揭示了高空视角下SSC的独特挑战。

Conclusion: OccuFly填补了空中语义场景补全研究的空白，其LiDAR-free数据生成方法适用于主流无人机平台，为未来高空3D感知与场景理解提供了重要基础和评估标准。

Abstract: Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

Abstract (中文翻译): 语义场景补全（Semantic Scene Completion, SSC）对于移动机器人中的三维感知至关重要，因为它能够通过联合估计密集体素占用状态和每个体素的语义信息，实现对场景的整体理解。尽管SSC在自动驾驶等地面场景中已被广泛研究，但在自主飞行等空中场景中仍鲜有探索，从而限制了下游应用的发展。此外，LiDAR传感器是当前SSC数据生成的主要模态，但由于飞行法规、质量和能耗限制，以及从高空视角获取的LiDAR点云过于稀疏，大多数无人飞行器（UAV）难以部署LiDAR。为解决上述问题，我们提出了OccuFly——首个基于相机的真实世界空中SSC基准数据集，该数据集在春季、夏季、秋季和冬季分别于50米、40米和30米高度采集，涵盖城市、工业和乡村场景，提供22个语义类别，且数据格式遵循现有规范以方便与已有研究无缝集成。更重要的是，我们提出了一种无需LiDAR、仅依赖相机模态的数据生成框架，而相机在现代无人机上已广泛配备。该框架利用传统三维重建技术，将部分带标注的二维掩码自动提升至重建的点云中，从而显著减少人工三维标注的工作量。最后，我们在OccuFly上对当前最先进的方法进行了基准测试，揭示了高空视角带来的特有挑战，构建了一个全面的视觉基准，用于推动整体空中三维场景理解的研究。

</details>


### [4] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: NullBUS 是一种多模态混合监督框架，通过引入可为空的提示（nullable prompts）在乳腺超声图像分割任务中同时利用有提示和无提示的数据，在三个公开数据集上达到 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺超声（BUS）数据集中许多缺乏可靠的文本或空间提示信息，导致基于提示的分割方法只能在小规模多模态子集上训练，限制了模型的鲁棒性和泛化能力。

Method: 提出 NullBUS 框架，引入可学习的空嵌入（null embeddings）与存在掩码（presence masks）构成“可为空提示”，使模型在提示缺失时退化为仅依赖图像信息，而在提示存在时加以利用，从而在一个统一模型中联合学习有提示与无提示的数据。

Result: 在三个公开 BUS 数据集组成的统一测试集上，NullBUS 取得了平均 IoU 0.8568 和平均 Dice 0.9103 的结果，展现出在混合提示可用性条件下的最优性能。

Conclusion: NullBUS 有效解决了 BUS 分割中提示信息缺失的问题，提升了模型在真实场景中的适用性和鲁棒性，为多模态医学图像分割提供了新思路。

Abstract: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

Abstract (中文翻译): 乳腺超声（BUS）分割为计算机辅助诊断和治疗规划提供了关键的病灶边界信息。尽管可提示方法在提供文本或空间提示时能够提升分割性能和肿瘤轮廓描绘效果，但许多公开的 BUS 数据集缺乏可靠的元数据或报告，这限制了模型只能在小规模多模态子集上进行训练，降低了鲁棒性。我们提出了 NullBUS，这是一种多模态混合监督框架，能够在单一模型中同时从有提示和无提示的图像中学习。为应对文本缺失问题，我们引入了“可为空提示”（nullable prompts），通过可学习的空嵌入与存在掩码实现，使得在元数据缺失时可退回到仅依赖图像证据，而在元数据存在时则利用文本信息。在三个公开 BUS 数据集组成的统一池上评估，NullBUS 实现了平均 IoU 为 0.8568、平均 Dice 为 0.9103 的结果，在混合提示可用条件下展现出当前最优的性能。

</details>


### [5] [Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation](https://arxiv.org/abs/2512.20815)
*Reeshad Khan amd John Gauch*

Main category: cs.CV

TL;DR: 本文提出一种任务驱动的端到端RAW-to-task协同设计框架，将光学、传感器建模与轻量语义分割网络联合优化，在KITTI-360上显著提升mIoU，同时保持模型小巧（约1M参数）和实时性（~28 FPS），证明了全栈协同优化在自动驾驶感知中的有效性与部署潜力。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶感知流程将相机设计与下游任务解耦，采用固定光学系统和为人眼优化的手工ISP，导致在去马赛克、降噪或量化过程中丢失对机器感知有用的信息，并迫使模型适应传感器伪影。

Method: 构建一个端到端的RAW-to-task协同设计框架，整合真实手机级镜头模型、可学习彩色滤光阵列（CFA）、泊松-高斯噪声过程和量化操作，并直接针对语义分割目标进行联合优化。

Result: 在KITTI-360数据集上，该方法相比固定流程持续提升mIoU，其中光学建模和CFA学习带来最大增益，尤其对细长或低光敏感类别；模型仅含约100万参数，运行速度达28 FPS，具备边缘部署能力。

Conclusion: 全栈协同优化（光学+传感器+网络）是实现高效、可靠且可部署的自动驾驶感知系统的有效路径。

Abstract: Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.

Abstract (中文翻译): 传统的自动驾驶感知流程将相机设计与下游感知任务解耦，依赖于固定光学系统和以人眼可视图像为目标的手工图像信号处理（ISP），而非面向机器语义。这种分离方式在去马赛克、降噪或量化过程中丢弃了有用信息，并迫使模型去适应传感器引入的伪影。我们提出了一种任务驱动的协同设计框架，将光学系统、传感器建模与轻量级语义分割网络统一为单一的端到端RAW-to-task流水线。该系统基于DeepLens[19]，整合了逼真的手机级镜头模型、可学习的彩色滤光阵列（CFA）、泊松-高斯噪声过程以及量化操作，并直接针对分割目标进行端到端优化。在KITTI-360上的评估表明，该方法相较于固定流程在mIoU上持续取得提升，其中光学建模和CFA学习带来的增益最为显著，尤其对细长结构或对低光照敏感的类别效果更佳。重要的是，这些鲁棒性提升是在一个紧凑的约100万参数模型上实现的，运行速度约为28 FPS，展示了其在边缘设备上的部署潜力。可视化与定量分析进一步揭示了协同设计的传感器如何根据语义结构自适应地调整图像采集过程，在模糊、噪声和低位深条件下仍能锐化边界并保持精度。这些发现共同表明，对光学、传感器和网络进行全栈协同优化，是迈向高效、可靠且可部署的自动驾驶感知系统的一条有效路径。

</details>


### [6] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

TL;DR: 本文提出了CHAMMI-75，一个包含75项不同生物研究的多通道显微图像开放数据集，用于训练能适应不同成像通道和条件的细胞形态学模型，从而提升跨研究的模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有用于量化细胞形态的机器学习模型通常仅针对单一显微成像类型进行训练，难以在不同生物研究间复用，原因包括技术规格不匹配（如通道数不同）或目标实验条件超出训练分布。

Method: 作者从公开资源中整理构建了CHAMMI-75数据集，该数据集包含来自75项不同生物研究的异构多通道显微图像，用于训练具有通道自适应能力的细胞形态模型。

Result: 实验表明，利用CHAMMI-75训练的模型在多通道生物成像任务中性能更优，主要得益于其在显微成像模态上的高度多样性。

Conclusion: 该工作为构建下一代适用于各类生物研究的通用细胞形态学模型奠定了基础。

Abstract: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

Abstract (中文翻译): 利用图像和机器学习对细胞形态进行量化已被证明是研究细胞对处理响应的强大工具。然而，用于量化细胞形态的模型通常仅使用单一类型的显微成像进行训练，导致这些模型专业化程度高，难以在不同生物研究之间复用，原因在于技术规格不匹配（例如通道数量不同），或者目标实验条件超出了模型训练分布。在此，我们提出了CHAMMI-75——一个开放获取的数据集，包含来自75项多样化生物研究的异构多通道显微图像。我们从公开可用资源中整理了这一数据集，旨在研究具备通道自适应能力、可处理任意显微图像类型的细胞形态模型。我们的实验表明，使用CHAMMI-75进行训练能够显著提升多通道生物成像任务的性能，这主要归功于其在显微成像模态方面的高度多样性。本研究为构建适用于各类生物研究的新一代细胞形态模型铺平了道路。

</details>


### [7] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

TL;DR: 本文提出一种自适应视觉预处理方法，通过动态调整输入分辨率和空间覆盖范围，在不修改FastVLM架构或重新训练的前提下，显著降低视觉语言模型的推理时间和视觉token数量。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在处理高分辨率图像时存在推理延迟高、计算成本大的问题。尽管已有如FastVLM等高效架构，但其仍采用静态视觉预处理策略，对视觉内容简单的图像造成冗余计算。

Method: 提出一种自适应视觉预处理方法，结合内容感知图像分析、自适应分辨率选择和内容感知裁剪，在视觉编码前减少视觉冗余，并与FastVLM无缝集成，无需修改模型结构或重新训练。

Result: 在DocVQA子集上的实验表明，该方法将每张图像的推理时间减少超过50%，平均生成时间降低，视觉token数量减少超过55%。

Conclusion: 内容感知的自适应预处理是一种轻量且有效的策略，可显著提升视觉语言模型在部署场景中的效率。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

Abstract (中文翻译): 视觉语言模型（VLMs）在多模态推理任务中表现出色，但由于推理延迟高和计算成本大，尤其是在处理高分辨率视觉输入时，其部署仍然具有挑战性。尽管最近的架构（如FastVLM）通过优化视觉编码器提高了效率，但现有流程仍依赖静态视觉预处理，导致对视觉上简单的输入产生冗余计算。在本研究中，我们提出了一种自适应视觉预处理方法，该方法根据图像内容特征动态调整输入分辨率和空间覆盖范围。所提出的方法结合了内容感知的图像分析、自适应分辨率选择和内容感知裁剪，在视觉编码之前减少视觉冗余。重要的是，该方法在不修改FastVLM架构或无需重新训练的情况下与其集成。我们在DocVQA数据集的一个子集上以仅推理设置评估了所提出的方法，重点关注面向效率的指标。实验结果表明，自适应预处理将每张图像的推理时间减少了超过50%，降低了平均完整生成时间，并且与基线流程相比，视觉token数量持续减少了55%以上。这些发现表明，输入感知的预处理是一种有效且轻量的策略，可提升视觉语言模型在部署导向场景中的效率。为便于复现，我们的实现作为FastVLM仓库的一个分支提供，包含所提方法的相关文件，可在 https://github.com/kmdavidds/mlfastlm 获取。

</details>


### [8] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

TL;DR: ALIVE 是一个本地运行的交互式讲座视频引擎，结合神经化身、内容感知检索与多模态交互，将传统被动观看转变为实时互动学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统讲座视频缺乏实时答疑机制，而现有基于大语言模型和神经化身的交互系统往往缺乏对讲座内容的理解、依赖云端服务，或未能在保护隐私的前提下统一整合检索与化身讲解。

Method: ALIVE 在本地硬件上运行，整合三项核心技术：(1) 基于 ASR 转录、LLM 优化和神经说话头合成的化身讲解；(2) 结合语义相似性与时间戳对齐的内容感知检索机制；(3) 支持文本/语音提问并以文本或化身形式返回答案的实时多模态交互。系统采用轻量嵌入模型、FAISS 检索和分段化身合成以保障响应速度。

Result: 在完整医学影像课程上验证了 ALIVE 的检索准确性、延迟表现和用户体验，结果表明其能提供准确、内容相关且具吸引力的实时学习支持。

Conclusion: ALIVE 展示了多模态 AI 与内容感知检索、本地部署相结合，可显著提升录播讲座的教学价值，为下一代交互式学习环境提供了可扩展路径。

Abstract: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

Abstract (中文翻译): 传统的讲座视频虽然灵活，但缺乏实时澄清机制，导致学习者在遇到困惑时不得不借助外部搜索。尽管大型语言模型和神经化身的最新进展为交互式学习带来了新机遇，但现有系统通常缺乏对讲座内容的理解，依赖云服务，或未能在统一且保护隐私的流程中整合检索与化身讲解功能。  
我们提出了 ALIVE（Avatar-Lecture Interactive Video Engine），它将被动的讲座观看转变为动态的实时学习体验。ALIVE 完全在本地硬件上运行，整合了三项核心技术：(1) 通过 ASR 转录、大语言模型优化和神经说话头合成生成的化身讲解；(2) 结合语义相似性与时间戳对齐的内容感知检索机制，用于定位上下文相关的讲座片段；(3) 实时多模态交互，允许学生暂停讲座并通过文本或语音提问，并获得基于讲座内容的文本或化身形式的回答。  
为保持响应速度，ALIVE 采用轻量级嵌入模型、基于 FAISS 的检索以及分段化身合成与渐进式预加载策略。我们在一整门医学影像课程上展示了该系统，并评估了其检索准确性、延迟特性和用户体验，结果表明 ALIVE 能提供准确、内容相关且引人入胜的实时学习支持。  
ALIVE 表明，当多模态 AI 与内容感知检索和本地部署相结合时，可显著提升录播讲座的教学价值，为构建下一代交互式学习环境提供了可扩展的路径。

</details>


### [9] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

TL;DR: 本文提出一种用于地下管线3D探地雷达（GPR）智能检测的新框架，通过三视图联合分析、改进YOLO模型（DCO-YOLO）和3D-DIoU空间匹配算法，显著提升了小目标识别精度与多视图融合鲁棒性，在真实数据上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决3D GPR地下管线检测中多视图特征关联弱、小尺度目标识别精度低以及复杂场景下鲁棒性不足的问题。

Method: 1）建立基于B/C/D-Scan三视图联合分析的三维管线特征评估方法；2）提出DCO-YOLO框架，融合DySample、CGLU和OutlookAttention机制以增强小目标边缘特征提取；3）设计3D-DIoU空间特征匹配算法，结合三维几何约束与中心距离惩罚项实现多视图标注自动关联。

Result: 在真实城市地下管线数据上的实验表明，所提方法在复杂多管线场景中达到96.2%准确率、93.3%召回率和96.7%mAP，分别比基线模型高2.0%、2.1%和0.9%；消融实验和Grad-CAM++可视化验证了模型对管线几何特征的关注能力提升。

Conclusion: 该研究将深度学习优化策略与3D GPR物理特性相结合，为地下管线的智能识别与定位提供了一种高效可靠的新技术框架。

Abstract: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

Abstract (中文翻译): 为解决利用三维探地雷达（3D GPR）进行地下管线检测时存在的多视图特征相关性弱、小尺度目标识别精度低以及复杂场景下鲁棒性不足等问题，本文提出了一种三维管线智能检测框架。首先，基于B/C/D-Scan三视图联合分析策略，通过FDTD方法获得的正演模拟结果与实测数据交叉验证，建立了三维管线三视图特征评估方法。其次，提出了DCO-YOLO框架，将DySample、CGLU和OutlookAttention等跨维度关联机制集成到原始YOLOv11算法中，显著提升了小尺度管线边缘特征的提取能力。此外，还提出了一种3D-DIoU空间特征匹配算法，融合三维几何约束与中心距离惩罚项，实现了多视图标注的自动关联，三视图融合策略有效解决了单视图检测中的固有模糊性。基于真实城市地下管线数据的实验表明，所提方法在复杂多管线场景中分别达到了96.2%的准确率、93.3%的召回率和96.7%的平均精度均值（mAP），较基线模型分别提高了2.0%、2.1%和0.9%。消融实验验证了动态特征增强模块的协同优化效果，Grad-CAM++热力图可视化也表明改进后的模型显著增强了对管线几何特征的关注能力。本研究将深度学习优化策略与3D GPR的物理特性相结合，为地下管线的智能识别与定位提供了一种高效且可靠的新技术框架。

</details>


### [10] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

TL;DR: NeRV360 是一种用于 360 度视频的高效隐式神经表示框架，通过仅解码用户视口而非整个全景帧，显著降低内存消耗并提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 将 NeRV 应用于高分辨率 360 度视频时存在内存占用高和解码速度慢的问题，限制了其在实时应用中的可行性。

Method: 提出 NeRV360 框架，将视口提取集成到解码过程中，并引入时空仿射变换模块，根据视角和时间进行条件解码，仅重建用户选定的视口区域。

Result: 在 6K 分辨率视频上的实验表明，与 HNeRV 相比，NeRV360 内存消耗减少 7 倍，解码速度提升 2.5 倍，并在客观图像质量指标上表现更优。

Conclusion: NeRV360 有效解决了高分辨率 360 度视频在隐式神经表示下的效率瓶颈，为实时应用提供了可行方案。

Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

Abstract (中文翻译): 用于视频的隐式神经表示（NeRV）在视频压缩方面展现出强大潜力。然而，将 NeRV 应用于高分辨率 360 度视频会导致内存占用过高和解码速度缓慢，使得实时应用难以实现。我们提出了 NeRV360，这是一种端到端框架，仅解码用户选定的视口，而非重建整个全景帧。与传统流程不同，NeRV360 将视口提取集成到解码过程中，并引入了一个时空仿射变换模块，以根据视角和时间进行条件解码。在 6K 分辨率视频上的实验表明，与代表性先前工作 HNeRV 相比，NeRV360 实现了 7 倍的内存消耗降低和 2.5 倍的解码速度提升，同时在客观图像质量指标上表现更佳。

</details>
