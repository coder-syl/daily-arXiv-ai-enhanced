<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段生成式数据增强框架，结合基于规则的口罩形变与无配对图像到图像的GAN转换，以生成更真实的戴口罩人脸样本，提升在数据稀缺和分布偏移情况下的检测与识别性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺和分布偏移，戴口罩人脸的检测与识别面临重大挑战，现有方法难以生成足够真实且多样化的训练样本。

Method: 提出一种两阶段生成式数据增强框架：首先使用基于规则的口罩形变，再通过无配对图像到图像的GAN进行转换；引入非口罩区域保留损失和随机噪声注入以稳定训练并提升样本多样性。

Result: 相比仅使用规则形变的方法，该方法在定性上表现更优，并能有效补充如IAMGAN等现有GAN方法；实验验证了所提组件的有效性。

Conclusion: 所提出的框架在生成真实、多样化戴口罩人脸样本方面具有优势，为面向人脸识别任务的数据中心化增强提供了可行方向和改进思路。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

Abstract (中文翻译): 数据稀缺和分布偏移给戴口罩人脸的检测与识别带来了重大挑战。我们提出了一种两阶段生成式数据增强框架，该框架结合了基于规则的口罩形变与基于GAN的无配对图像到图像转换，能够生成超越纯合成变换的真实戴口罩人脸样本。与仅使用规则形变的方法相比，所提出的方法在定性上表现出一致的改进效果，并可有效补充现有的如IAMGAN等基于GAN的戴口罩人脸生成方法。我们引入了非口罩区域保留损失和随机噪声注入机制，以稳定训练过程并增强样本多样性。实验结果验证了所提各组件的有效性，并为未来在人脸识别任务中开展以数据为中心的增强方法指明了方向。

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出JARVIS框架，通过引入I-JEPA自监督学习范式增强多模态大语言模型（MLLMs）的视觉理解能力，从而在不损害多模态推理能力的前提下，显著提升其在以视觉为中心任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在基础视觉推理任务上表现有限，主要因为其视觉理解依赖于主观且不完整的文本描述作为监督信号，且多模态指令微调数据规模远小于纯文本预训练，导致模型过度依赖语言先验而忽略视觉细节。

Method: 提出JARVIS框架，将I-JEPA自监督学习范式融入MLLM的标准视觉-语言对齐流程中；利用冻结的视觉基础模型作为上下文和目标编码器，同时训练以LLM早期层实现的预测器，使其从图像中学习结构与语义规律，减少对语言监督的依赖。

Result: 在多个标准MLLM基准测试中，JARVIS在不同LLM系列上均一致提升了以视觉为中心任务的性能，同时未损害多模态推理能力。

Conclusion: 通过引入自监督视觉增强机制，JARVIS有效弥补了MLLM在视觉理解方面的不足，为提升多模态模型的视觉推理能力提供了新思路。

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

Abstract (中文翻译): 多模态大语言模型（MLLMs）最近在连接视觉与语言方面展现出令人印象深刻的能力，但其在基础视觉推理任务上的表现仍显不足。这一局限性源于MLLMs主要从文本描述中学习视觉理解，而文本描述是一种主观且本质上不完整的监督信号。此外，与大规模纯文本预训练相比，多模态指令微调的数据规模较小，导致MLLMs过度拟合语言先验而忽视视觉细节。为解决这些问题，我们提出了JARVIS——一种受JEPA启发的MLLM自监督视觉增强框架。具体而言，我们将I-JEPA学习范式整合到MLLM训练的标准视觉-语言对齐流程中。该方法利用冻结的视觉基础模型作为上下文编码器和目标编码器，同时训练以大语言模型（LLM）早期层实现的预测器，使其无需完全依赖语言监督即可从图像中学习结构和语义规律。在标准MLLM基准上的大量实验表明，JARVIS在不同LLM系列中均能持续提升以视觉为中心任务的性能，且不会削弱多模态推理能力。我们的源代码已公开：https://github.com/aimagelab/JARVIS。

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: 本文提出了一种新的任务“稀疏接地视觉导航”（Sparsely Grounded Visual Navigation）和相应基准CityNav，用于评估多模态大语言模型（MLLMs）在真实城市环境中仅依靠视觉输入进行知识密集型导航的能力，并提出了新方法Verbalization of Path（VoP）以显著提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 当前对具身智能体的评估主要局限于以语言为中心或高度依赖模拟环境的基准，缺乏对现实世界中复杂、知识密集型推理能力的有效评测。

Method: 作者构建了CityNav基准，在四个全球城市中设置50多个决策点，要求MLLM驱动的智能体仅使用视觉输入和内部多模态推理进行导航。同时提出Verbalization of Path（VoP）方法，通过从MLLM中显式提取认知地图（关键地标和方向）来增强其导航能力。

Result: 实验表明，现有的最先进MLLM及标准推理技术（如思维链、反思）在此任务中表现不佳；而所提出的VoP方法能显著提高导航成功率。

Conclusion: 为了有效评估MLLM在真实世界中的具身推理能力，需要超越传统语言或模拟环境的评测方式。本文提出的CityNav基准和VoP方法为未来研究提供了重要基础。

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

Abstract (中文翻译): 利用多模态大语言模型（MLLMs）开发具身智能体在解决复杂的现实世界任务方面具有巨大潜力。然而，当前的评估基准仍主要以语言为中心或严重依赖模拟环境，很少考察实际应用场景中所需的细致且知识密集型的推理能力。为弥合这一关键差距，我们提出了“稀疏接地视觉导航”（Sparsely Grounded Visual Navigation）任务，专门用于评估MLLM在具有挑战性的、知识密集型的真实世界环境中的序列决策能力。我们通过CityNav这一综合性基准对该任务进行了具体实现，该基准涵盖全球四个不同城市，专门用于评估由原始MLLM驱动的智能体在城市导航中的表现。智能体必须仅依靠视觉输入和内部多模态推理，在没有额外环境标注或特定架构修改的情况下，依次通过50多个决策点。至关重要的是，智能体必须能够自主通过解读城市特有线索和识别地标实现定位，进行空间推理，并战略性地规划和执行通往目的地的路线。通过大量评估，我们证明了当前最先进的MLLM和标准推理技术（例如思维链、反思）在此具有挑战性的设定下表现明显不足。为此，我们提出了“路径言语化”（Verbalization of Path, VoP）方法，通过从MLLM中显式探查认知地图（关键地标及朝向目的地的方向），从而显著提升导航成功率。

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: R4 是一个无需训练的检索增强框架，通过构建四维（时空）结构化记忆，使视觉语言模型具备持久、可共享的世界模型，从而显著提升在动态环境中对时空信息的检索与推理能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过构建包含语义、空间和时间信息的四维内部表征来感知和推理环境，现有方法缺乏类似的能力。作者希望赋予视觉语言模型类似的结构化、持久记忆机制，以支持更强大的时空推理。

Method: 提出 R4 框架：在推理阶段将自然语言查询分解为语义、空间和时间键，在一个持续构建的四维知识库中进行检索；该知识库存储对象级语义描述及其在度量空间和时间中的锚点，无需训练即可集成到视觉语言模型的推理过程中。

Result: 在具身问答和导航基准测试中，R4 相比基线方法显著提升了时空信息的检索与推理性能。

Conclusion: R4 为动态环境中的具身智能体提供了一种新的四维推理范式，通过结构化、可共享的持久记忆机制，实现了无需训练的高效时空推理。

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

Abstract (中文翻译): 人类通过构建持久、结构化的内部表征，在四维时空中感知并推理周围环境，这些表征编码了语义含义、空间布局和时间动态。这种多模态记忆使人类能够回忆过去事件、推断未观测状态，并将新信息整合到依赖上下文的推理中。受此启发，我们提出了 R4——一种无需训练的检索增强推理框架，用于在四维时空空间中为视觉语言模型（VLMs）配备结构化的终身记忆。R4 通过在度量空间和时间中锚定对象级语义描述，持续构建一个四维知识数据库，形成可在多个智能体间共享的持久世界模型。在推理阶段，自然语言查询被分解为语义、空间和时间关键字，以检索相关观测，并将其整合进 VLM 的推理过程。与传统的检索增强生成方法不同，R4 的检索直接在四维空间中进行，从而实现无需训练的情节性和协作式推理。在具身问答和导航基准上的实验表明，R4 在时空信息的检索与推理方面显著优于基线方法，推动了动态环境中具身四维推理的新范式。

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出“感知观测站”（The Perceptual Observatory）框架，用于系统评估多模态大语言模型（MLLMs）在受控扰动下的感知能力，超越传统端到端准确率指标，关注其视觉基础、鲁棒性和推理保真度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）虽在规模上不断增长，但其感知能力缺乏系统性评估；多数模型仅扩展语言组件而沿用相同视觉编码器，难以判断其性能提升源于真实视觉理解还是依赖文本先验知识。现有评测方法过于关注任务准确率，忽视了鲁棒性、归因忠实度及扰动下的推理能力。

Method: 作者构建了“感知观测站”评测框架，涵盖两类垂直任务：(i) 简单视觉任务（如人脸匹配、图像中文本理解）；(ii) 从局部到全局的理解任务（如图像匹配、网格指向游戏、属性定位）。所有任务均基于含真值的人脸与文字数据集，并通过像素级增强和扩散模型生成的风格化错觉进行系统扰动。

Result: 该框架揭示了MLLMs在扰动条件下如何保持感知基础和关系结构，提供了对其感知能力优势与缺陷的深入洞察，超越了传统排行榜准确率的局限。

Conclusion: “感知观测站”为评估当前及未来MLLMs的感知能力提供了原则性基础，强调需关注模型在扰动下的视觉接地能力和结构保持能力，而非仅依赖端到端准确率。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

Abstract (中文翻译): 多模态大语言模型（MLLMs）的最新进展催生了日益强大的模型，但其感知能力仍未得到充分刻画。在实践中，大多数模型系列仅扩展语言组件，而重复使用几乎相同的视觉编码器（例如 Qwen2.5-VL 3B/7B/72B），这引发了关键问题：模型的进步究竟反映的是真实的视觉接地能力，还是依赖于互联网规模的文本世界知识？现有的评估方法侧重于端到端任务的准确率，忽略了鲁棒性、归因保真度以及在受控扰动下的推理能力。我们提出了“感知观测站”（The Perceptual Observatory）框架，从多个维度刻画MLLMs的感知特性：(i) 简单视觉任务，如人脸匹配和图像中文本理解能力；(ii) 从局部到全局的理解能力，包括图像匹配、网格指向游戏和属性定位，以测试通用的视觉接地能力。每个维度均基于包含人脸和文字真值的数据集，并通过基于像素的增强和基于扩散模型的风格化错觉进行系统性扰动。“感知观测站”超越了排行榜准确率，深入揭示了MLLMs在扰动下如何保持感知接地和关系结构，为分析当前及未来模型的优势与不足提供了原则性基础。

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: 本文提出CAMP-VLM，一种基于视觉语言模型的框架，用于从第三人称视角预测多个人类行为，通过结合视觉上下文和场景图的空间信息，并利用合成数据进行微调，在预测准确率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注以自我中心视角预测单人行为，但许多机器人应用需要从观察者（第三人称）视角理解多人行为，而当前缺乏适用于该任务的数据集和方法。

Method: 提出CAMP-VLM框架，整合视觉输入中的上下文特征与场景图中的空间感知；使用逼真模拟器生成的合成人类行为数据，结合监督微调（SFT）和直接偏好优化（DPO）对模型进行训练。

Result: CAMP-VLM在预测准确率上比最佳基线方法最高提升66.9%，并在合成与真实世界序列上验证了其泛化能力。

Conclusion: CAMP-VLM有效提升了从第三人称视角对多个人类行为的预测性能，为移动机器人在人群环境中的行为理解提供了可行方案。

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

Abstract (中文翻译): 准确预测人类行为对于在人群环境中运行的移动机器人至关重要。尽管以往的研究主要集中在从自我中心视角预测单人场景中的行为，但许多机器人应用需要从第三人称视角理解多个人类的行为。为此，我们提出了CAMP-VLM（上下文感知的多人行为预测）：一种基于视觉语言模型（VLM）的框架，通过融合视觉输入中的上下文特征和场景图中的空间感知信息，增强对人-场景交互的预测能力。由于缺乏适用于从观察者视角进行多人行为预测的合适数据集，我们利用逼真模拟器生成的合成人类行为数据对CAMP-VLM进行微调，并在合成数据和真实世界序列上评估模型的泛化能力。借助监督微调（SFT）和直接偏好优化（DPO），CAMP-VLM在预测准确率上比性能最佳的基线方法最高提升66.9%。

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 本文探索将视觉-语言模型（VLM）用于少样本多光谱目标检测，通过适配Grounding DINO和YOLO-World并融合文本、可见光与热成像模态，在FLIR和M3FD数据集上取得优于专用模型的性能，证明VLM的语义先验可有效迁移到新光谱模态，实现高效数据利用。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标检测在自动驾驶和监控等安全敏感应用中至关重要，但标注数据稀缺限制了深度检测器的训练；作者受视觉-语言模型（VLM）近期成功的启发，尝试将其用于少样本多光谱目标检测，以利用文本类别信息提供语义监督。

Method: 作者将两种代表性VLM检测器（Grounding DINO 和 YOLO-World）适配至多光谱输入，并提出一种有效机制融合文本、可见光与热成像模态。

Result: 在FLIR和M3FD两个多光谱图像基准上的实验表明，所提VLM检测器在少样本设置下显著优于使用同等数据训练的专用多光谱模型，并在全监督设置下也达到具有竞争力甚至更优的结果。

Conclusion: 大规模VLM学习到的语义先验能有效迁移到未见过的光谱模态，为数据高效的多光谱感知提供了有力途径。

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

Abstract (中文翻译): 多光谱目标检测对于自动驾驶和监控等对安全性要求较高的应用至关重要，因为在各种光照条件下实现鲁棒感知是必不可少的。然而，标注多光谱数据的稀缺严重限制了深度检测器的训练。在这种数据稀缺的场景下，文本类别信息可以作为一种有价值的语义监督来源。受近期视觉-语言模型（VLM）在计算机视觉领域取得成功的启发，我们探索其在少样本多光谱目标检测中的潜力。具体而言，我们将两种代表性的基于VLM的检测器——Grounding DINO和YOLO-World——适配用于处理多光谱输入，并提出了一种有效机制来融合文本、可见光与热成像模态。通过对两个流行的多光谱图像基准数据集FLIR和M3FD进行大量实验，我们证明了基于VLM的检测器不仅在少样本场景下表现卓越，显著优于使用同等数据训练的专用多光谱模型，而且在全监督设置下也能取得具有竞争力甚至更优的结果。我们的研究发现，大规模VLM所学习到的语义先验能够有效地迁移到未见过的光谱模态，为实现数据高效的多光谱感知提供了一条强有力的路径。

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 当前通用视觉语言模型（VLM）在农业识别任务中表现远逊于专用监督模型（如YOLO11），即使最强的闭源VLM（Gemini-3 Pro）在多项选择提示下平均准确率仅约62%，开放式回答则更低；评估方式显著影响性能排名，开源模型Qwen-VL-72B表现最佳但仍有差距；植物与杂草识别相对容易，病虫害识别最具挑战；现有VLM尚不适合作为独立农业诊断系统，但在约束性界面和领域适配策略下可作为辅助工具。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLM）被越来越多地视为通用视觉识别解决方案，但其在农业决策支持中的可靠性尚未得到充分评估。本文旨在系统评测多种开源与闭源VLM在农业分类任务上的实际性能，以明确其适用边界和改进方向。

Method: 作者在AgML集合中的27个农业分类数据集（涵盖162个类别，包括植物病害、虫害损伤及植物与杂草物种识别）上对多种VLM进行基准测试，采用零样本设置，并对比监督基线模型YOLO11。评估包含多项选择提示和开放式提示两种范式，并引入基于大语言模型（LLM）的语义判断来提升开放式回答的评分准确性，同时分析不同任务类型和模型类型的性能差异。

Result: 所有零样本VLM均显著落后于YOLO11；多项选择提示下最佳VLM（Gemini-3 Pro）平均准确率约62%，开放式提示原始准确率通常低于25%；LLM语义判断可将开放式准确率提升（如从21%升至30%）并改变模型排名；开源模型中Qwen-VL-72B表现最优；植物与杂草识别比病虫害识别更容易。

Conclusion: 当前现成的VLM尚不能作为独立的农业诊断系统，但在结合约束性交互界面、显式标签本体和领域感知评估策略的情况下，可作为有效的辅助组件。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

Abstract (中文翻译): 视觉语言模型（VLM）正越来越多地被提出作为视觉识别任务的通用解决方案，但其在农业决策支持中的可靠性仍知之甚少。我们在AgML数据集集合中的27个农业分类数据集上对一系列开源和闭源VLM进行了基准测试，这些数据集涵盖162个类别，涉及植物病害、虫害与损伤以及植物与杂草物种的识别。在所有任务中，零样本VLM的表现显著逊色于一个有监督的任务专用基线模型（YOLO11），后者始终比任何基础模型都获得明显更高的准确率。在多项选择提示下，表现最佳的VLM（Gemini-3 Pro）平均准确率约为62%，而开放式提示下的性能则低得多，原始准确率通常低于25%。采用基于大语言模型（LLM）的语义判断方法可提高开放式回答的准确率（例如，顶级模型的准确率从21%提升至30%），并改变模型的性能排名，表明评估方法会实质性地影响所得结论。在开源模型中，Qwen-VL-72B表现最佳，在受限提示下接近闭源模型性能，但仍落后于顶尖的专有系统。任务层面的分析显示，植物与杂草物种分类始终比虫害与损伤识别更容易，后者在所有模型中都是最具挑战性的类别。总体而言，这些结果表明，当前现成可用的VLM尚不适合作为独立的农业诊断系统，但在与约束性界面、显式标签本体和领域感知评估策略相结合时，可作为辅助组件发挥作用。

</details>


### [9] [Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings](https://arxiv.org/abs/2512.15993)
*Lars Beckers,Arno Waes,Aaron Van Campenhout,Toon Goedemé*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉感知与自适应决策的机器人割草框架，通过保留视觉多样性植被斑块来主动提升花园生物多样性。


<details>
  <summary>Details</summary>
Motivation: 传统割草方式导致城市草坪单一化，缺乏生态价值；现有再野化方法多为被动，难以在日常维护中兼顾美观与生态。因此需要一种能主动识别并保护植被多样性的智能割草系统。

Method: 利用在PlantNet300K上预训练的ResNet50网络提取具有生态意义的图像嵌入，通过计算特征空间中的全局偏离度作为无需物种标注的生物多样性指标，并据此动态控制割草机刀片启停，实现选择性割草。

Result: 在模拟草坪和真实花园数据集上的实验表明，所提方法的特征空间离散度与专家评估的生物多样性高度相关，验证了视觉多样性可作为生态丰富度的有效代理指标。

Conclusion: 该系统能将单调草坪转化为高生态价值的微型生境，若广泛部署，有望显著提升城市生物多样性。

Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.

Abstract (中文翻译): 本文提出了一种机器人割草框架，通过视觉感知与自适应决策主动增强花园生物多样性。与被动的再野化方法不同，该系统利用深度特征空间分析，在摄像头图像中识别并保留视觉上多样的植被斑块，并选择性地关闭割草刀片以实现保护。系统采用在PlantNet300K数据集上预训练的ResNet50网络生成具有生态意义的嵌入表示，并在此基础上构建一种全局偏离度指标，在无需物种级别监督的情况下估算生物多样性。该估算结果驱动一种选择性割草算法，动态切换割草与保护行为。该系统已在改装的商用机器人割草机上实现，并在受控模拟草坪和真实花园数据集上进行了验证。实验结果表明，嵌入空间的离散程度与专家对生物多样性的评估高度相关，证实了深度视觉多样性可作为生态丰富度的有效代理指标，也验证了所提割草决策方法的有效性。此类系统的广泛应用有望将生态价值低下的单一种植草坪转变为生机勃勃、富有价值的微型生物群落，从而提升城市生物多样性。

</details>


### [10] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出一种从初始图像和机器人关节状态出发、根据文本指令生成视频-动作对的方法，通过引入并行动作扩散模型、Bridge Attention机制和动作精炼模块，有效利用预训练视频扩散模型进行机器人策略学习，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用视频扩散模型进行机器人策略学习时面临动作标注缺失的问题，且多采用两阶段流程或单模态扩散模型，难以实现紧密的跨模态信息交互并充分利用预训练视频知识。

Method: (1) 在预训练视频扩散模型基础上并行引入专用动作扩散模型以保留预训练知识；(2) 设计Bridge Attention机制促进视频与动作之间的跨模态交互；(3) 构建动作精炼模块，将粗略动作转化为适用于低分辨率数据集的精确控制信号。

Result: 在多个公开基准和真实世界数据集上的实验表明，该方法生成的视频质量更高、动作更准确，显著优于现有基线方法。

Conclusion: 该方法为利用大规模视频数据进行机器人学习提供了一个可扩展的框架，有效解决了动作标注缺失和跨模态融合不足的问题。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

Abstract (中文翻译): 我们提出了一种根据文本指令生成视频-动作对的方法，该方法从初始图像观测和机器人关节状态出发。我们的方法自动为视频扩散模型提供动作标签，克服了动作标注普遍缺失的问题，从而使其能充分用于机器人策略学习。现有方法要么采用两阶段流程，限制了紧密耦合的跨模态信息共享，要么依赖于将单模态扩散模型适配为联合分布，无法充分利用预训练的视频知识。为克服这些局限，我们（1）在预训练视频扩散模型基础上扩展了一个并行的专用动作扩散模型，以保留预训练知识；（2）引入Bridge Attention机制，实现有效的跨模态交互；（3）设计了一个动作精炼模块，将粗略动作转换为适用于低分辨率数据集的精确控制。在多个公开基准和真实世界数据集上的广泛评估表明，我们的方法生成的视频质量更高、动作更准确，并显著优于现有基线方法，为利用大规模视频数据进行机器人学习提供了一个可扩展的框架。

</details>
