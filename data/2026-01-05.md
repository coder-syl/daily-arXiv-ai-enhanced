<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld 是一个实时多模态4D世界建模框架，通过生成-重建-引导闭环机制，在保证时空一致性和物理合理性的前提下，实现低延迟、长时程的动态场景生成与记忆。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽具高视觉质量，但在实时交互、长时程一致性及动态场景持久记忆方面存在不足，难以作为实用的世界模型。因此，亟需一种能统一视频生成、动态重建与长期记忆的系统性方法。

Method: 提出 TeleWorld 框架，采用“生成-重建-引导”范式：将生成的视频流持续重建为动态4D时空表示，并以此引导后续生成以维持一致性；结合基于自回归扩散的视频模型、Macro-from-Micro Planning（MMPL）分层规划策略以减少误差累积，以及高效 Distribution Matching Distillation（DMD）技术实现实时合成。

Result: 实验表明，TeleWorld 在静态与动态场景理解、长期一致性及实时生成效率方面表现优异，成功整合了动态对象建模与静态场景表示。

Conclusion: TeleWorld 为构建具备交互能力、记忆机制和计算可行性的实用型世界模型迈出了关键一步，有望推动多模态生成与具身智能的发展。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

Abstract (中文翻译): 世界模型旨在赋予人工智能系统以连贯且时间一致的方式表示、生成并与动态环境交互的能力。尽管近期的视频生成模型在视觉质量上表现出色，但它们在实时交互、长时程一致性以及对动态场景的持久记忆方面仍存在局限，阻碍了其向实用世界模型的演进。在本报告中，我们提出了 TeleWorld——一个实时多模态4D世界建模框架，该框架在一个闭环系统中统一了视频生成、动态场景重建和长期世界记忆。TeleWorld 引入了一种新颖的“生成-重建-引导”范式：生成的视频流被持续重建为动态4D时空表示，进而引导后续生成过程，以维持空间、时间和物理一致性。为支持低延迟的长时程生成，我们采用了一种基于自回归扩散的视频模型，并结合 Macro-from-Micro Planning（MMPL）分层规划方法（将误差累积从帧级降低到片段级）以及高效的 Distribution Matching Distillation（DMD）技术，从而在实际计算预算下实现实时合成。我们的方法在统一的4D框架内无缝整合了动态对象建模与静态场景表示，推动世界模型向实用、可交互且计算高效的方向发展。大量实验表明，TeleWorld 在静态与动态世界理解、长期一致性以及实时生成效率方面均取得了优异性能，标志着迈向具备交互能力和记忆机制的实用型世界模型的重要一步，适用于多模态生成与具身智能。

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 本文通过优化噪声而非依赖传统引导或后处理方法，有效缓解文本到图像生成中的模式崩溃问题，在保持生成质量的同时显著提升多样性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在相同提示下生成的图像存在明显的模式崩溃问题，限制了输出的多样性。

Method: 提出一种基于噪声优化的方法，通过设计简单的噪声优化目标，并探索具有不同频率特性的噪声初始化策略，以提升生成结果的多样性和质量。

Result: 实验表明，所提出的噪声优化方法在生成质量和多样性方面均优于现有方法。

Conclusion: 噪声优化是一种有效缓解模式崩溃、同时保持模型保真度的可行途径，且不同频率特性的噪声初始化有助于进一步提升性能。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

Abstract (中文翻译): 当代文本到图像模型在给定相同文本提示时生成的多个图像表现出令人惊讶的模式崩溃现象。尽管先前工作尝试通过引导机制调控模型或生成大量候选图像后再进行筛选来解决该问题，本文则另辟蹊径，通过噪声优化来提升生成结果的多样性。具体而言，我们证明了一种简单的噪声优化目标能够在保持基础模型保真度的同时缓解模式崩溃问题。我们还分析了噪声的频率特性，并表明采用具有不同频率分布的替代噪声初始化方式可以改进优化过程和搜索效果。实验结果表明，噪声优化在生成质量和多样性方面均取得了更优的结果。

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了Spatial4D-Bench，一个大规模、多任务的4D空间智能评测基准，用于全面评估多模态大语言模型（MLLMs）在4D空间推理方面的能力，并揭示了当前主流MLLMs在此类任务中的显著不足。


<details>
  <summary>Details</summary>
Motivation: 人类天然具备4D空间智能，能够理解物体随时间变化的运动与形态，而现有研究缺乏对多模态大语言模型是否具备类似能力的系统评估。当前的空间智能评测基准通常规模小、多样性有限，难以全面衡量MLLMs的4D空间认知能力。

Method: 作者构建了Spatial4D-Bench，包含约4万个问答对，覆盖18个明确定义的任务，并将这些任务系统划分为六类认知维度：物体理解、场景理解、空间关系理解、时空关系理解、空间推理和时空推理。

Result: 在Spatial4D-Bench上对多种前沿开源和闭源MLLM进行评测，结果显示这些模型在路径规划、动作识别、物理合理性推理等多个4D空间推理任务上存在明显短板。

Conclusion: 当前MLLM尚未达到人类水平的4D空间智能；Spatial4D-Bench可作为结构化、全面的评测工具，推动未来更强大MLLM的研发。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

Abstract (中文翻译): 4D空间智能涉及感知和处理物体如何随时间移动或变化。人类天然具备4D空间智能，支撑着广泛的空间推理能力。多模态大语言模型（MLLMs）在多大程度上能够实现人类水平的4D空间智能？为此，我们提出了Spatial4D-Bench——一个通用的4D空间智能评测基准，旨在全面评估MLLMs的4D空间推理能力。与现有规模较小或多样性有限的空间智能基准不同，Spatial4D-Bench提供了一个大规模、多任务的评测基准，包含约40,000个问答对，涵盖18个明确定义的任务。我们将这些任务系统地组织为六类认知维度：物体理解、场景理解、空间关系理解、时空关系理解、空间推理和时空推理。因此，Spatial4D-Bench为评估MLLMs的空间认知能力提供了一个结构化且全面的基准，覆盖了与人类空间智能相媲美的广泛任务。我们在Spatial4D-Bench上对多种最先进的开源和闭源MLLM进行了评测，揭示了它们在路径规划、动作识别和物理合理性推理等多种4D空间推理任务上的显著局限性。我们希望本研究的发现能为社区提供有价值的见解，并期望该基准能够促进更强大MLLM的发展，以迈向人类水平的4D空间智能。更多资源请参见我们的项目页面。

</details>


### [4] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为SMAGNet的多模态深度学习模型，利用SAR作为主输入，并自适应融合可能部分缺失的MSI数据，以提升洪水后水体范围制图的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在洪水应急响应阶段，及时准确的水体范围信息至关重要。虽然SAR数据常用于此任务，但结合SAR与MSI（多光谱成像）数据的多模态方法更具潜力，尤其是在灾后可用影像有限的情况下。然而，如何在MSI数据部分缺失时自适应地将其整合到基于SAR的制图流程中，仍是未被充分探索的问题。

Method: 作者提出了空间掩码自适应门控网络（SMAGNet），一种多模态深度学习模型。该模型以SAR数据为主要输入，通过特征融合机制自适应地整合互补的MSI数据，即使MSI数据部分或完全缺失也能有效工作。

Result: 在C2S-MS Floods数据集上的实验表明，SMAGNet在不同MSI数据可用性水平下均优于其他多模态模型。即使MSI数据完全缺失，其性能也与仅使用SAR数据训练的U-Net模型相当。

Conclusion: SMAGNet不仅提升了模型对缺失数据的鲁棒性，也增强了多模态深度学习在现实世界洪水管理场景中的适用性。

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

Abstract (中文翻译): 在洪水事件期间绘制水体范围对于减灾、准备、响应和恢复等所有阶段的有效灾害管理至关重要。特别是在响应阶段，及时准确的信息尤为重要，此时主要采用合成孔径雷达（SAR）数据来生成水体范围图。最近，通过多模态方法利用SAR和多光谱成像（MSI）数据的互补特性，已成为利用深度学习模型推进水体范围制图的一种有前景的策略。当在洪峰期间或之后不久获取的及时灾后观测数据有限时，这种方法尤其有益，因为它能够利用所有可用影像进行更准确的灾后水体范围制图。然而，如何将部分可用的MSI数据自适应地整合到基于SAR的灾后水体范围制图过程中，目前仍缺乏深入研究。为填补这一研究空白，我们提出了空间掩码自适应门控网络（SMAGNet），这是一种多模态深度学习模型，以SAR数据作为灾后水体范围制图的主要输入，并通过特征融合整合互补的MSI数据。在C2S-MS Floods数据集上的实验表明，SMAGNet在不同MSI数据可用性水平下的预测性能始终优于其他多模态深度学习模型。此外，我们发现，即使MSI数据完全缺失，SMAGNet的性能在统计上仍与仅使用SAR数据训练的U-Net模型相当。这些结果表明，SMAGNet不仅增强了模型对缺失数据的鲁棒性，也提高了多模态深度学习在现实世界洪水管理场景中的适用性。

</details>


### [5] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 本文提出压缩地图先验（CMP），一种从历史行驶数据中学习空间先验的轻量级方法，仅需32KB/km²存储，可无缝集成到主流3D感知系统中，在nuScenes数据集上显著提升多种架构的3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶视觉系统通常将每个地点视为首次访问，忽略了大量道路已被反复行驶的事实，未能有效利用历史遍历信息来提升感知性能。

Method: 提出压缩地图先验（Compressed Map Priors, CMP）框架，利用二值化哈希图从历史轨迹中学习空间先验，存储开销仅为32KB/km²，比稠密存储减少20倍，并可轻松集成到现有3D感知系统中。

Result: 在nuScenes数据集上，CMP在多种3D目标检测架构中均带来显著且一致的性能提升，且几乎不增加额外计算开销。

Conclusion: 利用历史遍历数据构建轻量级空间先验是一种高效提升自动驾驶3D感知能力的方法，CMP为此提供了一个简单而有效的解决方案。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

Abstract (中文翻译): 人类驾驶员很少前往从未有人去过的地方。毕竟，每天都有成千上万的驾驶员使用繁忙的城市道路，其中只有一人能声称自己是第一个。自动驾驶计算机视觉系统也是如此。绝大多数自动驾驶视觉系统的部署区域都曾被访问过。然而，大多数自动驾驶车辆视觉系统却表现得好像每次都是第一次遇到某个地点。在本研究中，我们提出了压缩地图先验（Compressed Map Priors, CMP），这是一个简单而有效的框架，可从历史行驶轨迹中学习空间先验。该地图先验采用二值化哈希图，每平方公里仅需32KB存储空间，相比稠密存储方式减少了20倍。压缩地图先验能够轻松集成到主流3D感知系统中，几乎不带来额外计算开销，并在nuScenes数据集上显著且一致地提升了多种架构的3D目标检测性能。

</details>


### [6] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: GLASS是一种结合全局视图与局部高分辨率裁剪的图像检测架构，通过分层采样和注意力机制有效融合多尺度信息，在计算可行的前提下显著提升AI生成图像的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法通常对输入图像进行下采样，导致丢失关键的细粒度细节，影响检测效果。

Method: 提出GLASS架构，将全局缩放视图与多个通过空间分层采样获得的原始分辨率局部裁剪相结合，并利用注意力机制对局部区域进行加权聚合；该架构可嵌入各类视觉模型（如ViT、ResNet、ConvNeXt）中处理任意尺寸图像。

Result: 在多种骨干网络上实验表明，GLASS在合理计算开销下显著优于标准迁移学习方法，取得更高的预测性能。

Conclusion: GLASS有效保留并利用了图像中的局部细节与全局语义信息，为AI生成图像检测提供了一种高效且通用的解决方案。

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

Abstract (中文翻译): 生成式人工智能的快速发展使得AI生成的图像越来越逼真且具有高分辨率。大多数AI生成图像检测架构通常在将图像输入模型前对其进行下采样，这可能导致细粒度细节的丢失。本文提出了GLASS（Global-Local Attention with Stratified Sampling）架构，该架构结合了全局缩放视图与多个随机采样的局部裁剪区域。这些裁剪区域是通过空间分层采样高效选取的原始分辨率区域，并通过基于注意力的评分机制进行聚合。GLASS可集成到视觉模型中，以利用任意尺寸图像中的全局与局部信息。本文采用Vision Transformer、ResNet和ConvNeXt作为骨干网络进行实验，结果表明，在可行的计算约束下，GLASS相比标准迁移学习方法实现了更高的预测性能。

</details>


### [7] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: 本文提出了FCMBench-V1.0，一个面向金融信贷场景的大规模多模态基准数据集，包含18类核心证件、4,043张隐私合规图像和8,446个问答样本，通过感知、推理和鲁棒性三个维度评估视觉语言模型在真实金融任务中的表现，并展示了当前主流模型在此基准下的性能差距与鲁棒性不足。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在信用风险评估和文档审核中的广泛应用，亟需一个领域专用的基准来反映金融信贷特有的文档与工作流、包含信贷相关的理解能力与现实鲁棒性，并在保障隐私合规的同时保持实用价值。

Method: 作者构建了FCMBench-V1.0，涵盖18类核心证件，采用闭合的合成-拍摄流程生成4,043张隐私合规图像和8,446个问答样本。评估框架包括感知（3项基础任务）、推理（4项信贷专用任务）和鲁棒性（10种现实采集伪影）三个维度。所有样本通过人工合成模板并内部拍摄生成，避免使用网络或公开图像以防止预训练数据泄露。

Result: 在23个来自14家顶尖机构的视觉语言模型上的实验表明：商业模型Gemini 3 Pro取得最佳F1得分（64.61%），开源基线Qwen3-VL-235B得分为57.27%，而作者提出的金融专用模型Qfin-VL-Instruct以64.92%位居榜首。鲁棒性测试显示，即使顶尖模型在采集伪影下也出现明显性能下降。

Conclusion: FCMBench-V1.0能有效区分不同视觉语言模型在金融信贷多模态任务中的性能差异和鲁棒性短板，为该领域模型开发与评估提供了可靠基准。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

Abstract (中文翻译): 随着多模态AI在信用风险评估和文档审核中的广泛应用，迫切需要一个领域专用的基准，该基准应（1）反映金融信贷申请中特有的文档和工作流程，（2）包含信贷相关的理解能力和现实世界的鲁棒性，（3）在不牺牲实际效用的前提下确保隐私合规。为此，我们提出了FCMBench-V1.0——一个面向真实应用场景的大规模金融信贷多模态基准，涵盖18类核心证件，包含4,043张隐私合规图像和8,446个问答样本。FCMBench评估框架包含三个维度：感知、推理和鲁棒性，具体包括3项基础感知任务、4项需要对视觉证据进行决策导向理解的信贷专用推理任务，以及用于鲁棒性压力测试的10种现实世界采集伪影类型。为了兼顾合规性与真实性，我们通过一个封闭的合成-拍摄流程构建所有样本：手动合成带有虚拟内容的文档模板，并在内部拍摄符合场景的图像。该设计还通过避免使用网络来源或公开发布的图像，缓解了预训练数据泄露问题。FCMBench能够有效区分现代视觉语言模型之间的性能差异和鲁棒性表现。我们在来自14家顶级AI公司和研究机构的23个最先进视觉语言模型上进行了广泛实验。其中，商业模型Gemini 3 Pro取得了最佳F1（%）分数（64.61），开源基线Qwen3-VL-235B表现最佳（57.27），而我们提出的金融信贷专用模型Qfin-VL-Instruct获得了整体最高分（64.92）。鲁棒性评估表明，即使是最先进的模型，在面对采集伪影时也会出现明显的性能下降。

</details>


### [8] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 本文提出FaceFocalDesc任务，即对任意选定的人脸区域生成和识别包含面部动作单元（AU）、情绪状态和年龄估计的多属性自然语言描述，并构建了相应数据集；同时提出了基于Qwen2.5-VL微调的Focal-RegionFace模型，通过多阶段渐进式微调实现对局部特征的聚焦，在新基准上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有面部分析方法通常关注整张人脸，缺乏对任意局部区域进行细粒度多属性理解的能力。作者认为，系统若能聚焦于特定面部区域，将有助于更深入的理解与可控性，因此提出FaceFocalDesc这一新问题。

Method: 构建了一个针对任意人脸区域的多属性描述新数据集，包含丰富的区域级标注和自然语言描述；在此基础上，提出Focal-RegionFace模型，该模型基于Qwen2.5-VL视觉语言模型，通过多个渐进式微调阶段逐步增强对局部面部特征的关注能力，从而实现可解释的年龄估计、面部动作单元（FAU）和情绪检测。

Result: Focal-RegionFace在新构建的基准数据集上，在传统常用指标和新提出的指标上均取得了最佳性能。

Conclusion: 所提方法在细粒度、多属性、区域聚焦的人脸分析任务中具有显著有效性与通用性，验证了局部聚焦策略在面部状态理解中的优势。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

Abstract (中文翻译): 本文提出了一种面部分析中尚未被充分探索的问题：为任意选定的人脸区域生成并识别包含面部动作单元（AUs）、情绪状态和年龄估计的多属性自然语言描述（称为FaceFocalDesc）。作者认为，系统若能聚焦于个体面部区域，将有助于实现更深入的理解与更强的控制能力。为此，本文构建了一个面向任意人脸区域的新型多属性描述数据集，提供了丰富的区域级标注和自然语言描述。此外，本文提出了一种基于Qwen2.5-VL微调的视觉语言模型——Focal-RegionFace，用于面部状态分析。该模型通过多个渐进式微调阶段，逐步优化对局部面部特征的关注，从而实现可解释的年龄估计、面部动作单元（FAU）识别和情绪检测。实验结果表明，Focal-RegionFace在新构建的基准测试中，无论是在传统广泛使用的指标还是新提出的指标上，均取得了最佳性能，充分验证了其在细粒度多属性人脸区域聚焦分析场景中的有效性和通用性。

</details>


### [9] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

TL;DR: 本文提出DichroGAN，一种用于从卫星图像恢复海底真实颜色的条件生成对抗网络，通过联合估计大气辐射亮度与水下光传输，有效去除光吸收和散射影响，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于水体中光随深度呈指数衰减，从卫星影像中恢复海底在空气中的真实颜色极具挑战性。

Method: 提出DichroGAN模型，采用两阶段同步训练：前两个生成器利用高光谱图像立方体分别估计漫反射和镜面反射以获得大气场景辐射亮度；后两个生成器分别处理包含各波段特征的场景辐射亮度和估计水下光传输，依据水下成像方程联合去除光吸收与散射效应。

Result: 在基于PRISMA卫星构建的小型RGB-光谱配对数据集以及水下数据集上的大量实验表明，DichroGAN在海底颜色恢复任务中性能优于当前最先进的水下复原方法。

Conclusion: DichroGAN能有效从卫星图像中恢复海底在空气中的真实颜色，为遥感海洋观测提供了一种新思路。

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

Abstract (中文翻译): 由于水体中光随深度呈指数衰减，从卫星图像中恢复海底在空气中的真实颜色是一项具有挑战性的任务。本研究提出了DichroGAN，一种为此目的设计的条件生成对抗网络（cGAN）。DichroGAN采用两阶段同步训练策略：首先，两个生成器利用高光谱图像立方体估计漫反射和镜面反射，从而获得大气场景辐射亮度；随后，第三个生成器接收包含各光谱波段特征的生成场景辐射亮度作为输入，而第四个生成器则估计水下光传输。这些生成器协同工作，依据水下图像形成方程，去除光吸收和散射的影响，从而恢复海底在空气中的真实颜色。DichroGAN在由PRISMA卫星图像构建的小型数据集上进行训练，该数据集包含RGB图像及其对应的光谱波段和掩膜。在卫星和水下数据集上的大量实验表明，DichroGAN相较于当前最先进的水下复原技术具有竞争力的表现。

</details>


### [10] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: MorphAny3D 是一个无需训练的 3D 形变框架，利用结构化潜在（SLAT）表示，通过新颖的 Morphing Cross-Attention 和 Temporal-Fused Self-Attention 实现高质量、语义一致且时间平滑的 3D 形变，支持跨类别形变、解耦形变和 3D 风格迁移。


<details>
  <summary>Details</summary>
Motivation: 3D 形变在生成语义一致且时间平滑的变形方面仍具挑战性，尤其是在跨类别情况下。

Method: 提出 MorphAny3D 框架，利用结构化潜在（SLAT）表示，在 3D 生成器的注意力机制中智能融合源和目标 SLAT 特征。具体包括 Morphing Cross-Attention (MCA) 用于结构一致性，Temporal-Fused Self-Attention (TFSA) 用于时间一致性，以及姿态校正策略缓解形变过程中的姿态模糊问题。

Result: 大量实验表明，该方法在包括跨类别等具有挑战性的案例中，均能生成最先进的形变序列，并支持解耦形变和 3D 风格迁移等高级应用。

Conclusion: MorphAny3D 是一种高效、无需训练的 3D 形变方法，能够生成高质量、语义一致且时间平滑的形变结果，适用于多种应用场景并可推广至其他基于 SLAT 的生成模型。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

Abstract (中文翻译): 由于难以生成语义一致且时间平滑的变形，尤其是跨类别的变形，3D 形变仍然具有挑战性。我们提出了 MorphAny3D，这是一种无需训练的框架，利用结构化潜在（SLAT）表示来实现高质量的 3D 形变。我们的关键见解是，在 3D 生成器的注意力机制中智能地混合源和目标 SLAT 特征，可以自然地产生合理的形变序列。为此，我们引入了 Morphing Cross-Attention（MCA），用于融合源和目标信息以保证结构一致性；以及 Temporal-Fused Self-Attention（TFSA），通过整合前序帧的特征来增强时间一致性。此外，一种姿态校正策略进一步缓解了形变步骤中的姿态模糊问题。大量实验表明，我们的方法即使在具有挑战性的跨类别情况下，也能生成最先进的形变序列。MorphAny3D 还支持解耦形变和 3D 风格迁移等高级应用，并可推广到其他基于 SLAT 的生成模型。项目页面：https://xiaokunsun.github.io/MorphAny3D.github.io/。

</details>
