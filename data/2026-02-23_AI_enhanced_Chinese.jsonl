{"id": "2602.17768", "pdf": "https://arxiv.org/pdf/2602.17768", "abs": "https://arxiv.org/abs/2602.17768", "authors": ["Boda Lin", "Yongjie Zhu", "Xiaocheng Gong", "Wenyu Qin", "Meng Wang"], "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding", "categories": ["cs.CV"], "comment": "26 pages", "summary": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd0\u52a8\u5b66\u8ba1\u7b97\u4e0e\u8bed\u8a00\u89e3\u6790\u7684\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86\u7ec6\u7c92\u5ea6\u4eba\u4f53\u52a8\u4f5c\u7406\u89e3\u6570\u636e\u96c6KPM-Bench\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u8a00\u7684MoPE\u7b97\u6cd5\u7528\u4e8e\u63d0\u53d6\u52a8\u4f5c\u5c5e\u6027\u3001\u8bc4\u4f30\u5e76\u7f13\u89e3\u89c6\u9891\u63cf\u8ff0\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7ec6\u8282\u63cf\u8ff0\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5bb9\u6613\u4ea7\u751f\u4e25\u91cd\u5e7b\u89c9\uff0c\u5c24\u5176\u5728\u4ee5\u52a8\u4f5c\u4e3a\u6838\u5fc3\u7684\u89c6\u9891\u4e2d\uff0c\u5bf9\u590d\u6742\u80a2\u4f53\u52a8\u6001\u7684\u7cbe\u786e\u523b\u753b\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u878d\u5408\u8fd0\u52a8\u5b66\u8ba1\u7b97\u4e0e\u8bed\u8a00\u89e3\u6790\uff0c\u6784\u5efa\u4e86KPM-Bench\u6570\u636e\u96c6\uff1b\u540c\u65f6\u63d0\u51faMoPE\u7b97\u6cd5\uff0c\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u63d0\u53d6\u52a8\u4f5c\u7279\u5b9a\u5c5e\u6027\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230GRPO\u540e\u8bad\u7ec3\u6846\u67b6\u4e2d\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "KPM-Bench\u5305\u542b\u7ec6\u7c92\u5ea6\u89c6\u9891-\u63cf\u8ff0\u5bf9\u3001\u52a8\u4f5c\u7406\u89e3\u95ee\u7b54\u5bf9\u548c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u52a8\u4f5c\u63cf\u8ff0\u5e7b\u89c9\u7684\u8bc4\u6d4b\u96c6\uff1bMoPE\u7b97\u6cd5\u53ef\u72ec\u7acb\u4e8e\u5927\u6a21\u578b\u8fdb\u884c\u7cbe\u51c6\u5e7b\u89c9\u8bc4\u4f30\uff0c\u5e76\u663e\u8457\u63d0\u5347\u52a8\u4f5c\u76f8\u5173\u89c6\u9891\u63cf\u8ff0\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165KPM-Bench\u548cMoPE\u65b9\u6cd5\uff0c\u672c\u6587\u6709\u6548\u7f13\u89e3\u4e86\u89c6\u9891\u63cf\u8ff0\u4e2d\u7684\u52a8\u4f5c\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7406\u89e3\u548c\u53ef\u9760\u89c6\u9891\u63cf\u8ff0\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u57fa\u51c6\u3002", "summary_cn": "\u5c3d\u7ba1\u8fd1\u671f\u53d6\u5f97\u4e86\u4e00\u4e9b\u8fdb\u5c55\uff0c\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u5728\u51c6\u786e\u63cf\u8ff0\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7ec6\u8282\u65b9\u9762\u4ecd\u9762\u4e34\u663e\u8457\u5c40\u9650\uff0c\u5e76\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u8fd9\u4e9b\u6311\u6218\u5728\u751f\u6210\u4ee5\u52a8\u4f5c\u4e3a\u6838\u5fc3\u7684\u89c6\u9891\u63cf\u8ff0\u65f6\u5c24\u4e3a\u7a81\u51fa\uff0c\u56e0\u4e3a\u6b64\u7c7b\u4efb\u52a1\u8981\u6c42\u7cbe\u786e\u523b\u753b\u590d\u6742\u7684\u52a8\u4f5c\u548c\u80a2\u4f53\u52a8\u6001\uff0c\u800c\u8fd9\u4e00\u70b9\u5f80\u5f80\u88ab\u5ffd\u89c6\u3002\u4e3a\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u878d\u5408\u4e86\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u52a8\u4f5c\u8ba1\u7b97\u4e0e\u8bed\u8a00\u89e3\u6790\uff0c\u80fd\u591f\u5bf9\u590d\u6742\u4eba\u4f53\u52a8\u4f5c\u8fdb\u884c\u7ec6\u81f4\u5206\u89e3\u4e0e\u63cf\u8ff0\u3002\u57fa\u4e8e\u6b64\u6d41\u7a0b\uff0c\u6211\u4eec\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u8fd0\u52a8\u5b66\u89e3\u6790\u52a8\u4f5c\u57fa\u51c6\uff08Kinematic Parsing Motion Benchmark, KPM-Bench\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u4fc3\u8fdb\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7406\u89e3\u7684\u65b0\u578b\u5f00\u6e90\u6570\u636e\u96c6\u3002KPM-Bench\u5305\u542b\uff1a(i) \u5168\u9762\u5c55\u73b0\u590d\u6742\u52a8\u4f5c\u4e2d\u80a2\u4f53\u5c42\u7ea7\u52a8\u6001\u7684\u7ec6\u7c92\u5ea6\u89c6\u9891-\u63cf\u8ff0\u5bf9\uff1b(ii) \u4e13\u6ce8\u4e8e\u52a8\u4f5c\u7406\u89e3\u7684\u591a\u6837\u5316\u4e14\u5177\u6311\u6218\u6027\u7684\u95ee\u7b54\u5bf9\uff1b\u4ee5\u53ca(iii) \u4e00\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u8bc4\u6d4b\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u4e0e\u52a8\u4f5c\u63cf\u8ff0\u76f8\u5173\u7684\u5e7b\u89c9\u73b0\u8c61\u3002\u6b64\u5916\uff0c\u4e3a\u7cfb\u7edf\u6027\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u8a00\u7684\u201c\u52a8\u4f5c\u89e3\u6790\u4e0e\u63d0\u53d6\u201d\uff08Motion Parsing and Extraction, MoPE\uff09\u7b97\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u51c6\u786e\u63d0\u53d6\u52a8\u4f5c\u7279\u5b9a\u5c5e\u6027\u3002\u5229\u7528MoPE\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7cbe\u786e\u7684\u5e7b\u89c9\u8bc4\u4f30\u6307\u6807\uff0c\u8be5\u6307\u6807\u65e0\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6216\u7eaf\u8bed\u8a00\u6a21\u578b\u5373\u53ef\u8fd0\u884c\u3002\u901a\u8fc7\u5c06MoPE\u96c6\u6210\u5230GRPO\u540e\u8bad\u7ec3\u6846\u67b6\u4e2d\uff0c\u6211\u4eec\u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee5\u52a8\u4f5c\u4e3a\u6838\u5fc3\u7684\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2602.17770", "pdf": "https://arxiv.org/pdf/2602.17770", "abs": "https://arxiv.org/abs/2602.17770", "authors": ["Balamurugan Thambiraja", "Omid Taheri", "Radek Danecek", "Giorgio Becherini", "Gerard Pons-Moll", "Justus Thies"], "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "categories": ["cs.CV", "cs.LG"], "comment": "ICLR2026; Project page: https://balamuruganthambiraja.github.io/CLUTCH/", "summary": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e863D-HIW\u6570\u636e\u96c6\u548cCLUTCH\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u91ce\u5916\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\uff0c\u652f\u6301\u6587\u672c\u5230\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u548c\u624b\u90e8\u52a8\u753b\u63cf\u8ff0\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6709\u9650\u4e14\u6602\u8d35\u7684\u5f71\u68da\u91c7\u96c6\u6570\u636e\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u771f\u5b9e\u4e16\u754c\uff08in-the-wild\uff09\u573a\u666f\uff0c\u4e14\u5728\u52a8\u4f5c\u4fdd\u771f\u5ea6\u4e0e\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "(1) \u6784\u5efa\u5305\u542b32K\u4e2a3D\u624b\u90e8\u52a8\u4f5c\u5e8f\u5217\u53ca\u5176\u5bf9\u5e94\u6587\u672c\u76843D-HIW\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e3D\u624b\u90e8\u8ffd\u8e2a\u5668\u7684\u6807\u6ce8\u6d41\u7a0b\uff1b(2) \u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684CLUTCH\u7cfb\u7edf\uff0c\u5305\u542b\uff1aa) \u65b0\u578bVQ-VAE\u67b6\u6784SHIFT\u7528\u4e8e\u624b\u90e8\u52a8\u4f5c\u5206\u8bcd\uff0cb) \u51e0\u4f55\u7cbe\u70bc\u9636\u6bb5\u4ee5\u63d0\u5347\u52a8\u753b\u8d28\u91cf\u3002", "result": "\u5728\u6587\u672c\u5230\u52a8\u4f5c\u548c\u52a8\u4f5c\u5230\u6587\u672c\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u91ce\u5916\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684LLM\u9a71\u52a8\u52a8\u753b\u7cfb\u7edf\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u91ce\u5916\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u3002", "summary_cn": "\u624b\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u626e\u6f14\u7740\u6838\u5fc3\u89d2\u8272\uff0c\u4f46\u5bf9\u81ea\u7136\u624b\u90e8\u52a8\u4f5c\u7684\u5efa\u6a21\u4ecd\u9c9c\u6709\u63a2\u7d22\u3002\u73b0\u6709\u5904\u7406\u6587\u672c\u5230\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u6216\u624b\u90e8\u52a8\u753b\u63cf\u8ff0\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u52a8\u4f5c\u548c\u4e0a\u4e0b\u6587\u6709\u9650\u7684\u5f71\u68da\u91c7\u96c6\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u4f4e\u6210\u672c\u5730\u6269\u5c55\u5230\u201c\u91ce\u5916\u201d\u573a\u666f\u3002\u6b64\u5916\uff0c\u5f53\u524d\u6a21\u578b\u53ca\u5176\u8bad\u7ec3\u65b9\u6848\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u52a8\u753b\u4fdd\u771f\u5ea6\u548c\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\uff081\uff09\u5f15\u5165\u4e86\u201c\u91ce\u59163D\u624b\u90e8\u201d\uff083D-HIW\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b32K\u4e2a3D\u624b\u90e8\u52a8\u4f5c\u5e8f\u5217\u53ca\u5176\u5bf9\u9f50\u6587\u672c\uff1b\uff082\uff09\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u624b\u90e8\u52a8\u753b\u7cfb\u7edfCLUTCH\uff0c\u5176\u5305\u542b\u4e24\u9879\u5173\u952e\u521b\u65b0\uff1a(a) \u4e00\u79cd\u540d\u4e3aSHIFT\u7684\u65b0\u578bVQ-VAE\u67b6\u6784\uff0c\u7528\u4e8e\u5bf9\u624b\u90e8\u52a8\u4f5c\u8fdb\u884c\u5206\u8bcd\uff1b(b) \u4e00\u4e2a\u51e0\u4f55\u7cbe\u70bc\u9636\u6bb5\uff0c\u7528\u4e8e\u5fae\u8c03LLM\u3002\u4e3a\u6784\u5efa3D-HIW\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u6807\u6ce8\u6d41\u7a0b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u6700\u5148\u8fdb\u76843D\u624b\u90e8\u8ffd\u8e2a\u5668\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u6db5\u76d6\u5e7f\u6cdb\u573a\u666f\u7684\u5927\u89c4\u6a21\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u52a8\u4f5c\u89c6\u9891\u8bed\u6599\u5e93\u3002\u4e3a\u5145\u5206\u6355\u6349\u91ce\u5916\u52a8\u4f5c\uff0cCLUTCH\u91c7\u7528\u4e86SHIFT\u2014\u2014\u4e00\u79cd\u90e8\u4ef6-\u6a21\u6001\u5206\u89e3\u7684VQ-VAE\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002\u6700\u540e\uff0c\u4e3a\u63d0\u9ad8\u52a8\u753b\u8d28\u91cf\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u51e0\u4f55\u7cbe\u70bc\u9636\u6bb5\uff0c\u5728\u8be5\u9636\u6bb5\u4e2d\uff0cCLUTCH\u901a\u8fc7\u76f4\u63a5\u4f5c\u7528\u4e8e\u89e3\u7801\u540e\u624b\u90e8\u52a8\u4f5c\u53c2\u6570\u7684\u91cd\u5efa\u635f\u5931\u8fdb\u884c\u534f\u540c\u76d1\u7763\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u5230\u52a8\u4f5c\u548c\u52a8\u4f5c\u5230\u6587\u672c\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u91ce\u5916\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u57fa\u51c6\u3002\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u5c06\u88ab\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2602.17785", "pdf": "https://arxiv.org/pdf/2602.17785", "abs": "https://arxiv.org/abs/2602.17785", "authors": ["Xinwei Ju", "Rema Daher", "Danail Stoyanov", "Sophia Bano", "Francisco Vasconcelos"], "title": "Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision", "categories": ["cs.CV"], "comment": "14 pages, 6 figures; early accepted by IPCAI2026", "summary": "Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPRISM\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fb9\u7f18\u68c0\u6d4b\u4e0e\u5149\u7167\u89e3\u8026\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5728\u7ed3\u80a0\u955c\u573a\u666f\u4e2d\u5b9e\u73b0\u5355\u76ee\u6df1\u5ea6\u4e0e\u4f4d\u59ff\u4f30\u8ba1\u7684SOTA\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u771f\u5b9e\u6570\u636e\u81ea\u76d1\u7763\u8bad\u7ec3\u548c\u89c6\u9891\u5e27\u7387\u5bf9\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u3002", "motivation": "\u7ed3\u80a0\u955c\u8f85\u52a9\u5bfc\u822a\u4e2d\u5355\u76ee\u6df1\u5ea6\u4e0e\u4f4d\u59ff\u4f30\u8ba1\u9762\u4e34\u65e0\u7eb9\u7406\u8868\u9762\u3001\u590d\u6742\u5149\u7167\u3001\u5f62\u53d8\u53ca\u7f3a\u4e4f\u53ef\u9760\u4f53\u5185\u771f\u503c\u6570\u636e\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u6709\u6548\u5229\u7528\u89e3\u5256\u7ed3\u6784\u4e0e\u5149\u7167\u5148\u9a8c\u63d0\u5347\u51e0\u4f55\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faPRISM\uff08Pose-Refinement with Intrinsic Shading and edge Maps\uff09\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u878d\u5408\u57fa\u4e8e\u5b66\u4e60\u7684\u8fb9\u7f18\u68c0\u6d4b\uff08\u5982DexiNed/HED\uff09\u83b7\u53d6\u9ad8\u9891\u8fb9\u754c\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u672c\u5f81\u5206\u89e3\u6a21\u5757\u5206\u79bb\u660e\u6697\uff08shading\uff09\u4e0e\u53cd\u5c04\uff08reflectance\uff09\uff0c\u5229\u7528\u660e\u6697\u7ebf\u7d22\u8f85\u52a9\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff1a(1) \u771f\u5b9e\u6570\u636e\u4e0a\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u4f18\u4e8e\u903c\u771f\u6a21\u578b\u6570\u636e\u7684\u76d1\u7763\u8bad\u7ec3\uff1b(2) \u89c6\u9891\u5e27\u7387\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u9700\u9488\u5bf9\u6570\u636e\u96c6\u5b9a\u5236\u91c7\u6837\u7b56\u7565\u3002", "conclusion": "PRISM\u6709\u6548\u5229\u7528\u89e3\u5256\u4e0e\u5149\u7167\u5148\u9a8c\u63d0\u5347\u7ed3\u80a0\u955c\u573a\u666f\u4e0b\u7684\u51e0\u4f55\u4f30\u8ba1\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u57df\u771f\u5b9e\u6027\u4e0e\u9ad8\u8d28\u91cf\u89c6\u9891\u91c7\u6837\u5728\u533b\u5b66\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "summary_cn": "\u5355\u76ee\u6df1\u5ea6\u4e0e\u4f4d\u59ff\u4f30\u8ba1\u5728\u7ed3\u80a0\u955c\u8f85\u52a9\u5bfc\u822a\u7684\u53d1\u5c55\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u56e0\u5176\u80fd\u591f\u901a\u8fc7\u51cf\u5c11\u76f2\u533a\u3001\u964d\u4f4e\u9057\u6f0f\u6216\u590d\u53d1\u6027\u75c5\u7076\u7684\u98ce\u9669\uff0c\u4ee5\u53ca\u51cf\u5c11\u68c0\u67e5\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\uff0c\u4ece\u800c\u6539\u5584\u7b5b\u67e5\u6548\u679c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b58\u5728\u65e0\u7eb9\u7406\u8868\u9762\u3001\u590d\u6742\u7684\u5149\u7167\u6a21\u5f0f\u3001\u7ec4\u7ec7\u5f62\u53d8\u4ee5\u53ca\u7f3a\u4e4f\u5e26\u6709\u53ef\u9760\u771f\u503c\u7684\u4f53\u5185\u6570\u636e\u96c6\uff0c\u8be5\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86PRISM\uff08Pose-Refinement with Intrinsic Shading and edge Maps\uff09\u2014\u2014\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u89e3\u5256\u7ed3\u6784\u548c\u5149\u7167\u5148\u9a8c\u6765\u5f15\u5bfc\u51e0\u4f55\u5b66\u4e60\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u72ec\u7279\u5730\u7ed3\u5408\u4e86\u8fb9\u7f18\u68c0\u6d4b\u4e0e\u4eae\u5ea6\u89e3\u8026\u4ee5\u63d0\u4f9b\u7ed3\u6784\u5f15\u5bfc\uff1a\u5177\u4f53\u800c\u8a00\uff0c\u4f7f\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u8fb9\u7f18\u68c0\u6d4b\u5668\uff08\u5982DexiNed\u6216HED\uff09\u63d0\u53d6\u7ec6\u8584\u4e14\u9ad8\u9891\u7684\u8fb9\u754c\u4ee5\u751f\u6210\u8fb9\u7f18\u56fe\uff1b\u540c\u65f6\u901a\u8fc7\u4e00\u4e2a\u672c\u5f81\u5206\u89e3\u6a21\u5757\u5c06\u660e\u6697\uff08shading\uff09\u4e0e\u53cd\u5c04\uff08reflectance\uff09\u5206\u79bb\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u660e\u6697\u7ebf\u7d22\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u3002\u5728\u591a\u4e2a\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u5bf9\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\uff0c\u4ee5\u786e\u7acb\u7ed3\u80a0\u955c\u4e0b\u4f4d\u59ff\u4e0e\u6df1\u5ea6\u4f30\u8ba1\u7684\u6700\u4f73\u5b9e\u8df5\u3002\u8be5\u5206\u6790\u5f97\u51fa\u4e24\u4e2a\u5b9e\u7528\u89c1\u89e3\uff1a(1) \u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u4f18\u4e8e\u5728\u903c\u771f\u6a21\u578b\u6570\u636e\u4e0a\u7684\u76d1\u7763\u8bad\u7ec3\uff0c\u51f8\u663e\u4e86\u57df\u771f\u5b9e\u6027\u76f8\u8f83\u4e8e\u771f\u503c\u53ef\u7528\u6027\u7684\u4f18\u52bf\uff1b(2) \u89c6\u9891\u5e27\u7387\u662f\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u6781\u5176\u5173\u952e\u56e0\u7d20\uff0c\u9700\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8fdb\u884c\u89c6\u9891\u5e27\u91c7\u6837\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2602.17793", "pdf": "https://arxiv.org/pdf/2602.17793", "abs": "https://arxiv.org/abs/2602.17793", "authors": ["Peide Zhu", "Linbin Lu", "Zhiqin Chen", "Xiong Chen"], "title": "LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLGD-Net\uff0c\u4e00\u79cd\u65e0\u9700\u751f\u6210\u865a\u62dfIHC\u56fe\u50cf\u3001\u76f4\u63a5\u4eceH&E\u5207\u7247\u9884\u6d4bHER2\u8868\u8fbe\u6c34\u5e73\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u548c\u9886\u57df\u77e5\u8bc6\u6b63\u5219\u5316\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6807\u51c6\u7684HER2\u514d\u75ab\u7ec4\u5316\uff08IHC\uff09\u67d3\u8272\u6d41\u7a0b\u8d44\u6e90\u6d88\u8017\u5927\u3001\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u4e14\u5728\u8bb8\u591a\u5730\u533a\u4e0d\u53ef\u7528\uff1b\u800c\u73b0\u6709\u57fa\u4e8eH&E\u5207\u7247\u9884\u6d4bHER2\u7684\u65b9\u6cd5\u4f9d\u8d56\u50cf\u7d20\u7ea7\u865a\u62df\u67d3\u8272\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u6613\u5f15\u5165\u4f2a\u5f71\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faLatent-Guided Dual-Stream Network\uff08LGD-Net\uff09\uff0c\u4e0d\u8fdb\u884c\u663e\u5f0f\u7684\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\uff0c\u800c\u662f\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u5c06H&E\u5f62\u6001\u7279\u5f81\u6620\u5c04\u5230\u5206\u5b50\u8868\u5f81\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u7531IHC\u6559\u5e08\u7f16\u7801\u5668\u5f15\u5bfc\uff1b\u540c\u65f6\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f85\u52a9\u4efb\u52a1\uff08\u5982\u7ec6\u80de\u6838\u5206\u5e03\u548c\u819c\u67d3\u8272\u5f3a\u5ea6\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u9886\u57df\u77e5\u8bc6\u6b63\u5219\u5316\u3002", "result": "\u5728\u516c\u5f00BCI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLGD-Net\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u4ec5\u9700\u5355\u6a21\u6001H&E\u8f93\u5165\u5373\u53ef\u9ad8\u6548\u63a8\u7406\u3002", "conclusion": "LGD-Net\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u65e0\u9700\u865a\u62df\u67d3\u8272\u7684HER2\u8bc4\u5206\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "summary_cn": "\u51c6\u786e\u8bc4\u4f30HER2\u8868\u8fbe\u6c34\u5e73\u5bf9\u4e8e\u4e73\u817a\u764c\u7684\u8bc4\u4f30\u548c\u9776\u5411\u6cbb\u7597\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6807\u51c6\u7684\u591a\u6b65\u9aa4\u514d\u75ab\u7ec4\u5316\uff08IHC\uff09\u67d3\u8272\u8fc7\u7a0b\u8d44\u6e90\u5bc6\u96c6\u3001\u6602\u8d35\u4e14\u8017\u65f6\uff0c\u5728\u8bb8\u591a\u5730\u533a\u5e38\u5e38\u65e0\u6cd5\u83b7\u5f97\u3002\u56e0\u6b64\uff0c\u76f4\u63a5\u4eceH&E\u5207\u7247\u9884\u6d4bHER2\u6c34\u5e73\u5df2\u6210\u4e3a\u4e00\u79cd\u6f5c\u5728\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5df2\u6709\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u4eceH&E\u56fe\u50cf\u751f\u6210\u7684\u865a\u62dfIHC\u56fe\u50cf\u8fdb\u884c\u81ea\u52a8HER2\u8bc4\u5206\u662f\u6709\u6548\u7684\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u50cf\u7d20\u7ea7\u7684\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5bb9\u6613\u4ea7\u751f\u91cd\u5efa\u4f2a\u5f71\uff0c\u4ece\u800c\u5bfc\u81f4\u8bca\u65ad\u9519\u8bef\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Latent-Guided Dual-Stream Network\uff08LGD-Net\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u663e\u5f0f\u7684\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\u3002LGD-Net\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u501f\u52a9\u6559\u5e08IHC\u7f16\u7801\u5668\u7684\u5f15\u5bfc\uff0c\u5b66\u4e60\u5c06H&E\u7684\u5f62\u6001\u5b66\u7279\u5f81\u76f4\u63a5\u6620\u5c04\u5230\u5206\u5b50\u8868\u5f81\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u4e3a\u786e\u4fdd\u5e7b\u89c9\u7279\u5f81\u80fd\u591f\u6355\u6349\u4e34\u5e8a\u4e0a\u76f8\u5173\u7684\u8868\u578b\uff0c\u6211\u4eec\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u8f85\u52a9\u6b63\u5219\u5316\u4efb\u52a1\uff08\u7279\u522b\u662f\u7ec6\u80de\u6838\u5206\u5e03\u548c\u819c\u67d3\u8272\u5f3a\u5ea6\uff09\u663e\u5f0f\u5730\u5c06\u4efb\u52a1\u7279\u5b9a\u7684\u9886\u57df\u77e5\u8bc6\u878d\u5165\u6a21\u578b\u8bad\u7ec3\u3002\u5728\u516c\u5f00BCI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLGD-Net\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u5355\u6a21\u6001H&E\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2602.17799", "pdf": "https://arxiv.org/pdf/2602.17799", "abs": "https://arxiv.org/abs/2602.17799", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u9065\u611f\u56fe\u50cf\u6587\u672c\u5f15\u5bfc\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5f0f\u548c\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0eSAM\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u3001\u6307\u4ee3\u8868\u8fbe\u548c\u63a8\u7406\u578b\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u56fe\u50cf\u6587\u672c\u5f15\u5bfc\u5206\u5272\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u53ef\u8bad\u7ec3\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\uff1b\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ec5\u4f7f\u7528\u73b0\u6709\u57fa\u7840\u6a21\u578b\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8be5\u4efb\u52a1\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1a1\uff09\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u5229\u7528CLIP\u4f5c\u4e3a\u63a9\u7801\u9009\u62e9\u5668\uff0c\u4eceSAM\u7684\u7f51\u683c\u63d0\u6848\u4e2d\u9009\u51fa\u5339\u914d\u6587\u672c\u63cf\u8ff0\u7684\u533a\u57df\uff1b2\uff09\u751f\u6210\u5f0f\u65b9\u6cd5\u901a\u8fc7GPT-5\u6216LoRA\u5fae\u8c03\u7684Qwen-VL\u751f\u6210\u70b9\u51fb\u63d0\u793a\u8f93\u5165SAM\uff0c\u5b9e\u73b0\u6307\u4ee3\u4e0e\u63a8\u7406\u5206\u5272\u3002\u6574\u4f53\u6846\u67b6\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\u6216\u4ec5\u9700\u8f7b\u91cfLoRA\u5fae\u8c03\u3002", "result": "\u572819\u4e2a\u9065\u611f\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff08\u6db5\u76d6\u5f00\u653e\u8bcd\u6c47\u3001\u6307\u4ee3\u548c\u63a8\u7406\u4efb\u52a1\uff09\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u5176\u4e2d\u5bf9\u6bd4\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e0a\u8fbe\u5230SOTA\uff0c\u751f\u6210\u5f0f\u65b9\u6cd5\u4e2dLoRA\u5fae\u8c03Qwen-VL\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u4ec5\u4f9d\u9760\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5373\u53ef\u9ad8\u6548\u5b9e\u73b0\u9065\u611f\u56fe\u50cf\u7684\u96f6\u6837\u672c\u6587\u672c\u5f15\u5bfc\u5206\u5272\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u901a\u7528\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "summary_cn": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u9065\u611f\u5f71\u50cf\u7684\u96f6\u6837\u672c\u6587\u672c\u5f15\u5bfc\u5206\u5272\u5f00\u8f9f\u4e86\u65b0\u673a\u9047\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u989d\u5916\u7684\u53ef\u8bad\u7ec3\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u9002\u7528\u6027\u3002\u672c\u6587\u7814\u7a76\u4e86\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4f9d\u9760\u73b0\u6709\u57fa\u7840\u6a21\u578b\u80fd\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u5b9e\u73b0\u57fa\u4e8e\u6587\u672c\u7684\u9065\u611f\u5206\u5272\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c06\u5bf9\u6bd4\u5f0f\u548c\u751f\u6210\u5f0fVLM\u4e0eSegment Anything Model\uff08SAM\uff09\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\u6216\u4ec5\u9700\u8f7b\u91cfLoRA\u5fae\u8c03\u7684\u6d41\u7a0b\u3002\u6211\u4eec\u7684\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u5229\u7528CLIP\u4f5c\u4e3a\u63a9\u7801\u9009\u62e9\u5668\uff0c\u4eceSAM\u7684\u7f51\u683c\u63d0\u6848\u4e2d\u9009\u51fa\u4e0e\u6587\u672c\u63cf\u8ff0\u6700\u5339\u914d\u7684\u533a\u57df\uff0c\u5728\u5b8c\u5168\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff08OVSS\uff09\u3002\u540c\u65f6\uff0c\u6211\u4eec\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\u901a\u8fc7GPT-5\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u751f\u6210\u70b9\u51fb\u63d0\u793a\uff0c\u6216\u4f7f\u7528LoRA\u5fae\u8c03\u7684Qwen-VL\u6a21\u578b\u751f\u6210\u63d0\u793a\u6765\u9a71\u52a8SAM\uff0c\u5b9e\u73b0\u6307\u4ee3\u8868\u8fbe\u4e0e\u63a8\u7406\u578b\u5206\u5272\uff0c\u5176\u4e2d\u540e\u8005\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\u3002\u5728\u5305\u62ec\u5f00\u653e\u8bcd\u6c47\u3001\u6307\u4ee3\u548c\u63a8\u7406\u4efb\u52a1\u5728\u5185\u768419\u4e2a\u9065\u611f\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u5f3a\u5927\u80fd\u529b\u3002\u4ee3\u7801\u5c06\u53d1\u5e03\u4e8e https://github.com/josesosajs/trainfree-rs-segmentation\u3002"}}
{"id": "2602.17807", "pdf": "https://arxiv.org/pdf/2602.17807", "abs": "https://arxiv.org/abs/2602.17807", "authors": ["Narges Norouzi", "Idil Esen Zulfikar", "Niccol`o Cavagnero", "Tommie Kerssies", "Bastian Leibe", "Gijs Dubbelman", "Daan de Geus"], "title": "VidEoMT: Your ViT is Secretly Also a Video Segmentation Model", "categories": ["cs.CV"], "comment": null, "summary": "Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/", "AI": {"tldr": "VidEoMT \u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u7f16\u7801\u5668\u7684\u89c6\u9891\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67e5\u8be2\u4f20\u64ad\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u65f6\u5e8f\u5efa\u6a21\uff0c\u65e0\u9700\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\uff0c\u901f\u5ea6\u63d0\u53475\u201310\u500d\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u89c6\u9891\u5206\u5272\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\uff0c\u5e26\u6765\u67b6\u6784\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\uff1b\u53d7\u8fd1\u671f\u7eaf ViT \u5728\u56fe\u50cf\u5206\u5272\u4e2d\u6210\u529f\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u4e2a\u65e0\u9700\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\u7684\u7b80\u5355\u9ad8\u6548\u89c6\u9891\u5206\u5272\u6a21\u578b\u3002", "method": "\u63d0\u51fa VidEoMT \u6a21\u578b\uff0c\u5728\u4ec5\u7f16\u7801\u5668\u7684 ViT \u67b6\u6784\u4e2d\u5f15\u5165\u8f7b\u91cf\u7ea7\u67e5\u8be2\u4f20\u64ad\u673a\u5236\uff0c\u5c06\u524d\u4e00\u5e27\u7684\u67e5\u8be2\u590d\u7528\u4e8e\u5f53\u524d\u5e27\uff0c\u5e76\u7ed3\u5408\u4e0e\u65f6\u95f4\u65e0\u5173\u7684\u5b66\u4e60\u67e5\u8be2\u8fdb\u884c\u878d\u5408\uff0c\u4ee5\u517c\u987e\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u9002\u5e94\u6027\u3002", "result": "VidEoMT \u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe 160 FPS\uff08ViT-L \u9aa8\u5e72\uff09\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb 5\u201310 \u500d\u3002", "conclusion": "\u901a\u8fc7\u5728\u7eaf\u7f16\u7801\u5668 ViT \u4e2d\u8bbe\u8ba1\u7b80\u6d01\u6709\u6548\u7684\u67e5\u8be2\u4f20\u64ad\u4e0e\u878d\u5408\u673a\u5236\uff0cVidEoMT \u6210\u529f\u6d88\u9664\u4e86\u5bf9\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\u7684\u4f9d\u8d56\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "summary_cn": "\u73b0\u6709\u7684\u5728\u7ebf\u89c6\u9891\u5206\u5272\u6a21\u578b\u901a\u5e38\u5c06\u9010\u5e27\u5206\u5272\u5668\u4e0e\u590d\u6742\u7684\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\u76f8\u7ed3\u5408\u3002\u5c3d\u7ba1\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u6a21\u5757\u5f15\u5165\u4e86\u663e\u8457\u7684\u67b6\u6784\u590d\u6742\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u3002\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u5f53\u5177\u5907\u8db3\u591f\u5bb9\u91cf\u5e76\u7ecf\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u65f6\uff0c\u666e\u901a\u7684\u89c6\u89c9 Transformer\uff08ViT\uff09\u7f16\u7801\u5668\u65e0\u9700\u4e13\u7528\u6a21\u5757\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u7684\u56fe\u50cf\u5206\u5272\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Video Encoder-only Mask Transformer\uff08VidEoMT\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u7684\u4ec5\u7f16\u7801\u5668\u89c6\u9891\u5206\u5272\u6a21\u578b\uff0c\u65e0\u9700\u4e13\u7528\u8ddf\u8e2a\u6a21\u5757\u3002\u4e3a\u5728\u4ec5\u7f16\u7801\u5668\u7684 ViT \u4e2d\u5b9e\u73b0\u65f6\u5e8f\u5efa\u6a21\uff0cVidEoMT \u5f15\u5165\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u67e5\u8be2\u4f20\u64ad\u673a\u5236\uff0c\u901a\u8fc7\u590d\u7528\u524d\u4e00\u5e27\u7684\u67e5\u8be2\u6765\u8de8\u5e27\u4f20\u9012\u4fe1\u606f\u3002\u4e3a\u4e86\u5728\u4fdd\u6301\u65f6\u5e8f\u4e00\u81f4\u6027\u7684\u540c\u65f6\u9002\u5e94\u65b0\u5185\u5bb9\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u79cd\u67e5\u8be2\u878d\u5408\u7b56\u7565\uff0c\u5c06\u4f20\u64ad\u7684\u67e5\u8be2\u4e0e\u4e00\u7ec4\u4e0e\u65f6\u5e8f\u65e0\u5173\u7684\u53ef\u5b66\u4e60\u67e5\u8be2\u76f8\u7ed3\u5408\u3002\u56e0\u6b64\uff0cVidEoMT \u5728\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e86\u8ddf\u8e2a\u5668\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u901f\u5ea6\u63d0\u53475\u81f310\u500d\uff0c\u4f7f\u7528 ViT-L \u9aa8\u5e72\u7f51\u7edc\u65f6\u6700\u9ad8\u53ef\u8fbe\u6bcf\u79d2160\u5e27\u3002\u4ee3\u7801\uff1ahttps://www.tue-mps.org/videomt/"}}
{"id": "2602.17814", "pdf": "https://arxiv.org/pdf/2602.17814", "abs": "https://arxiv.org/abs/2602.17814", "authors": ["Adrian Catalin Lutu", "Eduard Poesina", "Radu Tudor Ionescu"], "title": "VQPP: Video Query Performance Prediction Benchmark", "categories": ["cs.CV", "cs.IR", "cs.LG"], "comment": null, "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u5185\u5bb9\u89c6\u9891\u68c0\u7d22\uff08CBVR\uff09\u7684\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08QPP\uff09\u57fa\u51c6VQPP\uff0c\u5305\u542b\u4e24\u4e2a\u6587\u672c-\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u5927\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\u6539\u5199\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08QPP\uff09\u5728\u6587\u672c\u548c\u56fe\u50cf\u68c0\u7d22\u4e2d\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u57fa\u4e8e\u5185\u5bb9\u7684\u89c6\u9891\u68c0\u7d22\uff08CBVR\uff09\u9886\u57df\u4ecd\u5c5e\u7a7a\u767d\u3002\u4e3a\u63a8\u52a8\u8be5\u65b9\u5411\u7684\u53d1\u5c55\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u89c6\u9891\u68c0\u7d22\u7684QPP\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u540d\u4e3aVQPP\u7684\u57fa\u51c6\uff0c\u5305\u542b56K\u6587\u672c\u67e5\u8be2\u548c51K\u89c6\u9891\uff0c\u6db5\u76d6\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u5b98\u65b9\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u5212\u5206\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u591a\u79cd\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u5e76\u5c06\u6700\u4f73\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u67e5\u8be2\u6539\u5199\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u80fd\u591f\u53d6\u5f97\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u53ef\u5728\u5b9e\u9645\u68c0\u7d22\u524d\u90e8\u7f72\u3002\u6b64\u5916\uff0c\u5c06\u8be5\u9884\u6d4b\u5668\u7528\u4e8eLLM\u7684\u67e5\u8be2\u6539\u5199\u4efb\u52a1\u4e5f\u9a8c\u8bc1\u4e86VQPP\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "VQPP\u662f\u9996\u4e2a\u9762\u5411\u89c6\u9891\u68c0\u7d22\u7684QPP\u57fa\u51c6\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5e73\u53f0\uff0c\u5e76\u5c55\u793a\u4e86QPP\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u67e5\u8be2\u6539\u5199\uff09\u4e2d\u7684\u6f5c\u529b\u3002", "summary_cn": "\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08QPP\uff09\u662f\u4e00\u9879\u91cd\u8981\u4e14\u88ab\u5e7f\u6cdb\u7814\u7a76\u7684\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\uff0c\u5177\u6709\u591a\u79cd\u5e94\u7528\u573a\u666f\uff0c\u4f8b\u5982\u67e5\u8be2\u6539\u5199\u3001\u67e5\u8be2\u6269\u5c55\u548c\u68c0\u7d22\u7cfb\u7edf\u9009\u62e9\u7b49\u3002\u8be5\u4efb\u52a1\u4e3b\u8981\u5728\u6587\u672c\u548c\u56fe\u50cf\u68c0\u7d22\u7684\u80cc\u666f\u4e0b\u8fdb\u884c\u7814\u7a76\uff0c\u800c\u9762\u5411\u57fa\u4e8e\u5185\u5bb9\u7684\u89c6\u9891\u68c0\u7d22\uff08CBVR\uff09\u7684QPP\u5219\u57fa\u672c\u672a\u88ab\u63a2\u7d22\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08VQPP\uff09\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\u3002VQPP\u603b\u5171\u5305\u542b5.6\u4e07\u4e2a\u6587\u672c\u67e5\u8be2\u548c5.1\u4e07\u4e2a\u89c6\u9891\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b98\u65b9\u7684\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u5212\u5206\uff0c\u4ee5\u4fc3\u8fdb\u76f4\u63a5\u6bd4\u8f83\u548c\u53ef\u590d\u73b0\u7684\u7ed3\u679c\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u591a\u79cd\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u4e3a\u672a\u6765\u5728\u89c6\u9891\u9886\u57df\u63a2\u7d22QPP\u5efa\u7acb\u4e86\u4e00\u4e2a\u5177\u6709\u4ee3\u8868\u6027\u7684\u57fa\u51c6\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u80fd\u591f\u83b7\u5f97\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4ece\u800c\u5728\u6267\u884c\u68c0\u7d22\u6b65\u9aa4\u4e4b\u524d\u5b9e\u73b0\u76f8\u5173\u5e94\u7528\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u5c06\u8868\u73b0\u6700\u4f73\u7684\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u5229\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u65b9\u6cd5\u5728\u67e5\u8be2\u6539\u5199\u4efb\u52a1\u4e0a\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5c55\u793a\u4e86VQPP\u7684\u9002\u7528\u6027\u3002\u6211\u4eec\u5728https://github.com/AdrianLutu/VQPP\u53d1\u5e03\u4e86\u8be5\u57fa\u51c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2602.17854", "pdf": "https://arxiv.org/pdf/2602.17854", "abs": "https://arxiv.org/abs/2602.17854", "authors": ["Domonkos Varga"], "title": "On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szir\u00e1nyi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51faLiu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u59ff\u6001\u8bc6\u522b\u65b9\u6cd5\u56e0\u91c7\u7528\u5e27\u7ea7\u968f\u673a\u5212\u5206\u8bad\u7ec3/\u6d4b\u8bd5\u96c6\uff0c\u5bfc\u81f4\u6570\u636e\u6cc4\u9732\uff0c\u5176\u62a5\u544a\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5bf9\u672a\u89c1\u4e2a\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30Liu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5176\u8bc4\u4f30\u534f\u8bae\u662f\u5426\u5408\u7406\uff0c\u80fd\u5426\u771f\u6b63\u8861\u91cf\u6a21\u578b\u5728\u9762\u5bf9\u65b0\u7528\u6237\u65f6\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5176\u516c\u5f00\u7684\u6df7\u6dc6\u77e9\u9635\u3001\u5b66\u4e60\u66f2\u7ebf\u548c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u5f0f\uff0c\u63ed\u793a\u5176\u5e27\u7ea7\u968f\u673a\u5212\u5206\u5bfc\u81f4\u540c\u4e00\u53d7\u8bd5\u8005\u7684\u6570\u636e\u540c\u65f6\u51fa\u73b0\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e2d\uff0c\u4ece\u800c\u9020\u6210\u6570\u636e\u6cc4\u9732\u3002", "result": "\u8bc1\u660e\u8be5\u65b9\u6cd5\u7684\u9ad8\u51c6\u786e\u7387\u6e90\u4e8e\u6570\u636e\u6cc4\u9732\uff0c\u5176\u8bc4\u4f30\u7ed3\u679c\u4e0d\u80fd\u53cd\u6620\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u4e2a\u4f53\u7684\u624b\u52bf\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u5f3a\u8c03\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u4e2d\uff0c\u5c24\u5176\u662f\u9762\u5411\u65e0\u4eba\u673a-\u4eba\u7c7b\u4ea4\u4e92\u7b49\u9700\u8bc6\u522b\u65b0\u7528\u6237\u624b\u52bf\u7684\u5e94\u7528\u573a\u666f\uff0c\u5fc5\u987b\u91c7\u7528\u53d7\u8bd5\u8005\u72ec\u7acb\u7684\u6570\u636e\u5212\u5206\u65b9\u5f0f\u3002", "summary_cn": "\u672c\u6587\u5bf9Liu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u8fdb\u884c\u4e86\u65b9\u6cd5\u8bba\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce8\u5176\u8bc4\u4f30\u534f\u8bae\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u6307\u51fa\uff0c\u5176\u6240\u62a5\u544a\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u6e90\u4e8e\u5e27\u7ea7\u522b\u7684\u968f\u673a\u8bad\u7ec3-\u6d4b\u8bd5\u5212\u5206\uff0c\u8fd9\u79cd\u5212\u5206\u4e0d\u53ef\u907f\u514d\u5730\u5c06\u540c\u4e00\u53d7\u8bd5\u8005\u7684\u6570\u636e\u6837\u672c\u540c\u65f6\u6df7\u5165\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u4ece\u800c\u5bfc\u81f4\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u9732\u3002\u901a\u8fc7\u68c0\u67e5\u5176\u516c\u5f00\u7684\u6df7\u6dc6\u77e9\u9635\u3001\u5b66\u4e60\u66f2\u7ebf\u548c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u5f0f\uff0c\u6211\u4eec\u8bc1\u660e\u8be5\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u8861\u91cf\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u4e2a\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u7684\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u4e2d\u91c7\u7528\u53d7\u8bd5\u8005\u72ec\u7acb\u6570\u636e\u5212\u5206\u7684\u91cd\u8981\u6027\uff0c\u5c24\u5176\u662f\u5728\u65e0\u4eba\u673a-\u4eba\u7c7b\u4ea4\u4e92\u7b49\u9700\u8981\u53ef\u9760\u8bc6\u522b\u5148\u524d\u672a\u89c1\u8fc7\u4eba\u5458\u6240\u505a\u624b\u52bf\u7684\u5e94\u7528\u573a\u666f\u4e2d\u3002"}}
{"id": "2602.17869", "pdf": "https://arxiv.org/pdf/2602.17869", "abs": "https://arxiv.org/abs/2602.17869", "authors": ["Yuxiao Chen", "Jue Wang", "Zhikang Zhang", "Jingru Yi", "Xu Zhang", "Yang Zou", "Zhaowei Cai", "Jianbo Yuan", "Xinyu Li", "Hao Yang", "Davide Modolo"], "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models", "categories": ["cs.CV"], "comment": null, "summary": "With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668\uff08AVS\uff09\u548c\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u65f6\u7a7a\u538b\u7f29\u5668\uff08SVC\uff09\uff0c\u6709\u6548\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5e76\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u957f\u89c6\u9891\u5177\u6709\u9ad8\u5ea6\u5197\u4f59\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u6709\u9650\u5185\u5b58\u4e0b\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u5927\u91cf\u5e27\u5e76\u4ece\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u4fe1\u606f\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u80fd\u517c\u987e\u6548\u7387\u4e0e\u6027\u80fd\u7684\u957f\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u5305\u542b\u57fa\u4e8e\u4fe1\u606f\u5bc6\u5ea6\u7684\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668\uff08AVS\uff09\u548c\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668\uff08SVC\uff09\uff0c\u5e76\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u96c6\u6210\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u548c\u6807\u51c6\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u957f\u89c6\u9891\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5173\u952e\u5224\u522b\u4fe1\u606f\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7387\u5904\u7406\uff0c\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002", "summary_cn": "\u968f\u7740\u89c6\u9891\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u7684\u6700\u65b0\u8fdb\u5c55\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u663e\u8457\u6210\u5c31\uff0c\u5bf9\u65f6\u957f\u53ef\u8fbe\u6570\u5341\u5206\u949f\u7684\u957f\u89c6\u9891\u8fdb\u884c\u5206\u6790\u5df2\u53d8\u5f97\u53ef\u884c\u4e14\u65e5\u76ca\u666e\u904d\u3002\u7136\u800c\uff0c\u89c6\u9891\u5e8f\u5217\u56fa\u6709\u7684\u5197\u4f59\u6027\u7ed9\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u4e24\u4e2a\u65b9\u9762\uff1a1\uff09\u5728\u5185\u5b58\u9650\u5236\u4e0b\u9ad8\u6548\u5730\u7eb3\u5165\u66f4\u591a\u5e27\uff1b2\uff09\u4ece\u6d77\u91cf\u8f93\u5165\u6570\u636e\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u4fe1\u606f\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u65b0\u578b\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u5bc6\u5ea6\u7684\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668\uff08AVS\uff09\u548c\u4e00\u4e2a\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u96c6\u6210\u7684\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668\uff08SVC\uff09\u3002\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5177\u6709\u4e24\u5927\u4f18\u52bf\uff1a\u80fd\u591f\u81ea\u9002\u5e94\u4e14\u9ad8\u6548\u5730\u4ece\u4e0d\u540c\u957f\u5ea6\u7684\u89c6\u9891\u5e8f\u5217\u4e2d\u6355\u6349\u5173\u952e\u4fe1\u606f\uff0c\u5e76\u5728\u4fdd\u6301\u91cd\u8981\u5224\u522b\u4fe1\u606f\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u3002\u8be5\u6846\u67b6\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u548c\u6807\u51c6\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e0a\u5747\u8868\u73b0\u7a81\u51fa\uff0c\u51f8\u663e\u4e86\u5176\u5728\u5904\u7406\u957f\u65f6\u95f4\u89c6\u9891\u5e8f\u5217\u590d\u6742\u6027\u65b9\u9762\u7684\u901a\u7528\u6027\u4e0e\u6709\u6548\u6027\u3002"}}
{"id": "2602.17871", "pdf": "https://arxiv.org/pdf/2602.17871", "abs": "https://arxiv.org/abs/2602.17871", "authors": ["Dhruba Ghosh", "Yuhui Zhang", "Ludwig Schmidt"], "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.", "AI": {"tldr": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u79cd\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u4ecd\u663e\u4e0d\u8db3\uff1b\u7814\u7a76\u53d1\u73b0\u66f4\u5f3a\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u63d0\u5347\u7ec6\u7c92\u5ea6\u6027\u80fd\u5c24\u4e3a\u5173\u952e\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u3001\u6587\u6863\u7406\u89e3\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u4f20\u7edf\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u65e8\u5728\u63a2\u7a76\u9020\u6210\u8fd9\u79cd\u6027\u80fd\u5dee\u5f02\u7684\u539f\u56e0\u3002", "method": "\u4f5c\u8005\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u5206\u7c7b\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u8fd1\u671f\u7684VLMs\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u7ec4\u4ef6\uff08\u5982\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u7f16\u7801\u5668\u3001\u9884\u8bad\u7ec3\u7b56\u7565\uff09\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u66f4\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u6240\u6709\u57fa\u51c6\u5e26\u6765\u5747\u8861\u63d0\u5347\uff0c\u800c\u66f4\u5f3a\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5219\u663e\u8457\u66f4\u6709\u5229\u4e8e\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff1b\u6b64\u5916\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u89e3\u51bb\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u63d0\u5347VLMs\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u9700\u91cd\u70b9\u5173\u6ce8\u89c6\u89c9\u7f16\u7801\u5668\u8d28\u91cf\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u8fd9\u4e3a\u672a\u6765\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u4e2d\u5fc3\u578b\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "summary_cn": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5404\u7c7b\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u6db5\u76d6\u89c6\u89c9\u63a8\u7406\u3001\u6587\u6863\u7406\u89e3\u548c\u591a\u6a21\u6001\u5bf9\u8bdd\u7b49\u9886\u57df\u3002\u8fd9\u4e9b\u8fdb\u6b65\u4f53\u73b0\u5728\u57fa\u4e8e\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u3001\u5bf9\u9f50\u67b6\u6784\u548c\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u7684\u591a\u79cdVLMs\u4e2d\u3002\u7136\u800c\uff0c\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u6d4b\u8bd5\u7ec6\u7c92\u5ea6\u89c6\u89c9\u77e5\u8bc6\u7684\u4f20\u7edf\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u8868\u73b0\u6ede\u540e\u3002\u6211\u4eec\u5bf9\u5927\u91cf\u8fd1\u671fVLMs\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u8bc6\u522b\u51fa\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u4e0e\u5176\u4ed6\u89c6\u89c9\u57fa\u51c6\u4e4b\u95f4\u6027\u80fd\u8131\u8282\u7684\u6f5c\u5728\u56e0\u7d20\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u6d88\u878d\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u4f7f\u7528\u66f4\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\u80fd\u540c\u7b49\u63d0\u5347\u6240\u6709\u57fa\u51c6\u7684\u5f97\u5206\uff0c\u800c\u66f4\u5f3a\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5219\u5bf9\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u5e26\u6765\u4e0d\u6210\u6bd4\u4f8b\u7684\u66f4\u5927\u63d0\u5347\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u73b0\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u540c\u6837\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u89e3\u51bb\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u65f6\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u589e\u5f3aVLMs\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u548c\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u80fd\u529b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
