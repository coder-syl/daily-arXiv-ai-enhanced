<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Future Optical Flow Prediction Improves Robot Control & Video Generation](https://arxiv.org/abs/2601.10781)
*Kanchana Ranasinghe,Honglu Zhou,Yu Fang,Luyu Yang,Le Xue,Ran Xu,Caiming Xiong,Silvio Savarese,Michael S Ryoo,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: FOFPred 是一个结合视觉语言模型（VLM）与扩散架构的语言条件光流预测模型，利用大规模网络人类活动视频数据进行训练，展现出在机器人控制和视频生成等下游任务中的跨领域通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从噪声大、非结构化的现实世界数据中学习可泛化的密集空间运动表征（如光流）方面仍面临挑战，亟需一种能有效利用大规模多模态数据进行未来运动预测的模型。

Method: 提出 FOFPred 模型，融合统一的视觉语言模型（VLM）与扩散架构，通过关键的数据预处理技术和强大的图像预训练，从网络规模的人类活动视频-文本对中学习语言引导的光流预测。

Result: 在语言驱动的机器人操作和视频生成任务中，FOFPred 表现出优异的跨域性能，验证了其架构设计和利用多样化网络数据进行可扩展学习的有效性。

Conclusion: 统一的 VLM-扩散架构结合大规模网络数据，为未来光流预测提供了强大且通用的解决方案，在控制与生成任务中均具有应用潜力。

Abstract: Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.

Abstract (中文翻译): 未来运动表征（如光流）在控制和生成任务中具有巨大价值。然而，预测可泛化的空间密集运动表征仍是一个关键挑战，且从噪声大的真实世界数据中学习此类预测方法的研究相对较少。我们提出了 FOFPred，这是一种新颖的语言条件光流预测模型，采用统一的视觉语言模型（VLM）与扩散架构。这种独特组合实现了强大的多模态推理能力，并具备像素级的生成保真度，用于未来运动预测。我们的模型在大规模网络人类活动数据（一种高度可扩展但非结构化的数据源）上进行训练。为从这些含噪声的视频-字幕数据中提取有意义的信号，我们采用了关键的数据预处理技术，并结合具有强大图像预训练能力的统一架构。所训练的模型进一步被拓展用于解决控制和生成领域的两项不同下游任务。在语言驱动设置下的机器人操作和视频生成任务中的评估结果，验证了 FOFPred 的跨领域通用性，也证实了统一的 VLM-扩散架构以及从多样化网络数据中进行可扩展学习对未来光流预测的价值。

</details>


### [2] [ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research](https://arxiv.org/abs/2601.10802)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 本文提出了一个名为ICONIC-444的大规模工业图像数据集，包含310万张图像和444个类别，旨在推动分布外（OOD）检测研究，并提供了22种先进OOD方法的基准结果。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测研究受限于缺乏大规模、高质量且具有明确定义OOD类别（涵盖从近到远OOD难度）的数据集，难以同时支持细粒度和粗粒度的计算机视觉任务。

Method: 作者构建了ICONIC-444数据集，该数据集由原型工业分拣机采集，包含超过310万张RGB图像，覆盖444个类别，并设计了四个参考任务用于OOD检测评估。

Result: 在ICONIC-444上对22种最先进的后处理OOD检测方法进行了基准测试，提供了基线结果，验证了该数据集在不同任务复杂度下支持严谨OOD评估的能力。

Conclusion: ICONIC-444填补了现有OOD检测数据集的空白，为未来研究提供了结构化、多样化且贴近真实工业场景的基准平台。

Abstract: Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.

Abstract (中文翻译): 当前分布外（OOD）检测研究的进展受限于缺乏大规模、高质量的数据集，这些数据集需具备明确定义的OOD类别（涵盖从近到远OOD的不同难度级别），并能同时支持细粒度和粗粒度的计算机视觉任务。为解决这一局限性，我们提出了ICONIC-444（Image Classification and OOD Detection with Numerous Intricate Complexities），这是一个专门用于OOD检测研究的大规模工业图像数据集，包含超过310万张RGB图像，涵盖444个类别。该数据集由一台原型工业分拣机采集，高度模拟真实世界任务。ICONIC-444通过提供结构化且多样化的数据，补充了现有数据集，适用于在各种任务复杂度下进行严格的OOD评估。我们在ICONIC-444中定义了四个参考任务，用于对OOD检测研究进行基准测试，并为22种最先进的后处理OOD检测方法提供了基线结果。

</details>


### [3] [A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems](https://arxiv.org/abs/2601.10819)
*Yizhou Wang,Sameer Pusegaonkar,Yuxing Wang,Anqi Li,Vishal Kumar,Chetan Sethi,Ganapathy Aiyer,Yun He,Kartikay Thakkar,Swapnil Rathi,Bhushan Rupde,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种针对大规模基础设施环境优化的Sparse4D框架，通过引入世界坐标几何先验和遮挡感知ReID嵌入模块，在AI City Challenge 2025上实现了45.22的HOTA性能；同时利用NVIDIA COSMOS进行无监督生成式数据增强，并开发了TensorRT插件以实现实时部署，单个Blackwell GPU可支持64路以上摄像头流。


<details>
  <summary>Details</summary>
Motivation: 将自动驾驶中“由内向外”的感知模型迁移到工业基础设施中“由外向内”的静态多相机网络面临相机布局异构和严重遮挡等挑战，亟需专门优化的3D感知与多目标多相机跟踪方法。

Method: 基于Sparse4D框架，引入绝对世界坐标几何先验和遮挡感知的ReID嵌入模块；采用NVIDIA COSMOS进行生成式数据增强以弥合Sim2Real差距；并开发了针对MSDA操作的TensorRT插件以加速推理。

Result: 在AI City Challenge 2025基准上达到45.22的HOTA指标，为当前最优；硬件加速实现2.15倍提速，单块Blackwell GPU可处理64路以上并发视频流。

Conclusion: 所提方法有效解决了静态多相机网络下的3D感知与MTMC跟踪难题，在精度与实时性方面均取得显著进展，适用于大规模工业基础设施的数字化转型。

Abstract: Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.

Abstract (中文翻译): 精确的3D物体感知与多目标多相机（MTMC）跟踪是工业基础设施数字化转型的基础。然而，将“由内向外”的自动驾驶模型迁移到“由外向内”的静态相机网络面临巨大挑战，主要源于相机布局的异构性和极端遮挡问题。本文提出了一种专为大规模基础设施环境优化的Sparse4D框架。该系统利用绝对世界坐标几何先验，并引入一种遮挡感知的ReID嵌入模块，以在分布式传感器网络中保持目标身份稳定性。为在无需人工标注的情况下弥合仿真到现实（Sim2Real）的域差距，我们采用基于NVIDIA COSMOS框架的生成式数据增强策略，生成多样化的环境风格，从而提升模型对表观变化的鲁棒性。在AI City Challenge 2025基准测试中，我们的纯视觉方案取得了45.22的HOTA指标，达到当前最优水平。此外，为满足实时部署需求，我们开发了针对多尺度可变形聚合（MSDA）操作的优化TensorRT插件。该硬件加速实现可在现代GPU架构上获得2.15倍的速度提升，使单块Blackwell级GPU能够支持超过64路并发相机视频流。

</details>


### [4] [Can Vision-Language Models Understand Construction Workers? An Exploratory Study](https://arxiv.org/abs/2601.10835)
*Hieu Bui,Nathaniel E. Chodosh,Arash Tavakoli*

Main category: cs.CV

TL;DR: 本文评估了三种主流视觉语言模型（GPT-4o、Florence 2 和 LLaVa-1.5）在建筑工地静态图像中识别人类行为与情绪的能力。结果表明，GPT-4o 表现最佳，而所有模型在区分语义相近类别时仍存在困难，需进一步改进以提升实际应用可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在建筑流程中的广泛应用，其理解并响应人类行为的能力对实现安全高效的人机协作至关重要。由于建筑领域标注数据稀缺，且监测工人行为与情绪对安全和效率意义重大，无需大量领域特定训练的视觉语言模型（VLMs）成为有潜力的解决方案。

Method: 研究使用包含1000张图像的数据集，每张图像标注了10种行为和10种情绪类别，通过标准化推理流程和多种评估指标，对GPT-4o、Florence 2和LLaVa-1.5三种VLM进行性能评估。

Result: GPT-4o在行为识别（F1: 0.756，准确率: 0.799）和情绪识别（F1: 0.712，准确率: 0.773）上表现最优；Florence 2次之；LLaVa-1.5整体表现最弱。所有模型在区分语义相近类别（如团队协作与与主管沟通）时均存在困难。

Conclusion: 通用视觉语言模型可为建筑场景中的人类行为识别提供基础能力，但要实现实用级可靠性，仍需引入领域适配、时序建模或多模态感知等改进措施。

Abstract: As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.

Abstract (中文翻译): 随着机器人越来越多地融入建筑工作流程，其理解和响应人类行为的能力对于实现安全高效的人机协作至关重要。视觉语言模型（VLMs）作为视觉理解任务中的一种新兴工具，具备在无需大量领域特定训练的情况下识别人类行为的潜力。这一特性使其在建筑领域尤其具有吸引力——该领域标注数据稀缺，而对工人行为和情绪状态的监控又对安全性和生产效率至关重要。本研究评估了三种主流VLM（GPT-4o、Florence 2 和 LLaVa-1.5）在静态工地图像中检测建筑工人行为与情绪的表现。我们使用一个经过整理的包含1000张图像的数据集，每张图像均标注了10种行为类别和10种情绪类别，并通过标准化推理流程和多种评估指标对各模型输出进行评估。结果显示，GPT-4o在两项任务中均持续取得最高分：行为识别的平均F1得分为0.756、准确率为0.799；情绪识别的F1得分为0.712、准确率为0.773。Florence 2表现中等，行为和情绪识别的F1得分分别为0.497和0.414；LLaVa-1.5整体表现最弱，行为和情绪识别的F1得分分别为0.466和0.461。混淆矩阵分析表明，所有模型在区分语义相近的类别（例如“团队协作”与“与主管沟通”）方面均存在困难。尽管结果表明通用VLM可在建筑环境中提供人类行为识别的基础能力，但要实现现实场景中的可靠应用，仍需进一步改进，例如引入领域适配、时序建模或多模态感知等方法。

</details>


### [5] [One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection](https://arxiv.org/abs/2601.10836)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 对21种OOD检测方法在56个ImageNet模型上的研究表明，ID准确率与OOD性能并非单调相关，且训练策略、检测器选择和OOD性能之间存在强依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注提升分布内（ID）准确率，但对现代训练流程与OOD检测器之间的相互作用缺乏系统探索。

Method: 固定使用ResNet-50架构，在56个通过不同训练策略获得的ImageNet模型上，对21种后验式SOTA OOD检测方法进行基准测试，并在8个OOD测试集上评估性能。

Result: 发现ID准确率与OOD性能呈非单调关系：准确率提升初期OOD性能改善，但当训练策略使准确率超过基线后，OOD性能反而下降；同时，训练策略、检测器选择与OOD性能高度相关。

Conclusion: 不存在一种在所有情况下都最优的OOD检测方法，需根据具体训练策略选择合适的检测器。

Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.

Abstract (中文翻译): 分布外（OOD）检测对于在开放世界环境中部署鲁棒可靠的机器学习系统至关重要。尽管OOD检测器不断取得进展，但其与旨在最大化分布内（ID）准确率和泛化能力的现代训练流程之间的相互作用仍缺乏深入研究。本文通过一项全面的实证研究探讨了这一联系。我们固定采用广泛使用的ResNet-50架构，在通过多种训练策略获得的56个ImageNet训练模型上，对21种事后（post-hoc）的最先进OOD检测方法进行了基准测试，并在八个OOD测试集上评估其性能。与“更高的ID准确率意味着更好的OOD检测性能”这一普遍假设相反，我们发现二者之间存在非单调关系：OOD性能起初随准确率提升而改善，但一旦先进训练方法将准确率推高至基线以上，OOD性能反而下降。此外，我们观察到训练策略、检测器选择与最终OOD性能之间存在强烈依赖关系，表明并不存在一种普遍最优的方法。

</details>


### [6] [Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification](https://arxiv.org/abs/2601.10854)
*Mohammad Rasras,Iuliana Marin,Serban Radu,Irina Mocanu*

Main category: cs.CV

TL;DR: 本文研究了在提高视频帧分辨率的同时减少时间信息对3D ResNet模型（MC3、R3D、R(2+1)D）动作识别性能的影响，并通过引入多种注意力机制（如CBAM、TCN、多头注意力等）构建新变体，在UCF101数据集上验证其效果，发现多头注意力增强的R(2+1)D变体达到88.98%准确率，强调了时间特征缺失对高分辨率模型性能的重要影响。


<details>
  <summary>Details</summary>
Motivation: 探究在提升视频帧分辨率的同时减少时间维度信息对3D CNN动作识别模型性能的影响，并评估不同注意力机制对这类受限时间模型的补偿效果。

Method: 在原始MC3、R3D和R(2+1)D模型基础上添加Dropout层，并为每种模型设计十种新变体，分别集成CBAM、TCN、多头注意力和通道注意力等模块，以分析各注意力机制对受限时间模型性能的影响。

Result: 在UCF101数据集上，加入多头注意力的改进型R(2+1)D模型取得88.98%的准确率；不同变体在整体性能相近的情况下，类别级准确率表现各异。

Conclusion: 时间特征的缺失显著影响高分辨率动作识别模型的性能，而引入特定注意力机制可在一定程度上缓解该问题，但不同注意力模块对各类别的影响存在差异。

Abstract: Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.

Abstract (中文翻译): 由于人类动作识别在计算机视觉中具有广泛的应用，已成为重要的研究热点。基于3D ResNet的CNN模型（尤其是MC3、R3D和R(2+1)D）采用不同的卷积滤波器来提取时空特征。本文研究了在提高视频帧分辨率的同时减少时间维度所捕获信息的影响。为此，我们首先在三个原始模型的基础上构建了结构相似的版本，并在最终分类器前添加了Dropout层；随后，针对每种基础模型进一步开发了十种新变体，这些变体在其架构中集成了特定的注意力模块，包括卷积块注意力模块（CBAM）、时间卷积网络（TCN）以及多头注意力和通道注意力机制，旨在观察这些模块对受限时间模型性能的影响程度。所有模型在UCF101数据集上的测试结果表明，将多头注意力机制加入改进后的R(2+1)D模型可达到88.98%的准确率。本文得出结论：在新构建的高分辨率模型中，缺失的时间特征对性能具有显著影响；尽管各变体在整体性能提升方面相似，但在类别级别的准确率上表现出不同的行为。

</details>


### [7] [Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation](https://arxiv.org/abs/2601.10880)
*Chongcong Jiang,Tianxingjian Ding,Chuhan Song,Jiachen Tu,Ziyang Yan,Yihua Shao,Zhenyi Wang,Yuzhang Shang,Tianyu Han,Yu Tian*

Main category: cs.CV

TL;DR: 本文提出Medical SAM3，通过对SAM3在大规模多模态医学图像数据上进行全参数微调，显著提升了其在医学图像分割中的性能，尤其在语义模糊、形态复杂和3D长程依赖等挑战性场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: 原始SAM3模型在医学图像分割中受限于严重的域偏移、缺乏有效的空间提示以及对复杂解剖结构和体数据推理能力不足；作者发现其在医学数据上性能大幅下降，且依赖强几何先验（如真实边界框），因此需超越仅靠提示工程的策略，进行整体模型适配。

Method: 在涵盖10种医学成像模态、33个数据集的大规模2D/3D医学图像及对应分割掩码和文本提示上，对SAM3进行全参数微调，构建Medical SAM3模型。

Result: 在多种器官、成像模态和维度设置下，Medical SAM3均取得一致且显著的性能提升，尤其在语义模糊、复杂形态和3D长程上下文等困难场景中优势明显。

Conclusion: Medical SAM3是一个通用的、文本引导的医学图像分割基础模型，证明了在严重域偏移下，整体模型适配对实现鲁棒提示驱动分割至关重要。

Abstract: Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.

Abstract (中文翻译): 诸如SAM3之类的可提示分割基础模型通过交互式和基于概念的提示展现了强大的泛化能力。然而，它们在医学图像分割中的直接适用性仍受到严重域偏移、缺乏有效的空间提示以及需要对复杂解剖结构和体数据进行推理等因素的限制。本文提出了Medical SAM3——一个用于通用提示驱动医学图像分割的基础模型，该模型通过对SAM3在大规模异构的2D和3D医学影像数据集（包含配对的分割掩码和文本提示）上进行全参数微调而获得。通过对原始SAM3的系统分析，我们发现其在医学数据上的性能显著下降，其表面上的竞争力很大程度上依赖于强几何先验（例如由真实标签导出的边界框）。这些发现促使我们超越仅依赖提示工程的策略，进行全面的模型适配。通过在涵盖10种医学成像模态的33个数据集上微调SAM3的模型参数，Medical SAM3获得了稳健的领域特定表征，同时保留了提示驱动的灵活性。在多种器官、成像模态和维度上的大量实验表明，该模型始终取得显著性能提升，尤其在语义模糊、形态复杂和具有长程3D上下文等挑战性场景中表现优异。我们的结果确立了Medical SAM3作为医学影像领域通用、文本引导的分割基础模型的地位，并强调了在严重域偏移条件下，整体模型适配对于实现鲁棒提示驱动分割的重要性。代码和模型将发布于 https://github.com/AIM-Research-Lab/Medical-SAM3。

</details>


### [8] [FrankenMotion: Part-level Human Motion Generation and Composition](https://arxiv.org/abs/2601.10909)
*Chuqiao Li,Xianghui Xie,Yong Cao,Andreas Geiger,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 本文提出FrankenMotion，一个基于扩散模型的部位感知人体动作生成框架，并构建了首个包含原子级、时间感知的部位级文本标注的高质量动作数据集，实现对身体各部位动作的时空精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的人体动作生成方法主要依赖序列级或动作级描述，缺乏细粒度的部位级动作标注，导致对身体各部位动作的可控性不足。

Method: 利用大语言模型（LLM）构建了一个高质量的动作数据集，该数据集包含原子级、时间感知的部位级文本标注；在此基础上，提出FrankenMotion扩散模型框架，每个身体部位由其自身的时间结构化文本提示引导生成动作。

Result: 实验表明，FrankenMotion在新设定下优于所有经过适配和重新训练的基线模型，并能组合生成训练中未见过的动作。

Conclusion: 本文首次实现了具有原子级、时间感知的部位级动作标注，并提出了支持空间（身体部位）与时间（原子动作）双重控制的动作生成模型，显著提升了文本驱动动作生成的精细控制能力。

Abstract: Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

Abstract (中文翻译): 近年来，基于文本提示生成人体动作取得了显著进展。然而，由于缺乏细粒度的部位级动作标注，现有方法主要依赖序列级或动作级描述，限制了对各个身体部位动作的可控性。在本研究中，我们利用大语言模型（LLM）的推理能力，构建了一个高质量的动作数据集，其中包含原子级、时间感知的部位级文本标注。与以往仅提供固定时间段同步部位字幕或仅依赖全局序列标签的数据集不同，我们的数据集在精细的时间分辨率下捕捉了异步且语义各异的部位运动。基于该数据集，我们提出了一个基于扩散模型的部位感知动作生成框架——FrankenMotion，其中每个身体部位均由其自身的时间结构化文本提示进行引导。据我们所知，这是首个提供原子级、时间感知的部位级动作标注的工作，并实现了同时具备空间（身体部位）和时间（原子动作）控制能力的动作生成模型。实验表明，FrankenMotion在我们设定下的性能优于所有经过适配和重新训练的先前基线模型，并且能够组合生成训练期间未见过的动作。我们的代码和数据集将在论文发表后公开。

</details>


### [9] [Classification of Chest XRay Diseases through image processing and analysis techniques](https://arxiv.org/abs/2601.10913)
*Santiago Martínez Novoa,María Catalina Ibáñez,Lina Gómez Mesa,Jeremias Kramer*

Main category: cs.CV

TL;DR: 本文综述了用于胸部X光多分类任务的多种方法（包括DenseNet121），通过实验比较其性能，分析所提方法的不足，并开源了一个Web应用及代码。


<details>
  <summary>Details</summary>
Motivation: 胸部X光图像是诊断胸腔疾病最常用的放射学检查手段之一，因此需要有效且可靠的多分类方法来辅助诊断。

Method: 综述并实验比较了多种胸部X光图像多分类方法，包括DenseNet121，并开发了一个开源的Web应用。

Result: 对不同方法进行了性能测试，识别出当前方法的局限性。

Conclusion: 所研究的方法在胸部X光多分类任务中具有一定效果，但仍存在不足，未来可通过改进模型和数据策略提升性能。

Abstract: Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML

Abstract (中文翻译): 多分类胸部X光图像是一种用于诊断胸腔疾病最常见的放射学检查形式。在本研究中，我们简要概述了用于解决该任务的若干方法，包括DenseNet121。此外，我们部署了一个开源的基于Web的应用程序。在研究中，我们对不同方法进行了测试，以比较其性能表现，并深入分析了所提出方法的弱点，同时提出了未来改进的思路。我们的代码已公开于：https://github.com/AML4206-MINE20242/Proyecto_AML

</details>


### [10] [Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images](https://arxiv.org/abs/2601.10917)
*Pouya Afshin,David Helminiak,Tianling Niu,Julie M. Jorns,Tina Yen,Bing Yu,Dong Hye Ye*

Main category: cs.CV

TL;DR: 本文提出一种结合自监督学习（SSL）与潜在扩散模型（LDM）的方法，用于生成高质量的深紫外荧光扫描显微镜（DUV-FSM）合成图像，以缓解标注数据稀缺问题，并提升乳腺癌保乳手术中切缘评估的深度学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌保乳手术（BCS）需要术中精确评估切缘以保留健康组织，而深紫外荧光扫描显微镜（DUV-FSM）虽能提供快速高分辨率成像，但缺乏标注数据限制了深度学习模型的训练。

Method: 作者提出一种由自监督学习（SSL）引导的潜在扩散模型（LDM），利用微调后的DINO教师模型嵌入来增强合成图像中的细胞结构语义信息；随后将真实与合成图像拼接用于微调Vision Transformer（ViT），并通过图像块预测聚合实现全切片（WSI）级别分类。

Result: 在5折交叉验证实验中，该方法达到96.47%的准确率，FID分数降至45.72，显著优于类别条件基线模型。

Conclusion: 所提方法有效缓解了DUV-FSM图像标注数据稀缺的问题，显著提升了切缘评估模型的性能，具有临床应用潜力。

Abstract: Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.

Abstract (中文翻译): 乳腺癌保乳手术（BCS）需要在术中精确评估切缘以保留健康组织。深紫外荧光扫描显微镜（DUV-FSM）为此提供了快速、高分辨率的表面成像能力；然而，标注的DUV数据稀缺，阻碍了鲁棒深度学习模型的训练。为解决此问题，我们提出一种由自监督学习（SSL）引导的潜在扩散模型（LDM），用于生成高质量的合成训练图像块。通过使用微调后的DINO教师模型嵌入来引导LDM，我们将丰富的细胞结构语义细节注入合成数据中。我们将真实图像块与合成图像块结合，用于微调Vision Transformer（ViT），并采用图像块预测聚合策略进行全切片（WSI）级别的分类。基于5折交叉验证的实验表明，我们的方法达到了96.47%的准确率，并将FID分数降低至45.72，显著优于类别条件基线方法。

</details>
